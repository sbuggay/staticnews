<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1703062865710" as="style"/><link rel="stylesheet" href="styles.css?v=1703062865710"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://shihab-shahriar.github.io//blog/2023/Cuda-vs-Rocm-A-Case-Study-Through-Random-Number-Libraries/">CUDA vs. ROCm: A case study</a> <span class="domain">(<a href="https://shihab-shahriar.github.io">shihab-shahriar.github.io</a>)</span></div><div class="subtext"><span>shihab</span> | <span>44 comments</span></div><br/><div><div id="38703634" class="c"><input type="checkbox" id="c-38703634" checked=""/><div class="controls bullet"><span class="by">cherryteastain</span><span>|</span><a href="#38704304">next</a><span>|</span><label class="collapse" for="c-38703634">[-]</label><label class="expand" for="c-38703634">[13 more]</label></div><br/><div class="children"><div class="content">I wish AMD would just drop ROCm at this stage, and focus on SYCL. The rocRAND&#x2F;hipRAND woes in this article are if anything showing ROCm in a better light than it really is; here it at least worked and performed within the same ballpark as CUDA. Often it simply does not work at all, or if it works it&#x27;s behind by a lot more. At work I simply gave up on our 4x Radeon Pro W6800 workstation because launching Tensorflow with more than 1 GPU would cause a kernel panic every time, and AMD engineers never offered a fix other than &quot;reinstall Ubuntu&quot;.<p>ROCm feels like such a half assed product that (to me at least) feels like it&#x27;s been made to tick a box and look cool in corporate presentations. It&#x27;s not made with the proper mindset to compete against CUDA. Lisa Su claims they&#x27;re doubling down on ROCm but to me it feels like they&#x27;re falling behind relative to Nvidia, not catching up.<p>Banding together with Intel to support SYCL would in my opinion<p>1. Ensure there&#x27;s a lot more momentum behind a single, cross-platform, industry-standard competitor<p>2. Entice other industry heavyweights like MSFT, Qualcomm, ARM etc to also take the cross-platform solutions more seriously<p>3. Encourage heavy investment into the developer experience and tooling for the cross-platform solution</div><br/><div id="38703800" class="c"><input type="checkbox" id="c-38703800" checked=""/><div class="controls bullet"><span class="by">arcanus</span><span>|</span><a href="#38703634">parent</a><span>|</span><a href="#38706296">next</a><span>|</span><label class="collapse" for="c-38703800">[-]</label><label class="expand" for="c-38703800">[3 more]</label></div><br/><div class="children"><div class="content">SYCL is a terrible option. The performance of any application on SYCL is currently quite poor. Why drop ROCm (used on the world&#x27;s largest supercomputer?) for an even more unproven product that AMD has no control over the roadmap?<p>&gt; Banding together with Intel to support SYCL would in my opinion<p>Except that Intel has more control over SYCL and has repeatedly hurt AMD products with anticompetitive behavior in the past. Why would AMD permit their software to be controlled be a competitor?<p>AMD is executing with ROCm NOW, with multiple Top500 supercomputer wins and deployment in major cloud vendors with MI300X. Yes, the software needs to improve, but the practice of throwing out software to start over is not a good strategy.<p>AMD has far more momentum in the data center than INTC at present.</div><br/><div id="38703933" class="c"><input type="checkbox" id="c-38703933" checked=""/><div class="controls bullet"><span class="by">shihab</span><span>|</span><a href="#38703634">root</a><span>|</span><a href="#38703800">parent</a><span>|</span><a href="#38706298">next</a><span>|</span><label class="collapse" for="c-38703933">[-]</label><label class="expand" for="c-38703933">[1 more]</label></div><br/><div class="children"><div class="content">I agree about ROCm.<p>When it comes to comparison to SYCL, HIP is much closer to the spirit of SYCL than ROCm is. Both aim to help writing a single codebase that&#x27;ll run across multiple hardwares. For now though, the trajectory of SYCL appears much more promising to me than HIP. HIP is already split in two parts for CPU and GPU, which is baffling, and neither part seems to receive much love from AMD.</div><br/></div></div><div id="38706298" class="c"><input type="checkbox" id="c-38706298" checked=""/><div class="controls bullet"><span class="by">pjmlp</span><span>|</span><a href="#38703634">root</a><span>|</span><a href="#38703800">parent</a><span>|</span><a href="#38703933">prev</a><span>|</span><a href="#38706296">next</a><span>|</span><label class="collapse" for="c-38706298">[-]</label><label class="expand" for="c-38706298">[1 more]</label></div><br/><div class="children"><div class="content">It hardly matters if most desktops and laptops will be running CUDA&#x2F;SYCL instead.</div><br/></div></div></div></div><div id="38706296" class="c"><input type="checkbox" id="c-38706296" checked=""/><div class="controls bullet"><span class="by">zozbot234</span><span>|</span><a href="#38703634">parent</a><span>|</span><a href="#38703800">prev</a><span>|</span><a href="#38704491">next</a><span>|</span><label class="collapse" for="c-38706296">[-]</label><label class="expand" for="c-38706296">[1 more]</label></div><br/><div class="children"><div class="content">SYCL is practically comparable to OpenCL which is already getting unofficial support (including on older hardware that Rocm does not support) via Mesa&#x27;s RustiCL.  Additionally clspv and Mesa&#x27;s Zink are both exploring OpenCL (and potentially SYCL)-on-Vulkan, though that option involves lots of quirks since the standards have evolved independently (and are even based on different varieties of SPIR-V).</div><br/></div></div><div id="38704491" class="c"><input type="checkbox" id="c-38704491" checked=""/><div class="controls bullet"><span class="by">pokeypokes</span><span>|</span><a href="#38703634">parent</a><span>|</span><a href="#38706296">prev</a><span>|</span><a href="#38704698">next</a><span>|</span><label class="collapse" for="c-38704491">[-]</label><label class="expand" for="c-38704491">[4 more]</label></div><br/><div class="children"><div class="content">Important to understand that with these kinds of libraries, it&#x27;s one thing to get everything working, and a whole other to have it all be fast. Given finite resources AMD are (rightfully) focused on the second, for a narrower set of use cases - namely big customers with big pockets, using the latest compute cards.<p>See, for example, the world&#x27;s fastest super computer. That&#x27;s a whole lot more than a presentation tick box.<p>As someone doing relatively small scale work, on gaming cards, you just aren&#x27;t the target user. While these kinds of users are well represented on forums, they&#x27;re a rounding error in terms of actual $.<p>Switching to SYCL makes no sense. They need to unseat Nvidia, and specifically CUDA. The whole point of HIP is to clone CUDA, make it easy for users to switch and piggy back on Nvidias success.<p>I used to work in this space (not at AMD lol), and generally I don&#x27;t think the comments on HN are fair to AMD. Obviously ROCm has a long way to go (CUDA and libs are some truly amazing work), but it&#x27;s going.</div><br/><div id="38705293" class="c"><input type="checkbox" id="c-38705293" checked=""/><div class="controls bullet"><span class="by">throwaway81523</span><span>|</span><a href="#38703634">root</a><span>|</span><a href="#38704491">parent</a><span>|</span><a href="#38706328">next</a><span>|</span><label class="collapse" for="c-38705293">[-]</label><label class="expand" for="c-38705293">[2 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t know about the libs but CUDA itself is a C++ dialect targeted at a family of quirky array processors (GPU&#x27;s).  That takes some dev work by compiler hackers but doesn&#x27;t seem amazing.  Am I missing something?  It&#x27;s of course still easy to mess up such a project by not bringing enough clue, and AMD might have done that with ROCm.  Is that what happened, or is it something different?</div><br/><div id="38706326" class="c"><input type="checkbox" id="c-38706326" checked=""/><div class="controls bullet"><span class="by">pjmlp</span><span>|</span><a href="#38703634">root</a><span>|</span><a href="#38705293">parent</a><span>|</span><a href="#38706328">next</a><span>|</span><label class="collapse" for="c-38706326">[-]</label><label class="expand" for="c-38706326">[1 more]</label></div><br/><div class="children"><div class="content">CUDA is more than a C++ dialect, which is a big thing people keep missing with all those &quot;CUDA replacements&quot;.<p>Until CUDA 3.0, it was similar to OpenCL, a C dialect, however afterwards it became a C, C++ dialect, with common infrastructure PTX.<p>PGI targeted PTX, with their C, C++, and very relevant, Fortran compilers for HPC.<p>PGI was acquired by NVidia, and became the main set of CUDA compilers.<p>Given PTX, many other languages started targeting CUDA as well, Java, .NET, Haskell, Julia, at very least.<p>NVidia is now invested into a Python JIT for CUDA as well.<p>So yeah, while C++20 is the main language in CUDA, there is also a whole ecosystem of programming languages, that the &quot;CUDA replacements&quot; keep ignoring.</div><br/></div></div></div></div><div id="38706328" class="c"><input type="checkbox" id="c-38706328" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#38703634">root</a><span>|</span><a href="#38704491">parent</a><span>|</span><a href="#38705293">prev</a><span>|</span><a href="#38704698">next</a><span>|</span><label class="collapse" for="c-38706328">[-]</label><label class="expand" for="c-38706328">[1 more]</label></div><br/><div class="children"><div class="content">&gt;As someone doing relatively small scale work, on gaming cards, you just aren&#x27;t the target user.<p>He didn&#x27;t use gaming cards. He bought the expensive $1.5k &quot;workstation&quot; graphics cards.
What exactly is one going to use these GPUs for, other than GPGPU? Play video games? Really?<p>&gt;While these kinds of users are well represented on forums, they&#x27;re a rounding error in terms of actual $.<p>There are only three thousand developers who have ever committed to pytorch. Compared to a datacenter contract, that is a rounding error in terms of sales. Hence AMD shouldn&#x27;t waste time on letting people independently work on AMD support for pytorch. They should only work on pytorch, whenever there is a big contract.<p>However, switching to SYCL makes no sense because Mesa is getting SYCL support. The likelihood that the mesa drivers cause a kernel panic is much lower.</div><br/></div></div></div></div><div id="38704698" class="c"><input type="checkbox" id="c-38704698" checked=""/><div class="controls bullet"><span class="by">kimixa</span><span>|</span><a href="#38703634">parent</a><span>|</span><a href="#38704491">prev</a><span>|</span><a href="#38703920">next</a><span>|</span><label class="collapse" for="c-38704698">[-]</label><label class="expand" for="c-38704698">[3 more]</label></div><br/><div class="children"><div class="content">I feel this is falling into the rewrite trap - rather than continuing to develop an existing but imperfect system, throw it all away for something that is unproven and currently in a notably worse state.<p>What about rocm makes it fundamentally by design that much worse? And what about sycl makes it better? Rocm is already based on many standard shared OSS projects, like llvm and clang. Often the same ones SYCL stacks are based on. What about SYCL makes it so superior that it&#x27;ll make up for throwing all the current work away?</div><br/><div id="38706526" class="c"><input type="checkbox" id="c-38706526" checked=""/><div class="controls bullet"><span class="by">cherryteastain</span><span>|</span><a href="#38703634">root</a><span>|</span><a href="#38704698">parent</a><span>|</span><a href="#38706335">next</a><span>|</span><label class="collapse" for="c-38706526">[-]</label><label class="expand" for="c-38706526">[1 more]</label></div><br/><div class="children"><div class="content">&gt; What about rocm makes it fundamentally by design that much worse? And what about sycl makes it better?<p>The fact that SYCL compiles device code to SPIR-V instead of a device specific ISA like HIP for instance. The decision to do the latter, alone, is what causes ROCm to have support for a very narrow range of cards which significantly hampers adoption.</div><br/></div></div><div id="38706335" class="c"><input type="checkbox" id="c-38706335" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#38703634">root</a><span>|</span><a href="#38704698">parent</a><span>|</span><a href="#38706526">prev</a><span>|</span><a href="#38703920">next</a><span>|</span><label class="collapse" for="c-38706335">[-]</label><label class="expand" for="c-38706335">[1 more]</label></div><br/><div class="children"><div class="content">Most likely the ability to run software on the GPU. When I last used ROCm I got both bad performance and it was prone to crashing.</div><br/></div></div></div></div><div id="38703920" class="c"><input type="checkbox" id="c-38703920" checked=""/><div class="controls bullet"><span class="by">cyanydeez</span><span>|</span><a href="#38703634">parent</a><span>|</span><a href="#38704698">prev</a><span>|</span><a href="#38704304">next</a><span>|</span><label class="collapse" for="c-38703920">[-]</label><label class="expand" for="c-38703920">[1 more]</label></div><br/><div class="children"><div class="content">hate to break it to you but with the mi300, they&#x27;re clearly going in the same direction.<p>I looked at their current compatibility chart for rocm and it&#x27;s clearly just starting.</div><br/></div></div></div></div><div id="38704304" class="c"><input type="checkbox" id="c-38704304" checked=""/><div class="controls bullet"><span class="by">frognumber</span><span>|</span><a href="#38703634">prev</a><span>|</span><a href="#38703619">next</a><span>|</span><label class="collapse" for="c-38704304">[-]</label><label class="expand" for="c-38704304">[4 more]</label></div><br/><div class="children"><div class="content">That&#x27;s more-or-less my experience with AMD, only worse. Critical thing too are burned developers like myself.<p>I&#x27;m looking forward to Intel v. NVidia. Arc A770 is a pretty serious competitor. It&#x27;s the lowest-cost way to run OPT-175B.<p>Given a 7-slot motherboard, $270 * 7 = $1890 for 112GB of VRAM in one computer. That&#x27;s sweet. Compute speed would be on-par with top-of-the-line NVidia workstation GPU.<p>Three of those are enough to run the largest open-source LLMs at around $9000.<p>We&#x27;re just drivers + libraries + documentation away, and Intel is not bad at drivers + libraries + documentation.</div><br/><div id="38705489" class="c"><input type="checkbox" id="c-38705489" checked=""/><div class="controls bullet"><span class="by">ParetoOptimal</span><span>|</span><a href="#38704304">parent</a><span>|</span><a href="#38706476">next</a><span>|</span><label class="collapse" for="c-38705489">[-]</label><label class="expand" for="c-38705489">[1 more]</label></div><br/><div class="children"><div class="content">&gt;  Arc A770 is a pretty serious competitor. It&#x27;s the lowest-cost way to run OPT-175B.<p>With what software?</div><br/></div></div><div id="38706476" class="c"><input type="checkbox" id="c-38706476" checked=""/><div class="controls bullet"><span class="by">sangnoir</span><span>|</span><a href="#38704304">parent</a><span>|</span><a href="#38705489">prev</a><span>|</span><a href="#38705467">next</a><span>|</span><label class="collapse" for="c-38706476">[-]</label><label class="expand" for="c-38706476">[1 more]</label></div><br/><div class="children"><div class="content">If AMD can significantly undercut Nvidia on perf&#x2F;$ like they did to Intel with Zen 1; developers will fall over themselves to improve the software. When the performance price-point is low enough, the pain will be worth it.</div><br/></div></div><div id="38705467" class="c"><input type="checkbox" id="c-38705467" checked=""/><div class="controls bullet"><span class="by">vGPU</span><span>|</span><a href="#38704304">parent</a><span>|</span><a href="#38706476">prev</a><span>|</span><a href="#38703619">next</a><span>|</span><label class="collapse" for="c-38705467">[-]</label><label class="expand" for="c-38705467">[1 more]</label></div><br/><div class="children"><div class="content">Sure but what’s the energy and overhead tradeoff?</div><br/></div></div></div></div><div id="38703619" class="c"><input type="checkbox" id="c-38703619" checked=""/><div class="controls bullet"><span class="by">ekelsen</span><span>|</span><a href="#38704304">prev</a><span>|</span><a href="#38704419">next</a><span>|</span><label class="collapse" for="c-38703619">[-]</label><label class="expand" for="c-38703619">[3 more]</label></div><br/><div class="children"><div class="content">AMD attempted responses go all the way back to 2007 when CUDA first debuted with &quot;Close to Metal&quot; (<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Close_to_Metal" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Close_to_Metal</a>).  They&#x27;ve had nearly 20 years to fix the situation and have failed to do so.  Maybe some third party player like Lamini AI will do what they couldn&#x27;t and get acquired for it.</div><br/><div id="38703833" class="c"><input type="checkbox" id="c-38703833" checked=""/><div class="controls bullet"><span class="by">shihab</span><span>|</span><a href="#38703619">parent</a><span>|</span><a href="#38704419">next</a><span>|</span><label class="collapse" for="c-38703833">[-]</label><label class="expand" for="c-38703833">[2 more]</label></div><br/><div class="children"><div class="content">The thing about modern AI, it&#x27;s that operations involved here (e.g. Dense matmuls) are lot simpler and GPU friendly than what you&#x27;d find in a typical HPC applications. This means you can get pretty close to peak hardware performance using high-level languages like Python or OpenAI&#x27;s Triton. I think it&#x27;s unlikely that the push to improve ROCm&#x27;s standard libraries will come from an AI-focused startup</div><br/><div id="38704648" class="c"><input type="checkbox" id="c-38704648" checked=""/><div class="controls bullet"><span class="by">cavisne</span><span>|</span><a href="#38703619">root</a><span>|</span><a href="#38703833">parent</a><span>|</span><a href="#38704419">next</a><span>|</span><label class="collapse" for="c-38704648">[-]</label><label class="expand" for="c-38704648">[1 more]</label></div><br/><div class="children"><div class="content">Even with AMD working on hard on specific benchmarks for marketing purposes they could not get close to peak hardware performance on their brand new chips.<p><a href="https:&#x2F;&#x2F;www.semianalysis.com&#x2F;p&#x2F;amd-mi300-performance-faster-than" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.semianalysis.com&#x2F;p&#x2F;amd-mi300-performance-faster-...</a></div><br/></div></div></div></div></div></div><div id="38704419" class="c"><input type="checkbox" id="c-38704419" checked=""/><div class="controls bullet"><span class="by">quanto</span><span>|</span><a href="#38703619">prev</a><span>|</span><a href="#38704118">next</a><span>|</span><label class="collapse" for="c-38704419">[-]</label><label class="expand" for="c-38704419">[9 more]</label></div><br/><div class="children"><div class="content">AMD cards + ROCm are used in top supercomputers for (non-deep learning) HPC.  Why is this the case?<p>I understand that AMD GPUs offer better cost efficiency for F32 &amp; F64 FLOPs, RAM, and wattage.  But however, if ROCm is such a half baked piece, shouldn&#x27;t that advantage be gone?  What drives AMD adoption in the HPC space then?</div><br/><div id="38704542" class="c"><input type="checkbox" id="c-38704542" checked=""/><div class="controls bullet"><span class="by">ip26</span><span>|</span><a href="#38704419">parent</a><span>|</span><a href="#38704740">next</a><span>|</span><label class="collapse" for="c-38704542">[-]</label><label class="expand" for="c-38704542">[1 more]</label></div><br/><div class="children"><div class="content">I assume a deal like that involves a lot of direct support and optimization by ROCm team for the workload the supercomputer is running, rather than generic awesomeness at everything.<p>That’s been my experience with most large enterprise software, really. Amazing at its most critical competency, core dumps when I try something a little unusual.</div><br/></div></div><div id="38704740" class="c"><input type="checkbox" id="c-38704740" checked=""/><div class="controls bullet"><span class="by">cavisne</span><span>|</span><a href="#38704419">parent</a><span>|</span><a href="#38704542">prev</a><span>|</span><a href="#38706163">next</a><span>|</span><label class="collapse" for="c-38704740">[-]</label><label class="expand" for="c-38704740">[1 more]</label></div><br/><div class="children"><div class="content">Nvidia has pretty epic margins in the datacenter space, so some of it will be that. Epic because their software advantage in AI lets them charge a premium, so why bother competing for the absolute cheapest $&#x2F;FLOPS contract.<p>More generally LLM training is weird because its a supercomputing workload being executed by Silicon Valley devs. So they want to use open source frameworks, find answers on stack overflow, use cloud providers etc. And it&#x27;s new enough that random PHD&#x27;s can invent stuff like Flash Attention without being employed by Nvidia.<p>Normal supercomputer users dont care about any of that.</div><br/></div></div><div id="38706163" class="c"><input type="checkbox" id="c-38706163" checked=""/><div class="controls bullet"><span class="by">my123</span><span>|</span><a href="#38704419">parent</a><span>|</span><a href="#38704740">prev</a><span>|</span><a href="#38705991">next</a><span>|</span><label class="collapse" for="c-38706163">[-]</label><label class="expand" for="c-38706163">[1 more]</label></div><br/><div class="children"><div class="content">&gt; AMD cards + ROCm are used in top supercomputers for (non-deep learning) HPC. Why is this the case?<p>Public bidding processes are different and much more reliant solely on price. As such you can expect them to pick different choices than the private market.<p>This can end up picking outright duds (see Aurora)</div><br/></div></div><div id="38705991" class="c"><input type="checkbox" id="c-38705991" checked=""/><div class="controls bullet"><span class="by">nestorD</span><span>|</span><a href="#38704419">parent</a><span>|</span><a href="#38706163">prev</a><span>|</span><a href="#38704490">next</a><span>|</span><label class="collapse" for="c-38705991">[-]</label><label class="expand" for="c-38705991">[1 more]</label></div><br/><div class="children"><div class="content">Part of the reason is that at the scale of a country like the US, if you are going to have several supercomputers build in the coming years, you want to diversify to avoid putting all your eggs in the same basket (and to giving a preferential treatment to a single vendor).<p>So, as long as they can get a given set of applications to run above a given performance threshold, alternative vendors have a shot.</div><br/></div></div><div id="38704490" class="c"><input type="checkbox" id="c-38704490" checked=""/><div class="controls bullet"><span class="by">taw28</span><span>|</span><a href="#38704419">parent</a><span>|</span><a href="#38705991">prev</a><span>|</span><a href="#38704450">next</a><span>|</span><label class="collapse" for="c-38704490">[-]</label><label class="expand" for="c-38704490">[3 more]</label></div><br/><div class="children"><div class="content">There is certainly a requirement that prevents payout on the contract for those supers if performance numbers aren’t met. Further, those performance numbers are from real apps, so the system WILL be useful. Not getting paid is not an option, so performance&#x2F;usability will come.</div><br/><div id="38704519" class="c"><input type="checkbox" id="c-38704519" checked=""/><div class="controls bullet"><span class="by">quanto</span><span>|</span><a href="#38704419">root</a><span>|</span><a href="#38704490">parent</a><span>|</span><a href="#38704450">next</a><span>|</span><label class="collapse" for="c-38704519">[-]</label><label class="expand" for="c-38704519">[2 more]</label></div><br/><div class="children"><div class="content">how does that contractual obligation translate to technical implementation?  Do those supercomputers get an optimized version of ROCm to fulfill said obligation?</div><br/><div id="38705471" class="c"><input type="checkbox" id="c-38705471" checked=""/><div class="controls bullet"><span class="by">p_l</span><span>|</span><a href="#38704419">root</a><span>|</span><a href="#38704519">parent</a><span>|</span><a href="#38704450">next</a><span>|</span><label class="collapse" for="c-38705471">[-]</label><label class="expand" for="c-38705471">[1 more]</label></div><br/><div class="children"><div class="content">Vendor iterates until client gets an usable environment, even if that means 50 forks of different libraries with custom patches that in the end work only on that one system.</div><br/></div></div></div></div></div></div><div id="38704450" class="c"><input type="checkbox" id="c-38704450" checked=""/><div class="controls bullet"><span class="by">throwawaymaths</span><span>|</span><a href="#38704419">parent</a><span>|</span><a href="#38704490">prev</a><span>|</span><a href="#38704118">next</a><span>|</span><label class="collapse" for="c-38704450">[-]</label><label class="expand" for="c-38704450">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not how sales works for those kinds of installations</div><br/></div></div></div></div><div id="38704118" class="c"><input type="checkbox" id="c-38704118" checked=""/><div class="controls bullet"><span class="by">anvuong</span><span>|</span><a href="#38704419">prev</a><span>|</span><a href="#38706200">next</a><span>|</span><label class="collapse" for="c-38704118">[-]</label><label class="expand" for="c-38704118">[2 more]</label></div><br/><div class="children"><div class="content">“Please note the library is being actively developed, and is known to be incomplet; it might also be incorrekt and there could be a few bad bugs lurking.”<p>That gives me a good laugh.</div><br/><div id="38705310" class="c"><input type="checkbox" id="c-38705310" checked=""/><div class="controls bullet"><span class="by">thrtythreeforty</span><span>|</span><a href="#38704118">parent</a><span>|</span><a href="#38706200">next</a><span>|</span><label class="collapse" for="c-38705310">[-]</label><label class="expand" for="c-38705310">[1 more]</label></div><br/><div class="children"><div class="content">I use exactly the same verbiage in my draft documents. I think it&#x27;s a reference to C++ standards draft documents, which have that on the cover.  I don&#x27;t know if that&#x27;s the original usage though, it&#x27;s just where I saw it first.</div><br/></div></div></div></div><div id="38706200" class="c"><input type="checkbox" id="c-38706200" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#38704118">prev</a><span>|</span><a href="#38706262">next</a><span>|</span><label class="collapse" for="c-38706200">[-]</label><label class="expand" for="c-38706200">[1 more]</label></div><br/><div class="children"><div class="content">Recent video of Lisa Su, good watch:<p><a href="https:&#x2F;&#x2F;twitter.com&#x2F;TheSixFiveMedia&#x2F;status&#x2F;1737177221490450594" rel="nofollow noreferrer">https:&#x2F;&#x2F;twitter.com&#x2F;TheSixFiveMedia&#x2F;status&#x2F;17371772214904505...</a></div><br/></div></div><div id="38706262" class="c"><input type="checkbox" id="c-38706262" checked=""/><div class="controls bullet"><span class="by">Zetobal</span><span>|</span><a href="#38706200">prev</a><span>|</span><a href="#38705075">next</a><span>|</span><label class="collapse" for="c-38706262">[-]</label><label class="expand" for="c-38706262">[2 more]</label></div><br/><div class="children"><div class="content">I hate ROCm so much I can&#x27;t even describe how much I suffered because of this pos software. It wasn&#x27;t good 3 years ago but manageable now it&#x27;s just I don&#x27;t even know how they got it worse.<p>I really just wished my employer would give up on AMD for GPUs.</div><br/><div id="38706511" class="c"><input type="checkbox" id="c-38706511" checked=""/><div class="controls bullet"><span class="by">mnd999</span><span>|</span><a href="#38706262">parent</a><span>|</span><a href="#38705075">next</a><span>|</span><label class="collapse" for="c-38706511">[-]</label><label class="expand" for="c-38706511">[1 more]</label></div><br/><div class="children"><div class="content">It doesn’t even work on half of the GPUs (Navi23 hello). It’s like they’re not even trying which is insane when there’s so much cash on the table.</div><br/></div></div></div></div><div id="38705075" class="c"><input type="checkbox" id="c-38705075" checked=""/><div class="controls bullet"><span class="by">outside1234</span><span>|</span><a href="#38706262">prev</a><span>|</span><a href="#38703987">next</a><span>|</span><label class="collapse" for="c-38705075">[-]</label><label class="expand" for="c-38705075">[7 more]</label></div><br/><div class="children"><div class="content">How is there not an OpenGL at this point in this space?  It seems like it is in all of the hyperscalers benefit to get this going - why haven’t they?</div><br/><div id="38705210" class="c"><input type="checkbox" id="c-38705210" checked=""/><div class="controls bullet"><span class="by">jacoblambda</span><span>|</span><a href="#38705075">parent</a><span>|</span><a href="#38703987">next</a><span>|</span><label class="collapse" for="c-38705210">[-]</label><label class="expand" for="c-38705210">[6 more]</label></div><br/><div class="children"><div class="content">I mean there&#x27;s OpenCL which has widespread support and is pretty much about as pleasant to write as OpenGL. It has existed for a very, very long time but people hate writing in it so they just defaulted to CUDA and ignored everyone but NVIDIA.</div><br/><div id="38705334" class="c"><input type="checkbox" id="c-38705334" checked=""/><div class="controls bullet"><span class="by">thrtythreeforty</span><span>|</span><a href="#38705075">root</a><span>|</span><a href="#38705210">parent</a><span>|</span><a href="#38703987">next</a><span>|</span><label class="collapse" for="c-38705334">[-]</label><label class="expand" for="c-38705334">[5 more]</label></div><br/><div class="children"><div class="content">Doesn&#x27;t it take, like, 100 lines of code to call a kernel in OpenCL? Compared to Nvidia&#x27;s 1?<p>Developer friction is huge. Don&#x27;t discount the amount of boilerplate it takes to get started with something.</div><br/><div id="38706344" class="c"><input type="checkbox" id="c-38706344" checked=""/><div class="controls bullet"><span class="by">pjmlp</span><span>|</span><a href="#38705075">root</a><span>|</span><a href="#38705334">parent</a><span>|</span><a href="#38706009">next</a><span>|</span><label class="collapse" for="c-38706344">[-]</label><label class="expand" for="c-38706344">[1 more]</label></div><br/><div class="children"><div class="content">Additionally OpenCL was stuck in C99, Khronos ignored C++ for a long time, ignored Fortran to this day, and never cared about other languages.<p>CUDA is C, C++ and Fortran for at least a decade, and with PTX created a third party ecosystem that SPIR still doesn&#x27;t have to this day.</div><br/></div></div><div id="38706009" class="c"><input type="checkbox" id="c-38706009" checked=""/><div class="controls bullet"><span class="by">dotnet00</span><span>|</span><a href="#38705075">root</a><span>|</span><a href="#38705334">parent</a><span>|</span><a href="#38706344">prev</a><span>|</span><a href="#38706189">next</a><span>|</span><label class="collapse" for="c-38706009">[-]</label><label class="expand" for="c-38706009">[1 more]</label></div><br/><div class="children"><div class="content">Bigger than it taking a lot of boilerplate is the issue that there was never sufficiently widespread and bug free support for mixing CPU and GPU code in OpenCL in the way CUDA allows.<p>So you end up having to do a lot more work to port stuff over. When porting stuff to CUDA, you can share a lot of code. Simplifies the task so much.</div><br/></div></div><div id="38706189" class="c"><input type="checkbox" id="c-38706189" checked=""/><div class="controls bullet"><span class="by">jacoblambda</span><span>|</span><a href="#38705075">root</a><span>|</span><a href="#38705334">parent</a><span>|</span><a href="#38706009">prev</a><span>|</span><a href="#38705946">next</a><span>|</span><label class="collapse" for="c-38706189">[-]</label><label class="expand" for="c-38706189">[1 more]</label></div><br/><div class="children"><div class="content">Yep. That was the point I was making. Much like OpenGL, writing OpenCL is absolutely miserable so people just stuck with less portable alternatives that didn&#x27;t write like pulling teeth.</div><br/></div></div><div id="38705946" class="c"><input type="checkbox" id="c-38705946" checked=""/><div class="controls bullet"><span class="by">throwaway0665</span><span>|</span><a href="#38705075">root</a><span>|</span><a href="#38705334">parent</a><span>|</span><a href="#38706189">prev</a><span>|</span><a href="#38703987">next</a><span>|</span><label class="collapse" for="c-38705946">[-]</label><label class="expand" for="c-38705946">[1 more]</label></div><br/><div class="children"><div class="content">Just like OpenGL</div><br/></div></div></div></div></div></div></div></div><div id="38703987" class="c"><input type="checkbox" id="c-38703987" checked=""/><div class="controls bullet"><span class="by">dpflan</span><span>|</span><a href="#38705075">prev</a><span>|</span><label class="collapse" for="c-38703987">[-]</label><label class="expand" for="c-38703987">[2 more]</label></div><br/><div class="children"><div class="content">Are there use-cases for LLMs assisting in developing this software? Basically, I&#x27;m wondering if LLMs for developing a GPU-API exist and how can they can accelerate development such that this &quot;moat&quot; becomes more of a river that other can join?</div><br/><div id="38706308" class="c"><input type="checkbox" id="c-38706308" checked=""/><div class="controls bullet"><span class="by">seanhunter</span><span>|</span><a href="#38703987">parent</a><span>|</span><label class="collapse" for="c-38706308">[-]</label><label class="expand" for="c-38706308">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m struggling to see how that helps given the current problem they have.  What they have is several extremely half-assed implementations of somewhat different APIs that are basically the same and abysmal documentation. Pointing an LLM at it would make yet another half-assed implemetation of yet another different API, where what documentation it had would be convincingly wrong a bunch of the time.<p>What they need is:<p>1) Make one API [1] that builds out of the box, works really well and gets the best out of their hardware right away.  Bonus points if its not a total pig to use.<p>2) Take the thing that they built for 1 and make a straightforward python binding for it that works for tensorflow and pytorch.[2]<p>3) Have a beer on me. Seriously I would buy them one.<p>Ideally there&#x27;s an optional step 0 which is to stop making all kinds of press statements saying they are taking this seriously and paying sloppy journalists to say they are catching up in the race against nvidia if they can&#x27;t actually get their act together to do steps 1 and 2.<p>[1] In c, c++ or rust so it&#x27;s easy to embed in python and other languages etc<p>[2] By work here I mean the definition of done is a normal person can do `pip install tensorflow` and it will actually use an amd gpu with hardware accelleration right away.  Not some bullshit where you have to jump through a bunch of hoops and apply a whole lot of custom patches and it kinda sorta sometimes works but actually mostly just either falls back to CPU or crashes half the time</div><br/></div></div></div></div></div></div></div></div></div></body></html>