<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1684486857597" as="style"/><link rel="stylesheet" href="styles.css?v=1684486857597"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://twitter.com/wunderwuzzi23/status/1659411665853779971">Let ChatGPT visit a website and have your email stolen</a> <span class="domain">(<a href="https://twitter.com">twitter.com</a>)</span></div><div class="subtext"><span>wunderwuzzi23</span> | <span>25 comments</span></div><br/><div><div id="35998275" class="c"><input type="checkbox" id="c-35998275" checked=""/><div class="controls bullet"><span class="by">samwillis</span><span>|</span><a href="#35998744">next</a><span>|</span><label class="collapse" for="c-35998275">[-]</label><label class="expand" for="c-35998275">[8 more]</label></div><br/><div class="children"><div class="content">So I think what&#x27;s happing here:<p>1. The WebPilot plug-in is used for accessing the webpage that the user is asking to summarise<p>2. The prompt injection on that webpage then triggers the Zappier plugin to access the users email. This depends on the user having that plugin setup and enabled with access to their email.<p>3. The WebPilot plugin is then used again to exfiltrate the data.<p>It would be nice if the prompt on that page was shown, but I understand why they have removed it.<p>Essentially what OpenAI need to add is a positive assertion from the user each time a plugin is triggered indicating the actions that are going to take place. It&#x27;s somewhat mad that that has not been implemented, &quot;move fast and break things&quot;.</div><br/><div id="35998531" class="c"><input type="checkbox" id="c-35998531" checked=""/><div class="controls bullet"><span class="by">TheDong</span><span>|</span><a href="#35998275">parent</a><span>|</span><a href="#35998367">next</a><span>|</span><label class="collapse" for="c-35998531">[-]</label><label class="expand" for="c-35998531">[4 more]</label></div><br/><div class="children"><div class="content">I think we want a slightly finer grained permission model.<p>I think specifically we want a plugin to be able to indicate if it&#x27;s safe for it to be triggered without the user&#x27;s direct consent, and then the user themselves can also override any plugin to force it to need user consent or not.<p>For example, if we have a plugin that can fetch the weather for a single city, and a user asks &quot;Where, in the top 20 cities, is it the hottest?&quot;, I don&#x27;t think we want 20 prompts. I think the weather plugin can mark itself as &quot;I cannot do anything dangerous, just call me, no worries&quot;.<p>On the other hand, the zappier plugin, which can access email and perform proper actions, should be able to indicate &quot;all these APIs are sensitive&quot;<p>You might want to have a mix, where the weather plugin has &quot;GetUserOwnCityWeather&quot; (requires permissions &#x2F; user intent), and &quot;GetAnyCityWeather&quot; (public, no user intent required).</div><br/><div id="35999094" class="c"><input type="checkbox" id="c-35999094" checked=""/><div class="controls bullet"><span class="by">koboll</span><span>|</span><a href="#35998275">root</a><span>|</span><a href="#35998531">parent</a><span>|</span><a href="#35999011">next</a><span>|</span><label class="collapse" for="c-35999094">[-]</label><label class="expand" for="c-35999094">[1 more]</label></div><br/><div class="children"><div class="content">&gt;I think specifically we want a plugin to be able to indicate if it&#x27;s safe for it to be triggered without the user&#x27;s direct consent<p>It should really be on an endpoint by endpoint basis, since a single plugin can have several different endpoints.</div><br/></div></div><div id="35999011" class="c"><input type="checkbox" id="c-35999011" checked=""/><div class="controls bullet"><span class="by">freehorse</span><span>|</span><a href="#35998275">root</a><span>|</span><a href="#35998531">parent</a><span>|</span><a href="#35999094">prev</a><span>|</span><a href="#35998367">next</a><span>|</span><label class="collapse" for="c-35999011">[-]</label><label class="expand" for="c-35999011">[2 more]</label></div><br/><div class="children"><div class="content">We basically need to treat this sort of LLM usage as an operating system of sort, with some actions requiring administrator privileges granted as in an operating system.</div><br/><div id="35999173" class="c"><input type="checkbox" id="c-35999173" checked=""/><div class="controls bullet"><span class="by">Kevcmk</span><span>|</span><a href="#35998275">root</a><span>|</span><a href="#35999011">parent</a><span>|</span><a href="#35998367">next</a><span>|</span><label class="collapse" for="c-35999173">[-]</label><label class="expand" for="c-35999173">[1 more]</label></div><br/><div class="children"><div class="content">I believe that what is being missed in this thread is that, as it stands, user consent can be forged by prompt injection.</div><br/></div></div></div></div></div></div><div id="35998367" class="c"><input type="checkbox" id="c-35998367" checked=""/><div class="controls bullet"><span class="by">wunderwuzzi23</span><span>|</span><a href="#35998275">parent</a><span>|</span><a href="#35998531">prev</a><span>|</span><a href="#35999232">next</a><span>|</span><label class="collapse" for="c-35998367">[-]</label><label class="expand" for="c-35998367">[1 more]</label></div><br/><div class="children"><div class="content">That summarizes it well, and your conclusion is the same with what I meant with &quot;human in the loop&quot;.<p>I did remove the prompt since I didn&#x27;t want to share &quot;shell code&quot;, but it&#x27;s basically just natural language. Just asking what you want it to do. ChatGPT seems to automatically determine when to invoke a certain plugin once enabled.</div><br/></div></div><div id="35999232" class="c"><input type="checkbox" id="c-35999232" checked=""/><div class="controls bullet"><span class="by">h1fra</span><span>|</span><a href="#35998275">parent</a><span>|</span><a href="#35998367">prev</a><span>|</span><a href="#35999054">next</a><span>|</span><label class="collapse" for="c-35999232">[-]</label><label class="expand" for="c-35999232">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for summarising, I never used plugin and I had hard time understanding the tweet</div><br/></div></div></div></div><div id="35998744" class="c"><input type="checkbox" id="c-35998744" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#35998275">prev</a><span>|</span><a href="#35998229">next</a><span>|</span><label class="collapse" for="c-35998744">[-]</label><label class="expand" for="c-35998744">[1 more]</label></div><br/><div class="children"><div class="content">Looks like a working proof of concept of the data exfiltration attack I describe here: <a href="https:&#x2F;&#x2F;simonwillison.net&#x2F;2023&#x2F;Apr&#x2F;14&#x2F;worst-that-can-happen&#x2F;#data-exfiltration" rel="nofollow">https:&#x2F;&#x2F;simonwillison.net&#x2F;2023&#x2F;Apr&#x2F;14&#x2F;worst-that-can-happen&#x2F;...</a></div><br/></div></div><div id="35998229" class="c"><input type="checkbox" id="c-35998229" checked=""/><div class="controls bullet"><span class="by">yosito</span><span>|</span><a href="#35998744">prev</a><span>|</span><a href="#35998287">next</a><span>|</span><label class="collapse" for="c-35998229">[-]</label><label class="expand" for="c-35998229">[3 more]</label></div><br/><div class="children"><div class="content">There is some essential context missing from this screenshot to understand what is going on and if it&#x27;s significant or not. Where is ChatGPT getting the information about which email account to hack? That has to be provided either to the Wuzzi webpage, or to WebPilot or directly to ChatGPT somewhere. I noticed recently while developing a ChatGPT plugin that the HTTP requests that the plugin makes happen directly in my browser, and not on an OpenAI server. The implication, as far as I can tell, is that the Wuzzi webpage is somehow stealing some authentication token or something from your browser if you&#x27;re logged into your email. Definitely a plausible attack vector, but I&#x27;d like to know more.</div><br/><div id="35998295" class="c"><input type="checkbox" id="c-35998295" checked=""/><div class="controls bullet"><span class="by">wunderwuzzi23</span><span>|</span><a href="#35998229">parent</a><span>|</span><a href="#35998287">next</a><span>|</span><label class="collapse" for="c-35998295">[-]</label><label class="expand" for="c-35998295">[2 more]</label></div><br/><div class="children"><div class="content">Its the other plugin that has the authentication token to the emails (it&#x27;s part of setup if you use certain plugins).<p>So its not CSRF, but a confused deputy problem with plugins, Cross Plugin Request Forgery for lack of a better term.</div><br/><div id="35998375" class="c"><input type="checkbox" id="c-35998375" checked=""/><div class="controls bullet"><span class="by">yosito</span><span>|</span><a href="#35998229">root</a><span>|</span><a href="#35998295">parent</a><span>|</span><a href="#35998287">next</a><span>|</span><label class="collapse" for="c-35998375">[-]</label><label class="expand" for="c-35998375">[1 more]</label></div><br/><div class="children"><div class="content">It generally seems like a bad idea to give a plugin an authentication token if it&#x27;s in a shared environment with an untrusted plugin. Maybe obvious to a typical HN reader, but probably something OpenAI should mitigate or warn about before plugins go public.<p>Plugins probably need some kind of permission model for accessing credentials, and the UI could surface warnings when activating credentialed plugins together with other plugins in the same environment. And any plugin that uses credentials, should have some kind of &quot;Approve&#x2F;Deny&quot; dialog each time it executes.<p>As a side note, I was using the letters CRSF as the internal name for the plugin I&#x27;m testing, so reading CSRF is messing with my brain.</div><br/></div></div></div></div></div></div><div id="35998287" class="c"><input type="checkbox" id="c-35998287" checked=""/><div class="controls bullet"><span class="by">rahimnathwani</span><span>|</span><a href="#35998229">prev</a><span>|</span><a href="#35998272">next</a><span>|</span><label class="collapse" for="c-35998287">[-]</label><label class="expand" for="c-35998287">[2 more]</label></div><br/><div class="children"><div class="content">When you start a ChatGPT session with GPT-4 and Plugins Beta, no plugins are enabled by default. You can pick up to 3 plugins to enable, and you are restricted to plugins you&#x27;ve added from the &#x27;Plugin store&#x27;.<p>If I were going to start a GPT-4 session and ask it to summarize a web page, I&#x27;d probably use the &#x27;Browsing&#x27; version, which wouldn&#x27;t have access to any plugins, let alone those specific ones.</div><br/></div></div><div id="35998272" class="c"><input type="checkbox" id="c-35998272" checked=""/><div class="controls bullet"><span class="by">qiqitori</span><span>|</span><a href="#35998287">prev</a><span>|</span><a href="#35998193">next</a><span>|</span><label class="collapse" for="c-35998272">[-]</label><label class="expand" for="c-35998272">[1 more]</label></div><br/><div class="children"><div class="content">Watching the video linked in his previous tweet (<a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=PIY5ZVktiGs">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=PIY5ZVktiGs</a>) I finally understood what the author was talking about.<p>Are there a lot of people who use plugins with ChatGPT? Are there any interesting use cases?</div><br/></div></div><div id="35998193" class="c"><input type="checkbox" id="c-35998193" checked=""/><div class="controls bullet"><span class="by">wcrossbow</span><span>|</span><a href="#35998272">prev</a><span>|</span><a href="#35998216">next</a><span>|</span><label class="collapse" for="c-35998193">[-]</label><label class="expand" for="c-35998193">[1 more]</label></div><br/><div class="children"><div class="content">It was already predicted in this forum that this would happen sooner rather than later. I remember somebody already posted a simpler PoC more than a week ago.  Nothing good comes from letting a 3rd party, the attacking site, give arbitrary instruction to your systems on how to behave. Maybe OpenAI wants regulation to stop this kind of thing. What would be a funny development though is if they ended up themselves ̶o̶u̶t̶ ̶o̶f̶ ̶t̶h̶e̶ ̶m̶o̶a̶t̶ not being compliant.<p>Edit: The last sentence didn&#x27;t made sense without too much context. Clarified, but kept the original striked-through</div><br/></div></div><div id="35998216" class="c"><input type="checkbox" id="c-35998216" checked=""/><div class="controls bullet"><span class="by">danpalmer</span><span>|</span><a href="#35998193">prev</a><span>|</span><a href="#35998350">next</a><span>|</span><label class="collapse" for="c-35998216">[-]</label><label class="expand" for="c-35998216">[3 more]</label></div><br/><div class="children"><div class="content">Supposedly &quot;Be concise&quot; in a prompt can save 40% on API costs. I wonder how effective attacks against API cost would be. I suspect it would be very easy to get ChatGPT to output as much text as the API or quota will allow.<p>It feels like what I imagine the early internet was, where you could stick `&#x27;; drop table users;` into email fields and ruin someone&#x27;s day. There&#x27;s so much scope for security issues as LLMs are woven into services, exposed through cracks to users.<p>Everywhere that could be SQL&#x2F;database injected, or shell injected, can now be LLM injected. The difference being that where databases&#x2F;shells are intended to work with a strict character set and have separation of data and command channels, LLMs don&#x27;t currently have that separation and are designed to use unstructured data so ensuring they aren&#x27;t being attacked cannot be known in the general case.<p>I suspect that over time LLM use will fall into two categories: sandboxed LLMs where users can interact directly for creative purposes, and LLMs that untrusted users do not get direct access to, where user input is narrowly specified, and sanitised by a sandboxed LLM.</div><br/><div id="35998554" class="c"><input type="checkbox" id="c-35998554" checked=""/><div class="controls bullet"><span class="by">singularity2001</span><span>|</span><a href="#35998216">parent</a><span>|</span><a href="#35998350">next</a><span>|</span><label class="collapse" for="c-35998554">[-]</label><label class="expand" for="c-35998554">[2 more]</label></div><br/><div class="children"><div class="content">I wish there was a &quot;default prompt&quot; so that I don&#x27;t have to add &quot;think step by step, be concise&quot; every single time</div><br/><div id="35998819" class="c"><input type="checkbox" id="c-35998819" checked=""/><div class="controls bullet"><span class="by">danpalmer</span><span>|</span><a href="#35998216">root</a><span>|</span><a href="#35998554">parent</a><span>|</span><a href="#35998350">next</a><span>|</span><label class="collapse" for="c-35998819">[-]</label><label class="expand" for="c-35998819">[1 more]</label></div><br/><div class="children"><div class="content">There is a default prompt built into every one of these systems. If it doesn&#x27;t contain those instructions it&#x27;s probably for good reason. ChatGPT seems to target a more creative output, for which I could imagine &quot;think step by step, be concise&quot; is not appropriate.</div><br/></div></div></div></div></div></div><div id="35998350" class="c"><input type="checkbox" id="c-35998350" checked=""/><div class="controls bullet"><span class="by">yawnxyz</span><span>|</span><a href="#35998216">prev</a><span>|</span><a href="#35998186">next</a><span>|</span><label class="collapse" for="c-35998350">[-]</label><label class="expand" for="c-35998350">[1 more]</label></div><br/><div class="children"><div class="content">Ha for some reason I thought this more as a feature, less as a bug</div><br/></div></div><div id="35998186" class="c"><input type="checkbox" id="c-35998186" checked=""/><div class="controls bullet"><span class="by">FanaHOVA</span><span>|</span><a href="#35998350">prev</a><span>|</span><a href="#35998519">next</a><span>|</span><label class="collapse" for="c-35998186">[-]</label><label class="expand" for="c-35998186">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Why no human in the loop?<p>&quot;Stop Generating&quot; is right there?</div><br/><div id="35998265" class="c"><input type="checkbox" id="c-35998265" checked=""/><div class="controls bullet"><span class="by">Gigachad</span><span>|</span><a href="#35998186">parent</a><span>|</span><a href="#35999051">next</a><span>|</span><label class="collapse" for="c-35998265">[-]</label><label class="expand" for="c-35998265">[1 more]</label></div><br/><div class="children"><div class="content">Than relies on you being fast enough to hit the button. Given how fast other tools like Bard are, that isn’t a viable solution.</div><br/></div></div><div id="35999051" class="c"><input type="checkbox" id="c-35999051" checked=""/><div class="controls bullet"><span class="by">freehorse</span><span>|</span><a href="#35998186">parent</a><span>|</span><a href="#35998265">prev</a><span>|</span><a href="#35998519">next</a><span>|</span><label class="collapse" for="c-35999051">[-]</label><label class="expand" for="c-35999051">[1 more]</label></div><br/><div class="children"><div class="content">That assumes that you realise what is going on fast enough. An actual malicious prompt injection would try to obfuscate things so that they confuse the user.</div><br/></div></div></div></div></div></div></div></div></div></body></html>