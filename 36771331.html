<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1689843663954" as="style"/><link rel="stylesheet" href="styles.css?v=1689843663954"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.infoq.com/news/2023/07/yelp-corrupted-cassandra-rebuild/">Yelp rebuilds corrupted Cassandra cluster using its data streaming architecture</a> <span class="domain">(<a href="https://www.infoq.com">www.infoq.com</a>)</span></div><div class="subtext"><span>rgancarz</span> | <span>23 comments</span></div><br/><div><div id="36796919" class="c"><input type="checkbox" id="c-36796919" checked=""/><div class="controls bullet"><span class="by">hardwaresofton</span><span>|</span><a href="#36797191">next</a><span>|</span><label class="collapse" for="c-36796919">[-]</label><label class="expand" for="c-36796919">[15 more]</label></div><br/><div class="children"><div class="content">For those that care about <i>how</i> it got corrupted:<p><a href="https:&#x2F;&#x2F;engineeringblog.yelp.com&#x2F;2023&#x2F;01&#x2F;rebuilding-a-cassandra-cluster-using-yelps-data-pipeline.html#data-corruption-overview" rel="nofollow noreferrer">https:&#x2F;&#x2F;engineeringblog.yelp.com&#x2F;2023&#x2F;01&#x2F;rebuilding-a-cassan...</a><p>&gt; The investigation around the exception revealed that at-least one of the SSTable (Sorted String Table) rows was unordered, which caused the compaction operation to fail. SSTables are immutable files that are always sorted by the primary key<p>This was just... a bug in Cassandra? Is there anyone that can shed light on this? There seem to be plenty of people using Cassandra at scale -- is constantly repairing it normal practice?</div><br/><div id="36797336" class="c"><input type="checkbox" id="c-36797336" checked=""/><div class="controls bullet"><span class="by">t90fan</span><span>|</span><a href="#36796919">parent</a><span>|</span><a href="#36797672">next</a><span>|</span><label class="collapse" for="c-36797336">[-]</label><label class="expand" for="c-36797336">[5 more]</label></div><br/><div class="children"><div class="content">Running regular incremental repairs is the norm, as nodes will from time to time have trouble talking to each other due to real world network reasons, or will go down, for things like OS patching. We had a (daily) cron job for it. I come from the software side not the DBA side of things but my main advice from running Cassandra at scale in production (it was part of an Apigee stack) is don&#x27;t basically! It was very not realisable, would consume huge volumes of memory (especially during repairs), bandwidth (doing  a repair is very chatty as it has to sync lots of data) and disk space (tombstoning meant deleted records take up space until compaction runs), and was generally not much fun to manage, and it was difficult to hire people who knew much about it to do so. I would not build a solution myself using it going forward. We also had to periodically (weekly) do &quot;full&quot; repairs to work around Cassandra bugs, silent data corruption etc...</div><br/><div id="36797985" class="c"><input type="checkbox" id="c-36797985" checked=""/><div class="controls bullet"><span class="by">hardwaresofton</span><span>|</span><a href="#36796919">root</a><span>|</span><a href="#36797336">parent</a><span>|</span><a href="#36797497">next</a><span>|</span><label class="collapse" for="c-36797985">[-]</label><label class="expand" for="c-36797985">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for sharing your experience -- I know I&#x27;ve spent a lot of time in the past worrying about FS corruption, but generally expecting that the database sitting on top of it should never get corrupted, mostly because I use postgres so much.<p>I don&#x27;t have the experience you do in this situation, but my first reaction to this was definitely &quot;don&#x27;t use Cassandra&quot;. But I also never really understood the use-case where Cassandra shines as a solution either (seems like only companies with a lot of data really seem to get wins from it?)</div><br/></div></div><div id="36797497" class="c"><input type="checkbox" id="c-36797497" checked=""/><div class="controls bullet"><span class="by">rickette</span><span>|</span><a href="#36796919">root</a><span>|</span><a href="#36797336">parent</a><span>|</span><a href="#36797985">prev</a><span>|</span><a href="#36797998">next</a><span>|</span><label class="collapse" for="c-36797497">[-]</label><label class="expand" for="c-36797497">[1 more]</label></div><br/><div class="children"><div class="content">Can recommend <a href="https:&#x2F;&#x2F;cassandra-reaper.io&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;cassandra-reaper.io&#x2F;</a> for most of the management stuff you&#x27;re mentioning. Still not free though, running Cassandra requires (some) effort in my experience.</div><br/></div></div><div id="36797998" class="c"><input type="checkbox" id="c-36797998" checked=""/><div class="controls bullet"><span class="by">darkstar_16</span><span>|</span><a href="#36796919">root</a><span>|</span><a href="#36797336">parent</a><span>|</span><a href="#36797497">prev</a><span>|</span><a href="#36797628">next</a><span>|</span><label class="collapse" for="c-36797998">[-]</label><label class="expand" for="c-36797998">[1 more]</label></div><br/><div class="children"><div class="content">We run a Cassandra cluster in production and its a pretty small cluster yet all that you mentioned seems to resonate. We do use Cassandra reaper to automate some of the tasks but no one wants to touch Cassandra in general in the team.</div><br/></div></div><div id="36797628" class="c"><input type="checkbox" id="c-36797628" checked=""/><div class="controls bullet"><span class="by">TideAd</span><span>|</span><a href="#36796919">root</a><span>|</span><a href="#36797336">parent</a><span>|</span><a href="#36797998">prev</a><span>|</span><a href="#36797672">next</a><span>|</span><label class="collapse" for="c-36797628">[-]</label><label class="expand" for="c-36797628">[1 more]</label></div><br/><div class="children"><div class="content">Scaling up also takes up a lot of resources so you&#x27;re never able to scale up in response to load without hosing your database even more.</div><br/></div></div></div></div><div id="36797672" class="c"><input type="checkbox" id="c-36797672" checked=""/><div class="controls bullet"><span class="by">pbreit</span><span>|</span><a href="#36796919">parent</a><span>|</span><a href="#36797336">prev</a><span>|</span><a href="#36798117">next</a><span>|</span><label class="collapse" for="c-36797672">[-]</label><label class="expand" for="c-36797672">[8 more]</label></div><br/><div class="children"><div class="content">One wonders if Yelp could just run on a basic Postgres setup. It&#x27;s not too much data, the data is relatively unimportant and the traffic is modest and mostly from US. How do the setups get so complicated?</div><br/><div id="36798249" class="c"><input type="checkbox" id="c-36798249" checked=""/><div class="controls bullet"><span class="by">mrweasel</span><span>|</span><a href="#36796919">root</a><span>|</span><a href="#36797672">parent</a><span>|</span><a href="#36797774">next</a><span>|</span><label class="collapse" for="c-36798249">[-]</label><label class="expand" for="c-36798249">[1 more]</label></div><br/><div class="children"><div class="content">* * *</div><br/></div></div><div id="36797774" class="c"><input type="checkbox" id="c-36797774" checked=""/><div class="controls bullet"><span class="by">avereveard</span><span>|</span><a href="#36796919">root</a><span>|</span><a href="#36797672">parent</a><span>|</span><a href="#36798249">prev</a><span>|</span><a href="#36798061">next</a><span>|</span><label class="collapse" for="c-36797774">[-]</label><label class="expand" for="c-36797774">[4 more]</label></div><br/><div class="children"><div class="content">Yelp was founded 2004, 2004 postgres was different than 2024 postgres and storage and server options werent as powerful as today either.<p>There were half a million restaurants back then or thereabout, and they wanted to store ratings and comments.<p>That is not something you&#x27;d be able to put on one single database in 2004.<p>Why didn&#x27;t they simplify afterward I can&#x27;t imagine, but I can see how a business directory at that age looked at the numbers and went yeah not in a database.</div><br/><div id="36797903" class="c"><input type="checkbox" id="c-36797903" checked=""/><div class="controls bullet"><span class="by">onlypositive</span><span>|</span><a href="#36796919">root</a><span>|</span><a href="#36797774">parent</a><span>|</span><a href="#36797909">next</a><span>|</span><label class="collapse" for="c-36797903">[-]</label><label class="expand" for="c-36797903">[2 more]</label></div><br/><div class="children"><div class="content">Cassandra was initially released in 2008.<p>Back in 2004 I was running larger MySQL databases than yelp has now:<p><a href="https:&#x2F;&#x2F;www.enterpriseappstoday.com&#x2F;stats&#x2F;yelp-statistics.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.enterpriseappstoday.com&#x2F;stats&#x2F;yelp-statistics.ht...</a><p>&gt; That is not something you&#x27;d be able to put on one single database in 2004.<p>Sorry, but it was, plenty of companies had much larger databases back then running plain old  master slave replication.<p>But even if it wasn&#x27;t: just don&#x27;t run it all on one db? Nothing says you need to be able to join on the restaurant table and the comments table. Put them on different servers. That&#x27;s all you&#x27;re doing with Casandra anyway.</div><br/><div id="36798060" class="c"><input type="checkbox" id="c-36798060" checked=""/><div class="controls bullet"><span class="by">tomnipotent</span><span>|</span><a href="#36796919">root</a><span>|</span><a href="#36797903">parent</a><span>|</span><a href="#36797909">next</a><span>|</span><label class="collapse" for="c-36798060">[-]</label><label class="expand" for="c-36798060">[1 more]</label></div><br/><div class="children"><div class="content">Postgres didn&#x27;t ship built-in replication until 2010, and prior replication solutions like Slony were not options I would have enjoyed building a business around.</div><br/></div></div></div></div><div id="36797909" class="c"><input type="checkbox" id="c-36797909" checked=""/><div class="controls bullet"><span class="by">weego</span><span>|</span><a href="#36796919">root</a><span>|</span><a href="#36797774">parent</a><span>|</span><a href="#36797903">prev</a><span>|</span><a href="#36798061">next</a><span>|</span><label class="collapse" for="c-36797909">[-]</label><label class="expand" for="c-36797909">[1 more]</label></div><br/><div class="children"><div class="content">I mean, for a start Cassandra wasn&#x27;t available for years after that date, but either way, whatever Postgres was like back then Cassandra was much more of a piece of shit. Smells like implementing what the cool kids were doing to me.</div><br/></div></div></div></div><div id="36798061" class="c"><input type="checkbox" id="c-36798061" checked=""/><div class="controls bullet"><span class="by">redeyedtreefrog</span><span>|</span><a href="#36796919">root</a><span>|</span><a href="#36797672">parent</a><span>|</span><a href="#36797774">prev</a><span>|</span><a href="#36798117">next</a><span>|</span><label class="collapse" for="c-36798061">[-]</label><label class="expand" for="c-36798061">[2 more]</label></div><br/><div class="children"><div class="content">I found this on their tech blog from 2016: <a href="https:&#x2F;&#x2F;engineeringblog.yelp.com&#x2F;2016&#x2F;08&#x2F;how-we-scaled-our-ad-analytics-with-cassandra.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;engineeringblog.yelp.com&#x2F;2016&#x2F;08&#x2F;how-we-scaled-our-a...</a><p>Whether they really needed cassandra or whether it was just using fancy complicated tech for the sake of it, I couldn&#x27;t say.</div><br/><div id="36798128" class="c"><input type="checkbox" id="c-36798128" checked=""/><div class="controls bullet"><span class="by">beoberha</span><span>|</span><a href="#36796919">root</a><span>|</span><a href="#36798061">parent</a><span>|</span><a href="#36798117">next</a><span>|</span><label class="collapse" for="c-36798128">[-]</label><label class="expand" for="c-36798128">[1 more]</label></div><br/><div class="children"><div class="content">Yeah sounds like they liked it because of the infinitely scalable storage and high write throughput, but don’t really get into if they were having issues with all that on MySQL. Also, they use the term “analytics” a lot, but it seems to be a different use case than typical OLAP workloads since they were serving this data to customers adhoc.</div><br/></div></div></div></div></div></div><div id="36798117" class="c"><input type="checkbox" id="c-36798117" checked=""/><div class="controls bullet"><span class="by">CHY872</span><span>|</span><a href="#36796919">parent</a><span>|</span><a href="#36797672">prev</a><span>|</span><a href="#36797191">next</a><span>|</span><label class="collapse" for="c-36798117">[-]</label><label class="expand" for="c-36798117">[1 more]</label></div><br/><div class="children"><div class="content">I have &gt;5 years of experience using Cassandra in production, involving thousands of clusters storing petabytes of data. My conclusion from that time is that Cassandra is simply not robust enough to be a general purpose database (the team are working on it but they&#x27;re coming from a really rough starting place) - there are lots of ways to cause data corruption, and Cassandra does enough dynamic repairing that it can be hard to catch this before your backups are dropped due to time windowing. Unfortunately, the juice may still be worth the squeeze - Cassandra&#x27;s storage model lends itself very nicely to disaster recovery workflows in a way which something like Oracle or FoundationDB does not (and it&#x27;s Cassandra so you&#x27;ll need it!), while the ability to horizontally scale gets you out of so many operations issues. If you&#x27;ve got a schema which works well in Cassandra, you&#x27;ve probably solved a lot of the issues you might have.<p>Example of fairly standard Cassandra bug (don&#x27;t know if present on latest release, certainly was a year or two ago): When you add a new node to the cluster, it &#x27;bootstraps&#x27;, where it copies ~1&#x2F;n the data from other nodes. When you are done bootstrapping, it&#x27;s copied a bunch of data from other nodes, but the other nodes still contain that data. You then run &#x27;cleanups&#x27; on the other nodes to remove the (now stale and unusable) data so as to get your disk space back.<p>If you accidentally run a cleanup on the new node as it is being bootstrapped, it will succeed, you will delete all the data that&#x27;s been copied over so far, and Cassandra will _not_ terminate the bootstrap. Everything will be green, but your new node will suddenly be using 0 disk space. When the bootstrap finishes, possibly days later, your cluster will be immediately corrupted due to violated replication guarantees - but only on data that hasn&#x27;t been read or written over that period, because if it was written it&#x27;ll be re-replicated, and if it was read Cassandra will silently repair at this time. Repairs resolve the issue, but if you&#x27;ve made this mistake due to scripting, if you get unlucky it&#x27;s possible to just delete all replicas of some data between repairs.<p>Example of other Cassandra bug (again, might be outdated): Cassandra nodes identify themselves on startups with IPs, and the owned token ranges are not persisted, they&#x27;re streamed from other nodes in the cluster. If you&#x27;ve deployed your Cassandra in K8s and you reboot multiple nodes in one go and they swap IPs upon reboot, you may now find yourself in a split brain situation in which nodes magically forget they own certain data ranges and think they own each others data (or maybe it&#x27;s that the nodes still think they own the right ranges but other nodes think they own the wrong ranges). Wasn&#x27;t close enough to fully debug that one.<p>It&#x27;s a mess. Would seek to avoid problem spaces where I might need to use it again, though if by chance ended up in a space where it made sense, probably wouldn&#x27;t avoid the tech.</div><br/></div></div></div></div><div id="36797191" class="c"><input type="checkbox" id="c-36797191" checked=""/><div class="controls bullet"><span class="by">EgregiousCube</span><span>|</span><a href="#36796919">prev</a><span>|</span><a href="#36796773">next</a><span>|</span><label class="collapse" for="c-36797191">[-]</label><label class="expand" for="c-36797191">[1 more]</label></div><br/><div class="children"><div class="content">Love reading this, it&#x27;s a story about how a back of house team got to have an impact on the business in a more immediate way than they usually get to.  Data teams have a longer time horizon for when they succeed or fail than the glamorous lives of backend - let alone frontend! - engineering teams, which makes it harder to recognize the wins in the moment.<p>To any Yelp data engineers who might happen to read - good work, and it&#x27;s a good testament to the platform you provide.</div><br/></div></div><div id="36796773" class="c"><input type="checkbox" id="c-36796773" checked=""/><div class="controls bullet"><span class="by">mhio</span><span>|</span><a href="#36797191">prev</a><span>|</span><a href="#36796811">next</a><span>|</span><label class="collapse" for="c-36796773">[-]</label><label class="expand" for="c-36796773">[1 more]</label></div><br/><div class="children"><div class="content">Source: <a href="https:&#x2F;&#x2F;engineeringblog.yelp.com&#x2F;2023&#x2F;01&#x2F;rebuilding-a-cassandra-cluster-using-yelps-data-pipeline.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;engineeringblog.yelp.com&#x2F;2023&#x2F;01&#x2F;rebuilding-a-cassan...</a> (Jan 30, 2023)</div><br/></div></div><div id="36796811" class="c"><input type="checkbox" id="c-36796811" checked=""/><div class="controls bullet"><span class="by">jhgg</span><span>|</span><a href="#36796773">prev</a><span>|</span><a href="#36796788">next</a><span>|</span><label class="collapse" for="c-36796811">[-]</label><label class="expand" for="c-36796811">[1 more]</label></div><br/><div class="children"><div class="content">What is a bit unclear which isn&#x27;t addressed in the original blog post is how they actually were able to successfully scan over all the data and copy it to the new cluster, in spite of corrupt sstables. CDC would not handle back filling historical data. Would expect that to fail for certain token ranges. Perhaps they ended up deciding to discard them?</div><br/></div></div><div id="36796788" class="c"><input type="checkbox" id="c-36796788" checked=""/><div class="controls bullet"><span class="by">berkle4455</span><span>|</span><a href="#36796811">prev</a><span>|</span><a href="#36796572">next</a><span>|</span><label class="collapse" for="c-36796788">[-]</label><label class="expand" for="c-36796788">[1 more]</label></div><br/><div class="children"><div class="content">Despite best efforts, the shard and all of its data was lost. However due to their forward-thinking strategy it was limited to only 3-star and lower reviews for Yelp Premium clients!</div><br/></div></div><div id="36796572" class="c"><input type="checkbox" id="c-36796572" checked=""/><div class="controls bullet"><span class="by">grrdotcloud</span><span>|</span><a href="#36796788">prev</a><span>|</span><a href="#36797826">next</a><span>|</span><label class="collapse" for="c-36796572">[-]</label><label class="expand" for="c-36796572">[1 more]</label></div><br/><div class="children"><div class="content">I am excited to read about system recovery in most cases.</div><br/></div></div><div id="36797826" class="c"><input type="checkbox" id="c-36797826" checked=""/><div class="controls bullet"><span class="by">1letterunixname</span><span>|</span><a href="#36796572">prev</a><span>|</span><a href="#36796436">next</a><span>|</span><label class="collapse" for="c-36797826">[-]</label><label class="expand" for="c-36797826">[1 more]</label></div><br/><div class="children"><div class="content">Yelp: the mafia of restaurant protection rackets. I don&#x27;t care if they released a free quantum computer or solved world hunger because they&#x27;re still making a living off harming small businesses.<p><a href="https:&#x2F;&#x2F;www.yahoo.com&#x2F;lifestyle&#x2F;can-you-trust-yelp-crowd-funded-film-says-its-114144098147.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.yahoo.com&#x2F;lifestyle&#x2F;can-you-trust-yelp-crowd-fun...</a><p><a href="https:&#x2F;&#x2F;thehustle.co&#x2F;botto-bistro-1-star-yelp&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;thehustle.co&#x2F;botto-bistro-1-star-yelp&#x2F;</a><p><a href="https:&#x2F;&#x2F;nypost.com&#x2F;2014&#x2F;10&#x2F;13&#x2F;restaurant-fights-yelps-alleged-extortion&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;nypost.com&#x2F;2014&#x2F;10&#x2F;13&#x2F;restaurant-fights-yelps-allege...</a><p><a href="https:&#x2F;&#x2F;www.wired.com&#x2F;2010&#x2F;02&#x2F;yelp-sued-for-alleged-extortion&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.wired.com&#x2F;2010&#x2F;02&#x2F;yelp-sued-for-alleged-extortio...</a></div><br/></div></div><div id="36796436" class="c"><input type="checkbox" id="c-36796436" checked=""/><div class="controls bullet"><span class="by">Spivak</span><span>|</span><a href="#36797826">prev</a><span>|</span><label class="collapse" for="c-36796436">[-]</label><label class="expand" for="c-36796436">[1 more]</label></div><br/><div class="children"><div class="content">Can second that this is a very solid and reliable way to do any kind of migration, not just recovering from corruption. When possible being able to insert your own code in the middle of the transfer and shunt bad data to a dead letter queue is a life saver sometimes because the last thing you want to have happen is the xfer get stuck when it hits a pothole.</div><br/></div></div></div></div></div></div></div></body></html>