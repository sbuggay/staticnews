<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1686387663657" as="style"/><link rel="stylesheet" href="styles.css?v=1686387663657"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.economist.com/science-and-technology/2023/05/24/artificial-brains-are-helping-scientists-study-the-real-thing">Artificial brains help understand real brains</a> <span class="domain">(<a href="https://www.economist.com">www.economist.com</a>)</span></div><div class="subtext"><span>Jeff_Brown</span> | <span>41 comments</span></div><br/><div><div id="36263859" class="c"><input type="checkbox" id="c-36263859" checked=""/><div class="controls bullet"><span class="by">TaupeRanger</span><span>|</span><a href="#36264079">next</a><span>|</span><label class="collapse" for="c-36263859">[-]</label><label class="expand" for="c-36263859">[25 more]</label></div><br/><div class="children"><div class="content">Not really. The only real example given in the article is when you hook someone up to an fMRI machine, collect data about how the brain looks when it sees a certain image, and then have a computational statistics program (NOT an artificial brain, in any sense) do some number crunching and output the most likely thing it&#x27;s looking at based on things you specifically trained it to recognize beforehand. We learn precisely nothing from this, no medical or computer science advances are made from it, and it doesn&#x27;t remotely support the title of the article.</div><br/><div id="36264174" class="c"><input type="checkbox" id="c-36264174" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#36263859">parent</a><span>|</span><a href="#36267848">next</a><span>|</span><label class="collapse" for="c-36264174">[-]</label><label class="expand" for="c-36264174">[17 more]</label></div><br/><div class="children"><div class="content">I think the Economist article is exactly right - that despite the massive differences between ANNs and the brain, ANNs are indeed highly suggestive of how some aspects of the brain appear to work.<p>People can criticize the shortcomings of GPT-4, but it&#x27;s hard to argue that it&#x27;s at least capable of some level of reasoning (or functionally equivalent if you object to that word!). It&#x27;s not yet clear exactly how a Transformer works other than at mechanical level of the model architecture (vs the LLM &quot;running on&quot; the architecture), but we are at least starting to glean some knowledge of how the trained model is operating...<p>It seems that pairs of attention heads in consecutive layers are acting in coordination as &quot;induction heads&quot; that in one case are performing a kind of analogical(?) A&#x27;B&#x27; =&gt; AB match-and-copy type of operation. The induction head causes a context token A to be matched (via &quot;attention&quot; key query) with an earlier token A&#x27; whose following token B&#x27; then causes related token B to be copied to the residual stream at position following A.<p>This seems a very basic type of operation, and no doubt there&#x27;s a lot more interpretability research to be done, but given the resulting reasoning&#x2F;cognitive power (even in absense of any working memory or looping!), it seems we don&#x27;t need to go looking for overly complex exotic mechanisms to begin to understand how the cortex may be operating. It&#x27;s easy to imagine how this same type of embedded key matching might work in the cortex, perhaps with cortical columns acting as complex pattern matchers. Perhaps the brain&#x27;s well known ~7 item working memory corresponds to a &quot;context&quot; of sorts that is updated in same way as induction heads update the residual stream.<p>Anything I&#x27;ve written here about correspondence between transformer and cortex is of course massive speculation, but the point is that the ANN&#x27;s operation does indeed start to suggest how the brain, operating on similar sparse&#x2F;embedded representations, may be working.</div><br/><div id="36264503" class="c"><input type="checkbox" id="c-36264503" checked=""/><div class="controls bullet"><span class="by">amelius</span><span>|</span><a href="#36263859">root</a><span>|</span><a href="#36264174">parent</a><span>|</span><a href="#36264804">next</a><span>|</span><label class="collapse" for="c-36264503">[-]</label><label class="expand" for="c-36264503">[6 more]</label></div><br/><div class="children"><div class="content">But (how) does the human brain perform backpropagation?</div><br/><div id="36264594" class="c"><input type="checkbox" id="c-36264594" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#36263859">root</a><span>|</span><a href="#36264503">parent</a><span>|</span><a href="#36265467">next</a><span>|</span><label class="collapse" for="c-36264594">[-]</label><label class="expand" for="c-36264594">[4 more]</label></div><br/><div class="children"><div class="content">It probably doesn&#x27;t use backpropagation of gradients. Instead, the cortex appears to be a prediction engine that uses error feedback (perceptual reality vs prediction) to minimize prediction errors in a conceptually similar type of way. If every &quot;layer&quot; (cortical patch) is doing it&#x27;s own prediction and receiving feedback, then you don&#x27;t need any error propagation from one layer to the next.</div><br/><div id="36264813" class="c"><input type="checkbox" id="c-36264813" checked=""/><div class="controls bullet"><span class="by">marcosdumay</span><span>|</span><a href="#36263859">root</a><span>|</span><a href="#36264594">parent</a><span>|</span><a href="#36265467">next</a><span>|</span><label class="collapse" for="c-36264813">[-]</label><label class="expand" for="c-36264813">[3 more]</label></div><br/><div class="children"><div class="content">You just can&#x27;t go changing the layers of a neural network independently of each other if you are doing guided optimization.<p>For a start, it&#x27;s not even a given if increasing some weight will increase or decrease the result. The neurons are all tightly coupled.<p>You can do it on unguided optimization, what is one of the reasons I strongly suspect our brains use something similar to simulated annealing.</div><br/><div id="36265189" class="c"><input type="checkbox" id="c-36265189" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#36263859">root</a><span>|</span><a href="#36264813">parent</a><span>|</span><a href="#36265467">next</a><span>|</span><label class="collapse" for="c-36265189">[-]</label><label class="expand" for="c-36265189">[2 more]</label></div><br/><div class="children"><div class="content">Sure, we don&#x27;t know the exact details (Geoff Hinton spent much of his career trying to answer this question), but at the big picture level it does seem clear that the cortex is a prediction engine that minimizes prediction errors by feedback, and most likely does so in a localized way. Exactly how these prediction updates work is unknown.<p>Could you expand a bit on how you think simulated annealing could work?</div><br/></div></div></div></div></div></div><div id="36265467" class="c"><input type="checkbox" id="c-36265467" checked=""/><div class="controls bullet"><span class="by">canjobear</span><span>|</span><a href="#36263859">root</a><span>|</span><a href="#36264503">parent</a><span>|</span><a href="#36264594">prev</a><span>|</span><a href="#36264804">next</a><span>|</span><label class="collapse" for="c-36265467">[-]</label><label class="expand" for="c-36265467">[1 more]</label></div><br/><div class="children"><div class="content">Approximately, via predictive coding <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2006.04182" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2006.04182</a></div><br/></div></div></div></div><div id="36264804" class="c"><input type="checkbox" id="c-36264804" checked=""/><div class="controls bullet"><span class="by">foobarqux</span><span>|</span><a href="#36263859">root</a><span>|</span><a href="#36264174">parent</a><span>|</span><a href="#36264503">prev</a><span>|</span><a href="#36267848">next</a><span>|</span><label class="collapse" for="c-36264804">[-]</label><label class="expand" for="c-36264804">[10 more]</label></div><br/><div class="children"><div class="content">This is just false. Outside of the visual cortex there isn&#x27;t any evidence that brains work anything like GPT-4 or neural nets. Producing the same outputs isn&#x27;t evidence of anything.<p>At best you have just stated a hypothesis of how the brain might work, not any actual evidence supporting it.</div><br/><div id="36267934" class="c"><input type="checkbox" id="c-36267934" checked=""/><div class="controls bullet"><span class="by">sheepscreek</span><span>|</span><a href="#36263859">root</a><span>|</span><a href="#36264804">parent</a><span>|</span><a href="#36267222">next</a><span>|</span><label class="collapse" for="c-36267934">[-]</label><label class="expand" for="c-36267934">[2 more]</label></div><br/><div class="children"><div class="content">I think it is absurd to dismiss a comment for the lack of evidence on how the brain works. This is a thought experiment - nobody has any proof.<p>So you can’t say “this is just false” without providing counter evidence, which you don’t have.<p>The point here is to have a reference point, something to compare against, as we learn more about the brain.</div><br/><div id="36268286" class="c"><input type="checkbox" id="c-36268286" checked=""/><div class="controls bullet"><span class="by">jillesvangurp</span><span>|</span><a href="#36263859">root</a><span>|</span><a href="#36267934">parent</a><span>|</span><a href="#36267222">next</a><span>|</span><label class="collapse" for="c-36268286">[-]</label><label class="expand" for="c-36268286">[1 more]</label></div><br/><div class="children"><div class="content">The response is a bit knee-jerky and emotional. A lot of people find it upsetting that human thinking can be approximated by AI.<p>The demands for proof stem from this emotional response and a misunderstanding of how science actually works. There is plenty of evidence and theories. Most of that evidence is of an empirical nature. It suggests that GPT 4 can do some interesting things and that artificial neurons cluster and fire in ways similar to those in real brains. That&#x27;s a theory that is backed by some of this evidence. And it is of course not completely accidental because that sort of was the intention by those that constructed the neural networks. You could say that neural networks apparently work as intended.<p>The scientific way to dismiss that theory would be proving it wrong. That&#x27;s what science does: gather evidence and facts, come up with theories that explain those, and then try to find proof that counters those theories and explanations. And then you replace them with better ones. Falsifying theories is how science move forward. You don&#x27;t prove them right but you fail to prove them wrong. Insisting something is false without doing that is very unscientific.<p>Human brains are more than just neurons of course. The article actually calls that out. There&#x27;s a lot of chemistry in our brains that directly controls what it does. That&#x27;s why people enjoy taking certain drugs; those literally change the way our brain operates. Coffee is a drug that many people find useful. Learning especially is associated with endorphins. You get a little endorphin rush when you figure something out. Some people like this so much that they become scientists.<p>Artificial neural networks don&#x27;t really model any of that. But nobody is saying that they are the same; just that they do similar things in similar ways; as can be observed via experiments and the use of MRI scanners. We don&#x27;t really understand why that is but the similarity is easily observed and a valid theory is that an ANN captures enough of the complexity of a brain to be able to do interesting things. Which is of course backed up by plenty of empirical evidence in the form of people having used GOT-4. People seem to struggle to articulate what it is that is missing exactly that would prove that theory wrong. Lots of people that want that to be wrong, not a lot coming up with better theories. But of course some scientists are working on that and the prospect of them figuring this out is what truly scares some people.</div><br/></div></div></div></div><div id="36267222" class="c"><input type="checkbox" id="c-36267222" checked=""/><div class="controls bullet"><span class="by">rTX5CMRXIfFG</span><span>|</span><a href="#36263859">root</a><span>|</span><a href="#36264804">parent</a><span>|</span><a href="#36267934">prev</a><span>|</span><a href="#36265043">next</a><span>|</span><label class="collapse" for="c-36267222">[-]</label><label class="expand" for="c-36267222">[1 more]</label></div><br/><div class="children"><div class="content">Isn’t the hypothesis the point though? If you have existential evidence of a logical phenomenon occurring in a machine, then it’s worth directing your study to look for a similar (though not necessarily the exact) mechanism in organic matter, because logic is independent of the anatomy and physiology of the machine&#x2F;organism performing it? Besides, the propagation of 1’s and 0’s in computers has a very strong parallel with, say, how the brain transmits action potential to skeletal muscle, for example.</div><br/></div></div><div id="36265043" class="c"><input type="checkbox" id="c-36265043" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#36263859">root</a><span>|</span><a href="#36264804">parent</a><span>|</span><a href="#36267222">prev</a><span>|</span><a href="#36267387">next</a><span>|</span><label class="collapse" for="c-36265043">[-]</label><label class="expand" for="c-36265043">[4 more]</label></div><br/><div class="children"><div class="content">Sure - you can&#x27;t just look at how an ANN works and assume that&#x27;s how the brain does that too, but the ANN&#x27;s operation can act as inspiration to suggest or confirm the way the brain might be doing something.<p>It seems neuroscientists are good at discovering low level detail and perhaps not so good in general (visual cortex being somewhat of an exception) at putting the pieces together to suggest high level operations. ANNs seem complementary in that while their low level details are little like the brain, the connectionist architectures can be comparable, and we do know the top down operation (even though interpretation is an issue, more for some ANNs than others). If we assume that the cortex is doing some type of prediction error minimization then it&#x27;s likely to have found similar solutions to an ANN in cases where problem and connectivity are similar.</div><br/><div id="36265590" class="c"><input type="checkbox" id="c-36265590" checked=""/><div class="controls bullet"><span class="by">foobarqux</span><span>|</span><a href="#36263859">root</a><span>|</span><a href="#36265043">parent</a><span>|</span><a href="#36267387">next</a><span>|</span><label class="collapse" for="c-36265590">[-]</label><label class="expand" for="c-36265590">[3 more]</label></div><br/><div class="children"><div class="content">Again this is just a hypothesis and nothing has come from it despite the fact that it is very old. There isn&#x27;t any good reason for it to be true and no one has done anything to show that it is.</div><br/><div id="36266474" class="c"><input type="checkbox" id="c-36266474" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#36263859">root</a><span>|</span><a href="#36265590">parent</a><span>|</span><a href="#36267387">next</a><span>|</span><label class="collapse" for="c-36266474">[-]</label><label class="expand" for="c-36266474">[2 more]</label></div><br/><div class="children"><div class="content">Not sure what you&#x27;re referring to - what&#x27;s a hypothesis&#x2F;old ?</div><br/><div id="36266820" class="c"><input type="checkbox" id="c-36266820" checked=""/><div class="controls bullet"><span class="by">foobarqux</span><span>|</span><a href="#36263859">root</a><span>|</span><a href="#36266474">parent</a><span>|</span><a href="#36267387">next</a><span>|</span><label class="collapse" for="c-36266820">[-]</label><label class="expand" for="c-36266820">[1 more]</label></div><br/><div class="children"><div class="content">Pretty much any claim that the brain (outside the visual cortex) work anything like the way neural nets do.</div><br/></div></div></div></div></div></div></div></div><div id="36267387" class="c"><input type="checkbox" id="c-36267387" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#36263859">root</a><span>|</span><a href="#36264804">parent</a><span>|</span><a href="#36265043">prev</a><span>|</span><a href="#36267402">next</a><span>|</span><label class="collapse" for="c-36267387">[-]</label><label class="expand" for="c-36267387">[1 more]</label></div><br/><div class="children"><div class="content">Human speech processes seem similar to how LLMs work. There are brain injuries (Korsakoff&#x27;s syndrome) that cause confabulation, which is like ChatGPT hallucinating the answer to questions.</div><br/></div></div><div id="36267402" class="c"><input type="checkbox" id="c-36267402" checked=""/><div class="controls bullet"><span class="by">breck</span><span>|</span><a href="#36263859">root</a><span>|</span><a href="#36264804">parent</a><span>|</span><a href="#36267387">prev</a><span>|</span><a href="#36267848">next</a><span>|</span><label class="collapse" for="c-36267402">[-]</label><label class="expand" for="c-36267402">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Outside of the visual cortex there isn&#x27;t any evidence that brains work anything like GPT-4 or neural nets.<p>Are you sure? If you had to pick a number what would you pick?<p>I have never built a useful neural network myself so I can&#x27;t speak confidently, but I&#x27;ve read enough McCullough &amp; Pitts, Cybernetics, Minsky, et cetera, to have the knowledge that there&#x27;s a direct connection between a multitude of lab experiments done in the early 1900&#x27;s to quantify the behavior of neurons in various animals, and then develop the principle ideas of ANNs from those findings.<p>Of course, there are hundreds, if not thousands or more of &quot;components&quot; in the brain, and I&#x27;m not sure how many of those components we have digital analogs for. But I feel like GPT-4 makes me think we&#x27;ve crossed the 10% threshold.</div><br/></div></div></div></div></div></div><div id="36267848" class="c"><input type="checkbox" id="c-36267848" checked=""/><div class="controls bullet"><span class="by">raincom</span><span>|</span><a href="#36263859">parent</a><span>|</span><a href="#36264174">prev</a><span>|</span><a href="#36264562">next</a><span>|</span><label class="collapse" for="c-36267848">[-]</label><label class="expand" for="c-36267848">[1 more]</label></div><br/><div class="children"><div class="content">It is the new age phrenology!!</div><br/></div></div><div id="36264562" class="c"><input type="checkbox" id="c-36264562" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#36263859">parent</a><span>|</span><a href="#36267848">prev</a><span>|</span><a href="#36264079">next</a><span>|</span><label class="collapse" for="c-36264562">[-]</label><label class="expand" for="c-36264562">[6 more]</label></div><br/><div class="children"><div class="content">&gt; We learn precisely nothing from this<p>That&#x27;s literally never true about anything.</div><br/><div id="36264742" class="c"><input type="checkbox" id="c-36264742" checked=""/><div class="controls bullet"><span class="by">cosmojg</span><span>|</span><a href="#36263859">root</a><span>|</span><a href="#36264562">parent</a><span>|</span><a href="#36266442">next</a><span>|</span><label class="collapse" for="c-36264742">[-]</label><label class="expand" for="c-36264742">[3 more]</label></div><br/><div class="children"><div class="content">&gt;&gt; We learn precisely nothing from this<p>&gt; That&#x27;s literally never true about anything.<p>But if this were true, data compression would be impossible.</div><br/><div id="36265036" class="c"><input type="checkbox" id="c-36265036" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#36263859">root</a><span>|</span><a href="#36264742">parent</a><span>|</span><a href="#36266442">next</a><span>|</span><label class="collapse" for="c-36265036">[-]</label><label class="expand" for="c-36265036">[2 more]</label></div><br/><div class="children"><div class="content">OK you got me there I guess, but even learning that we&#x27;ve learned nothing is learning in itself. Otherwise the act of compressing wouldn&#x27;t give any more information.</div><br/><div id="36266452" class="c"><input type="checkbox" id="c-36266452" checked=""/><div class="controls bullet"><span class="by">User23</span><span>|</span><a href="#36263859">root</a><span>|</span><a href="#36265036">parent</a><span>|</span><a href="#36266442">next</a><span>|</span><label class="collapse" for="c-36266452">[-]</label><label class="expand" for="c-36266452">[1 more]</label></div><br/><div class="children"><div class="content">Read up on Kolmogorov complexity.</div><br/></div></div></div></div></div></div><div id="36266442" class="c"><input type="checkbox" id="c-36266442" checked=""/><div class="controls bullet"><span class="by">User23</span><span>|</span><a href="#36263859">root</a><span>|</span><a href="#36264562">parent</a><span>|</span><a href="#36264742">prev</a><span>|</span><a href="#36264749">next</a><span>|</span><label class="collapse" for="c-36266442">[-]</label><label class="expand" for="c-36266442">[1 more]</label></div><br/><div class="children"><div class="content">That’s true, some things actually cause us to learn falsehoods and spend indefinite amounts of time down the mistaken rabbit hole.</div><br/></div></div><div id="36264749" class="c"><input type="checkbox" id="c-36264749" checked=""/><div class="controls bullet"><span class="by">foobarqux</span><span>|</span><a href="#36263859">root</a><span>|</span><a href="#36264562">parent</a><span>|</span><a href="#36266442">prev</a><span>|</span><a href="#36264079">next</a><span>|</span><label class="collapse" for="c-36264749">[-]</label><label class="expand" for="c-36264749">[1 more]</label></div><br/><div class="children"><div class="content">What have we learned specifically in this case?</div><br/></div></div></div></div></div></div><div id="36264079" class="c"><input type="checkbox" id="c-36264079" checked=""/><div class="controls bullet"><span class="by">viableSprocket1</span><span>|</span><a href="#36263859">prev</a><span>|</span><a href="#36265974">next</a><span>|</span><label class="collapse" for="c-36264079">[-]</label><label class="expand" for="c-36264079">[1 more]</label></div><br/><div class="children"><div class="content">Oversimplifying for brevity (and there is definitely more nuance to this). This is basically the modeling approach:<p>1. Have a biological brain do a task, record neuronal data + task performance<p>2. Copy some of those biological features and implement in an ANN<p>3. Tune the many free parameters in the ANN on task performance<p>4. Show that the bio-inspired ANN performs better than SOTA and&#x2F;or shows &quot;signatures&quot; that are more brain-like.<p>The major criticisms of Yamins&#x27; (and similar) groups are either that correlation != causation, or correlation != understanding, or that it is tautological (bio-inspired ANNs will be more biological). I&#x27;m not sure how seriously this work is taken vs. true first principles theory.</div><br/></div></div><div id="36265974" class="c"><input type="checkbox" id="c-36265974" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#36264079">prev</a><span>|</span><a href="#36263567">next</a><span>|</span><label class="collapse" for="c-36265974">[-]</label><label class="expand" for="c-36265974">[1 more]</label></div><br/><div class="children"><div class="content">Yes indeed. Attempts to simulate human neurons have shown that a single neuron can be simulated with a realtively large ANN consisting of several layers. This tells us that human neurons are more computationally complex and capable compared to other animals and orders of magnitude more complex than neurons in ANNs.</div><br/></div></div><div id="36263567" class="c"><input type="checkbox" id="c-36263567" checked=""/><div class="controls bullet"><span class="by">eep_social</span><span>|</span><a href="#36265974">prev</a><span>|</span><a href="#36268030">next</a><span>|</span><label class="collapse" for="c-36263567">[-]</label><label class="expand" for="c-36263567">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;archive.is&#x2F;ryTWs" rel="nofollow">https:&#x2F;&#x2F;archive.is&#x2F;ryTWs</a></div><br/></div></div><div id="36268030" class="c"><input type="checkbox" id="c-36268030" checked=""/><div class="controls bullet"><span class="by">iso8859-1</span><span>|</span><a href="#36263567">prev</a><span>|</span><a href="#36266742">next</a><span>|</span><label class="collapse" for="c-36268030">[-]</label><label class="expand" for="c-36268030">[1 more]</label></div><br/><div class="children"><div class="content">Can someone explain to me why &quot;help understand&quot; is grammatically correct?<p>Why wouldn&#x27;t help be following by a gerund like on <a href="https:&#x2F;&#x2F;www.ef.com&#x2F;wwen&#x2F;english-resources&#x2F;english-grammar&#x2F;verbs-followed-gerunds&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.ef.com&#x2F;wwen&#x2F;english-resources&#x2F;english-grammar&#x2F;ve...</a> ?</div><br/></div></div><div id="36266742" class="c"><input type="checkbox" id="c-36266742" checked=""/><div class="controls bullet"><span class="by">pfdietz</span><span>|</span><a href="#36268030">prev</a><span>|</span><a href="#36264448">next</a><span>|</span><label class="collapse" for="c-36266742">[-]</label><label class="expand" for="c-36266742">[1 more]</label></div><br/><div class="children"><div class="content">This sounds like a conclusion that can be reached only when we understand real brains.</div><br/></div></div><div id="36264448" class="c"><input type="checkbox" id="c-36264448" checked=""/><div class="controls bullet"><span class="by">phkahler</span><span>|</span><a href="#36266742">prev</a><span>|</span><a href="#36266305">next</a><span>|</span><label class="collapse" for="c-36264448">[-]</label><label class="expand" for="c-36264448">[1 more]</label></div><br/><div class="children"><div class="content">For me the most interesting parallel is from (I think) GANs, and other generative AIs. This is similar to the idea in psychology that we are really doing a lot of projection with some correction based on sensory input - as opposed to actually perceiving everything around us.<p>Also, real synapses are one of the most abundant features of real brains and are the direct inspiration for NN weights. I&#x27;m not sure the artificial brains help understand real ones, but they do seem to validate some ideas we have about real ones.</div><br/></div></div><div id="36266305" class="c"><input type="checkbox" id="c-36266305" checked=""/><div class="controls bullet"><span class="by">worik</span><span>|</span><a href="#36264448">prev</a><span>|</span><a href="#36264817">next</a><span>|</span><label class="collapse" for="c-36266305">[-]</label><label class="expand" for="c-36266305">[1 more]</label></div><br/><div class="children"><div class="content">This article reminded me of looking at clouds, and seeing shapes.  Or constellations...</div><br/></div></div><div id="36264817" class="c"><input type="checkbox" id="c-36264817" checked=""/><div class="controls bullet"><span class="by">cosmojg</span><span>|</span><a href="#36266305">prev</a><span>|</span><a href="#36264490">next</a><span>|</span><label class="collapse" for="c-36264817">[-]</label><label class="expand" for="c-36264817">[2 more]</label></div><br/><div class="children"><div class="content">As a computational neuroscientist, I find myself both terribly disappointed and unfortunately reminded of Gell-Mann amnesia.</div><br/><div id="36267011" class="c"><input type="checkbox" id="c-36267011" checked=""/><div class="controls bullet"><span class="by">ggm</span><span>|</span><a href="#36264817">parent</a><span>|</span><a href="#36264490">next</a><span>|</span><label class="collapse" for="c-36267011">[-]</label><label class="expand" for="c-36267011">[1 more]</label></div><br/><div class="children"><div class="content">This ++<p>Somebody should have shown the journalist the paper on brain function in a dead fish under fMRI</div><br/></div></div></div></div><div id="36264490" class="c"><input type="checkbox" id="c-36264490" checked=""/><div class="controls bullet"><span class="by">kuprel</span><span>|</span><a href="#36264817">prev</a><span>|</span><a href="#36263582">next</a><span>|</span><label class="collapse" for="c-36264490">[-]</label><label class="expand" for="c-36264490">[1 more]</label></div><br/><div class="children"><div class="content">The left hand in that photo should be a humanoid robot hand. Didn’t actually read the article</div><br/></div></div></div></div></div></div></div></body></html>