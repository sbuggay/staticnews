<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1692262859981" as="style"/><link rel="stylesheet" href="styles.css?v=1692262859981"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://bellard.org/ts_server/ts_zip.html">Ts_zip: Text Compression Using Large Language Models</a> <span class="domain">(<a href="https://bellard.org">bellard.org</a>)</span></div><div class="subtext"><span>Deeg9rie9usi</span> | <span>131 comments</span></div><br/><div><div id="37153601" class="c"><input type="checkbox" id="c-37153601" checked=""/><div class="controls bullet"><span class="by">tysam_and</span><span>|</span><a href="#37153091">next</a><span>|</span><label class="collapse" for="c-37153601">[-]</label><label class="expand" for="c-37153601">[10 more]</label></div><br/><div class="children"><div class="content">The reason this works is because LLMs are trained to minimize the empirical risk over a large set of data (<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Empirical_risk_minimization" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Empirical_risk_minimization</a>), and it is where the log-likelihood-based measure comes from, oftentimes represented simply as the perplexity.<p>This is precisely because LLMs learn a representation of the nth-order Markov Chain, which by its nature is extraordinarily sparse. This allows for excellent compression, and I would assume (and have for a while, I suppose) the nearly-linear nature of LLMs allows for excellent interpolation between these very sparse states.<p>This lets us &#x27;break&#x27; the hard-combinatorial problem into one that can be softly, albeit with the fuzzy error that comes with any estimator.<p>Since computers cannot tractably compress markov chains past a certain point, this allows LLMs some opportunity to outpace traditional methods for compression, at least in theory.<p>Also, this method technically cheats since you need the language model to decompress, which counts towards your dictionary size. In a truly unbounded case, I would be interested to see which method wins. I&#x27;m assuming it&#x27;s the LLM.<p>Apologies for any factual inaccuracies or such, I am still learning many of these things. Many thanks and much love! &lt;3 :))))</div><br/><div id="37157346" class="c"><input type="checkbox" id="c-37157346" checked=""/><div class="controls bullet"><span class="by">fnordpiglet</span><span>|</span><a href="#37153601">parent</a><span>|</span><a href="#37154107">next</a><span>|</span><label class="collapse" for="c-37157346">[-]</label><label class="expand" for="c-37157346">[2 more]</label></div><br/><div class="children"><div class="content">I would note you can externalize your dictionary in a lot of schemes. For tight protocol level encoding in my custom protocols I often build the dictionary as part of compilation and deploy it compiled into both sides. It saves a lot of overhead on small message protocols. In this case the dictionary is probably absurdly large, but I think we are at a point where new compression techniques are going to have weird and unexpected trade offs.</div><br/><div id="37157365" class="c"><input type="checkbox" id="c-37157365" checked=""/><div class="controls bullet"><span class="by">tysam_and</span><span>|</span><a href="#37153601">root</a><span>|</span><a href="#37157346">parent</a><span>|</span><a href="#37154107">next</a><span>|</span><label class="collapse" for="c-37157365">[-]</label><label class="expand" for="c-37157365">[1 more]</label></div><br/><div class="children"><div class="content">Yes, in this case I&#x27;m counting the mathematical definition for a decoder that&#x27;s required when compressing large corpii of text.</div><br/></div></div></div></div><div id="37154107" class="c"><input type="checkbox" id="c-37154107" checked=""/><div class="controls bullet"><span class="by">blovescoffee</span><span>|</span><a href="#37153601">parent</a><span>|</span><a href="#37157346">prev</a><span>|</span><a href="#37153091">next</a><span>|</span><label class="collapse" for="c-37154107">[-]</label><label class="expand" for="c-37154107">[7 more]</label></div><br/><div class="children"><div class="content">If you want to learn something cool and related, checkout Autoencoders (or VAEs). They effectively compress information by forming representations of some data.</div><br/><div id="37154140" class="c"><input type="checkbox" id="c-37154140" checked=""/><div class="controls bullet"><span class="by">tysam_and</span><span>|</span><a href="#37153601">root</a><span>|</span><a href="#37154107">parent</a><span>|</span><a href="#37153091">next</a><span>|</span><label class="collapse" for="c-37154140">[-]</label><label class="expand" for="c-37154140">[6 more]</label></div><br/><div class="children"><div class="content">Indeed. They are extremely cool! &lt;3 :) I think in a way, every neural network works on compression! Though AEs &amp; VAEs are a bit more blatant in how they do it. It&#x27;s just always turned around a little, here and there. I have a pet theory that no neural network can work without compression (even generative ones that have constantly increasing layer depths! :D :)))) )</div><br/><div id="37155472" class="c"><input type="checkbox" id="c-37155472" checked=""/><div class="controls bullet"><span class="by">nerdponx</span><span>|</span><a href="#37153601">root</a><span>|</span><a href="#37154140">parent</a><span>|</span><a href="#37155485">next</a><span>|</span><label class="collapse" for="c-37155472">[-]</label><label class="expand" for="c-37155472">[2 more]</label></div><br/><div class="children"><div class="content">In a very loose sense, even plain linear regression is a form of lossy compression in the form of an orthogonal projection.</div><br/><div id="37155832" class="c"><input type="checkbox" id="c-37155832" checked=""/><div class="controls bullet"><span class="by">tysam_and</span><span>|</span><a href="#37153601">root</a><span>|</span><a href="#37155472">parent</a><span>|</span><a href="#37155485">next</a><span>|</span><label class="collapse" for="c-37155832">[-]</label><label class="expand" for="c-37155832">[1 more]</label></div><br/><div class="children"><div class="content">Oh, I appreciate this point, thanks for making it! I like it a lot. &lt;3 :))))<p>I think, perhaps, if inputs &gt; outputs and there is some dimensionality reduction (though I think orthogonality would be a trait of an ideal system [i.e. an emergent property approached in the limit], not one that is explicitly enforced each step).</div><br/></div></div></div></div><div id="37155485" class="c"><input type="checkbox" id="c-37155485" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#37153601">root</a><span>|</span><a href="#37154140">parent</a><span>|</span><a href="#37155472">prev</a><span>|</span><a href="#37153091">next</a><span>|</span><label class="collapse" for="c-37155485">[-]</label><label class="expand" for="c-37155485">[3 more]</label></div><br/><div class="children"><div class="content">The link between compression and intelligence is a popular theory. Indeed the linked work here came out of the Hutter Prize:<p>&gt; The goal of the Hutter Prize is to encourage research in artificial intelligence (AI). The organizers believe that text compression and AI are equivalent problems. Hutter proved that the optimal behavior of a goal-seeking agent in an unknown but computable environment is to guess at each step that the environment is probably controlled by one of the shortest programs consistent with all interaction so far.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Hutter_Prize" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Hutter_Prize</a></div><br/><div id="37155821" class="c"><input type="checkbox" id="c-37155821" checked=""/><div class="controls bullet"><span class="by">tysam_and</span><span>|</span><a href="#37153601">root</a><span>|</span><a href="#37155485">parent</a><span>|</span><a href="#37156308">next</a><span>|</span><label class="collapse" for="c-37155821">[-]</label><label class="expand" for="c-37155821">[1 more]</label></div><br/><div class="children"><div class="content">I agree that it is most certainly necessary to have compression to have intelligence, after all, it is the bridge from empirical examples to a learned policy. Oftentimes the subtle switch comes when people say that compression IS intelligence.<p>Similar to how metabolism in many ways is required for life, yet metabolism itself isn&#x27;t life.<p>One of the challenges is that the minimum MDL (Minimum Descriptor Length -- <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Minimum_description_length" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Minimum_description_length</a>) is intractable to prove directly, we can only prove that we are a bit closer to it than we were before. This of course becomes even more difficult in the temporal regime, as the amount of information to prove that an iterative mapping (i.e. a decision-making algorithm or what have you, in this case) over each time slice in a temporal system is nigh-impossible. We can tell _something_ about it because of the attractors generated by such a system, but even then, I think it&#x27;s something basically impossible to do in a closed-form manner.<p>That being said, I do believe compression is required for intelligence, and that deliciously drags in all of the info theory stuff, which is very fun indeed. Just seems like it gets really messy with that time component, but that&#x27;s just my 2 cents at least. :&#x27;(<p>I didn&#x27;t know that was related to the Hutter Prize. Very cool. I&#x27;d read a little bit about that prize before, and I&#x27;ll take another look at it now. Probably won&#x27;t start any work on anything because I really, really, really do not want to Collatz Conjecture myself again.</div><br/></div></div><div id="37156308" class="c"><input type="checkbox" id="c-37156308" checked=""/><div class="controls bullet"><span class="by">angch</span><span>|</span><a href="#37153601">root</a><span>|</span><a href="#37155485">parent</a><span>|</span><a href="#37155821">prev</a><span>|</span><a href="#37153091">next</a><span>|</span><label class="collapse" for="c-37156308">[-]</label><label class="expand" for="c-37156308">[1 more]</label></div><br/><div class="children"><div class="content">NNCP (Bellard&#x27;s prelude to ts_zip, using similar techniques) is not qualified for Hutter Prize, btw, because hardware and speed limitations specified by Hutter Prize.<p>&quot;Must run in ≲50 hours using a single CPU core and &lt;10GB RAM and &lt;100GB HDD on our test machine.&quot;
Which is an Intel Core i7-620M</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37153091" class="c"><input type="checkbox" id="c-37153091" checked=""/><div class="controls bullet"><span class="by">jandrese</span><span>|</span><a href="#37153601">prev</a><span>|</span><a href="#37156743">next</a><span>|</span><label class="collapse" for="c-37153091">[-]</label><label class="expand" for="c-37153091">[24 more]</label></div><br/><div class="children"><div class="content">I guess they&#x27;ve worked out to always get exactly the same text back?  Hopefully this won&#x27;t be a repeat of the photocopiers that used a similar compression technique and occasionally substituted letters in the copy.<p><a href="https:&#x2F;&#x2F;www.bbc.com&#x2F;news&#x2F;technology-23588202" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.bbc.com&#x2F;news&#x2F;technology-23588202</a></div><br/><div id="37153300" class="c"><input type="checkbox" id="c-37153300" checked=""/><div class="controls bullet"><span class="by">cbhl</span><span>|</span><a href="#37153091">parent</a><span>|</span><a href="#37153490">next</a><span>|</span><label class="collapse" for="c-37153300">[-]</label><label class="expand" for="c-37153300">[15 more]</label></div><br/><div class="children"><div class="content">The author, Fabrice Bellard, is well-known for his work on QEMU (among other things), but I&#x27;d treat the rest of the posts as &quot;idea sharing&quot; at an early stage. For example, while BPG (Better Portable Graphics) didn&#x27;t take off, the core idea (image formats based on video codecs) can be found in WebP (VP8, VP9) and AVIF (AV1) and HEIF (HEVC&#x2F;H.265).<p>For this, I&#x27;d look at comparable work in the audio codec space, including Lyra (<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2102.09660" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2102.09660</a>) and SoundStream (<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2107.03312" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2107.03312</a>).<p>For text compression, I think it would be novel if you could get a lossy but semantically equivalent decompression that was resilient to the exact hardware used for inference. I don&#x27;t think that&#x27;s what happened here, though, given the requirement for &quot;exact same GPU and model&quot;.</div><br/><div id="37153384" class="c"><input type="checkbox" id="c-37153384" checked=""/><div class="controls bullet"><span class="by">wood_spirit</span><span>|</span><a href="#37153091">root</a><span>|</span><a href="#37153300">parent</a><span>|</span><a href="#37155647">next</a><span>|</span><label class="collapse" for="c-37153384">[-]</label><label class="expand" for="c-37153384">[4 more]</label></div><br/><div class="children"><div class="content">Fabrice is also well known for creating Ffmpeg and discovering an algorithm for computing digits of PI <a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Fabrice_Bellard" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Fabrice_Bellard</a><p>He has also held the ‘world record’ for data compression since 2019 with nncp <a href="http:&#x2F;&#x2F;mattmahoney.net&#x2F;dc&#x2F;text.html#1085" rel="nofollow noreferrer">http:&#x2F;&#x2F;mattmahoney.net&#x2F;dc&#x2F;text.html#1085</a></div><br/><div id="37155764" class="c"><input type="checkbox" id="c-37155764" checked=""/><div class="controls bullet"><span class="by">foooorsyth</span><span>|</span><a href="#37153091">root</a><span>|</span><a href="#37153384">parent</a><span>|</span><a href="#37155806">next</a><span>|</span><label class="collapse" for="c-37155764">[-]</label><label class="expand" for="c-37155764">[2 more]</label></div><br/><div class="children"><div class="content">He is simply one of the most important programmers alive today. He should win a Turing award in his lifetime.</div><br/><div id="37157515" class="c"><input type="checkbox" id="c-37157515" checked=""/><div class="controls bullet"><span class="by">stevefan1999</span><span>|</span><a href="#37153091">root</a><span>|</span><a href="#37155764">parent</a><span>|</span><a href="#37155806">next</a><span>|</span><label class="collapse" for="c-37157515">[-]</label><label class="expand" for="c-37157515">[1 more]</label></div><br/><div class="children"><div class="content">I am afraid not. Turing awards are all theorists game, not engineers. He could get in the IEEE hall of fame though, I recommends.</div><br/></div></div></div></div><div id="37155806" class="c"><input type="checkbox" id="c-37155806" checked=""/><div class="controls bullet"><span class="by">rcme</span><span>|</span><a href="#37153091">root</a><span>|</span><a href="#37153384">parent</a><span>|</span><a href="#37155764">prev</a><span>|</span><a href="#37155647">next</a><span>|</span><label class="collapse" for="c-37155806">[-]</label><label class="expand" for="c-37155806">[1 more]</label></div><br/><div class="children"><div class="content">Don’t forget about QuickJS!</div><br/></div></div></div></div><div id="37155647" class="c"><input type="checkbox" id="c-37155647" checked=""/><div class="controls bullet"><span class="by">IggleSniggle</span><span>|</span><a href="#37153091">root</a><span>|</span><a href="#37153300">parent</a><span>|</span><a href="#37153384">prev</a><span>|</span><a href="#37156386">next</a><span>|</span><label class="collapse" for="c-37155647">[-]</label><label class="expand" for="c-37155647">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Semantically equivalent decompression&quot; is a very interesting idea, but I take issue with the idea that different words can share semantic equivalence. But I&#x27;ve also been reading a lot of poetry today, so subtle word choice feels important, even though I can recognize it may not be that important...a LLM could easily produce &quot;better&quot; or &quot;worse&quot; text than a human (while being lossy in either case), but then I have questions about the reason for using words in the first place.</div><br/></div></div><div id="37156386" class="c"><input type="checkbox" id="c-37156386" checked=""/><div class="controls bullet"><span class="by">crazygringo</span><span>|</span><a href="#37153091">root</a><span>|</span><a href="#37153300">parent</a><span>|</span><a href="#37155647">prev</a><span>|</span><a href="#37153490">next</a><span>|</span><label class="collapse" for="c-37156386">[-]</label><label class="expand" for="c-37156386">[9 more]</label></div><br/><div class="children"><div class="content">Wow. Lossy text compression is a... horrifying concept.</div><br/><div id="37158816" class="c"><input type="checkbox" id="c-37158816" checked=""/><div class="controls bullet"><span class="by">pluijzer</span><span>|</span><a href="#37153091">root</a><span>|</span><a href="#37156386">parent</a><span>|</span><a href="#37158496">next</a><span>|</span><label class="collapse" for="c-37158816">[-]</label><label class="expand" for="c-37158816">[1 more]</label></div><br/><div class="children"><div class="content">Yes, although, it brings some Philip K Dick-ian possibilities to computers, &#x27;ghosts&#x27; slipping into books and that sort of thing. Imagine, sending a highly compressed copy of Wikipedia to a mars colony. The signal got distorted during transit and upon decompressing it some weird entity slipped in.</div><br/></div></div><div id="37158496" class="c"><input type="checkbox" id="c-37158496" checked=""/><div class="controls bullet"><span class="by">bambax</span><span>|</span><a href="#37153091">root</a><span>|</span><a href="#37156386">parent</a><span>|</span><a href="#37158816">prev</a><span>|</span><a href="#37157894">next</a><span>|</span><label class="collapse" for="c-37158496">[-]</label><label class="expand" for="c-37158496">[1 more]</label></div><br/><div class="children"><div class="content">But why?<p>A summary is a form of lossy text compression, and it&#x27;s extremely useful.<p>That&#x27;s not what this is but if it can actually produce &quot;semantically equivalent&quot; decompressed text it&#x27;s fascinating.</div><br/></div></div><div id="37157894" class="c"><input type="checkbox" id="c-37157894" checked=""/><div class="controls bullet"><span class="by">fsiefken</span><span>|</span><a href="#37153091">root</a><span>|</span><a href="#37156386">parent</a><span>|</span><a href="#37158496">prev</a><span>|</span><a href="#37156900">next</a><span>|</span><label class="collapse" for="c-37157894">[-]</label><label class="expand" for="c-37157894">[1 more]</label></div><br/><div class="children"><div class="content">Check keyscript
<a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;FastWriting&#x2F;comments&#x2F;w7a4k1&#x2F;keyscript_shorthand&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;FastWriting&#x2F;comments&#x2F;w7a4k1&#x2F;keyscri...</a></div><br/></div></div><div id="37156900" class="c"><input type="checkbox" id="c-37156900" checked=""/><div class="controls bullet"><span class="by">Solvency</span><span>|</span><a href="#37153091">root</a><span>|</span><a href="#37156386">parent</a><span>|</span><a href="#37157894">prev</a><span>|</span><a href="#37157608">next</a><span>|</span><label class="collapse" for="c-37156900">[-]</label><label class="expand" for="c-37156900">[3 more]</label></div><br/><div class="children"><div class="content">Ww. Lssy txt cmprsn is hrffyng cncpt.<p>Or is it actly ok.</div><br/><div id="37156932" class="c"><input type="checkbox" id="c-37156932" checked=""/><div class="controls bullet"><span class="by">crazygringo</span><span>|</span><a href="#37153091">root</a><span>|</span><a href="#37156900">parent</a><span>|</span><a href="#37157761">next</a><span>|</span><label class="collapse" for="c-37156932">[-]</label><label class="expand" for="c-37156932">[1 more]</label></div><br/><div class="children"><div class="content">Jeez. Text squeezing in the manner of loss <i>is</i> a scary idea.</div><br/></div></div><div id="37157761" class="c"><input type="checkbox" id="c-37157761" checked=""/><div class="controls bullet"><span class="by">midoridensha</span><span>|</span><a href="#37153091">root</a><span>|</span><a href="#37156900">parent</a><span>|</span><a href="#37156932">prev</a><span>|</span><a href="#37157608">next</a><span>|</span><label class="collapse" for="c-37157761">[-]</label><label class="expand" for="c-37157761">[1 more]</label></div><br/><div class="children"><div class="content">Lossy text comparison is a horrifying concept?</div><br/></div></div></div></div><div id="37157608" class="c"><input type="checkbox" id="c-37157608" checked=""/><div class="controls bullet"><span class="by">supertrope</span><span>|</span><a href="#37153091">root</a><span>|</span><a href="#37156386">parent</a><span>|</span><a href="#37156900">prev</a><span>|</span><a href="#37156642">next</a><span>|</span><label class="collapse" for="c-37157608">[-]</label><label class="expand" for="c-37157608">[1 more]</label></div><br/><div class="children"><div class="content">Y ur account b lanc is $36 2.17</div><br/></div></div><div id="37156642" class="c"><input type="checkbox" id="c-37156642" checked=""/><div class="controls bullet"><span class="by">throwaway14356</span><span>|</span><a href="#37153091">root</a><span>|</span><a href="#37156386">parent</a><span>|</span><a href="#37157608">prev</a><span>|</span><a href="#37153490">next</a><span>|</span><label class="collapse" for="c-37156642">[-]</label><label class="expand" for="c-37156642">[1 more]</label></div><br/><div class="children"><div class="content">lossy text horror</div><br/></div></div></div></div></div></div><div id="37153490" class="c"><input type="checkbox" id="c-37153490" checked=""/><div class="controls bullet"><span class="by">ks2048</span><span>|</span><a href="#37153091">parent</a><span>|</span><a href="#37153300">prev</a><span>|</span><a href="#37154916">next</a><span>|</span><label class="collapse" for="c-37153490">[-]</label><label class="expand" for="c-37153490">[2 more]</label></div><br/><div class="children"><div class="content">If you can predict text very well, it can still be a good <i>lossless</i> compressor - you just have to encode the <i>errors</i> you make. Fewer errors, less data to store</div><br/><div id="37156742" class="c"><input type="checkbox" id="c-37156742" checked=""/><div class="controls bullet"><span class="by">mistercow</span><span>|</span><a href="#37153091">root</a><span>|</span><a href="#37153490">parent</a><span>|</span><a href="#37154916">next</a><span>|</span><label class="collapse" for="c-37156742">[-]</label><label class="expand" for="c-37156742">[1 more]</label></div><br/><div class="children"><div class="content">More specifically, you can use entropy coding to spend fewer bits the more likely you rated the token and more bits the less likely you rated the token. So &quot;errors&quot; aren&#x27;t a black and white thing. And since LLMs output log probabilities, they&#x27;re basically perfect for this.</div><br/></div></div></div></div><div id="37154916" class="c"><input type="checkbox" id="c-37154916" checked=""/><div class="controls bullet"><span class="by">OGWhales</span><span>|</span><a href="#37153091">parent</a><span>|</span><a href="#37153490">prev</a><span>|</span><a href="#37155291">next</a><span>|</span><label class="collapse" for="c-37154916">[-]</label><label class="expand" for="c-37154916">[3 more]</label></div><br/><div class="children"><div class="content">JBIG2 is what the NSO group exploited as part of their zero-click iMessage exploit. Google zero has a good write up if anyone hasn’t heard about it yet—it’s super cool:<p><a href="https:&#x2F;&#x2F;googleprojectzero.blogspot.com&#x2F;2021&#x2F;12&#x2F;a-deep-dive-into-nso-zero-click.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;googleprojectzero.blogspot.com&#x2F;2021&#x2F;12&#x2F;a-deep-dive-i...</a></div><br/><div id="37157793" class="c"><input type="checkbox" id="c-37157793" checked=""/><div class="controls bullet"><span class="by">Uptrenda</span><span>|</span><a href="#37153091">root</a><span>|</span><a href="#37154916">parent</a><span>|</span><a href="#37155242">next</a><span>|</span><label class="collapse" for="c-37157793">[-]</label><label class="expand" for="c-37157793">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Practical circuits<p>&gt;JBIG2 doesn&#x27;t have scripting capabilities, but when combined with a vulnerability, it does have the ability to emulate circuits of arbitrary logic gates operating on arbitrary memory. So why not just use that to build your own computer architecture and script that!? That&#x27;s exactly what this exploit does. Using over 70,000 segment commands defining logical bit operations, they define a small computer architecture with features such as registers and a full 64-bit adder and comparator which they use to search memory and perform arithmetic operations. It&#x27;s not as fast as Javascript, but it&#x27;s fundamentally computationally equivalent.<p>&gt;The bootstrapping operations for the sandbox escape exploit are written to run on this logic circuit and the whole thing runs in this weird, emulated environment created out of a single decompression pass through a JBIG2 stream. It&#x27;s pretty incredible, and at the same time, pretty terrifying.<p>Wow... that&#x27;s really out of the box. They built their own VM from a JBIG compression stream vuln and wrote an exploit to run on it... Not bad.</div><br/></div></div><div id="37155242" class="c"><input type="checkbox" id="c-37155242" checked=""/><div class="controls bullet"><span class="by">nocoiner</span><span>|</span><a href="#37153091">root</a><span>|</span><a href="#37154916">parent</a><span>|</span><a href="#37157793">prev</a><span>|</span><a href="#37155291">next</a><span>|</span><label class="collapse" for="c-37155242">[-]</label><label class="expand" for="c-37155242">[1 more]</label></div><br/><div class="children"><div class="content">This is so very much worth a read. It is almost literally mind blowing once you realize how deep the exploit goes.</div><br/></div></div></div></div><div id="37155291" class="c"><input type="checkbox" id="c-37155291" checked=""/><div class="controls bullet"><span class="by">CobrastanJorji</span><span>|</span><a href="#37153091">parent</a><span>|</span><a href="#37154916">prev</a><span>|</span><a href="#37153248">next</a><span>|</span><label class="collapse" for="c-37155291">[-]</label><label class="expand" for="c-37155291">[1 more]</label></div><br/><div class="children"><div class="content">Even if you haven&#x27;t, passing along a checksum of the correct original would still allow it to be used safely. Decompression would either be correct or fail to decompress (outside of considerations of intentional hash collision attacks).</div><br/></div></div><div id="37153248" class="c"><input type="checkbox" id="c-37153248" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#37153091">parent</a><span>|</span><a href="#37155291">prev</a><span>|</span><a href="#37156743">next</a><span>|</span><label class="collapse" for="c-37153248">[-]</label><label class="expand" for="c-37153248">[2 more]</label></div><br/><div class="children"><div class="content">Yeah, using a probabilistic predictor to do lossless compression has been known for a few decades.</div><br/><div id="37155381" class="c"><input type="checkbox" id="c-37155381" checked=""/><div class="controls bullet"><span class="by">stevefan1999</span><span>|</span><a href="#37153091">root</a><span>|</span><a href="#37153248">parent</a><span>|</span><a href="#37156743">next</a><span>|</span><label class="collapse" for="c-37155381">[-]</label><label class="expand" for="c-37155381">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Arithmetic_coding" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Arithmetic_coding</a></div><br/></div></div></div></div></div></div><div id="37156743" class="c"><input type="checkbox" id="c-37156743" checked=""/><div class="controls bullet"><span class="by">asicsp</span><span>|</span><a href="#37153091">prev</a><span>|</span><a href="#37153254">next</a><span>|</span><label class="collapse" for="c-37156743">[-]</label><label class="expand" for="c-37156743">[1 more]</label></div><br/><div class="children"><div class="content">See also: <a href="https:&#x2F;&#x2F;bellard.org&#x2F;nncp&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;bellard.org&#x2F;nncp&#x2F;</a><p>NNCP: Lossless Data Compression with Neural Networks<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=27244004">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=27244004</a> <i>(397 points | May 22, 2021 | 160 comments)</i></div><br/></div></div><div id="37153254" class="c"><input type="checkbox" id="c-37153254" checked=""/><div class="controls bullet"><span class="by">freedmand</span><span>|</span><a href="#37156743">prev</a><span>|</span><a href="#37153756">next</a><span>|</span><label class="collapse" for="c-37153254">[-]</label><label class="expand" for="c-37153254">[12 more]</label></div><br/><div class="children"><div class="content">Was it run on any text that was not feasibly training data for the LLMs? It wouldn&#x27;t be a fair comparison otherwise</div><br/><div id="37153903" class="c"><input type="checkbox" id="c-37153903" checked=""/><div class="controls bullet"><span class="by">NobodyNada</span><span>|</span><a href="#37153254">parent</a><span>|</span><a href="#37153737">next</a><span>|</span><label class="collapse" for="c-37153903">[-]</label><label class="expand" for="c-37153903">[8 more]</label></div><br/><div class="children"><div class="content">Every lossless “compression” algorithm makes some strings shorter and other strings longer, such that the <i>average</i> space savings against all possible strings is neutral at best. This is a fundamental consequence of the way information works — there’s no way to losslessly map (say) 8 bits of input into 7 bits of output.<p>The goal of compression, then, is to make common strings shorter while making uncommon strings longer. You can think of, say, UTF-8 as a simple compression algorithm: it would take 20 bits per character to encode all Unicode code points with a fixed-width encoding, but UTF-8 takes advantage of the fact that the most commonly used characters have a lot of leading zeroes (at least in English). So the characters of the English alphabet require only 8 bits to encode, but uncommon characters require up to 32 bits.<p>Thus, I would expect an LLM-based compression algorithm to do well on strings that were common in its training data, and make strings that were uncommon or absent slightly longer. If it did not do that, it would not be a lossless compression algorithm.</div><br/><div id="37154728" class="c"><input type="checkbox" id="c-37154728" checked=""/><div class="controls bullet"><span class="by">freedmand</span><span>|</span><a href="#37153254">root</a><span>|</span><a href="#37153903">parent</a><span>|</span><a href="#37156528">next</a><span>|</span><label class="collapse" for="c-37154728">[-]</label><label class="expand" for="c-37154728">[3 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t disagree with anything you said.<p>I&#x27;m saying more that if the compression algorithm is benchmarking against &quot;Alice in Wonderland&quot; and has consumed the entirety of &quot;Alice in Wonderland&quot; in training the LLM (along with popular paragraphs and sentences quoted elsewhere), then it might do particularly well at reciting lines from that book and thus be able to compress it extremely well. I&#x27;d be more interested in seeing the compression algorithm&#x27;s performance on new or unreleased works that would have no way of being training data.<p>As an extreme hypothetical, I could make a compression algorithm that is a table mapping an ID to an entire book and fill it with all the popular works. &quot;Alice in Wonderland&quot; would be replaced with a single short identifier string and achieve a ~0.001% compression ratio. An unseen work would be replaced with an &lt;unknown&gt; ID followed by the entire work and be slightly bigger. Then, I benchmark only the popular works and show insanely impressive results!<p>I have no doubt the LLM compressor would do really well on unseen works based on what you said above, but it&#x27;s not a fair look at its performance to run it on works it may have been explicitly trained on.</div><br/><div id="37158206" class="c"><input type="checkbox" id="c-37158206" checked=""/><div class="controls bullet"><span class="by">NobodyNada</span><span>|</span><a href="#37153254">root</a><span>|</span><a href="#37154728">parent</a><span>|</span><a href="#37155138">next</a><span>|</span><label class="collapse" for="c-37158206">[-]</label><label class="expand" for="c-37158206">[1 more]</label></div><br/><div class="children"><div class="content">Gotcha, sorry I completely misunderstood what you were asking! That’s a really insightful question that didn’t cross my mind at all.</div><br/></div></div><div id="37155138" class="c"><input type="checkbox" id="c-37155138" checked=""/><div class="controls bullet"><span class="by">ant6n</span><span>|</span><a href="#37153254">root</a><span>|</span><a href="#37154728">parent</a><span>|</span><a href="#37158206">prev</a><span>|</span><a href="#37156528">next</a><span>|</span><label class="collapse" for="c-37155138">[-]</label><label class="expand" for="c-37155138">[1 more]</label></div><br/><div class="children"><div class="content">Well, unless the weights are part of the compressed file.</div><br/></div></div></div></div><div id="37156528" class="c"><input type="checkbox" id="c-37156528" checked=""/><div class="controls bullet"><span class="by">red369</span><span>|</span><a href="#37153254">root</a><span>|</span><a href="#37153903">parent</a><span>|</span><a href="#37154728">prev</a><span>|</span><a href="#37157631">next</a><span>|</span><label class="collapse" for="c-37156528">[-]</label><label class="expand" for="c-37156528">[3 more]</label></div><br/><div class="children"><div class="content">&gt;Every lossless “compression” algorithm makes some strings shorter and other strings longer, such that the average space savings against all possible strings is neutral at best. This is a fundamental consequence of the way information works — there’s no way to losslessly map (say) 8 bits of input into 7 bits of output.<p>I think I&#x27;m missing a point here. Is it a provable fact that all lossless compression algorithms are neutral at best? Or just something that occurs in the real world?<p>I can&#x27;t think of a good example, so I&#x27;ll give a trivial example to explain my thinking.<p>It seems to me that you could have an algorithm which did not change any text, except the sentence &quot;Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo&quot;, which it encodes as something much shorter like &quot;&lt;Buffalo*8&gt;&quot;. The algorithm would be (very marginally) better than neutral. If this is true, then surely other improvements would be possible to make a lossless algorithm with real reduction.<p>(Edited a typo)</div><br/><div id="37157230" class="c"><input type="checkbox" id="c-37157230" checked=""/><div class="controls bullet"><span class="by">tarvaina</span><span>|</span><a href="#37153254">root</a><span>|</span><a href="#37156528">parent</a><span>|</span><a href="#37156761">next</a><span>|</span><label class="collapse" for="c-37157230">[-]</label><label class="expand" for="c-37157230">[1 more]</label></div><br/><div class="children"><div class="content">The impossibility of a general lossless compression algorithm is a consequence of the pigeon-hole principle.<p><a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Pigeonhole_principle" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Pigeonhole_principle</a></div><br/></div></div><div id="37156761" class="c"><input type="checkbox" id="c-37156761" checked=""/><div class="controls bullet"><span class="by">coryrc</span><span>|</span><a href="#37153254">root</a><span>|</span><a href="#37156528">parent</a><span>|</span><a href="#37157230">prev</a><span>|</span><a href="#37157631">next</a><span>|</span><label class="collapse" for="c-37156761">[-]</label><label class="expand" for="c-37156761">[1 more]</label></div><br/><div class="children"><div class="content">Then how do you encode an input of &quot;&lt;Buffalo*8&gt;&quot;?
If your answer is &quot;I&#x27;m using characters outside the alphabet&quot; then, there&#x27;s your answer, you&#x27;re wasting bits on them.</div><br/></div></div></div></div><div id="37157631" class="c"><input type="checkbox" id="c-37157631" checked=""/><div class="controls bullet"><span class="by">boywitharupee</span><span>|</span><a href="#37153254">root</a><span>|</span><a href="#37153903">parent</a><span>|</span><a href="#37156528">prev</a><span>|</span><a href="#37153737">next</a><span>|</span><label class="collapse" for="c-37157631">[-]</label><label class="expand" for="c-37157631">[1 more]</label></div><br/><div class="children"><div class="content">&gt; there’s no way to losslessly map (say) 8 bits of input into 7 bits of output.<p>This is due to the pigeonhole principle</div><br/></div></div></div></div><div id="37153737" class="c"><input type="checkbox" id="c-37153737" checked=""/><div class="controls bullet"><span class="by">thomasahle</span><span>|</span><a href="#37153254">parent</a><span>|</span><a href="#37153903">prev</a><span>|</span><a href="#37153756">next</a><span>|</span><label class="collapse" for="c-37153737">[-]</label><label class="expand" for="c-37153737">[3 more]</label></div><br/><div class="children"><div class="content">Exactly. This is id always a pitfall when benchmarking LLM based techniques. The enwiki8 dataset they use, for example, is for sure in the training data.<p>To know how the method performs on novel data, the authors have to come up with entirely new datasets, since anything already existing must be assumed probably contaminated.</div><br/><div id="37154841" class="c"><input type="checkbox" id="c-37154841" checked=""/><div class="controls bullet"><span class="by">makapuf</span><span>|</span><a href="#37153254">root</a><span>|</span><a href="#37153737">parent</a><span>|</span><a href="#37153756">next</a><span>|</span><label class="collapse" for="c-37154841">[-]</label><label class="expand" for="c-37154841">[2 more]</label></div><br/><div class="children"><div class="content">But doesn&#x27;t the size in benchmarks include the size of the binary decoder ? So the embedded trained data is accounted for (preventing a plain copy of wikipedia to be included in the decoder)</div><br/><div id="37154945" class="c"><input type="checkbox" id="c-37154945" checked=""/><div class="controls bullet"><span class="by">loeg</span><span>|</span><a href="#37153254">root</a><span>|</span><a href="#37154841">parent</a><span>|</span><a href="#37153756">next</a><span>|</span><label class="collapse" for="c-37154945">[-]</label><label class="expand" for="c-37154945">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think the compressed size statistics on this webpage include the size of the LLM needed to decode.  Some of these inputs are only a few 100 kB -- LLMs absolutely dwarf that.</div><br/></div></div></div></div></div></div></div></div><div id="37153756" class="c"><input type="checkbox" id="c-37153756" checked=""/><div class="controls bullet"><span class="by">elil17</span><span>|</span><a href="#37153254">prev</a><span>|</span><a href="#37153265">next</a><span>|</span><label class="collapse" for="c-37153756">[-]</label><label class="expand" for="c-37153756">[13 more]</label></div><br/><div class="children"><div class="content">Idea: lossy text compression, where the LLM replaces unlikely words with likelier synonyms.</div><br/><div id="37157337" class="c"><input type="checkbox" id="c-37157337" checked=""/><div class="controls bullet"><span class="by">radford-neal</span><span>|</span><a href="#37153756">parent</a><span>|</span><a href="#37154560">next</a><span>|</span><label class="collapse" for="c-37157337">[-]</label><label class="expand" for="c-37157337">[1 more]</label></div><br/><div class="children"><div class="content">From 1992: <a href="https:&#x2F;&#x2F;core.ac.uk&#x2F;download&#x2F;pdf&#x2F;44289908.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;core.ac.uk&#x2F;download&#x2F;pdf&#x2F;44289908.pdf</a></div><br/></div></div><div id="37154560" class="c"><input type="checkbox" id="c-37154560" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#37153756">parent</a><span>|</span><a href="#37157337">prev</a><span>|</span><a href="#37155619">next</a><span>|</span><label class="collapse" for="c-37154560">[-]</label><label class="expand" for="c-37154560">[8 more]</label></div><br/><div class="children"><div class="content">This is actually a very interesting idea, I hope someone would implement it and how. Would be fun to read the same text with stronger and stronger lossy compression.</div><br/><div id="37154933" class="c"><input type="checkbox" id="c-37154933" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#37153756">root</a><span>|</span><a href="#37154560">parent</a><span>|</span><a href="#37155619">next</a><span>|</span><label class="collapse" for="c-37154933">[-]</label><label class="expand" for="c-37154933">[7 more]</label></div><br/><div class="children"><div class="content">up-goer five: explained using only the ten hundred words people use the most often</div><br/><div id="37155274" class="c"><input type="checkbox" id="c-37155274" checked=""/><div class="controls bullet"><span class="by">davidzweig</span><span>|</span><a href="#37153756">root</a><span>|</span><a href="#37154933">parent</a><span>|</span><a href="#37155152">next</a><span>|</span><label class="collapse" for="c-37155274">[-]</label><label class="expand" for="c-37155274">[2 more]</label></div><br/><div class="children"><div class="content">Oh, this would be super neat for (our) language learning chatbot.. restricting the vocab the model uses.. if someone knows how this can be done. :)</div><br/><div id="37155720" class="c"><input type="checkbox" id="c-37155720" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#37153756">root</a><span>|</span><a href="#37155274">parent</a><span>|</span><a href="#37155152">next</a><span>|</span><label class="collapse" for="c-37155720">[-]</label><label class="expand" for="c-37155720">[1 more]</label></div><br/><div class="children"><div class="content">I guess you try to just zero the values that corresponds to unwanted tokens before the softmax.</div><br/></div></div></div></div><div id="37155152" class="c"><input type="checkbox" id="c-37155152" checked=""/><div class="controls bullet"><span class="by">ant6n</span><span>|</span><a href="#37153756">root</a><span>|</span><a href="#37154933">parent</a><span>|</span><a href="#37155274">prev</a><span>|</span><a href="#37155619">next</a><span>|</span><label class="collapse" for="c-37155152">[-]</label><label class="expand" for="c-37155152">[4 more]</label></div><br/><div class="children"><div class="content">Carried to us by the Podcaster of Phd comics!</div><br/><div id="37155510" class="c"><input type="checkbox" id="c-37155510" checked=""/><div class="controls bullet"><span class="by">kbenson</span><span>|</span><a href="#37153756">root</a><span>|</span><a href="#37155152">parent</a><span>|</span><a href="#37155619">next</a><span>|</span><label class="collapse" for="c-37155510">[-]</label><label class="expand" for="c-37155510">[3 more]</label></div><br/><div class="children"><div class="content">What?  Isn&#x27;t it Randall Munroe (XKCD)?</div><br/><div id="37158473" class="c"><input type="checkbox" id="c-37158473" checked=""/><div class="controls bullet"><span class="by">ant6n</span><span>|</span><a href="#37153756">root</a><span>|</span><a href="#37155510">parent</a><span>|</span><a href="#37155788">next</a><span>|</span><label class="collapse" for="c-37158473">[-]</label><label class="expand" for="c-37158473">[1 more]</label></div><br/><div class="children"><div class="content">Well, it was before that nnet compression.</div><br/></div></div><div id="37155788" class="c"><input type="checkbox" id="c-37155788" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#37153756">root</a><span>|</span><a href="#37155510">parent</a><span>|</span><a href="#37158473">prev</a><span>|</span><a href="#37155619">next</a><span>|</span><label class="collapse" for="c-37155788">[-]</label><label class="expand" for="c-37155788">[1 more]</label></div><br/><div class="children"><div class="content">Oops- I left out the link!
<a href="https:&#x2F;&#x2F;xkcd.com&#x2F;1133&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;xkcd.com&#x2F;1133&#x2F;</a><p>&quot;you will not go to space today&quot;</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37155619" class="c"><input type="checkbox" id="c-37155619" checked=""/><div class="controls bullet"><span class="by">mhitza</span><span>|</span><a href="#37153756">parent</a><span>|</span><a href="#37154560">prev</a><span>|</span><a href="#37156723">next</a><span>|</span><label class="collapse" for="c-37155619">[-]</label><label class="expand" for="c-37155619">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a very good approach also when you want to preserve anonymity. Writing idiosyncrasies can be used to distictely fingerprint us (with enough effort), llm&#x27;s could in theory enhance anonymity as long as we can run these things locally on cheap hardware.</div><br/><div id="37156876" class="c"><input type="checkbox" id="c-37156876" checked=""/><div class="controls bullet"><span class="by">andai</span><span>|</span><a href="#37153756">root</a><span>|</span><a href="#37155619">parent</a><span>|</span><a href="#37156723">next</a><span>|</span><label class="collapse" for="c-37156876">[-]</label><label class="expand" for="c-37156876">[1 more]</label></div><br/><div class="children"><div class="content">Stylometry is one thing, but your thoughts&#x2F;ideas will also give you away. For true anonymity, you&#x27;d have to let the thing do your thinking for you too!</div><br/></div></div></div></div><div id="37156723" class="c"><input type="checkbox" id="c-37156723" checked=""/><div class="controls bullet"><span class="by">canjobear</span><span>|</span><a href="#37153756">parent</a><span>|</span><a href="#37155619">prev</a><span>|</span><a href="#37153265">next</a><span>|</span><label class="collapse" for="c-37156723">[-]</label><label class="expand" for="c-37156723">[1 more]</label></div><br/><div class="children"><div class="content">Similar to human memory for text.</div><br/></div></div></div></div><div id="37153265" class="c"><input type="checkbox" id="c-37153265" checked=""/><div class="controls bullet"><span class="by">londons_explore</span><span>|</span><a href="#37153756">prev</a><span>|</span><a href="#37154357">next</a><span>|</span><label class="collapse" for="c-37153265">[-]</label><label class="expand" for="c-37153265">[9 more]</label></div><br/><div class="children"><div class="content">There are plenty of usecases for very slow and very good text compression.<p>Imagine you run a big email server for millions of users.<p>Any email that hasn&#x27;t been accessed for a month is probably a good candidate for compressing with this method.<p>In the unlikely event the user wishes to read an old email again, I&#x27;m sure they won&#x27;t mind waiting 0.1 seconds for 10 kilobytes of email (ie. a screenfull) to decompress.</div><br/><div id="37153463" class="c"><input type="checkbox" id="c-37153463" checked=""/><div class="controls bullet"><span class="by">eklitzke</span><span>|</span><a href="#37153265">parent</a><span>|</span><a href="#37153366">next</a><span>|</span><label class="collapse" for="c-37153463">[-]</label><label class="expand" for="c-37153463">[4 more]</label></div><br/><div class="children"><div class="content">As soon as someone uses a POP or IMAP client to fetch all of their mail your server is going to catch on fire. Even if you had POP&#x2F;IMAP disabled, the computation costs to decode data are enormous with this method, which is precisely why people don&#x27;t use the most maximally compressing codecs for anything other than true archival applications (e.g. database backups which, unlike old email, really are expected to be accessed almost never).</div><br/><div id="37153543" class="c"><input type="checkbox" id="c-37153543" checked=""/><div class="controls bullet"><span class="by">londons_explore</span><span>|</span><a href="#37153265">root</a><span>|</span><a href="#37153463">parent</a><span>|</span><a href="#37153697">next</a><span>|</span><label class="collapse" for="c-37153543">[-]</label><label class="expand" for="c-37153543">[2 more]</label></div><br/><div class="children"><div class="content">Sounds like a good reason to just ship the compressed data to the customer and say &quot;decompress it yourself - this bellard guy makes a decompressor!&quot;</div><br/><div id="37154709" class="c"><input type="checkbox" id="c-37154709" checked=""/><div class="controls bullet"><span class="by">etiam</span><span>|</span><a href="#37153265">root</a><span>|</span><a href="#37153543">parent</a><span>|</span><a href="#37153697">next</a><span>|</span><label class="collapse" for="c-37154709">[-]</label><label class="expand" for="c-37154709">[1 more]</label></div><br/><div class="children"><div class="content"><i>That</i> sounds like recipe for really poor relations with the sizeable fraction of customers who are more concerned with getting their data back than delighted to be introduced to a cutting-edge text compression method...</div><br/></div></div></div></div><div id="37153697" class="c"><input type="checkbox" id="c-37153697" checked=""/><div class="controls bullet"><span class="by">londons_explore</span><span>|</span><a href="#37153265">root</a><span>|</span><a href="#37153463">parent</a><span>|</span><a href="#37153543">prev</a><span>|</span><a href="#37153366">next</a><span>|</span><label class="collapse" for="c-37153697">[-]</label><label class="expand" for="c-37153697">[1 more]</label></div><br/><div class="children"><div class="content">A full IMAP sync of a 20 gigabyte inbox from gmail takes ~12 hours.<p>Thats 400 kilobytes per second.   And much of that is image attachments and stuff - perhaps only 10 kilobytes&#x2F;second of text.<p>Maybe Google already does this?</div><br/></div></div></div></div><div id="37153366" class="c"><input type="checkbox" id="c-37153366" checked=""/><div class="controls bullet"><span class="by">CharlesW</span><span>|</span><a href="#37153265">parent</a><span>|</span><a href="#37153463">prev</a><span>|</span><a href="#37153331">next</a><span>|</span><label class="collapse" for="c-37153366">[-]</label><label class="expand" for="c-37153366">[3 more]</label></div><br/><div class="children"><div class="content">&gt; <i>Any email that hasn&#x27;t been accessed for a month is probably a good candidate for compressing with this method.</i><p>You don&#x27;t think &quot;the same exact GPU model and program versions must be used for compression and decompression&quot; rules it out as more than an interesting experiment, especially given that storage is cheap and continues to get cheaper?</div><br/><div id="37153524" class="c"><input type="checkbox" id="c-37153524" checked=""/><div class="controls bullet"><span class="by">londons_explore</span><span>|</span><a href="#37153265">root</a><span>|</span><a href="#37153366">parent</a><span>|</span><a href="#37153331">next</a><span>|</span><label class="collapse" for="c-37153524">[-]</label><label class="expand" for="c-37153524">[2 more]</label></div><br/><div class="children"><div class="content">I think they mean the exact same model data (for the GPU).    I don&#x27;t think it requires the exact same model of GPU.</div><br/><div id="37154162" class="c"><input type="checkbox" id="c-37154162" checked=""/><div class="controls bullet"><span class="by">CharlesW</span><span>|</span><a href="#37153265">root</a><span>|</span><a href="#37153524">parent</a><span>|</span><a href="#37153331">next</a><span>|</span><label class="collapse" for="c-37154162">[-]</label><label class="expand" for="c-37154162">[1 more]</label></div><br/><div class="children"><div class="content">I’m also reading it as &quot;the same exact GPU [LLM] and program versions must be used for compression and decompression&quot;.</div><br/></div></div></div></div></div></div><div id="37153331" class="c"><input type="checkbox" id="c-37153331" checked=""/><div class="controls bullet"><span class="by">otabdeveloper4</span><span>|</span><a href="#37153265">parent</a><span>|</span><a href="#37153366">prev</a><span>|</span><a href="#37154357">next</a><span>|</span><label class="collapse" for="c-37153331">[-]</label><label class="expand" for="c-37153331">[1 more]</label></div><br/><div class="children"><div class="content">Old email needs to be indexed for search, and those indexes might be as large as the original text.</div><br/></div></div></div></div><div id="37154357" class="c"><input type="checkbox" id="c-37154357" checked=""/><div class="controls bullet"><span class="by">fsiefken</span><span>|</span><a href="#37153265">prev</a><span>|</span><a href="#37153583">next</a><span>|</span><label class="collapse" for="c-37154357">[-]</label><label class="expand" for="c-37154357">[4 more]</label></div><br/><div class="children"><div class="content">Impressive! But an order of a magnitude slower then non-LLM compression techniques.<p>I try to compare with the results from the top 9 of this enwik8 compression test by Matt Mahoney:
<a href="https:&#x2F;&#x2F;www.mattmahoney.net&#x2F;dc&#x2F;text.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.mattmahoney.net&#x2F;dc&#x2F;text.html</a><p>The durilca compressor by Dmitry Shkarin (what happened to him?) has been the fastest compressor in the top since it&#x27;s debut in 2006.<p>The original size, enwik8, is 10^8 bytes. The compressed size is the compressend enwik8 plus the size of a zip archive containing the decompressor.<p>The bpb value of rwkv_430M on enwik8 is 0.948 bpb, the 7B models will be lower, perhaps around 0.800. So if my calculations are correct, the LLM&#x27;s can perform 50% better then the best performing conventional compressors excluding the zipped LLM decompressor code (I am unsure about the size).<p>The bpb ratio for each existing program can be calculated as:
bpb ratio=(Original size (enwik9)Total size (enwik9+prog) )×8<p>nncp v3.1:
Given compressed size is not provided, we cannot calculate bpb for nncp v3.1 using enwik8.<p>cmix v19:
bpb=(14,837,987+223,485108)×8bpb=(10814,837,987+223,485 )×8
bpb≈1.205<p>tensorflow-compress v4:
bpb=(15,905,037+55,283108)×8bpb=(10815,905,037+55,283 )×8
bpb≈1.272<p>cmix-hp 10 Jun 2021:
Given compressed size is not provided, we cannot calculate bpb for cmix-hp using enwik8.<p>fast-cmix:
Given compressed size is not provided, we cannot calculate bpb for fast-cmix using enwik8.<p>starlit 31 May 2021:
bpb=(15,215,107108)×8bpb=(10815,215,107 )×8
bpb≈1.217<p>phda9 1.8:
bpb=(15,010,414+42,944108)×8bpb=(10815,010,414+42,944 )×8
bpb≈1.205<p>paq8px_v206fix1:
bpb=(15,849,084+402,949108)×8bpb=(10815,849,084+402,949 )×8
bpb≈1.265<p>durilca&#x27;kingsize:
bpb=(16,209,167+407,477108)×8bpb=(10816,209,167+407,477 )×8
bpb≈1.317</div><br/><div id="37154449" class="c"><input type="checkbox" id="c-37154449" checked=""/><div class="controls bullet"><span class="by">londons_explore</span><span>|</span><a href="#37154357">parent</a><span>|</span><a href="#37153583">next</a><span>|</span><label class="collapse" for="c-37154449">[-]</label><label class="expand" for="c-37154449">[3 more]</label></div><br/><div class="children"><div class="content">&gt; The bpb value of rwkv_430M on enwik8 is 0.948 bpb<p>This is an unfair test, because the rwkv_430M model was almost certainly trained on wikipedia.   Testing using training data is a big no-no in the ML world - you get unrealistically good results.<p>To test this properly, it needs to be run on a bit of newly written text which is nowhere on the internet.   Obviously writing 100 MB of human written text which is not already on the internet is rather a big task...</div><br/><div id="37155465" class="c"><input type="checkbox" id="c-37155465" checked=""/><div class="controls bullet"><span class="by">ndriscoll</span><span>|</span><a href="#37154357">root</a><span>|</span><a href="#37154449">parent</a><span>|</span><a href="#37154648">next</a><span>|</span><label class="collapse" for="c-37155465">[-]</label><label class="expand" for="c-37155465">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s fair because the model itself is included in the compressed size. You&#x27;re not trying to extrapolate predictions. It makes perfect sense for compression &quot;at rest&quot; applications to intentionally overfit on the data you&#x27;re trying to compress if that gives a smaller total size(model ++ compressed text).<p>If you were going to standardize on a foundational model that gets delivered to billions of devices to be used in network transfers (where you just transfer the compressed text and assume the other side already has the model), <i>then</i> it makes sense to optimize for generalizability. In that case it would make sense to exclude the model itself from the compressed size. Basically like how brotli gets to &quot;cheat&quot; by including a 120KB pre-defined dictionary.<p>Edit: Actually that can&#x27;t be right. The numbers on <a href="https:&#x2F;&#x2F;www.mattmahoney.net&#x2F;dc&#x2F;text.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.mattmahoney.net&#x2F;dc&#x2F;text.html</a> use that methodology, but the numbers in OP here can&#x27;t be, since a zip compressed version of rwkv_430M is ~650 MB, so obviously it&#x27;s not being included or Alice in Wonderland would have an abysmal ratio.</div><br/></div></div><div id="37154648" class="c"><input type="checkbox" id="c-37154648" checked=""/><div class="controls bullet"><span class="by">etiam</span><span>|</span><a href="#37154357">root</a><span>|</span><a href="#37154449">parent</a><span>|</span><a href="#37155465">prev</a><span>|</span><a href="#37153583">next</a><span>|</span><label class="collapse" for="c-37154648">[-]</label><label class="expand" for="c-37154648">[1 more]</label></div><br/><div class="children"><div class="content">Newly declassified material maybe? Or just newly digitized?</div><br/></div></div></div></div></div></div><div id="37153583" class="c"><input type="checkbox" id="c-37153583" checked=""/><div class="controls bullet"><span class="by">AceJohnny2</span><span>|</span><a href="#37154357">prev</a><span>|</span><a href="#37153277">next</a><span>|</span><label class="collapse" for="c-37153583">[-]</label><label class="expand" for="c-37153583">[3 more]</label></div><br/><div class="children"><div class="content">This is hilarious.<p>In compression contest circles, there&#x27;s long been this concept of &quot;AI compression&quot; (long before the current LLM wave), considered the upper limit of compression algorithms. This is based on the idea of the Dictionary... IIRC, some compression systems include the Dictionary as part of the decompressor, to save space in the payload, but then that Dictionary is fixed and may be suboptimal for the data. Others include it in the payload, so that it&#x27;s optimal for the data, which most do.<p>But you could achieve incredible compression of, say, The Complete Works Of Shakespeare if you knew your peer knew about The Complete Works Of Shakespeare (had a rich enough dictionary) and just send them that designator of the payload! Or, you could send them a particular high-resolution picture of a wheat field with cypresses under a blue sky with an appropriately specific &quot;description&quot;.<p>What Fabrice did is gone ahead and put this theoretical idea in practice. And that&#x27;s hilarious.</div><br/><div id="37154239" class="c"><input type="checkbox" id="c-37154239" checked=""/><div class="controls bullet"><span class="by">sterlind</span><span>|</span><a href="#37153583">parent</a><span>|</span><a href="#37154592">next</a><span>|</span><label class="collapse" for="c-37154239">[-]</label><label class="expand" for="c-37154239">[1 more]</label></div><br/><div class="children"><div class="content">Notably, I think this would fail on the Wikipedia compression contest because that counts the size of the decompression program (model in this case), so that it measures Kolmogorov complexity. And RWKV is surely too big. A neat thing to think about though, and maybe a winning idea if a more space-efficient model can be found.</div><br/></div></div></div></div><div id="37153277" class="c"><input type="checkbox" id="c-37153277" checked=""/><div class="controls bullet"><span class="by">ttoinou</span><span>|</span><a href="#37153583">prev</a><span>|</span><a href="#37154324">next</a><span>|</span><label class="collapse" for="c-37153277">[-]</label><label class="expand" for="c-37153277">[1 more]</label></div><br/><div class="children"><div class="content">Aren&#x27;t the LLM supposed to have &quot;seen&quot; those text corpus before ? I would expect that a very small prompt could generate the whole requested text</div><br/></div></div><div id="37154324" class="c"><input type="checkbox" id="c-37154324" checked=""/><div class="controls bullet"><span class="by">sterlind</span><span>|</span><a href="#37153277">prev</a><span>|</span><a href="#37155342">next</a><span>|</span><label class="collapse" for="c-37154324">[-]</label><label class="expand" for="c-37154324">[2 more]</label></div><br/><div class="children"><div class="content">How does this work? I know that LLMs predict the log-likelihood of the next token in a sequence.. so I guess you initialize the LLM from scratch, say that each token of the input text is the Nth most likely possibility, and record what N is for that token? And then you do some combination of run-length encoding and Huffman trees or something to record the Ns?<p>Since this is not open-source, but seems simple to do, perhaps I&#x27;ll make my own.</div><br/><div id="37155191" class="c"><input type="checkbox" id="c-37155191" checked=""/><div class="controls bullet"><span class="by">Scene_Cast2</span><span>|</span><a href="#37154324">parent</a><span>|</span><a href="#37155342">next</a><span>|</span><label class="collapse" for="c-37155191">[-]</label><label class="expand" for="c-37155191">[1 more]</label></div><br/><div class="children"><div class="content">A slightly more elegant method would be to throw an arithmetic coder on the output vector (normalized and represented as an interval) and record the interval represented by the real token to be compressed. This would allow for better compression ratios than your approach, I think (due to arbitrary probability support).<p>My potential issue with NNs for compression is that sometimes they predict near zero for some probabilities that actually occur in real data - in which case the number of bytes needed to encode would blow up. But that can be somewhat guarded against (perhaps by limiting the lower bound of output probabilities).</div><br/></div></div></div></div><div id="37155342" class="c"><input type="checkbox" id="c-37155342" checked=""/><div class="controls bullet"><span class="by">TheMiddleMan</span><span>|</span><a href="#37154324">prev</a><span>|</span><a href="#37153357">next</a><span>|</span><label class="collapse" for="c-37155342">[-]</label><label class="expand" for="c-37155342">[4 more]</label></div><br/><div class="children"><div class="content">Is it reasonable to imagine a future where most devices come with a, say, 500MB file used for decompressing?<p>Imagine the potential bandwidth savings. I bet this has applications as a modern &quot;Dial-up accelerator&quot; for people with slow connections and fast hardware.</div><br/><div id="37156577" class="c"><input type="checkbox" id="c-37156577" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#37155342">parent</a><span>|</span><a href="#37155405">next</a><span>|</span><label class="collapse" for="c-37156577">[-]</label><label class="expand" for="c-37156577">[2 more]</label></div><br/><div class="children"><div class="content">It seems likely that to me we will eventually start using AI models for compression and decompression of text, images, 3d models, programs, video, and streaming. AI has the potential for much higher compression. It&#x27;s actually quite feasible for most PCs and laptops to keep in memory multiple GBs models. And the typical PC game is dozens of GBs.</div><br/><div id="37157583" class="c"><input type="checkbox" id="c-37157583" checked=""/><div class="controls bullet"><span class="by">Turskarama</span><span>|</span><a href="#37155342">root</a><span>|</span><a href="#37156577">parent</a><span>|</span><a href="#37155405">next</a><span>|</span><label class="collapse" for="c-37157583">[-]</label><label class="expand" for="c-37157583">[1 more]</label></div><br/><div class="children"><div class="content">You run into a problem where the model needs to be exactly the same on both ends.  If your model is a couple of MB then updating it on occasion is no problem, but if it&#x27;s several GB then you DO have a problem.<p>Then if you want to decompress something that was archived 10 years ago you need to make sure you can find and install the old compression model.</div><br/></div></div></div></div><div id="37155405" class="c"><input type="checkbox" id="c-37155405" checked=""/><div class="controls bullet"><span class="by">landgenoot</span><span>|</span><a href="#37155342">parent</a><span>|</span><a href="#37156577">prev</a><span>|</span><a href="#37153357">next</a><span>|</span><label class="collapse" for="c-37155405">[-]</label><label class="expand" for="c-37155405">[1 more]</label></div><br/><div class="children"><div class="content">And could you use that file to compress itself?</div><br/></div></div></div></div><div id="37153357" class="c"><input type="checkbox" id="c-37153357" checked=""/><div class="controls bullet"><span class="by">albertzeyer</span><span>|</span><a href="#37155342">prev</a><span>|</span><a href="#37155555">next</a><span>|</span><label class="collapse" for="c-37153357">[-]</label><label class="expand" for="c-37153357">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The same exact GPU model and program versions must be used for compression and decompression.<p>This makes the whole approach really difficult to be useful in practice. Basically whenever you have a method that relies on 100% reproducible float numbers.<p>What are ways around that? Why do the quantized variants actually have the same problem?</div><br/></div></div><div id="37155555" class="c"><input type="checkbox" id="c-37155555" checked=""/><div class="controls bullet"><span class="by">dmillar</span><span>|</span><a href="#37153357">prev</a><span>|</span><a href="#37153341">next</a><span>|</span><label class="collapse" for="c-37155555">[-]</label><label class="expand" for="c-37155555">[1 more]</label></div><br/><div class="children"><div class="content">Zstandard (zstd), with a properly trained dictionary, could do well against larger (than alice29) texts. I doubt you&#x27;d see ~0.4 bpb, but 1-1.5 seems achievable. And it would be ~100% accurate with no GPU or other overhead necessary.<p>A quick experiment using a small selection of truncated texts yielded 1.8 bpb against alice29. I imagine other texts could do better.</div><br/></div></div><div id="37153341" class="c"><input type="checkbox" id="c-37153341" checked=""/><div class="controls bullet"><span class="by">MaximilianEmel</span><span>|</span><a href="#37155555">prev</a><span>|</span><a href="#37156309">next</a><span>|</span><label class="collapse" for="c-37153341">[-]</label><label class="expand" for="c-37153341">[3 more]</label></div><br/><div class="children"><div class="content">&quot;The same exact GPU model ... must be used for compression and decompression.&quot;<p>This seems like a massive issue for actual use. Are there really not some workarounds to get deterministic output?</div><br/><div id="37153560" class="c"><input type="checkbox" id="c-37153560" checked=""/><div class="controls bullet"><span class="by">slimsag</span><span>|</span><a href="#37153341">parent</a><span>|</span><a href="#37153660">next</a><span>|</span><label class="collapse" for="c-37153560">[-]</label><label class="expand" for="c-37153560">[1 more]</label></div><br/><div class="children"><div class="content">Unfortunately GPUs (and even CPUs&#x27; SIMD) floating point math is riddled with cross-platform determinism issues; hardware manufacturers intentionally trade that in order to get faster math operations in general, because although behavior of floating point is defined in IEEE 754, you get rounding errors for each operation.<p>Compiler optimizations (and remember, GPU drivers each use their own compiler behind the scenes to translate to their actual hardware architecture) can alter rounding errors of each operation, and parallel execution - which differs from hardware to hardware - also affects it.<p>Some APIs (Cuda?) let you disable all optimizations and there are ways to get cross-platform determinism, but in general it&#x27;s much much slower if you want bit-for-bit equality across different hardware.<p>SPIR-V&#x2F;Vulkan for example[0] only define an error range based in ULP for some operations - not bit-for-bit equality.<p>[0] <a href="https:&#x2F;&#x2F;registry.khronos.org&#x2F;vulkan&#x2F;specs&#x2F;1.2-extensions&#x2F;html&#x2F;vkspec.html#spirvenv-precision-operation" rel="nofollow noreferrer">https:&#x2F;&#x2F;registry.khronos.org&#x2F;vulkan&#x2F;specs&#x2F;1.2-extensions&#x2F;htm...</a></div><br/></div></div><div id="37153660" class="c"><input type="checkbox" id="c-37153660" checked=""/><div class="controls bullet"><span class="by">johndough</span><span>|</span><a href="#37153341">parent</a><span>|</span><a href="#37153560">prev</a><span>|</span><a href="#37156309">next</a><span>|</span><label class="collapse" for="c-37153660">[-]</label><label class="expand" for="c-37153660">[1 more]</label></div><br/><div class="children"><div class="content">Reproducible results across GPUs are theoretically possible, but it would take some effort to implement and be slower. At least the primitive operations (addition, multiplication, etc.) are there: <a href="https:&#x2F;&#x2F;docs.nvidia.com&#x2F;cuda&#x2F;floating-point&#x2F;index.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;docs.nvidia.com&#x2F;cuda&#x2F;floating-point&#x2F;index.html</a></div><br/></div></div></div></div><div id="37156309" class="c"><input type="checkbox" id="c-37156309" checked=""/><div class="controls bullet"><span class="by">luke-stanley</span><span>|</span><a href="#37153341">prev</a><span>|</span><a href="#37155958">next</a><span>|</span><label class="collapse" for="c-37156309">[-]</label><label class="expand" for="c-37156309">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m curious how cmix 19 compares to rwkv_169M (the LLM with the fastest compression speed and also low GPU memory use), since cmix 19 still had a better compression ratio at the moment. Anyone know?</div><br/></div></div><div id="37155958" class="c"><input type="checkbox" id="c-37155958" checked=""/><div class="controls bullet"><span class="by">spiritplumber</span><span>|</span><a href="#37156309">prev</a><span>|</span><a href="#37153167">next</a><span>|</span><label class="collapse" for="c-37155958">[-]</label><label class="expand" for="c-37155958">[2 more]</label></div><br/><div class="children"><div class="content">I wrote a dictionary compression type thing a while ago, should I post it somewhere?</div><br/><div id="37156359" class="c"><input type="checkbox" id="c-37156359" checked=""/><div class="controls bullet"><span class="by">bcjordan</span><span>|</span><a href="#37155958">parent</a><span>|</span><a href="#37153167">next</a><span>|</span><label class="collapse" for="c-37156359">[-]</label><label class="expand" for="c-37156359">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d be curious to check it out!<p>I&#x27;ve been wondering this last year about the use case of compressing highly repetitive logs in a streaming fashion, and whether fine-tuning a model or combination of LLM &#x2F; datastore might make a sort of adaptive online compression perform well (e.g. allowing central versioned coordination of compression steps&#x2F;layers as they evolve)</div><br/></div></div></div></div><div id="37153167" class="c"><input type="checkbox" id="c-37153167" checked=""/><div class="controls bullet"><span class="by">leumassuehtam</span><span>|</span><a href="#37155958">prev</a><span>|</span><a href="#37153292">next</a><span>|</span><label class="collapse" for="c-37153167">[-]</label><label class="expand" for="c-37153167">[1 more]</label></div><br/><div class="children"><div class="content">Interesting application for the RWKV family of models.</div><br/></div></div><div id="37153292" class="c"><input type="checkbox" id="c-37153292" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#37153167">prev</a><span>|</span><a href="#37153617">next</a><span>|</span><label class="collapse" for="c-37153292">[-]</label><label class="expand" for="c-37153292">[1 more]</label></div><br/><div class="children"><div class="content">Intriguing promise, but I can&#x27;t find technical details or source, only binary exectuables. Anyone else have luck?<p>The &quot;(and hopefully decompress)&quot; would usually make me think it was like when people would &quot;compress&quot; sequences into emoji using ChatGPT...but its bellard</div><br/></div></div><div id="37153617" class="c"><input type="checkbox" id="c-37153617" checked=""/><div class="controls bullet"><span class="by">londons_explore</span><span>|</span><a href="#37153292">prev</a><span>|</span><a href="#37154708">next</a><span>|</span><label class="collapse" for="c-37153617">[-]</label><label class="expand" for="c-37153617">[6 more]</label></div><br/><div class="children"><div class="content">Bellard still does great work...     But no longer releases anything opensource.<p>In turn, that makes it far less interesting to me.<p>Especially since the majority of what he releases is more of a cool tech demo than a product in it&#x27;s own right.    A closed source tech demo really isn&#x27;t of much use to anyone who doesn&#x27;t have the source code to extend it.</div><br/><div id="37154216" class="c"><input type="checkbox" id="c-37154216" checked=""/><div class="controls bullet"><span class="by">DSingularity</span><span>|</span><a href="#37153617">parent</a><span>|</span><a href="#37154587">next</a><span>|</span><label class="collapse" for="c-37154216">[-]</label><label class="expand" for="c-37154216">[4 more]</label></div><br/><div class="children"><div class="content">Why did he stop?</div><br/><div id="37154374" class="c"><input type="checkbox" id="c-37154374" checked=""/><div class="controls bullet"><span class="by">londons_explore</span><span>|</span><a href="#37153617">root</a><span>|</span><a href="#37154216">parent</a><span>|</span><a href="#37154587">next</a><span>|</span><label class="collapse" for="c-37154374">[-]</label><label class="expand" for="c-37154374">[3 more]</label></div><br/><div class="children"><div class="content">I can&#x27;t speak for him...<p>But I stopped open sourcing some of my work when I got aggressive users demanding I support them for free.   I realised I was pouring hours into triaging bugs, explaining why PR&#x27;s weren&#x27;t good enough, fixing other people&#x27;s corner cases, and settling disputes, without really getting much back.    Sure, my projects were getting a following and there were lots of happy users, but the actual benefit to me of that happening was negative.<p>One middle ground I used for a while was releasing my projects as a tar file only, not a git repo.   Someone else can then make a git repo and maintain the project.    Looks like Bellard did the same for some projects.</div><br/><div id="37154575" class="c"><input type="checkbox" id="c-37154575" checked=""/><div class="controls bullet"><span class="by">dakotasmith</span><span>|</span><a href="#37153617">root</a><span>|</span><a href="#37154374">parent</a><span>|</span><a href="#37154828">next</a><span>|</span><label class="collapse" for="c-37154575">[-]</label><label class="expand" for="c-37154575">[1 more]</label></div><br/><div class="children"><div class="content">I recently came across Curio (<a href="https:&#x2F;&#x2F;github.com&#x2F;dabeaz&#x2F;curio">https:&#x2F;&#x2F;github.com&#x2F;dabeaz&#x2F;curio</a>) &amp; really appreciated their FAQ section on contributing. I plan to use it in any future open source projects:<p>Q: Can I contribute?<p>A: Curio is not a community-based project seeking developers or maintainers. However, having it work reliably is important. If you&#x27;ve found a bug or have an idea for making it better, please file an issue.</div><br/></div></div><div id="37154828" class="c"><input type="checkbox" id="c-37154828" checked=""/><div class="controls bullet"><span class="by">londons_explore</span><span>|</span><a href="#37153617">root</a><span>|</span><a href="#37154374">parent</a><span>|</span><a href="#37154575">prev</a><span>|</span><a href="#37154587">next</a><span>|</span><label class="collapse" for="c-37154828">[-]</label><label class="expand" for="c-37154828">[1 more]</label></div><br/><div class="children"><div class="content">Code is a tool.   Often you build a tool to complete a task.<p>Open Sourcing the tools lets others do <i>their</i> tasks.  But you run the risk of becoming an unpaid toolmaker and unpaid tool maintainer, and getting none of your own tasks done.<p>In a way, github&#x27;s biggest opensource committers are in a way the &#x27;gig economy&#x27; workers of the tech world.   Paid in just the occasional few bucks of donation for work which would otherwise be worth hundreds of thousands of dollars done with a salary.</div><br/></div></div></div></div></div></div></div></div><div id="37154708" class="c"><input type="checkbox" id="c-37154708" checked=""/><div class="controls bullet"><span class="by">lwneal</span><span>|</span><a href="#37153617">prev</a><span>|</span><a href="#37153139">next</a><span>|</span><label class="collapse" for="c-37154708">[-]</label><label class="expand" for="c-37154708">[3 more]</label></div><br/><div class="children"><div class="content">A man goes to prison, and the first night while he&#x27;s laying in bed, he hears someone yell out, &quot;44!&quot;, followed by laughter from the other prisoners.<p>Puzzled, he laid back down, but then he heard someone else yell out, &quot;72!&quot;, followed by even more laughter.<p>&quot;What&#x27;s going on?&quot; he asked his cellmate.<p>&quot;Well, we&#x27;ve all heard every joke so many times, we&#x27;ve given them each a number to make it easier.&quot;<p>&quot;Oh,&quot; he says, &quot;can I try?&quot;<p>&quot;Sure, go ahead.&quot;<p>So, he yells out &quot;102!&quot; and the place goes nuts. People are whooping and laughing in a hysteria. He looks at his cellmate rolling on the ground laughing.<p>&quot;Wow, good joke, huh?&quot;<p>&quot;Yeah! We ain&#x27;t never heard that one before!&quot;</div><br/><div id="37157399" class="c"><input type="checkbox" id="c-37157399" checked=""/><div class="controls bullet"><span class="by">redsaz</span><span>|</span><a href="#37154708">parent</a><span>|</span><a href="#37154725">next</a><span>|</span><label class="collapse" for="c-37157399">[-]</label><label class="expand" for="c-37157399">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve not heard that version before, I like it. The way I usually hear it end:<p>So, he yells out &quot;102!&quot; and... Crickets.<p>&quot;What&#x27;d I do wrong?&quot;<p>&quot;Ehh, you must not have told it right.&quot;<p>(...Or, in this case, &quot;you&#x27;re probably not using the right model GPU&quot;)</div><br/></div></div></div></div><div id="37153139" class="c"><input type="checkbox" id="c-37153139" checked=""/><div class="controls bullet"><span class="by">optimalsolver</span><span>|</span><a href="#37154708">prev</a><span>|</span><a href="#37154669">next</a><span>|</span><label class="collapse" for="c-37153139">[-]</label><label class="expand" for="c-37153139">[21 more]</label></div><br/><div class="children"><div class="content">Reminder that compression = intelligence:<p><a href="https:&#x2F;&#x2F;mattmahoney.net&#x2F;dc&#x2F;rationale.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;mattmahoney.net&#x2F;dc&#x2F;rationale.html</a><p>...which leads to the theory of mind known as &quot;compressionism&quot;:<p><a href="https:&#x2F;&#x2F;ceur-ws.org&#x2F;Vol-1419&#x2F;paper0045.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;ceur-ws.org&#x2F;Vol-1419&#x2F;paper0045.pdf</a></div><br/><div id="37153484" class="c"><input type="checkbox" id="c-37153484" checked=""/><div class="controls bullet"><span class="by">tysam_and</span><span>|</span><a href="#37153139">parent</a><span>|</span><a href="#37156764">next</a><span>|</span><label class="collapse" for="c-37153484">[-]</label><label class="expand" for="c-37153484">[5 more]</label></div><br/><div class="children"><div class="content">No. Compression == information, not intelligence. This is a common mistake that people make, and is a popular expression making the rounds right now, but it is incorrect.<p>Intelligence _may arise_ from information. Information _necessarily arises_ from compression.<p>It is very similar, to me, to the concept of humors (<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Humorism" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Humorism</a>). Empirically, it was observed that these things had some correlation, and they occurred together, but they are not directly causative. Similarly with the theory of spontaneous generation (<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Spontaneous_generation" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Spontaneous_generation</a>), which was another theory spawned (heh, as it were), from casual causal correlation (again, as it were).<p>This can be shown rather easily when you show that the time-dynamics of an exemplar system with high information diverge from the time dynamics of a system with high intelligence, i.e. there is a structural component to the information embedded in the system such that the temporal aspects of applied information are somehow necessary for intelligence.<p>I hope to release some work at least tangentially related to this within the next few years (though of course it is a bit more high-flying and depends on some other in-flight work). If we are to attempt to move towards AGI, I really think we need to stick with the mathematical basics. Neural networks, although they&#x27;ve been made out to be really complicated, are actually quite simple in terms of the concepts powering them, I believe. That&#x27;s something I&#x27;m currently working on putting together and trying to distill to its essence. That will also be at least 1-2 years out, and likely before any temporally-related work, all going well. In my experience, this all is just a personal belief from a great deal of time and thought with the minutia of neural networks. That said, I could perhaps be biased with some notion of simplicity, since I&#x27;ve worked with them long enough for some concepts to feel comfortably second-nature.</div><br/><div id="37154836" class="c"><input type="checkbox" id="c-37154836" checked=""/><div class="controls bullet"><span class="by">derefr</span><span>|</span><a href="#37153139">root</a><span>|</span><a href="#37153484">parent</a><span>|</span><a href="#37154219">next</a><span>|</span><label class="collapse" for="c-37154836">[-]</label><label class="expand" for="c-37154836">[2 more]</label></div><br/><div class="children"><div class="content">I think what the GP meant is not that <i>compressed data</i> has intelligence, but rather that a <i>compressor</i> is necessarily &quot;intelligent&quot; in correlation to the degree that it compresses well on arbitrary novel datasets it wasn&#x27;t tuned for. At least insofar as we define &quot;intelligence&quot; with measures like IQ, that relate to &quot;number of examples of a pattern required to notice the pattern.&quot;<p>To compress data, you have to derive some interesting insight about that data that allows the data to be projected into a lower-dimensional, lower-entropy embedding. (In pop-neurological terms, you have to &quot;make a neuron&quot; that represents a concept, so that you can re-phrase your dataset into a star-schema expressed in its connections to that concept.)<p>The more <i>different kinds of things</i> a single compressor can do that for — without the compressor itself being made larger — the more &quot;intelligent&quot; the compressor is, no?</div><br/><div id="37155108" class="c"><input type="checkbox" id="c-37155108" checked=""/><div class="controls bullet"><span class="by">tysam_and</span><span>|</span><a href="#37153139">root</a><span>|</span><a href="#37154836">parent</a><span>|</span><a href="#37154219">next</a><span>|</span><label class="collapse" for="c-37155108">[-]</label><label class="expand" for="c-37155108">[1 more]</label></div><br/><div class="children"><div class="content">Yes, that is the point I&#x27;m addressing, I&#x27;m using Shannon&#x27;s definition of information and making an argument about the _temporal_ aspects of it. If you&#x27;re looking for a hint, it&#x27;s in the Lyapunov exponents of the system, which do not come necessarily from compression alone, though compression I feel is a critical part of the process.<p>Hence, the set vs superset argument.</div><br/></div></div></div></div><div id="37154219" class="c"><input type="checkbox" id="c-37154219" checked=""/><div class="controls bullet"><span class="by">bbstats</span><span>|</span><a href="#37153139">root</a><span>|</span><a href="#37153484">parent</a><span>|</span><a href="#37154836">prev</a><span>|</span><a href="#37156764">next</a><span>|</span><label class="collapse" for="c-37154219">[-]</label><label class="expand" for="c-37154219">[2 more]</label></div><br/><div class="children"><div class="content">OTOH true intelligence can still poorly compress things.</div><br/><div id="37154233" class="c"><input type="checkbox" id="c-37154233" checked=""/><div class="controls bullet"><span class="by">tysam_and</span><span>|</span><a href="#37153139">root</a><span>|</span><a href="#37154219">parent</a><span>|</span><a href="#37156764">next</a><span>|</span><label class="collapse" for="c-37154233">[-]</label><label class="expand" for="c-37154233">[1 more]</label></div><br/><div class="children"><div class="content">Any ERM-based method will have this flaw (if it is a flaw -- I&#x27;d possibly consider it a tradeoff) -- it is the nature of compression itself. I would make the argument, I think, that I believe that this disrupts the &#x27;information&#x27; layer of the hierarchy, not the &#x27;intelligence&#x27;, though that is just my personal 2 cents of course.</div><br/></div></div></div></div></div></div><div id="37156764" class="c"><input type="checkbox" id="c-37156764" checked=""/><div class="controls bullet"><span class="by">canjobear</span><span>|</span><a href="#37153139">parent</a><span>|</span><a href="#37153484">prev</a><span>|</span><a href="#37153345">next</a><span>|</span><label class="collapse" for="c-37156764">[-]</label><label class="expand" for="c-37156764">[1 more]</label></div><br/><div class="children"><div class="content">Compression is equivalent to learning but it&#x27;s not clear to me that that gets you all the way to intelligent action. In reinforcement learning terms, it can get you asymptotically to a perfect model of the environment, but it&#x27;s not clear to me that it would tell you how to find the highest-value action in that environment.</div><br/></div></div><div id="37153345" class="c"><input type="checkbox" id="c-37153345" checked=""/><div class="controls bullet"><span class="by">ks2048</span><span>|</span><a href="#37153139">parent</a><span>|</span><a href="#37156764">prev</a><span>|</span><a href="#37153591">next</a><span>|</span><label class="collapse" for="c-37153345">[-]</label><label class="expand" for="c-37153345">[10 more]</label></div><br/><div class="children"><div class="content">I would say compression and intelligence are closely related, not equal. For example, zstd is a pretty good compressor, but does any one think it should be called a pretty good “AI”?</div><br/><div id="37154206" class="c"><input type="checkbox" id="c-37154206" checked=""/><div class="controls bullet"><span class="by">londons_explore</span><span>|</span><a href="#37153139">root</a><span>|</span><a href="#37153345">parent</a><span>|</span><a href="#37153398">next</a><span>|</span><label class="collapse" for="c-37154206">[-]</label><label class="expand" for="c-37154206">[4 more]</label></div><br/><div class="children"><div class="content">Read the paper...<p>Zstd, while it might be better than gzip or bzip, is still a very poor compressor compared to an ideal compressor (which hasn&#x27;t yet been discovered).<p>That is why zstd acts like a rather bad AI.     Note that if you wanted to use zstd as an AI, you would patch out of the source code checksum checks, and you would then feed it a file to decompress (The cat sat on the mat), followed by a few bytes of random noise.<p>A great compressor would output:  The cat sat on the mat.  It was comfortable, so he then lay down to sleep.<p>A medium compressor would output:   The cat sat on the mat. bat cat cat mat sat bat.<p>A terrible compressor would output:  The cat sat on the mat.  D7s&quot;&#x2F;r %we<p>See how each is using knowledge at different levels to generate a completion.   Notice also how that few bytes generates different amounts of output depending on the compressors level of world understanding, and therefore compression ratio.</div><br/><div id="37156751" class="c"><input type="checkbox" id="c-37156751" checked=""/><div class="controls bullet"><span class="by">canjobear</span><span>|</span><a href="#37153139">root</a><span>|</span><a href="#37154206">parent</a><span>|</span><a href="#37154383">next</a><span>|</span><label class="collapse" for="c-37156751">[-]</label><label class="expand" for="c-37156751">[1 more]</label></div><br/><div class="children"><div class="content">Concretely gzip will output something like &quot;theudcanvas. ;cm,zumhmcyoetter toauuo long a one aay,;wvbu.mvns. x the dtls and enso.;k.like bla.njv&quot;<p><a href="https:&#x2F;&#x2F;github.com&#x2F;Futrell&#x2F;ziplm">https:&#x2F;&#x2F;github.com&#x2F;Futrell&#x2F;ziplm</a></div><br/></div></div><div id="37154383" class="c"><input type="checkbox" id="c-37154383" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#37153139">root</a><span>|</span><a href="#37154206">parent</a><span>|</span><a href="#37156751">prev</a><span>|</span><a href="#37153398">next</a><span>|</span><label class="collapse" for="c-37154383">[-]</label><label class="expand" for="c-37154383">[2 more]</label></div><br/><div class="children"><div class="content">&gt; A terrible compressor would output: The cat sat on the mat. Dsr %we3 9T23 }£{D:rg!@ !jv£dP$<p>LLMs are sort of unable to do this because they use a fixed tokenizer instead of raw bytes. That means they won&#x27;t output binary garbage even early on + saves a lot of memory, but it may hurt learning things like capitalization, rhyming, etc we think are obvious.</div><br/><div id="37154480" class="c"><input type="checkbox" id="c-37154480" checked=""/><div class="controls bullet"><span class="by">londons_explore</span><span>|</span><a href="#37153139">root</a><span>|</span><a href="#37154383">parent</a><span>|</span><a href="#37153398">next</a><span>|</span><label class="collapse" for="c-37154480">[-]</label><label class="expand" for="c-37154480">[1 more]</label></div><br/><div class="children"><div class="content">Even if you trained an LLM with a simplified tokenizer that simply had 256 tokens for each of 256 possible ascii characters, you would see the same result.</div><br/></div></div></div></div></div></div><div id="37153398" class="c"><input type="checkbox" id="c-37153398" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#37153139">root</a><span>|</span><a href="#37153345">parent</a><span>|</span><a href="#37154206">prev</a><span>|</span><a href="#37153878">next</a><span>|</span><label class="collapse" for="c-37153398">[-]</label><label class="expand" for="c-37153398">[4 more]</label></div><br/><div class="children"><div class="content">Wasn&#x27;t there a paper recently about a fairly simple wrapper around a common conpression algorithm beating LLMs on classification tasks?</div><br/><div id="37153599" class="c"><input type="checkbox" id="c-37153599" checked=""/><div class="controls bullet"><span class="by">jmholla</span><span>|</span><a href="#37153139">root</a><span>|</span><a href="#37153398">parent</a><span>|</span><a href="#37153878">next</a><span>|</span><label class="collapse" for="c-37153599">[-]</label><label class="expand" for="c-37153599">[3 more]</label></div><br/><div class="children"><div class="content">I thought that was about LLMs being trained on compressed data. But I might be thinking about a different paper.</div><br/><div id="37153684" class="c"><input type="checkbox" id="c-37153684" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#37153139">root</a><span>|</span><a href="#37153599">parent</a><span>|</span><a href="#37153878">next</a><span>|</span><label class="collapse" for="c-37153684">[-]</label><label class="expand" for="c-37153684">[2 more]</label></div><br/><div class="children"><div class="content">Gzip + kNN for text classification:<p><a href="https:&#x2F;&#x2F;aclanthology.org&#x2F;2023.findings-acl.426.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;aclanthology.org&#x2F;2023.findings-acl.426.pdf</a></div><br/><div id="37154082" class="c"><input type="checkbox" id="c-37154082" checked=""/><div class="controls bullet"><span class="by">junipertea</span><span>|</span><a href="#37153139">root</a><span>|</span><a href="#37153684">parent</a><span>|</span><a href="#37153878">next</a><span>|</span><label class="collapse" for="c-37154082">[-]</label><label class="expand" for="c-37154082">[1 more]</label></div><br/><div class="children"><div class="content">Not LLM, just BERT, also did not actually outperform it.<p>source: <a href="https:&#x2F;&#x2F;kenschutte.com&#x2F;gzip-knn-paper&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;kenschutte.com&#x2F;gzip-knn-paper&#x2F;</a></div><br/></div></div></div></div></div></div></div></div><div id="37153878" class="c"><input type="checkbox" id="c-37153878" checked=""/><div class="controls bullet"><span class="by">willis936</span><span>|</span><a href="#37153139">root</a><span>|</span><a href="#37153345">parent</a><span>|</span><a href="#37153398">prev</a><span>|</span><a href="#37153591">next</a><span>|</span><label class="collapse" for="c-37153878">[-]</label><label class="expand" for="c-37153878">[1 more]</label></div><br/><div class="children"><div class="content">Someone in sales with a product that uses zstd.</div><br/></div></div></div></div><div id="37153591" class="c"><input type="checkbox" id="c-37153591" checked=""/><div class="controls bullet"><span class="by">YeGoblynQueenne</span><span>|</span><a href="#37153139">parent</a><span>|</span><a href="#37153345">prev</a><span>|</span><a href="#37153548">next</a><span>|</span><label class="collapse" for="c-37153591">[-]</label><label class="expand" for="c-37153591">[1 more]</label></div><br/><div class="children"><div class="content">&gt;&gt; Reminder that compression = intelligence:<p>If I conclude from this that you mean that gzip is intelligent, will I be accused of bad faith?</div><br/></div></div><div id="37153548" class="c"><input type="checkbox" id="c-37153548" checked=""/><div class="controls bullet"><span class="by">burtonator</span><span>|</span><a href="#37153139">parent</a><span>|</span><a href="#37153591">prev</a><span>|</span><a href="#37154194">next</a><span>|</span><label class="collapse" for="c-37153548">[-]</label><label class="expand" for="c-37153548">[1 more]</label></div><br/><div class="children"><div class="content">There must be a relationship here between Shannon&#x27;s estimation that english is 1 bit of entropy per character and is highly redundant and &#x27;easy&#x27; to predict.<p>A highly advanced AI could compress the text and predict the next sequences easily.<p>This seems like a direct connection like electricity and magnetism.<p>And maybe that&#x27;s why English needs to be about 1 bit because we&#x27;re not very intelligent.</div><br/></div></div><div id="37154194" class="c"><input type="checkbox" id="c-37154194" checked=""/><div class="controls bullet"><span class="by">bbstats</span><span>|</span><a href="#37153139">parent</a><span>|</span><a href="#37153548">prev</a><span>|</span><a href="#37154589">next</a><span>|</span><label class="collapse" for="c-37154194">[-]</label><label class="expand" for="c-37154194">[1 more]</label></div><br/><div class="children"><div class="content">I would say compression has some congruency with intelligence.</div><br/></div></div></div></div><div id="37154669" class="c"><input type="checkbox" id="c-37154669" checked=""/><div class="controls bullet"><span class="by">hn_throwaway_99</span><span>|</span><a href="#37153139">prev</a><span>|</span><a href="#37154201">next</a><span>|</span><label class="collapse" for="c-37154669">[-]</label><label class="expand" for="c-37154669">[4 more]</label></div><br/><div class="children"><div class="content">My hope is that one day I&#x27;ll be reincarnated and the new me will produce 1&#x2F;1000 of the output of Fabrice Bellard. Whenever I see new posts from him I just think &quot;<i>How</i> can he do so much?&quot;</div><br/><div id="37157544" class="c"><input type="checkbox" id="c-37157544" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#37154669">parent</a><span>|</span><a href="#37154703">next</a><span>|</span><label class="collapse" for="c-37157544">[-]</label><label class="expand" for="c-37157544">[1 more]</label></div><br/><div class="children"><div class="content">Because we believe in &quot;I did this this big thing X, that computer scientists love, which is public&quot; &gt; &quot;I worked on a team and contributed to big thing Y, which is profitable but meh, and is private&quot;</div><br/></div></div><div id="37154703" class="c"><input type="checkbox" id="c-37154703" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#37154669">parent</a><span>|</span><a href="#37157544">prev</a><span>|</span><a href="#37154727">next</a><span>|</span><label class="collapse" for="c-37154703">[-]</label><label class="expand" for="c-37154703">[1 more]</label></div><br/><div class="children"><div class="content">The SI unit for productivity is measured in millibellards</div><br/></div></div></div></div><div id="37154201" class="c"><input type="checkbox" id="c-37154201" checked=""/><div class="controls bullet"><span class="by">ducktective</span><span>|</span><a href="#37154669">prev</a><span>|</span><label class="collapse" for="c-37154201">[-]</label><label class="expand" for="c-37154201">[3 more]</label></div><br/><div class="children"><div class="content">Man fears AGI,
AGI fears Fabrice Bellard.</div><br/><div id="37154977" class="c"><input type="checkbox" id="c-37154977" checked=""/><div class="controls bullet"><span class="by">optimalsolver</span><span>|</span><a href="#37154201">parent</a><span>|</span><a href="#37154598">next</a><span>|</span><label class="collapse" for="c-37154977">[-]</label><label class="expand" for="c-37154977">[1 more]</label></div><br/><div class="children"><div class="content">Woman inherits the Earth.</div><br/></div></div></div></div></div></div></div></div></div></body></html>