<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1694163660314" as="style"/><link rel="stylesheet" href="styles.css?v=1694163660314"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/samvher/bert-for-laptops/blob/main/BERT_for_laptops.ipynb">A BERT for laptops, from scratch</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>samvher</span> | <span>38 comments</span></div><br/><div><div id="37425131" class="c"><input type="checkbox" id="c-37425131" checked=""/><div class="controls bullet"><span class="by">samvher</span><span>|</span><a href="#37426395">next</a><span>|</span><label class="collapse" for="c-37425131">[-]</label><label class="expand" for="c-37425131">[14 more]</label></div><br/><div class="children"><div class="content">I built and trained a BERT on my gaming laptop (3070 RTX) to ~94% of BERT-base&#x27;s performance in ~17 hours* (BERT-base was trained on 4 TPUs for 4 days). This notebook goes over the whole process, from implementing and training a tokenizer, to pretraining, to finetuning. One feature that makes this BERT different from most (though not unique) is the use of relative position embeddings.<p>Edit - for anyone unsure about what &quot;BERT&quot; is or its relevance, it&#x27;s a transformer based natural language model just like GPT. However, where GPT is used to generate text, BERT is used to generate embeddings for input text that you can then use for predictive models (e.g. sentiment prediction), and that process is also demonstrated in the notebook.<p>*Edit 2 - The 17 hours are pretraining only, not including the time to train the tokenizer, or finetuning.</div><br/><div id="37428028" class="c"><input type="checkbox" id="c-37428028" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#37425131">parent</a><span>|</span><a href="#37426543">next</a><span>|</span><label class="collapse" for="c-37428028">[-]</label><label class="expand" for="c-37428028">[3 more]</label></div><br/><div class="children"><div class="content">&gt; it&#x27;s a transformer based natural language model just like GPT<p>its an encoder-decoder model whereas GPT is decoder only. feels like a pretty big difference, though in practice i honestly still dont have a strong grasp of how encoder-decoder is deficient to decoder-only when it comes to text generation. i get that BERT was designed for translation but why cant we scale it up and use it for textgen just the same</div><br/><div id="37429857" class="c"><input type="checkbox" id="c-37429857" checked=""/><div class="controls bullet"><span class="by">rolisz</span><span>|</span><a href="#37425131">root</a><span>|</span><a href="#37428028">parent</a><span>|</span><a href="#37428564">next</a><span>|</span><label class="collapse" for="c-37429857">[-]</label><label class="expand" for="c-37429857">[1 more]</label></div><br/><div class="children"><div class="content">BERT is encoder only and was designed for classification and natural language inference problems. The original Transformer was encoder-decoder and was designed for translation.<p>BERT can&#x27;t be used in an autoregressive way because it doesn&#x27;t output a new token, it simply generates embeddings from the existing tokens (you get one for each input token).</div><br/></div></div><div id="37428564" class="c"><input type="checkbox" id="c-37428564" checked=""/><div class="controls bullet"><span class="by">abhijitr</span><span>|</span><a href="#37425131">root</a><span>|</span><a href="#37428028">parent</a><span>|</span><a href="#37429857">prev</a><span>|</span><a href="#37426543">next</a><span>|</span><label class="collapse" for="c-37428564">[-]</label><label class="expand" for="c-37428564">[1 more]</label></div><br/><div class="children"><div class="content">BERT is encoder only..</div><br/></div></div></div></div><div id="37426543" class="c"><input type="checkbox" id="c-37426543" checked=""/><div class="controls bullet"><span class="by">amelius</span><span>|</span><a href="#37425131">parent</a><span>|</span><a href="#37428028">prev</a><span>|</span><a href="#37425915">next</a><span>|</span><label class="collapse" for="c-37426543">[-]</label><label class="expand" for="c-37426543">[3 more]</label></div><br/><div class="children"><div class="content">&gt; where GPT is used to generate text, BERT is used to generate embeddings for input text that you can then use for predictive models (e.g. sentiment prediction)<p>Ok, but isn&#x27;t text generation more general? E.g. you could ask it to predict the sentiment of a sentence and write the result as a sentence?</div><br/><div id="37430888" class="c"><input type="checkbox" id="c-37430888" checked=""/><div class="controls bullet"><span class="by">extasia</span><span>|</span><a href="#37425131">root</a><span>|</span><a href="#37426543">parent</a><span>|</span><a href="#37427151">next</a><span>|</span><label class="collapse" for="c-37430888">[-]</label><label class="expand" for="c-37430888">[1 more]</label></div><br/><div class="children"><div class="content">BERT is ~100m parameters. GPT3.5 is 1000x bigger. A Bert sized decoder model would likely generate nonsense<p>For most applications BERTs output is used to fine tune additional NN layers, eg for text classification</div><br/></div></div><div id="37427151" class="c"><input type="checkbox" id="c-37427151" checked=""/><div class="controls bullet"><span class="by">samvher</span><span>|</span><a href="#37425131">root</a><span>|</span><a href="#37426543">parent</a><span>|</span><a href="#37430888">prev</a><span>|</span><a href="#37425915">next</a><span>|</span><label class="collapse" for="c-37427151">[-]</label><label class="expand" for="c-37427151">[1 more]</label></div><br/><div class="children"><div class="content">Yeah my explanation was definitely a lossy summary. You can do similar things with GPT, but BERT is bidirectional, so for a given token it can take into account both tokens before and after it. GPT would only take into account tokens before it. Looking both ways can be helpful. Another comment in this thread explains the same (maybe clearer).</div><br/></div></div></div></div><div id="37425915" class="c"><input type="checkbox" id="c-37425915" checked=""/><div class="controls bullet"><span class="by">amrrs</span><span>|</span><a href="#37425131">parent</a><span>|</span><a href="#37426543">prev</a><span>|</span><a href="#37425854">next</a><span>|</span><label class="collapse" for="c-37425915">[-]</label><label class="expand" for="c-37425915">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m sorry if it&#x27;s off-topic but did the laptop hold up well? Did you have any special cooling mechanism?</div><br/><div id="37425958" class="c"><input type="checkbox" id="c-37425958" checked=""/><div class="controls bullet"><span class="by">samvher</span><span>|</span><a href="#37425131">root</a><span>|</span><a href="#37425915">parent</a><span>|</span><a href="#37425854">next</a><span>|</span><label class="collapse" for="c-37425958">[-]</label><label class="expand" for="c-37425958">[3 more]</label></div><br/><div class="children"><div class="content">Haha fair question. I didn&#x27;t make any special changes, I just left the lid open and put the laptop in a ventilated spot. I&#x27;m actually in the tropics, so I guess Lenovo scores some points here (the laptop is a Legion 5 Pro).</div><br/><div id="37426148" class="c"><input type="checkbox" id="c-37426148" checked=""/><div class="controls bullet"><span class="by">gerdesj</span><span>|</span><a href="#37425131">root</a><span>|</span><a href="#37425958">parent</a><span>|</span><a href="#37425854">next</a><span>|</span><label class="collapse" for="c-37426148">[-]</label><label class="expand" for="c-37426148">[2 more]</label></div><br/><div class="children"><div class="content">If you want to really cook your lap, try running Gentoo!  Emerging (compiling) Firefox, glibc, gcc, Libre office and a few of their friends will soon show you how good the cooling is.<p>A few years back <i>cough</i> I upgraded gcc from 3 to 4 and emerged system and then world.  That was over 1200 packages.  It took about a week.  That was in the days when I used a Windows wifi driver and some unholy magic to get a connection.  I parked the laptop on a table with the lid open and two metal rods lifting it up 6&quot; for airflow.  I left the nearby window open a bit too.</div><br/><div id="37428917" class="c"><input type="checkbox" id="c-37428917" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#37425131">root</a><span>|</span><a href="#37426148">parent</a><span>|</span><a href="#37425854">next</a><span>|</span><label class="collapse" for="c-37428917">[-]</label><label class="expand" for="c-37428917">[1 more]</label></div><br/><div class="children"><div class="content">Going off on a tangent: I used to use Gentoo in the past.  I suspect, if you use one of the common processors, compiling your own binaries doesn&#x27;t really give you any performance benefits, does it?<p>(I have to admit I stopped using Gentoo mostly because it encouraged me to endlessly fiddle with my system, and it would invariably end up broken somehow.  That&#x27;s entirely my fault, and not Gentoo&#x27;s.  I switched to Archlinux as my distribution of choice, and I manage to hold myself back enough not to destroy my installation.)</div><br/></div></div></div></div></div></div></div></div><div id="37425854" class="c"><input type="checkbox" id="c-37425854" checked=""/><div class="controls bullet"><span class="by">unixhero</span><span>|</span><a href="#37425131">parent</a><span>|</span><a href="#37425915">prev</a><span>|</span><a href="#37426395">next</a><span>|</span><label class="collapse" for="c-37425854">[-]</label><label class="expand" for="c-37425854">[3 more]</label></div><br/><div class="children"><div class="content">Do you wish to share your source code with the community?</div><br/><div id="37425879" class="c"><input type="checkbox" id="c-37425879" checked=""/><div class="controls bullet"><span class="by">samvher</span><span>|</span><a href="#37425131">root</a><span>|</span><a href="#37425854">parent</a><span>|</span><a href="#37426395">next</a><span>|</span><label class="collapse" for="c-37425879">[-]</label><label class="expand" for="c-37425879">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s all behind the submission link! I&#x27;ve set it up so that you can run it start to end, if you want. The only thing I&#x27;m not 100% sure about is resource requirements - I have an 8GB GPU and 32GB of RAM, it could be that if you have less than that you&#x27;d run into out of memory errors. Those would be fairly straightforward to fix, though (honestly I&#x27;d be happy to help if someone runs into this).</div><br/><div id="37425907" class="c"><input type="checkbox" id="c-37425907" checked=""/><div class="controls bullet"><span class="by">unixhero</span><span>|</span><a href="#37425131">root</a><span>|</span><a href="#37425879">parent</a><span>|</span><a href="#37426395">next</a><span>|</span><label class="collapse" for="c-37425907">[-]</label><label class="expand" for="c-37425907">[1 more]</label></div><br/><div class="children"><div class="content">Thanks! Cool, no worries, I have more onboard gpu memory avail</div><br/></div></div></div></div></div></div></div></div><div id="37426395" class="c"><input type="checkbox" id="c-37426395" checked=""/><div class="controls bullet"><span class="by">ceph_</span><span>|</span><a href="#37425131">prev</a><span>|</span><a href="#37427310">next</a><span>|</span><label class="collapse" for="c-37426395">[-]</label><label class="expand" for="c-37426395">[12 more]</label></div><br/><div class="children"><div class="content">Since the article never defines what a BERT is:<p>(Bidirectional Encoder Representations from Transformers)<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;BERT_(language_model)" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;BERT_(language_model)</a></div><br/><div id="37426674" class="c"><input type="checkbox" id="c-37426674" checked=""/><div class="controls bullet"><span class="by">heliophobicdude</span><span>|</span><a href="#37426395">parent</a><span>|</span><a href="#37427933">next</a><span>|</span><label class="collapse" for="c-37426674">[-]</label><label class="expand" for="c-37426674">[8 more]</label></div><br/><div class="children"><div class="content">The biggest distinction in architecture between BERT and GPT is that BERT looks both ways from a given token. This helps give context to a token. This is what made BERT great at the time because the surrounding text, before and after, could change the meaning of the token we are at. You could essentially fill in the middle, or rather correct what&#x27;s in the middle after it&#x27;s been said. I believe this is why Apple is using it for iOS 17&#x27;s auto-correct.<p>GPT predicts the next word by only look back at what we have seen so far. In other words, it&#x27;s auto regressive.</div><br/><div id="37426773" class="c"><input type="checkbox" id="c-37426773" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#37426395">root</a><span>|</span><a href="#37426674">parent</a><span>|</span><a href="#37428677">next</a><span>|</span><label class="collapse" for="c-37426773">[-]</label><label class="expand" for="c-37426773">[4 more]</label></div><br/><div class="children"><div class="content">Remember not to confuse interface with implementation. GPT&#x27;s interface is a single stream of tokens - so if you want it to see before and after context, that just means you have to encode them into a single stream.</div><br/><div id="37427936" class="c"><input type="checkbox" id="c-37427936" checked=""/><div class="controls bullet"><span class="by">huac</span><span>|</span><a href="#37426395">root</a><span>|</span><a href="#37426773">parent</a><span>|</span><a href="#37428677">next</a><span>|</span><label class="collapse" for="c-37427936">[-]</label><label class="expand" for="c-37427936">[3 more]</label></div><br/><div class="children"><div class="content">this comment is downvoted but it is correct. while fill-in-the-middle is less obvious in the decoder-only paradigm, it is still possible. one example is code-llama <a href="https:&#x2F;&#x2F;ai.meta.com&#x2F;blog&#x2F;code-llama-large-language-model-coding&#x2F;-" rel="nofollow noreferrer">https:&#x2F;&#x2F;ai.meta.com&#x2F;blog&#x2F;code-llama-large-language-model-cod...</a> it is a variant of llama 2 (GPT-style decoder-only) but it supports infilling<p>&gt; [W]e split training documents at the character level into a prefix, a middle part[,] and a suffix with the splitting locations sampled independently from a uniform distribution over the document length. We apply this transformation with a probability of 0.9 and to documents that are not cut across multiple model contexts only. We randomly format half of the splits in the prefix-suffix-middle (PSM) format and the other half in the compatible suffix-prefix-middle (SPM) format described in Bavarian et al. (2022, App. D). We extend Llama 2’s tokenizer with four special tokens that mark the beginning of the prefix, the middle part or the suffix, and the end of the infilling span<p>Tokens are super powerful :)</div><br/><div id="37428056" class="c"><input type="checkbox" id="c-37428056" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#37426395">root</a><span>|</span><a href="#37427936">parent</a><span>|</span><a href="#37428677">next</a><span>|</span><label class="collapse" for="c-37428056">[-]</label><label class="expand" for="c-37428056">[2 more]</label></div><br/><div class="children"><div class="content">yes but code llama also found that the PSM format was inferior to the SPM format presumably because those hard cuts lose context. the &quot;real&quot; fill-in-the-middle of BERT is i think more likely to model language compared to the &quot;faux&quot; F-i-t-m of flinging prefixes and suffixes around</div><br/><div id="37429146" class="c"><input type="checkbox" id="c-37429146" checked=""/><div class="controls bullet"><span class="by">huac</span><span>|</span><a href="#37426395">root</a><span>|</span><a href="#37428056">parent</a><span>|</span><a href="#37428677">next</a><span>|</span><label class="collapse" for="c-37429146">[-]</label><label class="expand" for="c-37429146">[1 more]</label></div><br/><div class="children"><div class="content">where is that reported? in Table 14 I see PSM performing much better than SPM. I also see a note about the SPM performance which attributes the degradation to the tokenizer edge cases<p>&gt;  As an example, our model would complete the string &#x27;enu&#x27; with &#x27;emrate&#x27; instead of &#x27;merate&#x27; which shows awareness of the logical situation of the code but incomplete understanding of how tokens map to character-level spelling.<p>that doesn&#x27;t really feel like a failure of language modeling to me</div><br/></div></div></div></div></div></div></div></div><div id="37428677" class="c"><input type="checkbox" id="c-37428677" checked=""/><div class="controls bullet"><span class="by">fbdab103</span><span>|</span><a href="#37426395">root</a><span>|</span><a href="#37426674">parent</a><span>|</span><a href="#37426773">prev</a><span>|</span><a href="#37429738">next</a><span>|</span><label class="collapse" for="c-37428677">[-]</label><label class="expand" for="c-37428677">[2 more]</label></div><br/><div class="children"><div class="content">&gt;I believe this is why Apple is using it for iOS 17&#x27;s auto-correct.<p>Talk about a negative endorsement. I am continually disappointed in the auto correction implementation.</div><br/><div id="37430044" class="c"><input type="checkbox" id="c-37430044" checked=""/><div class="controls bullet"><span class="by">pilotneko</span><span>|</span><a href="#37426395">root</a><span>|</span><a href="#37428677">parent</a><span>|</span><a href="#37429738">next</a><span>|</span><label class="collapse" for="c-37430044">[-]</label><label class="expand" for="c-37430044">[1 more]</label></div><br/><div class="children"><div class="content">To be fair, iOS 17 is not out yet. If you are currently using it, you are beta testing it.</div><br/></div></div></div></div><div id="37429738" class="c"><input type="checkbox" id="c-37429738" checked=""/><div class="controls bullet"><span class="by">3abiton</span><span>|</span><a href="#37426395">root</a><span>|</span><a href="#37426674">parent</a><span>|</span><a href="#37428677">prev</a><span>|</span><a href="#37427933">next</a><span>|</span><label class="collapse" for="c-37429738">[-]</label><label class="expand" for="c-37429738">[1 more]</label></div><br/><div class="children"><div class="content">Aren&#x27;t both of them based on embeddings?</div><br/></div></div></div></div><div id="37427933" class="c"><input type="checkbox" id="c-37427933" checked=""/><div class="controls bullet"><span class="by">willis936</span><span>|</span><a href="#37426395">parent</a><span>|</span><a href="#37426674">prev</a><span>|</span><a href="#37427310">next</a><span>|</span><label class="collapse" for="c-37427933">[-]</label><label class="expand" for="c-37427933">[3 more]</label></div><br/><div class="children"><div class="content">I find the acronym unfortunate because it overloads a previously unique acronym in an adjacent space.  It costs nothing to avoid these issues.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Bit_error_rate#Bit_error_rate_tester" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Bit_error_rate#Bit_error_rate_...</a></div><br/><div id="37430734" class="c"><input type="checkbox" id="c-37430734" checked=""/><div class="controls bullet"><span class="by">ooterness</span><span>|</span><a href="#37426395">root</a><span>|</span><a href="#37427933">parent</a><span>|</span><a href="#37428634">next</a><span>|</span><label class="collapse" for="c-37430734">[-]</label><label class="expand" for="c-37430734">[1 more]</label></div><br/><div class="children"><div class="content">Yes, I visited this thread hoping for BERT software that can find memory errors, either due to radiation upset or signal integrity issues.<p>Now my disappointment is immeassurable and my day is ruined.</div><br/></div></div><div id="37428634" class="c"><input type="checkbox" id="c-37428634" checked=""/><div class="controls bullet"><span class="by">xpe</span><span>|</span><a href="#37426395">root</a><span>|</span><a href="#37427933">parent</a><span>|</span><a href="#37430734">prev</a><span>|</span><a href="#37427310">next</a><span>|</span><label class="collapse" for="c-37428634">[-]</label><label class="expand" for="c-37428634">[1 more]</label></div><br/><div class="children"><div class="content">&gt; It costs nothing to avoid these issues.<p>No. There are at least two kinds of costs. First, It takes time to search &#x27;adjacent&#x27; domains. Second, by reducing your available acronyms&#x2F;initialisms, you make it harder to map your architecture name onto those letters.<p>It is fun to think of some of the alternative BERT names that &quot;could have been&quot;, such as BIDET = BIDirectional Encoder representations from Transformers.</div><br/></div></div></div></div></div></div><div id="37427310" class="c"><input type="checkbox" id="c-37427310" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#37426395">prev</a><span>|</span><a href="#37426008">next</a><span>|</span><label class="collapse" for="c-37427310">[-]</label><label class="expand" for="c-37427310">[5 more]</label></div><br/><div class="children"><div class="content">This is a fantastic notebook, thanks very much for sharing.<p>&quot;If you want to run the full notebook on a full size model, expect training the tokenizer to take ~15 hours, pretraining with the MLM objective to take ~17 hours (on a 3070 RTX, adjust expectations for your own system), and finetuning to take about an hour&quot;<p>I wonder how hard it would be to modify this code to run on a 64GB M2 Mac.<p>It&#x27;s frustrating how much potential that platform has for this kind of thing (given the way the GPU shares memory with the CPU) that isn&#x27;t yet harnessed because most of the ecosystem is built around NVIDIA and CUDA.</div><br/><div id="37429241" class="c"><input type="checkbox" id="c-37429241" checked=""/><div class="controls bullet"><span class="by">Artgor</span><span>|</span><a href="#37427310">parent</a><span>|</span><a href="#37427859">next</a><span>|</span><label class="collapse" for="c-37429241">[-]</label><label class="expand" for="c-37429241">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I wonder how hard it would be to modify this code to run on a 64GB M2 Mac.<p>It isn&#x27;t that hard, I was able to run in on M1.
The changes are:<p>remove or modify multiprocessing - it doesn&#x27;t work on Mac the same way as in the code;<p>replace `device = &quot;cuda&quot;` with `device = &quot;mps&quot;`<p>In this line `        att_idxs = (torch.clamp(torch.arange(context_size)[None, :] - torch.arange(context_size)[:, None], -pos_emb_radius, pos_emb_radius-1) % pos_emb_size).to(&quot;cuda&quot;)` replace cuda with &quot;mps&quot;<p>in `optim.AdamW` remove `fused=True` - we can&#x27;t do it without CUDA<p>Replace 
```with autocast(device_type=&#x27;cuda&#x27;, dtype=torch.float16):
    _, loss = mlm_head(bert(batch_data_torch_xs[mb_start_idx:mb_end_idx]), batch_data_torch_ys[mb_start_idx:mb_end_idx])
```<p>with simply `_, loss = mlm_head(bert(batch_data_torch_xs[mb_start_idx:mb_end_idx]), batch_data_torch_ys[mb_start_idx:mb_end_idx])`<p>replace `scaler.scale(corrected_loss).backward()` with `corrected_loss.backward()`<p>replace
```
        scaler.unscale_(optimizer)
        scaler.step(optimizer)
        scaler.update()
        ```
with `optimizer.step()`<p>It should work.</div><br/></div></div><div id="37427859" class="c"><input type="checkbox" id="c-37427859" checked=""/><div class="controls bullet"><span class="by">smoldesu</span><span>|</span><a href="#37427310">parent</a><span>|</span><a href="#37429241">prev</a><span>|</span><a href="#37426008">next</a><span>|</span><label class="collapse" for="c-37427859">[-]</label><label class="expand" for="c-37427859">[3 more]</label></div><br/><div class="children"><div class="content">&gt; It&#x27;s frustrating how much potential that platform has for this kind of thing (given the way the GPU shares memory with the CPU) that isn&#x27;t yet harnessed because most of the ecosystem is built around NVIDIA and CUDA.<p>I&#x27;m sure it&#x27;s frustrating from a consumer perspective, but it should be no surprise why Nvidia won here. CUDA shipped unified memory addressing ten years before the M1 hit shelves. On top of that, their architecture and OS support is top-notch, you can ship your CUDA code on anything from a $250 Jetson to a $300,000 DGX system, and their hardware is relatively ubiquitous.<p>The frustrating thing is how companies like Apple and Nvidia insist on being each other&#x27;s enemies. Only consumers feel the pain when researchers discover cool stuff like this and want to share.</div><br/><div id="37431036" class="c"><input type="checkbox" id="c-37431036" checked=""/><div class="controls bullet"><span class="by">pmontra</span><span>|</span><a href="#37427310">root</a><span>|</span><a href="#37427859">parent</a><span>|</span><a href="#37426008">next</a><span>|</span><label class="collapse" for="c-37431036">[-]</label><label class="expand" for="c-37431036">[2 more]</label></div><br/><div class="children"><div class="content">You can add an Nvidia card to basically every kind of hardware, desktop, server or most importantly rent in the cloud. You must buy a Mac to use a M1&#x2F;M2. Given the cost of some cards it could make sense but then everybody using that software will have to buy a Mac too.</div><br/><div id="37431184" class="c"><input type="checkbox" id="c-37431184" checked=""/><div class="controls bullet"><span class="by">regularfry</span><span>|</span><a href="#37427310">root</a><span>|</span><a href="#37431036">parent</a><span>|</span><a href="#37426008">next</a><span>|</span><label class="collapse" for="c-37431184">[-]</label><label class="expand" for="c-37431184">[1 more]</label></div><br/><div class="children"><div class="content">A lot of people have M1s or M2s already, who who would have to pay for access to an NVidia card.  I think that&#x27;s the disconnect.  It&#x27;s more about making use of what a lot of people (and let&#x27;s not forget a lot of <i>developers</i>) already have.<p>I&#x27;ve personally got an 8GB M1 Macbook as my work development machine, and while I&#x27;m having a <i>lot</i> of fun with llama.cpp it does feel somewhat disconnected from the bulk of the ML ecosystem.</div><br/></div></div></div></div></div></div></div></div><div id="37426008" class="c"><input type="checkbox" id="c-37426008" checked=""/><div class="controls bullet"><span class="by">drtournier</span><span>|</span><a href="#37427310">prev</a><span>|</span><a href="#37426580">next</a><span>|</span><label class="collapse" for="c-37426008">[-]</label><label class="expand" for="c-37426008">[2 more]</label></div><br/><div class="children"><div class="content">This is awesome. Long time I wanted to pre-train a language model in Brazilian Portuguese and just got a 24GB 3090. This is the perfect timing. Thank you for sharing your notebook.</div><br/><div id="37426077" class="c"><input type="checkbox" id="c-37426077" checked=""/><div class="controls bullet"><span class="by">samvher</span><span>|</span><a href="#37426008">parent</a><span>|</span><a href="#37426580">next</a><span>|</span><label class="collapse" for="c-37426077">[-]</label><label class="expand" for="c-37426077">[1 more]</label></div><br/><div class="children"><div class="content">Nice! You&#x27;d get a long way just by swapping out the wikipedia dataset being pulled in. Though you might need to still accommodate for some special characters as well, I don&#x27;t know how important those are in Brazilian Portuguese.</div><br/></div></div></div></div><div id="37426580" class="c"><input type="checkbox" id="c-37426580" checked=""/><div class="controls bullet"><span class="by">rdedev</span><span>|</span><a href="#37426008">prev</a><span>|</span><a href="#37426792">next</a><span>|</span><label class="collapse" for="c-37426580">[-]</label><label class="expand" for="c-37426580">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been thinking about doing something like this but only inference and written in rust. I thought it might be a good excuse to learn SIMD but it seemed like a big undertaking so I&#x27;ve been putting it aside for a long time</div><br/></div></div><div id="37426792" class="c"><input type="checkbox" id="c-37426792" checked=""/><div class="controls bullet"><span class="by">SushiHippie</span><span>|</span><a href="#37426580">prev</a><span>|</span><a href="#37430878">next</a><span>|</span><label class="collapse" for="c-37426792">[-]</label><label class="expand" for="c-37426792">[2 more]</label></div><br/><div class="children"><div class="content">Thanks for sharing, one thing that I really like is that your code is very clean, formatted and commented. Thats very rare to see when people share their jupyter notebooks.</div><br/><div id="37428695" class="c"><input type="checkbox" id="c-37428695" checked=""/><div class="controls bullet"><span class="by">xpe</span><span>|</span><a href="#37426792">parent</a><span>|</span><a href="#37430878">next</a><span>|</span><label class="collapse" for="c-37428695">[-]</label><label class="expand" for="c-37428695">[1 more]</label></div><br/><div class="children"><div class="content">And even rarer when they don&#x27;t share them.</div><br/></div></div></div></div><div id="37430878" class="c"><input type="checkbox" id="c-37430878" checked=""/><div class="controls bullet"><span class="by">ShadowBanThis01</span><span>|</span><a href="#37426792">prev</a><span>|</span><label class="collapse" for="c-37430878">[-]</label><label class="expand" for="c-37430878">[1 more]</label></div><br/><div class="children"><div class="content">You left out the Q*.</div><br/></div></div></div></div></div></div></div></body></html>