<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1710752451890" as="style"/><link rel="stylesheet" href="styles.css?v=1710752451890"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/xai-org/grok">Grok</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>pierre</span> | <span>354 comments</span></div><br/><div><div id="39740558" class="c"><input type="checkbox" id="c-39740558" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#39737367">next</a><span>|</span><label class="collapse" for="c-39740558">[-]</label><label class="expand" for="c-39740558">[8 more]</label></div><br/><div class="children"><div class="content">Has anyone outside of x.ai actually done inference with this model yet? And if so, have they provided details of the hardware? What type of AWS instance or whatever?<p>I think you can rent like an 8 x A100 or 8 x H100 and it&#x27;s &quot;affordable&quot; to play around with for at least a few minutes. But you would need to know exactly how to set up the GPU cluster.<p>Because I doubt it&#x27;s as simple as just &#x27;python run.py&#x27; to get it going.</div><br/><div id="39740852" class="c"><input type="checkbox" id="c-39740852" checked=""/><div class="controls bullet"><span class="by">a_wild_dandan</span><span>|</span><a href="#39740558">parent</a><span>|</span><a href="#39740817">next</a><span>|</span><label class="collapse" for="c-39740852">[-]</label><label class="expand" for="c-39740852">[4 more]</label></div><br/><div class="children"><div class="content">Someone could run Grok-1 on a 192GB M2 Mac when a 4-bit quant is released; I&#x27;m guessing that TheBloke is already working on it.</div><br/><div id="39740878" class="c"><input type="checkbox" id="c-39740878" checked=""/><div class="controls bullet"><span class="by">mohu</span><span>|</span><a href="#39740558">root</a><span>|</span><a href="#39740852">parent</a><span>|</span><a href="#39741190">next</a><span>|</span><label class="collapse" for="c-39740878">[-]</label><label class="expand" for="c-39740878">[1 more]</label></div><br/><div class="children"><div class="content">Fairly sure the bloke hasn&#x27;t created any new quants in a month.</div><br/></div></div><div id="39741190" class="c"><input type="checkbox" id="c-39741190" checked=""/><div class="controls bullet"><span class="by">hanselot</span><span>|</span><a href="#39740558">root</a><span>|</span><a href="#39740852">parent</a><span>|</span><a href="#39740878">prev</a><span>|</span><a href="#39740817">next</a><span>|</span><label class="collapse" for="c-39741190">[-]</label><label class="expand" for="c-39741190">[2 more]</label></div><br/><div class="children"><div class="content">TheBloke dissapeared near the day <a href="https:&#x2F;&#x2F;nvd.nist.gov&#x2F;vuln&#x2F;detail&#x2F;CVE-2024-23496" rel="nofollow">https:&#x2F;&#x2F;nvd.nist.gov&#x2F;vuln&#x2F;detail&#x2F;CVE-2024-23496</a> was published.<p>Of course there has been much speculation on this, I have no more information than this that can be backed up by facts, but the timing was suspicious.</div><br/><div id="39741325" class="c"><input type="checkbox" id="c-39741325" checked=""/><div class="controls bullet"><span class="by">oezi</span><span>|</span><a href="#39740558">root</a><span>|</span><a href="#39741190">parent</a><span>|</span><a href="#39740817">next</a><span>|</span><label class="collapse" for="c-39741325">[-]</label><label class="expand" for="c-39741325">[1 more]</label></div><br/><div class="children"><div class="content">Was any .gguf file hosted on HuggingFace found to be crafted in a way to exploit this?</div><br/></div></div></div></div></div></div><div id="39740817" class="c"><input type="checkbox" id="c-39740817" checked=""/><div class="controls bullet"><span class="by">zone411</span><span>|</span><a href="#39740558">parent</a><span>|</span><a href="#39740852">prev</a><span>|</span><a href="#39737367">next</a><span>|</span><label class="collapse" for="c-39740817">[-]</label><label class="expand" for="c-39740817">[3 more]</label></div><br/><div class="children"><div class="content">If you&#x27;re just looking to test it out, it&#x27;s probably easiest to wait for llama.cpp to add support (<a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;issues&#x2F;6120">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;issues&#x2F;6120</a>), and then you can run it slowly if you have enough RAM, or wait for one of the inference API providers like together.ai to add it. I&#x27;d like to add it to my NYT Connections benchmarks, and that&#x27;s my plan (though it will require changing the prompt since it&#x27;s a base model, not a chat&#x2F;instruct model).</div><br/><div id="39741024" class="c"><input type="checkbox" id="c-39741024" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#39740558">root</a><span>|</span><a href="#39740817">parent</a><span>|</span><a href="#39737367">next</a><span>|</span><label class="collapse" for="c-39741024">[-]</label><label class="expand" for="c-39741024">[2 more]</label></div><br/><div class="children"><div class="content">&gt;it&#x27;s probably easiest<p>Cheapest maybe, but easiest is just to rent a p4de.24xlarge from AWS for a couple hours to test (at around $40&#x2F;hour..).</div><br/><div id="39741134" class="c"><input type="checkbox" id="c-39741134" checked=""/><div class="controls bullet"><span class="by">zone411</span><span>|</span><a href="#39740558">root</a><span>|</span><a href="#39741024">parent</a><span>|</span><a href="#39737367">next</a><span>|</span><label class="collapse" for="c-39741134">[-]</label><label class="expand" for="c-39741134">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d expect more configuration issues in getting it to run on them than from a tested llama.cpp version, since this doesn&#x27;t seem like a polished release. But maybe.</div><br/></div></div></div></div></div></div></div></div><div id="39737367" class="c"><input type="checkbox" id="c-39737367" checked=""/><div class="controls bullet"><span class="by">extheat</span><span>|</span><a href="#39740558">prev</a><span>|</span><a href="#39740901">next</a><span>|</span><label class="collapse" for="c-39737367">[-]</label><label class="expand" for="c-39737367">[45 more]</label></div><br/><div class="children"><div class="content">At 8x86B, looks like the largest open model yet by far. Would be interesting to hear how many tokens it&#x27;s been trained on. Especially important for higher param models in order to efficiently utilize all those parameters.</div><br/><div id="39737922" class="c"><input type="checkbox" id="c-39737922" checked=""/><div class="controls bullet"><span class="by">swalsh</span><span>|</span><a href="#39737367">parent</a><span>|</span><a href="#39738856">next</a><span>|</span><label class="collapse" for="c-39737922">[-]</label><label class="expand" for="c-39737922">[37 more]</label></div><br/><div class="children"><div class="content">Considering how poor it is compared to other models, it really emphasises how important fine tuning is.  Models with MUCH smaller parameter counts are outperforming it in many metrics.</div><br/><div id="39737956" class="c"><input type="checkbox" id="c-39737956" checked=""/><div class="controls bullet"><span class="by">lukan</span><span>|</span><a href="#39737367">root</a><span>|</span><a href="#39737922">parent</a><span>|</span><a href="#39738564">next</a><span>|</span><label class="collapse" for="c-39737956">[-]</label><label class="expand" for="c-39737956">[34 more]</label></div><br/><div class="children"><div class="content">&quot;it really emphasises how important fine tuning is&quot;<p>Or rather the quality of the training data?</div><br/><div id="39738474" class="c"><input type="checkbox" id="c-39738474" checked=""/><div class="controls bullet"><span class="by">llm_trw</span><span>|</span><a href="#39737367">root</a><span>|</span><a href="#39737956">parent</a><span>|</span><a href="#39738301">next</a><span>|</span><label class="collapse" for="c-39738474">[-]</label><label class="expand" for="c-39738474">[18 more]</label></div><br/><div class="children"><div class="content">We don&#x27;t know since no one is releasing their data.<p>Calling these models open source is like calling a binary open source because you can download it.<p>Which in this day and age isn&#x27;t far from where were at.</div><br/><div id="39738586" class="c"><input type="checkbox" id="c-39738586" checked=""/><div class="controls bullet"><span class="by">DreamGen</span><span>|</span><a href="#39737367">root</a><span>|</span><a href="#39738474">parent</a><span>|</span><a href="#39741066">next</a><span>|</span><label class="collapse" for="c-39738586">[-]</label><label class="expand" for="c-39738586">[7 more]</label></div><br/><div class="children"><div class="content">A big distinction is that you can built on top (fine-tune) thus released models as well as if they released the pre-training data.</div><br/><div id="39739503" class="c"><input type="checkbox" id="c-39739503" checked=""/><div class="controls bullet"><span class="by">tarruda</span><span>|</span><a href="#39737367">root</a><span>|</span><a href="#39738586">parent</a><span>|</span><a href="#39738642">next</a><span>|</span><label class="collapse" for="c-39739503">[-]</label><label class="expand" for="c-39739503">[1 more]</label></div><br/><div class="children"><div class="content">You can fine tune without the pre training data too.<p>Mistral models are one example, they never released pre training data and there are many fine tunes.</div><br/></div></div><div id="39738642" class="c"><input type="checkbox" id="c-39738642" checked=""/><div class="controls bullet"><span class="by">llm_trw</span><span>|</span><a href="#39737367">root</a><span>|</span><a href="#39738586">parent</a><span>|</span><a href="#39739503">prev</a><span>|</span><a href="#39741066">next</a><span>|</span><label class="collapse" for="c-39738642">[-]</label><label class="expand" for="c-39738642">[5 more]</label></div><br/><div class="children"><div class="content">You can also build on top of binaries if you use gotos and machine code.</div><br/><div id="39740954" class="c"><input type="checkbox" id="c-39740954" checked=""/><div class="controls bullet"><span class="by">shwaj</span><span>|</span><a href="#39737367">root</a><span>|</span><a href="#39738642">parent</a><span>|</span><a href="#39741100">next</a><span>|</span><label class="collapse" for="c-39740954">[-]</label><label class="expand" for="c-39740954">[3 more]</label></div><br/><div class="children"><div class="content">This seems intentionally obtuse. What you say is true, but it is very obvious that this is <i>much</i> more of a pain than if you had the source code.  On the other hand, fine tuning is just as easy, regardless of whether you have the original training data.</div><br/><div id="39741175" class="c"><input type="checkbox" id="c-39741175" checked=""/><div class="controls bullet"><span class="by">samus</span><span>|</span><a href="#39737367">root</a><span>|</span><a href="#39740954">parent</a><span>|</span><a href="#39741100">next</a><span>|</span><label class="collapse" for="c-39741175">[-]</label><label class="expand" for="c-39741175">[2 more]</label></div><br/><div class="children"><div class="content">One could also disassemble an executable and build on top of it. Not for the faint of heart and probably illegal, but possible unless it was deliberately obfuscated. Compared to that, it is impossible with state-of-the-art methods to systematically extract the training data from an LLM model. Fragments yes, but not all of it.</div><br/><div id="39741618" class="c"><input type="checkbox" id="c-39741618" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#39737367">root</a><span>|</span><a href="#39741175">parent</a><span>|</span><a href="#39741100">next</a><span>|</span><label class="collapse" for="c-39741618">[-]</label><label class="expand" for="c-39741618">[1 more]</label></div><br/><div class="children"><div class="content">You can do better - generate synthetic data covering all topics. And to make it less prone to hallucination, use RAG or web search for reference material. The Phi-1.5 model was trained on 300B of synthetic tokens generated with chatGPT and it showed a 5x bump in efficiency, punching well above its line.<p>Synthetic data can be more diverse if you sample carefully with seeded concepts, and it can be more complex than average web text. You can even diff against a garden variety Mistral or LLaMA and only collect knowledge and skills they don&#x27;t already have. I call this approach &quot;Machine Study&quot;, where AI makes its own training data by studying its corpus and learning from other models.</div><br/></div></div></div></div></div></div><div id="39741100" class="c"><input type="checkbox" id="c-39741100" checked=""/><div class="controls bullet"><span class="by">adrianN</span><span>|</span><a href="#39737367">root</a><span>|</span><a href="#39738642">parent</a><span>|</span><a href="#39740954">prev</a><span>|</span><a href="#39741066">next</a><span>|</span><label class="collapse" for="c-39741100">[-]</label><label class="expand" for="c-39741100">[1 more]</label></div><br/><div class="children"><div class="content">Or shell scripts</div><br/></div></div></div></div></div></div><div id="39741066" class="c"><input type="checkbox" id="c-39741066" checked=""/><div class="controls bullet"><span class="by">cl3misch</span><span>|</span><a href="#39737367">root</a><span>|</span><a href="#39738474">parent</a><span>|</span><a href="#39738586">prev</a><span>|</span><a href="#39738828">next</a><span>|</span><label class="collapse" for="c-39741066">[-]</label><label class="expand" for="c-39741066">[1 more]</label></div><br/><div class="children"><div class="content">FWIW the Grok repo uses the term &quot;open weights&quot;.</div><br/></div></div><div id="39738828" class="c"><input type="checkbox" id="c-39738828" checked=""/><div class="controls bullet"><span class="by">swalsh</span><span>|</span><a href="#39737367">root</a><span>|</span><a href="#39738474">parent</a><span>|</span><a href="#39741066">prev</a><span>|</span><a href="#39740727">next</a><span>|</span><label class="collapse" for="c-39738828">[-]</label><label class="expand" for="c-39738828">[1 more]</label></div><br/><div class="children"><div class="content">We should just call it open weight models at this point.</div><br/></div></div><div id="39740727" class="c"><input type="checkbox" id="c-39740727" checked=""/><div class="controls bullet"><span class="by">boulos</span><span>|</span><a href="#39737367">root</a><span>|</span><a href="#39738474">parent</a><span>|</span><a href="#39738828">prev</a><span>|</span><a href="#39738821">next</a><span>|</span><label class="collapse" for="c-39740727">[-]</label><label class="expand" for="c-39740727">[2 more]</label></div><br/><div class="children"><div class="content">How about &quot;weights available&quot; as similar to the &quot;source available&quot; moniker?</div><br/><div id="39740797" class="c"><input type="checkbox" id="c-39740797" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#39737367">root</a><span>|</span><a href="#39740727">parent</a><span>|</span><a href="#39738821">next</a><span>|</span><label class="collapse" for="c-39740797">[-]</label><label class="expand" for="c-39740797">[1 more]</label></div><br/><div class="children"><div class="content">weights available or model available, but yes.</div><br/></div></div></div></div><div id="39738821" class="c"><input type="checkbox" id="c-39738821" checked=""/><div class="controls bullet"><span class="by">drexlspivey</span><span>|</span><a href="#39737367">root</a><span>|</span><a href="#39738474">parent</a><span>|</span><a href="#39740727">prev</a><span>|</span><a href="#39738301">next</a><span>|</span><label class="collapse" for="c-39738821">[-]</label><label class="expand" for="c-39738821">[6 more]</label></div><br/><div class="children"><div class="content">Their data is the twitter corpus which is public. Or do you want a dump of their database for free too?</div><br/><div id="39739032" class="c"><input type="checkbox" id="c-39739032" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#39737367">root</a><span>|</span><a href="#39738821">parent</a><span>|</span><a href="#39738881">next</a><span>|</span><label class="collapse" for="c-39739032">[-]</label><label class="expand" for="c-39739032">[1 more]</label></div><br/><div class="children"><div class="content">Twitter tweet data in itself is both highly idiosyncratic and short by design, which alone is not conductive towards training a LLM.</div><br/></div></div><div id="39738881" class="c"><input type="checkbox" id="c-39738881" checked=""/><div class="controls bullet"><span class="by">llm_trw</span><span>|</span><a href="#39737367">root</a><span>|</span><a href="#39738821">parent</a><span>|</span><a href="#39739032">prev</a><span>|</span><a href="#39738877">next</a><span>|</span><label class="collapse" for="c-39738881">[-]</label><label class="expand" for="c-39738881">[2 more]</label></div><br/><div class="children"><div class="content">Saying &quot;It&#x27;s just the twitter public corpus.&quot; is like saying &quot;Here&#x27;s the Linux Kernel, makefiles not included.&quot;</div><br/><div id="39739887" class="c"><input type="checkbox" id="c-39739887" checked=""/><div class="controls bullet"><span class="by">zx8080</span><span>|</span><a href="#39737367">root</a><span>|</span><a href="#39738881">parent</a><span>|</span><a href="#39738877">next</a><span>|</span><label class="collapse" for="c-39739887">[-]</label><label class="expand" for="c-39739887">[1 more]</label></div><br/><div class="children"><div class="content">Or even &quot;here&#x27;s the Linux Kernel makefiles, no sources included, enjoy&quot;.</div><br/></div></div></div></div></div></div></div></div><div id="39738301" class="c"><input type="checkbox" id="c-39738301" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#39737367">root</a><span>|</span><a href="#39737956">parent</a><span>|</span><a href="#39738474">prev</a><span>|</span><a href="#39738577">next</a><span>|</span><label class="collapse" for="c-39738301">[-]</label><label class="expand" for="c-39738301">[14 more]</label></div><br/><div class="children"><div class="content">that&#x27;s a subtle dig at the fact that they have all of Twitter as a training corpus to use, but we don&#x27;t know how they weight tweets. which, we know they&#x27;re not gonna be weighted evenly.</div><br/><div id="39738400" class="c"><input type="checkbox" id="c-39738400" checked=""/><div class="controls bullet"><span class="by">rezonant</span><span>|</span><a href="#39737367">root</a><span>|</span><a href="#39738301">parent</a><span>|</span><a href="#39738577">next</a><span>|</span><label class="collapse" for="c-39738400">[-]</label><label class="expand" for="c-39738400">[13 more]</label></div><br/><div class="children"><div class="content">I&#x27;m sure just like in X&#x27;s algorithms, @elon tweets are weighted heavily.</div><br/><div id="39738444" class="c"><input type="checkbox" id="c-39738444" checked=""/><div class="controls bullet"><span class="by">convery</span><span>|</span><a href="#39737367">root</a><span>|</span><a href="#39738400">parent</a><span>|</span><a href="#39739215">next</a><span>|</span><label class="collapse" for="c-39738444">[-]</label><label class="expand" for="c-39738444">[11 more]</label></div><br/><div class="children"><div class="content">The X algorithm is also opensource, so you can verify before commenting..</div><br/><div id="39739759" class="c"><input type="checkbox" id="c-39739759" checked=""/><div class="controls bullet"><span class="by">threeseed</span><span>|</span><a href="#39737367">root</a><span>|</span><a href="#39738444">parent</a><span>|</span><a href="#39738569">next</a><span>|</span><label class="collapse" for="c-39739759">[-]</label><label class="expand" for="c-39739759">[1 more]</label></div><br/><div class="children"><div class="content">X algorithm Github project hasn&#x27;t been updated in 8 months:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;twitter&#x2F;the-algorithm">https:&#x2F;&#x2F;github.com&#x2F;twitter&#x2F;the-algorithm</a><p>So clearly they aren&#x27;t running it in production.<p>Also they didn&#x27;t open source the list of people who are being artificially boosted e.g. Elon.</div><br/></div></div><div id="39738569" class="c"><input type="checkbox" id="c-39738569" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#39737367">root</a><span>|</span><a href="#39738444">parent</a><span>|</span><a href="#39739759">prev</a><span>|</span><a href="#39739215">next</a><span>|</span><label class="collapse" for="c-39738569">[-]</label><label class="expand" for="c-39738569">[9 more]</label></div><br/><div class="children"><div class="content">just because they open sourced it doesn&#x27;t mean that&#x27;s actually what they&#x27;re running on it though</div><br/><div id="39738684" class="c"><input type="checkbox" id="c-39738684" checked=""/><div class="controls bullet"><span class="by">chrisco255</span><span>|</span><a href="#39737367">root</a><span>|</span><a href="#39738569">parent</a><span>|</span><a href="#39738686">next</a><span>|</span><label class="collapse" for="c-39738684">[-]</label><label class="expand" for="c-39738684">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not like he needs boosting, he was one of Twitter&#x27;s top followed accounts long before he bought them. He&#x27;s pretty good at getting attention.</div><br/><div id="39739133" class="c"><input type="checkbox" id="c-39739133" checked=""/><div class="controls bullet"><span class="by">latexr</span><span>|</span><a href="#39737367">root</a><span>|</span><a href="#39738684">parent</a><span>|</span><a href="#39738686">next</a><span>|</span><label class="collapse" for="c-39739133">[-]</label><label class="expand" for="c-39739133">[1 more]</label></div><br/><div class="children"><div class="content">And yet it’s not enough to curb the desire to tip the scales.<p><a href="https:&#x2F;&#x2F;arstechnica.com&#x2F;tech-policy&#x2F;2023&#x2F;02&#x2F;report-musk-had-twitter-engineers-boost-his-tweets-after-biden-got-more-views&#x2F;" rel="nofollow">https:&#x2F;&#x2F;arstechnica.com&#x2F;tech-policy&#x2F;2023&#x2F;02&#x2F;report-musk-had-...</a></div><br/></div></div></div></div><div id="39738686" class="c"><input type="checkbox" id="c-39738686" checked=""/><div class="controls bullet"><span class="by">lukan</span><span>|</span><a href="#39737367">root</a><span>|</span><a href="#39738569">parent</a><span>|</span><a href="#39738684">prev</a><span>|</span><a href="#39739215">next</a><span>|</span><label class="collapse" for="c-39738686">[-]</label><label class="expand" for="c-39738686">[6 more]</label></div><br/><div class="children"><div class="content">No idea about the current state, but the open sourcing did show, they were favoring elon:<p><a href="https:&#x2F;&#x2F;mashable.com&#x2F;article&#x2F;twitter-releases-algorithm-showing-it-tracks-elon-musk-tweets" rel="nofollow">https:&#x2F;&#x2F;mashable.com&#x2F;article&#x2F;twitter-releases-algorithm-show...</a><p>And personally I never used Twitter much, but I certainly did not follow Elon Musk when I did - yet I had to see lot&#x27;s of his posts in my feed. Surely just coincidence.</div><br/><div id="39739334" class="c"><input type="checkbox" id="c-39739334" checked=""/><div class="controls bullet"><span class="by">maccaw</span><span>|</span><a href="#39737367">root</a><span>|</span><a href="#39738686">parent</a><span>|</span><a href="#39738857">next</a><span>|</span><label class="collapse" for="c-39739334">[-]</label><label class="expand" for="c-39739334">[2 more]</label></div><br/><div class="children"><div class="content">&gt; they were favoring elon<p>No, and that&#x27;s not what the article says either. They were just tracking how well his tweets were doing versus others. They were not favoring Elon.</div><br/><div id="39740863" class="c"><input type="checkbox" id="c-39740863" checked=""/><div class="controls bullet"><span class="by">lukan</span><span>|</span><a href="#39737367">root</a><span>|</span><a href="#39739334">parent</a><span>|</span><a href="#39738857">next</a><span>|</span><label class="collapse" for="c-39740863">[-]</label><label class="expand" for="c-39740863">[1 more]</label></div><br/><div class="children"><div class="content">&quot;They were just tracking how well his tweets were doing versus others. &quot;<p>Yeah, and adjusting it, so he comes out best. That was Musks demand, as the other article shows, that is linked inside, after a Biden tweet performed better than Musk:<p><a href="https:&#x2F;&#x2F;mashable.com&#x2F;article&#x2F;elon-musk-super-bowl-joe-biden-tweet" rel="nofollow">https:&#x2F;&#x2F;mashable.com&#x2F;article&#x2F;elon-musk-super-bowl-joe-biden-...</a><p>They officially boost people, who pay a little bit. Elon payed a lot.<p>And the source is clearly not the production source and never where in this shape - otherwise why sue someone, who open sourced it?<p>&quot;But, the release of this source code also comes days after Twitter forced Github to take down other parts of Twitter&#x27;s source code that was allegedly posted by a former employee without the company&#x27;s permission. So, clearly, there&#x27;s still plenty of Twitter that Musk still doesn&#x27;t want us to see.&quot;<p>Also, you probably missed that:<p>&quot;Zoë Schiffer of Platformer reported that Twitter actually removed part of the source code that affected the reach of Musk&#x27;s and other user&#x27;s tweets before releasing the algorithm to the public.&quot;<p>Which is consistent with quite some other statements, also from Twitter itself and the fact, that the source has not been updated in 8 months.<p>See also this HN comment and discussion about it:<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35391854">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35391854</a><p>&quot;But the underlying policies and models are almost entirely missing (there are a couple valuable components in [1]). Without those, we can&#x27;t evaluate the behavior and possible effects of &quot;the algorithm.&quot;&quot;</div><br/></div></div></div></div><div id="39738857" class="c"><input type="checkbox" id="c-39738857" checked=""/><div class="controls bullet"><span class="by">machdiamonds</span><span>|</span><a href="#39737367">root</a><span>|</span><a href="#39738686">parent</a><span>|</span><a href="#39739334">prev</a><span>|</span><a href="#39739341">next</a><span>|</span><label class="collapse" for="c-39738857">[-]</label><label class="expand" for="c-39738857">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not too hard to believe it is a coincidence when the most followed person on a platform shows up in your feed, especially if you follow tech accounts.</div><br/><div id="39738905" class="c"><input type="checkbox" id="c-39738905" checked=""/><div class="controls bullet"><span class="by">internetter</span><span>|</span><a href="#39737367">root</a><span>|</span><a href="#39738857">parent</a><span>|</span><a href="#39739341">next</a><span>|</span><label class="collapse" for="c-39738905">[-]</label><label class="expand" for="c-39738905">[1 more]</label></div><br/><div class="children"><div class="content">Did you not read the article linked in the comment you&#x27;re replying to?</div><br/></div></div></div></div><div id="39739341" class="c"><input type="checkbox" id="c-39739341" checked=""/><div class="controls bullet"><span class="by">jokethrowaway</span><span>|</span><a href="#39737367">root</a><span>|</span><a href="#39738686">parent</a><span>|</span><a href="#39738857">prev</a><span>|</span><a href="#39739215">next</a><span>|</span><label class="collapse" for="c-39739341">[-]</label><label class="expand" for="c-39739341">[1 more]</label></div><br/><div class="children"><div class="content">Sounds a bit far fetched<p>So changes in power users stats would also result in audience balancing?<p>Most likely the code was used for analytics and for tracking balance; Elon was a pain in the ass and asked to have custom analytics for his account and devs eventually added him as an audience to be able to get analytics about him easily. A bit dirty but it works.<p>Most likely the balancing code is somewhere else and it affects only republican &#x2F; democrats.</div><br/></div></div></div></div></div></div></div></div><div id="39739215" class="c"><input type="checkbox" id="c-39739215" checked=""/><div class="controls bullet"><span class="by">nonethewiser</span><span>|</span><a href="#39737367">root</a><span>|</span><a href="#39738400">parent</a><span>|</span><a href="#39738444">prev</a><span>|</span><a href="#39738577">next</a><span>|</span><label class="collapse" for="c-39739215">[-]</label><label class="expand" for="c-39739215">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;m sure just like in X&#x27;s algorithms, @elon tweets are weighted heavily.<p>Are you sure or is it the literal opposite and you’re just speculating?</div><br/></div></div></div></div></div></div><div id="39738577" class="c"><input type="checkbox" id="c-39738577" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#39737367">root</a><span>|</span><a href="#39737956">parent</a><span>|</span><a href="#39738301">prev</a><span>|</span><a href="#39738564">next</a><span>|</span><label class="collapse" for="c-39738577">[-]</label><label class="expand" for="c-39738577">[1 more]</label></div><br/><div class="children"><div class="content">Or even how much it was trained on this dataset, the amount of FLOPs.</div><br/></div></div></div></div><div id="39738564" class="c"><input type="checkbox" id="c-39738564" checked=""/><div class="controls bullet"><span class="by">lairv</span><span>|</span><a href="#39737367">root</a><span>|</span><a href="#39737922">parent</a><span>|</span><a href="#39737956">prev</a><span>|</span><a href="#39740596">next</a><span>|</span><label class="collapse" for="c-39738564">[-]</label><label class="expand" for="c-39738564">[1 more]</label></div><br/><div class="children"><div class="content">I would say it emphasises that training a good model is more than throwing random data and compute</div><br/></div></div><div id="39740596" class="c"><input type="checkbox" id="c-39740596" checked=""/><div class="controls bullet"><span class="by">make3</span><span>|</span><a href="#39737367">root</a><span>|</span><a href="#39737922">parent</a><span>|</span><a href="#39738564">prev</a><span>|</span><a href="#39738856">next</a><span>|</span><label class="collapse" for="c-39740596">[-]</label><label class="expand" for="c-39740596">[1 more]</label></div><br/><div class="children"><div class="content">no it empathizes the importance of training smaller models for longer, like the Mistral &quot;overtrained&quot; models</div><br/></div></div></div></div><div id="39738856" class="c"><input type="checkbox" id="c-39738856" checked=""/><div class="controls bullet"><span class="by">zone411</span><span>|</span><a href="#39737367">parent</a><span>|</span><a href="#39737922">prev</a><span>|</span><a href="#39737607">next</a><span>|</span><label class="collapse" for="c-39738856">[-]</label><label class="expand" for="c-39738856">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s actually not the largest. <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;google&#x2F;switch-c-2048" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;google&#x2F;switch-c-2048</a> is 1.6T parameters.</div><br/></div></div><div id="39737607" class="c"><input type="checkbox" id="c-39737607" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#39737367">parent</a><span>|</span><a href="#39738856">prev</a><span>|</span><a href="#39740901">next</a><span>|</span><label class="collapse" for="c-39737607">[-]</label><label class="expand" for="c-39737607">[6 more]</label></div><br/><div class="children"><div class="content">It’s not 8x86B. Total number of parameters is 314B.<p>Perhaps it’s 8x39B to fit on a single 8xA100 (40GB) server?</div><br/><div id="39739003" class="c"><input type="checkbox" id="c-39739003" checked=""/><div class="controls bullet"><span class="by">dheera</span><span>|</span><a href="#39737367">root</a><span>|</span><a href="#39737607">parent</a><span>|</span><a href="#39737742">next</a><span>|</span><label class="collapse" for="c-39739003">[-]</label><label class="expand" for="c-39739003">[3 more]</label></div><br/><div class="children"><div class="content">They all do this marketing bull.<p>Mixtral has an 8x7B model but it&#x27;s actually 46.7B, not 56B params.<p>Kinda similar to how 4K displays are 3840 pixels wide, not true 4K which would be 4096. Marketing people called it 4K, not engineers.</div><br/><div id="39741242" class="c"><input type="checkbox" id="c-39741242" checked=""/><div class="controls bullet"><span class="by">guitarlimeo</span><span>|</span><a href="#39737367">root</a><span>|</span><a href="#39739003">parent</a><span>|</span><a href="#39737742">next</a><span>|</span><label class="collapse" for="c-39741242">[-]</label><label class="expand" for="c-39741242">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve always thought of 4K as &quot;4x FullHD&quot;. In that way it makes sense.</div><br/><div id="39741661" class="c"><input type="checkbox" id="c-39741661" checked=""/><div class="controls bullet"><span class="by">mavhc</span><span>|</span><a href="#39737367">root</a><span>|</span><a href="#39741242">parent</a><span>|</span><a href="#39737742">next</a><span>|</span><label class="collapse" for="c-39741661">[-]</label><label class="expand" for="c-39741661">[1 more]</label></div><br/><div class="children"><div class="content">TV and Digital Cinema have different standards, because of course they do</div><br/></div></div></div></div></div></div><div id="39737742" class="c"><input type="checkbox" id="c-39737742" checked=""/><div class="controls bullet"><span class="by">cma</span><span>|</span><a href="#39737367">root</a><span>|</span><a href="#39737607">parent</a><span>|</span><a href="#39739003">prev</a><span>|</span><a href="#39737734">next</a><span>|</span><label class="collapse" for="c-39737742">[-]</label><label class="expand" for="c-39737742">[1 more]</label></div><br/><div class="children"><div class="content">Active parameters is  86B, so wouldn&#x27;t that be the size of the largest two experts (where they may all be the same) + the weights of the selector?</div><br/></div></div><div id="39737734" class="c"><input type="checkbox" id="c-39737734" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#39737367">root</a><span>|</span><a href="#39737607">parent</a><span>|</span><a href="#39737742">prev</a><span>|</span><a href="#39740901">next</a><span>|</span><label class="collapse" for="c-39737734">[-]</label><label class="expand" for="c-39737734">[1 more]</label></div><br/><div class="children"><div class="content">Most likely it&#x27;s a MoE of Grok-0 which would be 8x33B + 50B for the router.</div><br/></div></div></div></div></div></div><div id="39740901" class="c"><input type="checkbox" id="c-39740901" checked=""/><div class="controls bullet"><span class="by">nasir</span><span>|</span><a href="#39737367">prev</a><span>|</span><a href="#39740373">next</a><span>|</span><label class="collapse" for="c-39740901">[-]</label><label class="expand" for="c-39740901">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d be very curious to see how it performs especially on inputs that&#x27;s blocked by other models. Seems like Grok will differentiate itself from other OS models from a cencorship and alignment perspective.</div><br/></div></div><div id="39740373" class="c"><input type="checkbox" id="c-39740373" checked=""/><div class="controls bullet"><span class="by">joydeep314</span><span>|</span><a href="#39740901">prev</a><span>|</span><a href="#39737737">next</a><span>|</span><label class="collapse" for="c-39740373">[-]</label><label class="expand" for="c-39740373">[1 more]</label></div><br/><div class="children"><div class="content">Model weights on huggingface: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;xai-org&#x2F;grok-1" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;xai-org&#x2F;grok-1</a></div><br/></div></div><div id="39737737" class="c"><input type="checkbox" id="c-39737737" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#39740373">prev</a><span>|</span><a href="#39737438">next</a><span>|</span><label class="collapse" for="c-39737737">[-]</label><label class="expand" for="c-39737737">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Base model trained on a large amount of text data, not fine-tuned for any particular task.&quot;<p>Presumably the version they&#x27;ve been previewing on Twitter is an instruction-tuned model which behaves quite differently from these raw weights.</div><br/></div></div><div id="39737438" class="c"><input type="checkbox" id="c-39737438" checked=""/><div class="controls bullet"><span class="by">nylonstrung</span><span>|</span><a href="#39737737">prev</a><span>|</span><a href="#39737477">next</a><span>|</span><label class="collapse" for="c-39737438">[-]</label><label class="expand" for="c-39737438">[40 more]</label></div><br/><div class="children"><div class="content">For what reason would you want to use this instead of open source alternatives like Mistral</div><br/><div id="39737457" class="c"><input type="checkbox" id="c-39737457" checked=""/><div class="controls bullet"><span class="by">rvnx</span><span>|</span><a href="#39737438">parent</a><span>|</span><a href="#39738999">next</a><span>|</span><label class="collapse" for="c-39737457">[-]</label><label class="expand" for="c-39737457">[10 more]</label></div><br/><div class="children"><div class="content">Mistral opened their weights only for very small LLaMA-like model.</div><br/><div id="39737594" class="c"><input type="checkbox" id="c-39737594" checked=""/><div class="controls bullet"><span class="by">MallocVoidstar</span><span>|</span><a href="#39737438">root</a><span>|</span><a href="#39737457">parent</a><span>|</span><a href="#39738999">next</a><span>|</span><label class="collapse" for="c-39737594">[-]</label><label class="expand" for="c-39737594">[9 more]</label></div><br/><div class="children"><div class="content">I&#x27;m pretty sure Mixtral outperforms Grok-1 and uses much less memory to do it</div><br/><div id="39737905" class="c"><input type="checkbox" id="c-39737905" checked=""/><div class="controls bullet"><span class="by">cavisne</span><span>|</span><a href="#39737438">root</a><span>|</span><a href="#39737594">parent</a><span>|</span><a href="#39737646">next</a><span>|</span><label class="collapse" for="c-39737905">[-]</label><label class="expand" for="c-39737905">[2 more]</label></div><br/><div class="children"><div class="content">One of the interesting things when weights are open sourced is the community can often improve the results. See all the bugs fixed in Gemma for an example.</div><br/><div id="39739301" class="c"><input type="checkbox" id="c-39739301" checked=""/><div class="controls bullet"><span class="by">ein0p</span><span>|</span><a href="#39737438">root</a><span>|</span><a href="#39737905">parent</a><span>|</span><a href="#39737646">next</a><span>|</span><label class="collapse" for="c-39739301">[-]</label><label class="expand" for="c-39739301">[1 more]</label></div><br/><div class="children"><div class="content">Doubtful, for purely information theoretic and memory capacity reasons. It may outperform on some synthetic metrics, but in practice, to a human, larger models just feel “smarter” because they have a lot more density in their long tail where metrics never go</div><br/></div></div></div></div><div id="39737646" class="c"><input type="checkbox" id="c-39737646" checked=""/><div class="controls bullet"><span class="by">elfbargpt</span><span>|</span><a href="#39737438">root</a><span>|</span><a href="#39737594">parent</a><span>|</span><a href="#39737905">prev</a><span>|</span><a href="#39738999">next</a><span>|</span><label class="collapse" for="c-39737646">[-]</label><label class="expand" for="c-39737646">[6 more]</label></div><br/><div class="children"><div class="content">I&#x27;m a little out of touch, is there a way to see how Grok measures up to other models?</div><br/><div id="39737818" class="c"><input type="checkbox" id="c-39737818" checked=""/><div class="controls bullet"><span class="by">amrrs</span><span>|</span><a href="#39737438">root</a><span>|</span><a href="#39737646">parent</a><span>|</span><a href="#39738999">next</a><span>|</span><label class="collapse" for="c-39737818">[-]</label><label class="expand" for="c-39737818">[5 more]</label></div><br/><div class="children"><div class="content">Benchmarks here <a href="https:&#x2F;&#x2F;x.ai&#x2F;blog&#x2F;grok" rel="nofollow">https:&#x2F;&#x2F;x.ai&#x2F;blog&#x2F;grok</a></div><br/><div id="39737951" class="c"><input type="checkbox" id="c-39737951" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#39737438">root</a><span>|</span><a href="#39737818">parent</a><span>|</span><a href="#39738017">next</a><span>|</span><label class="collapse" for="c-39737951">[-]</label><label class="expand" for="c-39737951">[3 more]</label></div><br/><div class="children"><div class="content">And to compare, you can sort by MMLU on here: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;HuggingFaceH4&#x2F;open_llm_leaderboard" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;HuggingFaceH4&#x2F;open_llm_leaderb...</a>.<p>Edit: to include my self summary after review: There&#x27;s a good 100 models better than, a couple 1x7b even. Mixtral stomps it, half mixtral are universally better but one is close to same.</div><br/><div id="39738265" class="c"><input type="checkbox" id="c-39738265" checked=""/><div class="controls bullet"><span class="by">lossolo</span><span>|</span><a href="#39737438">root</a><span>|</span><a href="#39737951">parent</a><span>|</span><a href="#39738017">next</a><span>|</span><label class="collapse" for="c-39738265">[-]</label><label class="expand" for="c-39738265">[2 more]</label></div><br/><div class="children"><div class="content">This benchmark is mostly worthless, some of the top models there were trained on benchmark data, which is a known fact in the community.<p>The only reliable benchmark: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;lmsys&#x2F;chatbot-arena-leaderboard" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;lmsys&#x2F;chatbot-arena-leaderboar...</a></div><br/><div id="39739550" class="c"><input type="checkbox" id="c-39739550" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#39737438">root</a><span>|</span><a href="#39738265">parent</a><span>|</span><a href="#39738017">next</a><span>|</span><label class="collapse" for="c-39739550">[-]</label><label class="expand" for="c-39739550">[1 more]</label></div><br/><div class="children"><div class="content">No, it&#x27;s not &quot;mostly worthless&quot; and yes, some of the top models were removed a few months back from being trained on benchmark data.<p>I urge you to at least think through what alternative you propose before posting so aggressively in these situations. Lmsys doesn&#x27;t have Grok, or I would have included it. And having _some_ data is better than none.<p>I also had someone arguing with me 6 months back that we can&#x27;t trust any benchmarks at all from vendors, which would exclude the blog post. Instead of just repeating that back vehemently, I filled a gap. It&#x27;s important we don&#x27;t self-peasantize as a species, all data has its issues, that doesn&#x27;t mean we throw it all out.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="39738999" class="c"><input type="checkbox" id="c-39738999" checked=""/><div class="controls bullet"><span class="by">zozbot234</span><span>|</span><a href="#39737438">parent</a><span>|</span><a href="#39737457">prev</a><span>|</span><a href="#39737671">next</a><span>|</span><label class="collapse" for="c-39738999">[-]</label><label class="expand" for="c-39738999">[1 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t this Apache licensed?  Regardless, you can run multiple models concurrently on the same input using well-known ensemble techniques.  (Not to be confused with mixture-of-experts, which is more like training a single model where only a few blocks are chosen to be active at any given time - a kind of sparsity.)</div><br/></div></div><div id="39737671" class="c"><input type="checkbox" id="c-39737671" checked=""/><div class="controls bullet"><span class="by">verticalscaler</span><span>|</span><a href="#39737438">parent</a><span>|</span><a href="#39738999">prev</a><span>|</span><a href="#39737477">next</a><span>|</span><label class="collapse" for="c-39737671">[-]</label><label class="expand" for="c-39737671">[28 more]</label></div><br/><div class="children"><div class="content">Well if nothing else, this one might be significantly less nerfed. Very interesting to compare to the others.</div><br/></div></div></div></div><div id="39737477" class="c"><input type="checkbox" id="c-39737477" checked=""/><div class="controls bullet"><span class="by">pogue</span><span>|</span><a href="#39737438">prev</a><span>|</span><a href="#39738250">next</a><span>|</span><label class="collapse" for="c-39737477">[-]</label><label class="expand" for="c-39737477">[34 more]</label></div><br/><div class="children"><div class="content">Can someone explain why the weights are posted via a Bittorrent magnet link? I have no way to check the size at the moment, but isn&#x27;t that a bit unusual? There&#x27;s also only 21 seeders right now according to <a href="https:&#x2F;&#x2F;checker.openwebtorrent.com&#x2F;" rel="nofollow">https:&#x2F;&#x2F;checker.openwebtorrent.com&#x2F;</a></div><br/><div id="39737541" class="c"><input type="checkbox" id="c-39737541" checked=""/><div class="controls bullet"><span class="by">monkin</span><span>|</span><a href="#39737477">parent</a><span>|</span><a href="#39738264">next</a><span>|</span><label class="collapse" for="c-39737541">[-]</label><label class="expand" for="c-39737541">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s 318.24G<p><a href="https:&#x2F;&#x2F;academictorrents.com&#x2F;details&#x2F;5f96d43576e3d386c9ba65b883210a393b68210e" rel="nofollow">https:&#x2F;&#x2F;academictorrents.com&#x2F;details&#x2F;5f96d43576e3d386c9ba65b...</a></div><br/></div></div><div id="39738264" class="c"><input type="checkbox" id="c-39738264" checked=""/><div class="controls bullet"><span class="by">fzzzy</span><span>|</span><a href="#39737477">parent</a><span>|</span><a href="#39737541">prev</a><span>|</span><a href="#39737565">next</a><span>|</span><label class="collapse" for="c-39738264">[-]</label><label class="expand" for="c-39738264">[1 more]</label></div><br/><div class="children"><div class="content">It may become a tradition since weights are so large. Perhaps it started when the Llama torrent link leaked. Then, Mistral decided to release their weights using bittorrent.</div><br/></div></div><div id="39737565" class="c"><input type="checkbox" id="c-39737565" checked=""/><div class="controls bullet"><span class="by">bongodongobob</span><span>|</span><a href="#39737477">parent</a><span>|</span><a href="#39738264">prev</a><span>|</span><a href="#39741511">next</a><span>|</span><label class="collapse" for="c-39737565">[-]</label><label class="expand" for="c-39737565">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure why you wouldn&#x27;t tbh. That&#x27;s a lot of bandwidth.</div><br/></div></div><div id="39741511" class="c"><input type="checkbox" id="c-39741511" checked=""/><div class="controls bullet"><span class="by">ur-whale</span><span>|</span><a href="#39737477">parent</a><span>|</span><a href="#39737565">prev</a><span>|</span><a href="#39738757">next</a><span>|</span><label class="collapse" for="c-39741511">[-]</label><label class="expand" for="c-39741511">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Can someone explain why the weights are posted via a Bittorrent magnet link?<p>I think the best way to get an answer to that question is to try to host it yourself and see what happens.</div><br/></div></div><div id="39738757" class="c"><input type="checkbox" id="c-39738757" checked=""/><div class="controls bullet"><span class="by">raydev</span><span>|</span><a href="#39737477">parent</a><span>|</span><a href="#39741511">prev</a><span>|</span><a href="#39738287">next</a><span>|</span><label class="collapse" for="c-39738757">[-]</label><label class="expand" for="c-39738757">[1 more]</label></div><br/><div class="children"><div class="content">Spreads the burden&#x2F;cost of distributing a 300+GB file.</div><br/></div></div><div id="39738287" class="c"><input type="checkbox" id="c-39738287" checked=""/><div class="controls bullet"><span class="by">leumon</span><span>|</span><a href="#39737477">parent</a><span>|</span><a href="#39738757">prev</a><span>|</span><a href="#39737538">next</a><span>|</span><label class="collapse" for="c-39738287">[-]</label><label class="expand" for="c-39738287">[1 more]</label></div><br/><div class="children"><div class="content">Mistral did it too when they released their first open model. They just posted a magnet link on Twitter.</div><br/></div></div><div id="39737538" class="c"><input type="checkbox" id="c-39737538" checked=""/><div class="controls bullet"><span class="by">MallocVoidstar</span><span>|</span><a href="#39737477">parent</a><span>|</span><a href="#39738287">prev</a><span>|</span><a href="#39741264">next</a><span>|</span><label class="collapse" for="c-39737538">[-]</label><label class="expand" for="c-39737538">[1 more]</label></div><br/><div class="children"><div class="content">Distributing 300GB via torrent is cheaper than direct, assuming even a few other people seed</div><br/></div></div><div id="39741264" class="c"><input type="checkbox" id="c-39741264" checked=""/><div class="controls bullet"><span class="by">seydor</span><span>|</span><a href="#39737477">parent</a><span>|</span><a href="#39737538">prev</a><span>|</span><a href="#39737522">next</a><span>|</span><label class="collapse" for="c-39741264">[-]</label><label class="expand" for="c-39741264">[1 more]</label></div><br/><div class="children"><div class="content">my optimistic explanation is we are going back to the 2000s internet , but probably we are not</div><br/></div></div><div id="39737522" class="c"><input type="checkbox" id="c-39737522" checked=""/><div class="controls bullet"><span class="by">CamperBob2</span><span>|</span><a href="#39737477">parent</a><span>|</span><a href="#39741264">prev</a><span>|</span><a href="#39737508">next</a><span>|</span><label class="collapse" for="c-39737522">[-]</label><label class="expand" for="c-39737522">[19 more]</label></div><br/><div class="children"><div class="content">How else could&#x2F;should it be done?</div><br/><div id="39737561" class="c"><input type="checkbox" id="c-39737561" checked=""/><div class="controls bullet"><span class="by">pogue</span><span>|</span><a href="#39737477">root</a><span>|</span><a href="#39737522">parent</a><span>|</span><a href="#39737508">next</a><span>|</span><label class="collapse" for="c-39737561">[-]</label><label class="expand" for="c-39737561">[18 more]</label></div><br/><div class="children"><div class="content">I would have assumed they could just upload it to Github. If it has restrictions on file size I&#x27;m sure they could make multiple part compressed files.<p>Torrents can unfortunately die after a period of time if no one continues seeding it or if they don&#x27;t use a permanent web based seeder, which doesn&#x27;t appear to be the case.</div><br/><div id="39737656" class="c"><input type="checkbox" id="c-39737656" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#39737477">root</a><span>|</span><a href="#39737561">parent</a><span>|</span><a href="#39737655">next</a><span>|</span><label class="collapse" for="c-39737656">[-]</label><label class="expand" for="c-39737656">[4 more]</label></div><br/><div class="children"><div class="content">GitHub have a soft repository size limit of 5GB, documented here: <a href="https:&#x2F;&#x2F;docs.github.com&#x2F;en&#x2F;repositories&#x2F;working-with-files&#x2F;managing-large-files&#x2F;about-large-files-on-github#repository-size-limits" rel="nofollow">https:&#x2F;&#x2F;docs.github.com&#x2F;en&#x2F;repositories&#x2F;working-with-files&#x2F;m...</a><p>Soft size limit means &quot;If your repository excessively impacts our infrastructure, you might receive an email from GitHub Support asking you to take corrective action.&quot; - I know people who have received such emails.<p>Most model releases happen through Hugging Face which does not have such a size limit.</div><br/><div id="39738138" class="c"><input type="checkbox" id="c-39738138" checked=""/><div class="controls bullet"><span class="by">KomoD</span><span>|</span><a href="#39737477">root</a><span>|</span><a href="#39737656">parent</a><span>|</span><a href="#39738542">next</a><span>|</span><label class="collapse" for="c-39738138">[-]</label><label class="expand" for="c-39738138">[1 more]</label></div><br/><div class="children"><div class="content">They&#x27;d probably just charge you for it. They sell &quot;data packs&quot; for LFS.<p><a href="https:&#x2F;&#x2F;docs.github.com&#x2F;billing&#x2F;managing-billing-for-git-large-file-storage&#x2F;about-billing-for-git-large-file-storage" rel="nofollow">https:&#x2F;&#x2F;docs.github.com&#x2F;billing&#x2F;managing-billing-for-git-lar...</a></div><br/></div></div><div id="39738542" class="c"><input type="checkbox" id="c-39738542" checked=""/><div class="controls bullet"><span class="by">rezonant</span><span>|</span><a href="#39737477">root</a><span>|</span><a href="#39737656">parent</a><span>|</span><a href="#39738138">prev</a><span>|</span><a href="#39737655">next</a><span>|</span><label class="collapse" for="c-39738542">[-]</label><label class="expand" for="c-39738542">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;d bet Hugging Face would be happy to have hosted these canonically too, so not sure why that doesn&#x27;t happen more.</div><br/><div id="39738650" class="c"><input type="checkbox" id="c-39738650" checked=""/><div class="controls bullet"><span class="by">osanseviero</span><span>|</span><a href="#39737477">root</a><span>|</span><a href="#39738542">parent</a><span>|</span><a href="#39737655">next</a><span>|</span><label class="collapse" for="c-39738650">[-]</label><label class="expand" for="c-39738650">[1 more]</label></div><br/><div class="children"><div class="content">The model is also at <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;xai-org" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;xai-org</a></div><br/></div></div></div></div></div></div><div id="39737655" class="c"><input type="checkbox" id="c-39737655" checked=""/><div class="controls bullet"><span class="by">larrysalibra</span><span>|</span><a href="#39737477">root</a><span>|</span><a href="#39737561">parent</a><span>|</span><a href="#39737656">prev</a><span>|</span><a href="#39737723">next</a><span>|</span><label class="collapse" for="c-39737655">[-]</label><label class="expand" for="c-39737655">[1 more]</label></div><br/><div class="children"><div class="content">The great thing about torrents is that you (or anyone else who cares) can single-handedly solve the problem you&#x27;re complaining about by seeding the torrent.</div><br/></div></div><div id="39737723" class="c"><input type="checkbox" id="c-39737723" checked=""/><div class="controls bullet"><span class="by">sashank_1509</span><span>|</span><a href="#39737477">root</a><span>|</span><a href="#39737561">parent</a><span>|</span><a href="#39737655">prev</a><span>|</span><a href="#39738499">next</a><span>|</span><label class="collapse" for="c-39737723">[-]</label><label class="expand" for="c-39737723">[4 more]</label></div><br/><div class="children"><div class="content">No git would be impossible. I’ve never seen a repo even a few GB in size, if you are uploading non code files you really should not be using git. Git is a version management software for code. I often see repos which images and even videos checked in, please don’t, there are so many far better and more performant solutions out there.<p>The other approach would be to use AWS S3 or other cloud providers which would cost them money every time someone downloads their code, which is not their prerogative to pay for when they are releasing something for free. Torrents seems like the only good solution, unless someone hosts this on the cloud for free for everyone.</div><br/><div id="39738270" class="c"><input type="checkbox" id="c-39738270" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#39737477">root</a><span>|</span><a href="#39737723">parent</a><span>|</span><a href="#39738529">next</a><span>|</span><label class="collapse" for="c-39738270">[-]</label><label class="expand" for="c-39738270">[1 more]</label></div><br/><div class="children"><div class="content">Huggingface will disagree with impossible as their models are available via git, sometimes broken up in pth files.<p>Still, as far as sentiment goes, yeah git for model weights is an impedance mismatch for sure!</div><br/></div></div><div id="39738529" class="c"><input type="checkbox" id="c-39738529" checked=""/><div class="controls bullet"><span class="by">rezonant</span><span>|</span><a href="#39737477">root</a><span>|</span><a href="#39737723">parent</a><span>|</span><a href="#39738270">prev</a><span>|</span><a href="#39740299">next</a><span>|</span><label class="collapse" for="c-39738529">[-]</label><label class="expand" for="c-39738529">[1 more]</label></div><br/><div class="children"><div class="content">&gt; No git would be impossible. I’ve never seen a repo even a few GB in size, if you are uploading non code files you really should not be using git<p>It&#x27;s not actually a limitation in git itself, especially if you use Git LFS. People use Git for Unreal projects and big ones can be half a terabyte or more in size.</div><br/></div></div><div id="39740299" class="c"><input type="checkbox" id="c-39740299" checked=""/><div class="controls bullet"><span class="by">djhn</span><span>|</span><a href="#39737477">root</a><span>|</span><a href="#39737723">parent</a><span>|</span><a href="#39738529">prev</a><span>|</span><a href="#39738499">next</a><span>|</span><label class="collapse" for="c-39740299">[-]</label><label class="expand" for="c-39740299">[1 more]</label></div><br/><div class="children"><div class="content">Scott Chacon (github cofounder) mentioned in a recent talk that the Windows repo is 300GB <a href="https:&#x2F;&#x2F;youtu.be&#x2F;aolI_Rz0ZqY?si=MOo2eS6dsKKAxmsP" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;aolI_Rz0ZqY?si=MOo2eS6dsKKAxmsP</a></div><br/></div></div></div></div><div id="39738499" class="c"><input type="checkbox" id="c-39738499" checked=""/><div class="controls bullet"><span class="by">rezonant</span><span>|</span><a href="#39737477">root</a><span>|</span><a href="#39737561">parent</a><span>|</span><a href="#39737723">prev</a><span>|</span><a href="#39737623">next</a><span>|</span><label class="collapse" for="c-39738499">[-]</label><label class="expand" for="c-39738499">[4 more]</label></div><br/><div class="children"><div class="content">Others have pointed out that GitHub doesn&#x27;t allow that, but<p>&gt; Torrents can unfortunately die after a period of time if no one continues seeding it or if they don&#x27;t use a permanent web based seeder, which doesn&#x27;t appear to be the case.<p>So to can web links, especially when they are 300 GB and egressing out of AWS at $0.09&#x2F;GB or worse (in non-US regions). Each full download would cost $27 at that rate. 10,000 downloads would cost $270,000.<p>Sure you could go for something with a better cost model like R2, but you can&#x27;t beat using one or two unmetered connections on a VPN to constantly seed on Bittorrent, your pricing would be effectively free and reliability would be higher than if you just exposed a HTTP server on the Internet in such a way.</div><br/><div id="39739064" class="c"><input type="checkbox" id="c-39739064" checked=""/><div class="controls bullet"><span class="by">KomoD</span><span>|</span><a href="#39737477">root</a><span>|</span><a href="#39738499">parent</a><span>|</span><a href="#39737623">next</a><span>|</span><label class="collapse" for="c-39739064">[-]</label><label class="expand" for="c-39739064">[3 more]</label></div><br/><div class="children"><div class="content">&gt; and egressing out of AWS at $0.09&#x2F;GB<p>There&#x27;s a lot of seeders on the torrent that are actually AWS ips too, all with similar configurations which makes me believe that it&#x27;s probably xAI running them<p>&gt; on a VPN<p>That&#x27;s unnecessary, you don&#x27;t need a VPN?</div><br/><div id="39739490" class="c"><input type="checkbox" id="c-39739490" checked=""/><div class="controls bullet"><span class="by">rezonant</span><span>|</span><a href="#39737477">root</a><span>|</span><a href="#39739064">parent</a><span>|</span><a href="#39737623">next</a><span>|</span><label class="collapse" for="c-39739490">[-]</label><label class="expand" for="c-39739490">[2 more]</label></div><br/><div class="children"><div class="content">No you don&#x27;t, but if you wanted to host it from your gigabit office IP, you probably would want to.</div><br/><div id="39740041" class="c"><input type="checkbox" id="c-39740041" checked=""/><div class="controls bullet"><span class="by">KomoD</span><span>|</span><a href="#39737477">root</a><span>|</span><a href="#39739490">parent</a><span>|</span><a href="#39737623">next</a><span>|</span><label class="collapse" for="c-39740041">[-]</label><label class="expand" for="c-39740041">[1 more]</label></div><br/><div class="children"><div class="content">Why?</div><br/></div></div></div></div></div></div></div></div><div id="39737623" class="c"><input type="checkbox" id="c-39737623" checked=""/><div class="controls bullet"><span class="by">cedws</span><span>|</span><a href="#39737477">root</a><span>|</span><a href="#39737561">parent</a><span>|</span><a href="#39738499">prev</a><span>|</span><a href="#39737642">next</a><span>|</span><label class="collapse" for="c-39737623">[-]</label><label class="expand" for="c-39737623">[1 more]</label></div><br/><div class="children"><div class="content">GitHub may choose to throttle downloads or remove the files simply because they&#x27;re taking up too much bandwidth.<p>A torrent is less likely to go down in the short term.</div><br/></div></div><div id="39737642" class="c"><input type="checkbox" id="c-39737642" checked=""/><div class="controls bullet"><span class="by">xcv123</span><span>|</span><a href="#39737477">root</a><span>|</span><a href="#39737561">parent</a><span>|</span><a href="#39737623">prev</a><span>|</span><a href="#39737508">next</a><span>|</span><label class="collapse" for="c-39737642">[-]</label><label class="expand" for="c-39737642">[3 more]</label></div><br/><div class="children"><div class="content">This is not some crappy DVD rip on The Pirate Bay. It will be seeded as long as its relevant.<p>Twitter&#x2F;X has their own massive infrastructure and bandwidth to seed this indefinitely.</div><br/><div id="39738110" class="c"><input type="checkbox" id="c-39738110" checked=""/><div class="controls bullet"><span class="by">KomoD</span><span>|</span><a href="#39737477">root</a><span>|</span><a href="#39737642">parent</a><span>|</span><a href="#39737508">next</a><span>|</span><label class="collapse" for="c-39738110">[-]</label><label class="expand" for="c-39738110">[2 more]</label></div><br/><div class="children"><div class="content">Yeah, they can just leave some server running somewhere and just let it seed forever</div><br/></div></div></div></div></div></div></div></div><div id="39737508" class="c"><input type="checkbox" id="c-39737508" checked=""/><div class="controls bullet"><span class="by">pooloo</span><span>|</span><a href="#39737477">parent</a><span>|</span><a href="#39737522">prev</a><span>|</span><a href="#39737501">next</a><span>|</span><label class="collapse" for="c-39737508">[-]</label><label class="expand" for="c-39737508">[2 more]</label></div><br/><div class="children"><div class="content">Its likely over 100GB of data, so I wouldn&#x27;t say its necessarily unusual to spread out the bandwidth across multiple hosts.</div><br/><div id="39737581" class="c"><input type="checkbox" id="c-39737581" checked=""/><div class="controls bullet"><span class="by">pogue</span><span>|</span><a href="#39737477">root</a><span>|</span><a href="#39737508">parent</a><span>|</span><a href="#39737501">next</a><span>|</span><label class="collapse" for="c-39737581">[-]</label><label class="expand" for="c-39737581">[1 more]</label></div><br/><div class="children"><div class="content">Thanks! I searched and searched for a tool that would show me info via the web about a magnet link but nada</div><br/></div></div></div></div><div id="39737501" class="c"><input type="checkbox" id="c-39737501" checked=""/><div class="controls bullet"><span class="by">lambdaba</span><span>|</span><a href="#39737477">parent</a><span>|</span><a href="#39737508">prev</a><span>|</span><a href="#39737945">next</a><span>|</span><label class="collapse" for="c-39737501">[-]</label><label class="expand" for="c-39737501">[3 more]</label></div><br/><div class="children"><div class="content">Why not? Mistral was first to do it, it has become tradition.</div><br/><div id="39738495" class="c"><input type="checkbox" id="c-39738495" checked=""/><div class="controls bullet"><span class="by">orlp</span><span>|</span><a href="#39737477">root</a><span>|</span><a href="#39737501">parent</a><span>|</span><a href="#39737529">next</a><span>|</span><label class="collapse" for="c-39738495">[-]</label><label class="expand" for="c-39738495">[1 more]</label></div><br/><div class="children"><div class="content">BitTorrent is just an objectively superior method of delivering a lot of data to a lot of people.</div><br/></div></div><div id="39737529" class="c"><input type="checkbox" id="c-39737529" checked=""/><div class="controls bullet"><span class="by">gillesjacobs</span><span>|</span><a href="#39737477">root</a><span>|</span><a href="#39737501">parent</a><span>|</span><a href="#39738495">prev</a><span>|</span><a href="#39737945">next</a><span>|</span><label class="collapse" for="c-39737529">[-]</label><label class="expand" for="c-39737529">[1 more]</label></div><br/><div class="children"><div class="content">I believe it was Llama 1 that notoriously got leaked with a torrent on 4chan.</div><br/></div></div></div></div><div id="39737945" class="c"><input type="checkbox" id="c-39737945" checked=""/><div class="controls bullet"><span class="by">jiripospisil</span><span>|</span><a href="#39737477">parent</a><span>|</span><a href="#39737501">prev</a><span>|</span><a href="#39738250">next</a><span>|</span><label class="collapse" for="c-39737945">[-]</label><label class="expand" for="c-39737945">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t understand why you&#x27;re being downvoted for asking a legitimate question. People not familiar with model weights might be surprised that they are often in tens of gigabytes and in this case even more.</div><br/></div></div></div></div><div id="39738250" class="c"><input type="checkbox" id="c-39738250" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#39737477">prev</a><span>|</span><a href="#39737366">next</a><span>|</span><label class="collapse" for="c-39738250">[-]</label><label class="expand" for="c-39738250">[3 more]</label></div><br/><div class="children"><div class="content">Is this the first major model to be natively FP8? I was wondering why people hadn&#x27;t done it yet. Seems like a big win when hardware supports it.</div><br/><div id="39740869" class="c"><input type="checkbox" id="c-39740869" checked=""/><div class="controls bullet"><span class="by">a_wild_dandan</span><span>|</span><a href="#39738250">parent</a><span>|</span><a href="#39737366">next</a><span>|</span><label class="collapse" for="c-39740869">[-]</label><label class="expand" for="c-39740869">[2 more]</label></div><br/><div class="children"><div class="content">No, e.g. Yi-34B.</div><br/><div id="39740885" class="c"><input type="checkbox" id="c-39740885" checked=""/><div class="controls bullet"><span class="by">modeless</span><span>|</span><a href="#39738250">root</a><span>|</span><a href="#39740869">parent</a><span>|</span><a href="#39737366">next</a><span>|</span><label class="collapse" for="c-39740885">[-]</label><label class="expand" for="c-39740885">[1 more]</label></div><br/><div class="children"><div class="content">As far as I can tell Yi-34B is natively 16 bit float, the 8 bit version is quantized. <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;01-ai&#x2F;Yi-34B#quantization" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;01-ai&#x2F;Yi-34B#quantization</a></div><br/></div></div></div></div></div></div><div id="39737366" class="c"><input type="checkbox" id="c-39737366" checked=""/><div class="controls bullet"><span class="by">tosh</span><span>|</span><a href="#39738250">prev</a><span>|</span><a href="#39737398">next</a><span>|</span><label class="collapse" for="c-39737366">[-]</label><label class="expand" for="c-39737366">[30 more]</label></div><br/><div class="children"><div class="content">blog post: <a href="https:&#x2F;&#x2F;x.ai&#x2F;blog&#x2F;grok-os" rel="nofollow">https:&#x2F;&#x2F;x.ai&#x2F;blog&#x2F;grok-os</a><p><pre><code>  * 314B parameters (86B active at a time)
  * mixture of experts 8 (2 active at a time)
  * weights and architecture licensed under Apache 2.0
</code></pre>
(edit:) announcement blog post from last year
with benchmarks compared to Claude 2, GPT-3.5 and GPT-4: <a href="https:&#x2F;&#x2F;x.ai&#x2F;blog&#x2F;grok" rel="nofollow">https:&#x2F;&#x2F;x.ai&#x2F;blog&#x2F;grok</a><p>(edit2:)TL;DR: somewhat comparable to GPT-3.5, Mixtral and Qwen-1.5-72B in capability but way larger than the open weight models</div><br/><div id="39737559" class="c"><input type="checkbox" id="c-39737559" checked=""/><div class="controls bullet"><span class="by">OkGoDoIt</span><span>|</span><a href="#39737366">parent</a><span>|</span><a href="#39737832">next</a><span>|</span><label class="collapse" for="c-39737559">[-]</label><label class="expand" for="c-39737559">[16 more]</label></div><br/><div class="children"><div class="content">Is a model so huge that’s only at the level of GPT 3.5 actually good? That seems incredibly inefficient to me.</div><br/><div id="39738731" class="c"><input type="checkbox" id="c-39738731" checked=""/><div class="controls bullet"><span class="by">fwlr</span><span>|</span><a href="#39737366">root</a><span>|</span><a href="#39737559">parent</a><span>|</span><a href="#39738126">next</a><span>|</span><label class="collapse" for="c-39738731">[-]</label><label class="expand" for="c-39738731">[10 more]</label></div><br/><div class="children"><div class="content">OpenAI is valued at 90 billion and all they do is make GPT; Twitter is valued at 40 billion and this was essentially a vanity side-project by a cowboy CEO. Presuming that benchmarks and general “it’s about the level of 3.5” is accurate, it’s inefficient, but not incredibly inefficient imho</div><br/><div id="39740973" class="c"><input type="checkbox" id="c-39740973" checked=""/><div class="controls bullet"><span class="by">thekhatribharat</span><span>|</span><a href="#39737366">root</a><span>|</span><a href="#39738731">parent</a><span>|</span><a href="#39739143">next</a><span>|</span><label class="collapse" for="c-39740973">[-]</label><label class="expand" for="c-39740973">[1 more]</label></div><br/><div class="children"><div class="content">xAI is a separate entity, and not a X&#x2F;Twitter subsidiary.</div><br/></div></div><div id="39739143" class="c"><input type="checkbox" id="c-39739143" checked=""/><div class="controls bullet"><span class="by">pelorat</span><span>|</span><a href="#39737366">root</a><span>|</span><a href="#39738731">parent</a><span>|</span><a href="#39740973">prev</a><span>|</span><a href="#39738126">next</a><span>|</span><label class="collapse" for="c-39739143">[-]</label><label class="expand" for="c-39739143">[8 more]</label></div><br/><div class="children"><div class="content">&gt;  Twitter is valued at 40 billion<p>WAS vaulued at 44B.<p>Now?<p>Maybe 5 billion.</div><br/><div id="39740120" class="c"><input type="checkbox" id="c-39740120" checked=""/><div class="controls bullet"><span class="by">alvah</span><span>|</span><a href="#39737366">root</a><span>|</span><a href="#39739143">parent</a><span>|</span><a href="#39739642">next</a><span>|</span><label class="collapse" for="c-39740120">[-]</label><label class="expand" for="c-39740120">[1 more]</label></div><br/><div class="children"><div class="content">LOL @ $5 billion, but if it that was the valuation, you&#x27;d be making parent&#x27;s point stronger.</div><br/></div></div><div id="39739642" class="c"><input type="checkbox" id="c-39739642" checked=""/><div class="controls bullet"><span class="by">wongarsu</span><span>|</span><a href="#39737366">root</a><span>|</span><a href="#39739143">parent</a><span>|</span><a href="#39740120">prev</a><span>|</span><a href="#39738126">next</a><span>|</span><label class="collapse" for="c-39739642">[-]</label><label class="expand" for="c-39739642">[6 more]</label></div><br/><div class="children"><div class="content">Last I heard they lost 15% of their users, so let&#x27;s call it 36 billion.</div><br/><div id="39740549" class="c"><input type="checkbox" id="c-39740549" checked=""/><div class="controls bullet"><span class="by">dilyevsky</span><span>|</span><a href="#39737366">root</a><span>|</span><a href="#39739642">parent</a><span>|</span><a href="#39739840">next</a><span>|</span><label class="collapse" for="c-39740549">[-]</label><label class="expand" for="c-39740549">[1 more]</label></div><br/><div class="children"><div class="content">They weren&#x27;t even 44B when elon took the keys - he specifically tried to back out of the deal because 44B was insane peak &#x27;21 asset bubble price. In truth they were probably like 10-15B at that moment. And now that bunch of advertisers left due to we know who it&#x27;s probably about 10B</div><br/></div></div><div id="39739840" class="c"><input type="checkbox" id="c-39739840" checked=""/><div class="controls bullet"><span class="by">mceachen</span><span>|</span><a href="#39737366">root</a><span>|</span><a href="#39739642">parent</a><span>|</span><a href="#39740549">prev</a><span>|</span><a href="#39740098">next</a><span>|</span><label class="collapse" for="c-39739840">[-]</label><label class="expand" for="c-39739840">[1 more]</label></div><br/><div class="children"><div class="content">More like $13b.<p><a href="https:&#x2F;&#x2F;arstechnica.com&#x2F;tech-policy&#x2F;2024&#x2F;01&#x2F;since-elon-musks-twitter-purchase-firm-reportedly-lost-72-of-its-value&#x2F;" rel="nofollow">https:&#x2F;&#x2F;arstechnica.com&#x2F;tech-policy&#x2F;2024&#x2F;01&#x2F;since-elon-musks...</a></div><br/></div></div><div id="39740098" class="c"><input type="checkbox" id="c-39740098" checked=""/><div class="controls bullet"><span class="by">wraptile</span><span>|</span><a href="#39737366">root</a><span>|</span><a href="#39739642">parent</a><span>|</span><a href="#39739840">prev</a><span>|</span><a href="#39738126">next</a><span>|</span><label class="collapse" for="c-39740098">[-]</label><label class="expand" for="c-39740098">[3 more]</label></div><br/><div class="children"><div class="content">Twitter didn&#x27;t have direct competitors other than Mastodon when it was taken at 44B. Now there&#x27;s Threads, Bluesky and bigger Mastodon.</div><br/><div id="39740460" class="c"><input type="checkbox" id="c-39740460" checked=""/><div class="controls bullet"><span class="by">jsight</span><span>|</span><a href="#39737366">root</a><span>|</span><a href="#39740098">parent</a><span>|</span><a href="#39740789">next</a><span>|</span><label class="collapse" for="c-39740460">[-]</label><label class="expand" for="c-39740460">[1 more]</label></div><br/><div class="children"><div class="content">Honestly, none of those look like meaningful competitors at the moment.</div><br/></div></div><div id="39740789" class="c"><input type="checkbox" id="c-39740789" checked=""/><div class="controls bullet"><span class="by">squigglydonut</span><span>|</span><a href="#39737366">root</a><span>|</span><a href="#39740098">parent</a><span>|</span><a href="#39740460">prev</a><span>|</span><a href="#39738126">next</a><span>|</span><label class="collapse" for="c-39740789">[-]</label><label class="expand" for="c-39740789">[1 more]</label></div><br/><div class="children"><div class="content">None of these matter</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39738126" class="c"><input type="checkbox" id="c-39738126" checked=""/><div class="controls bullet"><span class="by">drak0n1c</span><span>|</span><a href="#39737366">root</a><span>|</span><a href="#39737559">parent</a><span>|</span><a href="#39738731">prev</a><span>|</span><a href="#39737799">next</a><span>|</span><label class="collapse" for="c-39738126">[-]</label><label class="expand" for="c-39738126">[4 more]</label></div><br/><div class="children"><div class="content">It’s designed to be actively searching real-time posts on X. Apples and oranges.</div><br/><div id="39738558" class="c"><input type="checkbox" id="c-39738558" checked=""/><div class="controls bullet"><span class="by">grey8</span><span>|</span><a href="#39737366">root</a><span>|</span><a href="#39738126">parent</a><span>|</span><a href="#39739816">next</a><span>|</span><label class="collapse" for="c-39738558">[-]</label><label class="expand" for="c-39738558">[1 more]</label></div><br/><div class="children"><div class="content">Why is that relevant to the size?<p>Post search on X is done as it is with any other data from any other source, you use RAG and function calling to insert the context.<p>&lt; 7B open source models can function call very well. In fact, Nous Hermes 2 Pro (7B) is benchmarking better at that then GPT-3.5.<p>Not related to the size, if I&#x27;m not mistaken.</div><br/></div></div><div id="39739816" class="c"><input type="checkbox" id="c-39739816" checked=""/><div class="controls bullet"><span class="by">pests</span><span>|</span><a href="#39737366">root</a><span>|</span><a href="#39738126">parent</a><span>|</span><a href="#39738558">prev</a><span>|</span><a href="#39739023">next</a><span>|</span><label class="collapse" for="c-39739816">[-]</label><label class="expand" for="c-39739816">[1 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t that... the same thing as search?</div><br/></div></div><div id="39739023" class="c"><input type="checkbox" id="c-39739023" checked=""/><div class="controls bullet"><span class="by">hn_20591249</span><span>|</span><a href="#39737366">root</a><span>|</span><a href="#39738126">parent</a><span>|</span><a href="#39739816">prev</a><span>|</span><a href="#39737799">next</a><span>|</span><label class="collapse" for="c-39739023">[-]</label><label class="expand" for="c-39739023">[1 more]</label></div><br/><div class="children"><div class="content">The data pipeline isn&#x27;t included in this release, and we already know it is a pretty simple RAG pipeline using qdrant, <a href="https:&#x2F;&#x2F;twitter.com&#x2F;qdrant_engine&#x2F;status&#x2F;1721097971830260030" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;qdrant_engine&#x2F;status&#x2F;1721097971830260030</a>.<p>Nothing about using data in &quot;real time&quot; predicates that the model parameters need to be this large, and is likely quite inefficient for their &quot;non-woke&quot; instructional use-case.</div><br/></div></div></div></div><div id="39737799" class="c"><input type="checkbox" id="c-39737799" checked=""/><div class="controls bullet"><span class="by">cma</span><span>|</span><a href="#39737366">root</a><span>|</span><a href="#39737559">parent</a><span>|</span><a href="#39738126">prev</a><span>|</span><a href="#39737832">next</a><span>|</span><label class="collapse" for="c-39737799">[-]</label><label class="expand" for="c-39737799">[1 more]</label></div><br/><div class="children"><div class="content">Since it is MoE, quantized it could be able to run on cheaper hardware with just consumer networking inbetween instead of needing epyc&#x2F;xeon levels of PCI-e lanes, nvlink, or infiniband type networking.  Or it could even run with people pooling smaller systems over slow internet links.</div><br/></div></div></div></div><div id="39737832" class="c"><input type="checkbox" id="c-39737832" checked=""/><div class="controls bullet"><span class="by">asciii</span><span>|</span><a href="#39737366">parent</a><span>|</span><a href="#39737559">prev</a><span>|</span><a href="#39737556">next</a><span>|</span><label class="collapse" for="c-39737832">[-]</label><label class="expand" for="c-39737832">[1 more]</label></div><br/><div class="children"><div class="content">I love the citation for image in the article<p>&gt; The cover image was generated using Midjourney based on the following prompt proposed by Grok: A 3D illustration of a neural network, with transparent nodes and glowing connections, showcasing the varying weights as different thicknesses and colors of the connecting lines.</div><br/></div></div><div id="39737556" class="c"><input type="checkbox" id="c-39737556" checked=""/><div class="controls bullet"><span class="by">TOMDM</span><span>|</span><a href="#39737366">parent</a><span>|</span><a href="#39737832">prev</a><span>|</span><a href="#39737826">next</a><span>|</span><label class="collapse" for="c-39737556">[-]</label><label class="expand" for="c-39737556">[2 more]</label></div><br/><div class="children"><div class="content">Mixtral is also comparable to gpt 3.5 and open.<p>At 8x7B it&#x27;s also a fraction of the size. Are there any benchmarks comparing Mixtral to Grok?</div><br/><div id="39737635" class="c"><input type="checkbox" id="c-39737635" checked=""/><div class="controls bullet"><span class="by">tosh</span><span>|</span><a href="#39737366">root</a><span>|</span><a href="#39737556">parent</a><span>|</span><a href="#39737826">next</a><span>|</span><label class="collapse" for="c-39737635">[-]</label><label class="expand" for="c-39737635">[1 more]</label></div><br/><div class="children"><div class="content">Mixtral announcement is here: <a href="https:&#x2F;&#x2F;mistral.ai&#x2F;news&#x2F;mixtral-of-experts&#x2F;" rel="nofollow">https:&#x2F;&#x2F;mistral.ai&#x2F;news&#x2F;mixtral-of-experts&#x2F;</a><p>Mixtral looks more economical @ capability to size (similar also for Qwen 1.5 72b)</div><br/></div></div></div></div><div id="39737826" class="c"><input type="checkbox" id="c-39737826" checked=""/><div class="controls bullet"><span class="by">tootie</span><span>|</span><a href="#39737366">parent</a><span>|</span><a href="#39737556">prev</a><span>|</span><a href="#39737398">next</a><span>|</span><label class="collapse" for="c-39737826">[-]</label><label class="expand" for="c-39737826">[10 more]</label></div><br/><div class="children"><div class="content">How is it that OpenAI was touted like it was some massive years-long effort that blew all AI research out of the water and now we have so many competitors popping up one after another?</div><br/><div id="39737984" class="c"><input type="checkbox" id="c-39737984" checked=""/><div class="controls bullet"><span class="by">longdog</span><span>|</span><a href="#39737366">root</a><span>|</span><a href="#39737826">parent</a><span>|</span><a href="#39737993">next</a><span>|</span><label class="collapse" for="c-39737984">[-]</label><label class="expand" for="c-39737984">[6 more]</label></div><br/><div class="children"><div class="content">You don&#x27;t need to be a cutting edge research scientist to train a SOTA LLM. You just need money for scaling. OpenAI&#x27;s &quot;secret&quot; was just their willingness to spend tens&#x2F;hundreds of millions without guaranteed returns, and RLHF&#x2F;instruct fine tuning, both of which are out of the bag now.</div><br/><div id="39738112" class="c"><input type="checkbox" id="c-39738112" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#39737366">root</a><span>|</span><a href="#39737984">parent</a><span>|</span><a href="#39737993">next</a><span>|</span><label class="collapse" for="c-39738112">[-]</label><label class="expand" for="c-39738112">[5 more]</label></div><br/><div class="children"><div class="content">Disagree. It took more than 12 months from the release of GPT-4 to someone else producing a model of equivalent quality, and that definitely wasn&#x27;t due to a shortage of investment from the competition.<p>There&#x27;s a huge amount of depth in training a really good LLM. Not helped by the fact that iteration is incredibly expensive - it might take several months (and millions of dollars) before you can tell if your new model is working well or if there was some mistake in the pipeline that lead to a poor quality result.<p>Almost all of the world-class LLMs outside of OpenAI&#x2F;DeepMind have been trained by people who previously worked at those organizations - giving them invaluable experience such that they could avoid the most expensive mistakes while training their new models.</div><br/><div id="39740714" class="c"><input type="checkbox" id="c-39740714" checked=""/><div class="controls bullet"><span class="by">barrell</span><span>|</span><a href="#39737366">root</a><span>|</span><a href="#39738112">parent</a><span>|</span><a href="#39741399">next</a><span>|</span><label class="collapse" for="c-39740714">[-]</label><label class="expand" for="c-39740714">[1 more]</label></div><br/><div class="children"><div class="content">While I do agree there is some amount of secret sauce, keep in mind the training takes several months. So from someone to see the success of GPT4, decide they want to invest that amount of money to train the same, raise the money to train the model, find someone competent to supervise the training, train the model for several months, then test and integrate it could easily be a year long even if there was no secret sauce.</div><br/></div></div><div id="39741399" class="c"><input type="checkbox" id="c-39741399" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#39737366">root</a><span>|</span><a href="#39738112">parent</a><span>|</span><a href="#39740714">prev</a><span>|</span><a href="#39738201">next</a><span>|</span><label class="collapse" for="c-39741399">[-]</label><label class="expand" for="c-39741399">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s still no model of equivalent quality to GPT-4.</div><br/></div></div><div id="39738201" class="c"><input type="checkbox" id="c-39738201" checked=""/><div class="controls bullet"><span class="by">lossolo</span><span>|</span><a href="#39737366">root</a><span>|</span><a href="#39738112">parent</a><span>|</span><a href="#39741399">prev</a><span>|</span><a href="#39738230">next</a><span>|</span><label class="collapse" for="c-39738201">[-]</label><label class="expand" for="c-39738201">[1 more]</label></div><br/><div class="children"><div class="content">Don’t overlook the training data (used for both training and instruction fine-tuning), it is one of the most crucial aspects, if not the most critical, given the significant differences observed in models with similar architectures.</div><br/></div></div><div id="39738230" class="c"><input type="checkbox" id="c-39738230" checked=""/><div class="controls bullet"><span class="by">echelon</span><span>|</span><a href="#39737366">root</a><span>|</span><a href="#39738112">parent</a><span>|</span><a href="#39738201">prev</a><span>|</span><a href="#39737993">next</a><span>|</span><label class="collapse" for="c-39738230">[-]</label><label class="expand" for="c-39738230">[1 more]</label></div><br/><div class="children"><div class="content">That only remains an advantage if they can continue climbing the gradient from their lead position. If they hit a snag in scaling, methodology, or research, everyone else on the planet catches up, and then it&#x27;s anyone&#x27;s game again.</div><br/></div></div></div></div></div></div><div id="39737993" class="c"><input type="checkbox" id="c-39737993" checked=""/><div class="controls bullet"><span class="by">cavisne</span><span>|</span><a href="#39737366">root</a><span>|</span><a href="#39737826">parent</a><span>|</span><a href="#39737984">prev</a><span>|</span><a href="#39738147">next</a><span>|</span><label class="collapse" for="c-39737993">[-]</label><label class="expand" for="c-39737993">[1 more]</label></div><br/><div class="children"><div class="content">LLM training is arcane and expensive to experiment with. So OpenAI had to waste a lot of time and GPU-hours on things that didn&#x27;t work to learn the tricks that did work.<p>Most of the competitors have lineage straight back to OpenAI, eg the lead of x.ai was previously at OpenAI and Deepmind. Likewise with Mistral and especially Anthropic.</div><br/></div></div><div id="39738147" class="c"><input type="checkbox" id="c-39738147" checked=""/><div class="controls bullet"><span class="by">jxy</span><span>|</span><a href="#39737366">root</a><span>|</span><a href="#39737826">parent</a><span>|</span><a href="#39737993">prev</a><span>|</span><a href="#39737908">next</a><span>|</span><label class="collapse" for="c-39738147">[-]</label><label class="expand" for="c-39738147">[1 more]</label></div><br/><div class="children"><div class="content">OpenAI still seems to be at the top, except for Anthropic, who may be close, in terms of the capabilities comparing gpt-4 and claude-opus.<p>This Grok-1 is a large model (~314B), which matches gpt-3.5 released 2 years ago, and at about the same level of much smaller models like, mixtral (~47B) and qwen-1.5 (~72B). Do you think it&#x27;s competitive?</div><br/></div></div><div id="39737908" class="c"><input type="checkbox" id="c-39737908" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#39737366">root</a><span>|</span><a href="#39737826">parent</a><span>|</span><a href="#39738147">prev</a><span>|</span><a href="#39737398">next</a><span>|</span><label class="collapse" for="c-39737908">[-]</label><label class="expand" for="c-39737908">[1 more]</label></div><br/><div class="children"><div class="content">Egg of Columbus.<p>Also, the general architecture is well documented, ChatGPT (specifically the chat interface, not GPT-3, not InstructGPT) is what made a lot of people <i>care</i>, and actually reproducing it requires someone wanting to in the first place.</div><br/></div></div></div></div></div></div><div id="39737398" class="c"><input type="checkbox" id="c-39737398" checked=""/><div class="controls bullet"><span class="by">hubraumhugo</span><span>|</span><a href="#39737366">prev</a><span>|</span><a href="#39737624">next</a><span>|</span><label class="collapse" for="c-39737398">[-]</label><label class="expand" for="c-39737398">[35 more]</label></div><br/><div class="children"><div class="content">When will we reach an upper limit&#x2F;dimishing returns in terms of number of parameters and mixture of experts?</div><br/><div id="39737418" class="c"><input type="checkbox" id="c-39737418" checked=""/><div class="controls bullet"><span class="by">andy99</span><span>|</span><a href="#39737398">parent</a><span>|</span><a href="#39737624">next</a><span>|</span><label class="collapse" for="c-39737418">[-]</label><label class="expand" for="c-39737418">[34 more]</label></div><br/><div class="children"><div class="content">We may have already - data is more important than anything else which is why nobody has beat GPT4 yet. Throwing more parameters or more compute at the problem only gets you so far. But Grok was never a contender so there is room to improve on it. It is one of the biggest models open sourced as mentioned, so will be interesting to take a look at for sure.</div><br/><div id="39737434" class="c"><input type="checkbox" id="c-39737434" checked=""/><div class="controls bullet"><span class="by">lambdaba</span><span>|</span><a href="#39737398">root</a><span>|</span><a href="#39737418">parent</a><span>|</span><a href="#39737455">next</a><span>|</span><label class="collapse" for="c-39737434">[-]</label><label class="expand" for="c-39737434">[17 more]</label></div><br/><div class="children"><div class="content">Claude 3 has *decisively* beat GPT-4, I wonder how all their attributes compare.</div><br/><div id="39738133" class="c"><input type="checkbox" id="c-39738133" checked=""/><div class="controls bullet"><span class="by">orbital-decay</span><span>|</span><a href="#39737398">root</a><span>|</span><a href="#39737434">parent</a><span>|</span><a href="#39737849">next</a><span>|</span><label class="collapse" for="c-39738133">[-]</label><label class="expand" for="c-39738133">[3 more]</label></div><br/><div class="children"><div class="content">Has it, though? LMSys Arena Leaderboard (blind ranking by humans) [0] positions Opus just below GPT-4 with a negligible ELO gap.<p>[0] <a href="https:&#x2F;&#x2F;chat.lmsys.org&#x2F;" rel="nofollow">https:&#x2F;&#x2F;chat.lmsys.org&#x2F;</a></div><br/><div id="39738377" class="c"><input type="checkbox" id="c-39738377" checked=""/><div class="controls bullet"><span class="by">espadrine</span><span>|</span><a href="#39737398">root</a><span>|</span><a href="#39738133">parent</a><span>|</span><a href="#39739180">next</a><span>|</span><label class="collapse" for="c-39738377">[-]</label><label class="expand" for="c-39738377">[1 more]</label></div><br/><div class="children"><div class="content">A number of AI companies have a naming&#x2F;reproducibility issue.<p>GPT4 Turbo, released last November, is a separate version that is much better than GPT-4 (winning 70% of human preferences in blind tests), released in March 2023.<p>Claude 3 Opus beats release-day GPT-4 (winning 60% of human preferences), but not GPT-4 Turbo.<p>In the LMSys leaderboard, release-day GPT-4 is labeled gpt-4-0314, and GPT4 Turbo is labeled gpt-4-1106-preview.</div><br/></div></div><div id="39739180" class="c"><input type="checkbox" id="c-39739180" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#39737398">root</a><span>|</span><a href="#39738133">parent</a><span>|</span><a href="#39738377">prev</a><span>|</span><a href="#39737849">next</a><span>|</span><label class="collapse" for="c-39739180">[-]</label><label class="expand" for="c-39739180">[1 more]</label></div><br/><div class="children"><div class="content">Chatbot Arena is not a blind ranking.<p>Many, if not most, users intentionally ask the models questions to tease out their canned disclaimers: so they know exactly which model is answering.<p>On one hand it&#x27;s fair to say disclaimers affect the usefulness of the model, but on the other I don&#x27;t think most people are solely asking these LLMs to produce meth or say &quot;fuck&quot;, and that has an outsized effect on the usefulness of Chatbot Arena as a general benchmark.<p>I personally recommend people use it at most as a way to directly test specific LLMs and ignore it as a benchmark.</div><br/></div></div></div></div><div id="39737849" class="c"><input type="checkbox" id="c-39737849" checked=""/><div class="controls bullet"><span class="by">swalsh</span><span>|</span><a href="#39737398">root</a><span>|</span><a href="#39737434">parent</a><span>|</span><a href="#39738133">prev</a><span>|</span><a href="#39737639">next</a><span>|</span><label class="collapse" for="c-39737849">[-]</label><label class="expand" for="c-39737849">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know if Claude is &quot;smarter&quot; in any significant way.  But its harder working.  I can ask it for some code, and I never get a placeholder.  It dutifully gives me the code I need.</div><br/><div id="39737974" class="c"><input type="checkbox" id="c-39737974" checked=""/><div class="controls bullet"><span class="by">lambdaba</span><span>|</span><a href="#39737398">root</a><span>|</span><a href="#39737849">parent</a><span>|</span><a href="#39737639">next</a><span>|</span><label class="collapse" for="c-39737974">[-]</label><label class="expand" for="c-39737974">[1 more]</label></div><br/><div class="children"><div class="content">It understands instructions better, it&#x27;s rarer to have it misunderstand, and I have to be less careful with prompting.</div><br/></div></div></div></div><div id="39737639" class="c"><input type="checkbox" id="c-39737639" checked=""/><div class="controls bullet"><span class="by">stainablesteel</span><span>|</span><a href="#39737398">root</a><span>|</span><a href="#39737434">parent</a><span>|</span><a href="#39737849">prev</a><span>|</span><a href="#39737697">next</a><span>|</span><label class="collapse" for="c-39737639">[-]</label><label class="expand" for="c-39737639">[9 more]</label></div><br/><div class="children"><div class="content">i like some of claudes answers better, but it doesnt seem to be a better coder imo</div><br/><div id="39737684" class="c"><input type="checkbox" id="c-39737684" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#39737398">root</a><span>|</span><a href="#39737639">parent</a><span>|</span><a href="#39738661">next</a><span>|</span><label class="collapse" for="c-39737684">[-]</label><label class="expand" for="c-39737684">[7 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve found it to be significantly better for code than GPT-4 - I&#x27;ve had multiple examples where the GPT-4 solution contained bugs but the Claude 3 Opus solution was exactly what I wanted. One recent example: <a href="https:&#x2F;&#x2F;fedi.simonwillison.net&#x2F;@simon&#x2F;112057299607427949" rel="nofollow">https:&#x2F;&#x2F;fedi.simonwillison.net&#x2F;@simon&#x2F;112057299607427949</a><p>How well models work varies wildly according to your personal prompting style though - it&#x27;s possible I just have a prompting style which happens to work better with Claude 3.</div><br/><div id="39737865" class="c"><input type="checkbox" id="c-39737865" checked=""/><div class="controls bullet"><span class="by">asciii</span><span>|</span><a href="#39737398">root</a><span>|</span><a href="#39737684">parent</a><span>|</span><a href="#39737846">next</a><span>|</span><label class="collapse" for="c-39737865">[-]</label><label class="expand" for="c-39737865">[1 more]</label></div><br/><div class="children"><div class="content">&gt; according to your personal prompting style though<p>I like the notion of someone’s personal prompting style (seems like a proxy for those that can prepare a question with context about the other’s knowledge) - that’s interesting for these systems in future job interviews</div><br/></div></div><div id="39737846" class="c"><input type="checkbox" id="c-39737846" checked=""/><div class="controls bullet"><span class="by">bugglebeetle</span><span>|</span><a href="#39737398">root</a><span>|</span><a href="#39737684">parent</a><span>|</span><a href="#39737865">prev</a><span>|</span><a href="#39738661">next</a><span>|</span><label class="collapse" for="c-39737846">[-]</label><label class="expand" for="c-39737846">[5 more]</label></div><br/><div class="children"><div class="content">What is your code prompting style for Claude? I’ve tried to repurpose some of my GPT-4 ones for Claude and have noticed some degradation. I use the “Act as a software developer&#x2F;write a spec&#x2F;implement step-by-step” CoT style.</div><br/><div id="39738141" class="c"><input type="checkbox" id="c-39738141" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#39737398">root</a><span>|</span><a href="#39737846">parent</a><span>|</span><a href="#39738643">next</a><span>|</span><label class="collapse" for="c-39738141">[-]</label><label class="expand" for="c-39738141">[3 more]</label></div><br/><div class="children"><div class="content">Almost impossible to describe prompting style, but here are some examples of how I&#x27;ve used Claude 3:<p><a href="https:&#x2F;&#x2F;gist.github.com&#x2F;simonw&#x2F;4cecde4a729f4da0b5059b50c8e01359" rel="nofollow">https:&#x2F;&#x2F;gist.github.com&#x2F;simonw&#x2F;4cecde4a729f4da0b5059b50c8e01...</a> - writing a Python function<p><a href="https:&#x2F;&#x2F;gist.github.com&#x2F;simonw&#x2F;408fcf28e9fc6bb2233aae694f8cd1f4" rel="nofollow">https:&#x2F;&#x2F;gist.github.com&#x2F;simonw&#x2F;408fcf28e9fc6bb2233aae694f8cd...</a> - most sophisticated example, building a JavaScript command palette<p><a href="https:&#x2F;&#x2F;gist.github.com&#x2F;simonw&#x2F;2002e2b56a97053bd9302a34e0b83074" rel="nofollow">https:&#x2F;&#x2F;gist.github.com&#x2F;simonw&#x2F;2002e2b56a97053bd9302a34e0b83...</a> - asking it to refactor some existing code<p>I don&#x27;t use the &quot;Act as a X&quot; format any more, I&#x27;m not at all convinced it has a noticeable impact on quality. I think it&#x27;s yet another example of LLM superstition.</div><br/><div id="39738253" class="c"><input type="checkbox" id="c-39738253" checked=""/><div class="controls bullet"><span class="by">lgas</span><span>|</span><a href="#39737398">root</a><span>|</span><a href="#39738141">parent</a><span>|</span><a href="#39738371">next</a><span>|</span><label class="collapse" for="c-39738253">[-]</label><label class="expand" for="c-39738253">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I don&#x27;t use the &quot;Act as a X&quot; format any more, I&#x27;m not at all convinced it has a noticeable impact on quality. I think it&#x27;s yet another example of LLM superstition.<p>It&#x27;s very contextually dependent.  You really have to things like this for your specific task, with your specific model, etc.  Sometimes it helps, sometimes it hurts, and sometimes it does nothing at all.</div><br/></div></div><div id="39738371" class="c"><input type="checkbox" id="c-39738371" checked=""/><div class="controls bullet"><span class="by">bugglebeetle</span><span>|</span><a href="#39737398">root</a><span>|</span><a href="#39738141">parent</a><span>|</span><a href="#39738253">prev</a><span>|</span><a href="#39738643">next</a><span>|</span><label class="collapse" for="c-39738371">[-]</label><label class="expand" for="c-39738371">[1 more]</label></div><br/><div class="children"><div class="content">Super helpful! Thanks!</div><br/></div></div></div></div><div id="39738643" class="c"><input type="checkbox" id="c-39738643" checked=""/><div class="controls bullet"><span class="by">furyofantares</span><span>|</span><a href="#39737398">root</a><span>|</span><a href="#39737846">parent</a><span>|</span><a href="#39738141">prev</a><span>|</span><a href="#39738661">next</a><span>|</span><label class="collapse" for="c-39738643">[-]</label><label class="expand" for="c-39738643">[1 more]</label></div><br/><div class="children"><div class="content">I didn&#x27;t know people were still doing this &quot;act as etc etc&quot; instructional prompting.<p>I just tell it my coding problem. Or when making something from scratch, ask for small things and incrementally add.</div><br/></div></div></div></div></div></div><div id="39738661" class="c"><input type="checkbox" id="c-39738661" checked=""/><div class="controls bullet"><span class="by">furyofantares</span><span>|</span><a href="#39737398">root</a><span>|</span><a href="#39737639">parent</a><span>|</span><a href="#39737684">prev</a><span>|</span><a href="#39737697">next</a><span>|</span><label class="collapse" for="c-39738661">[-]</label><label class="expand" for="c-39738661">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve found it significantly better than GPT4 for code and it&#x27;s become my go-to for coding.<p>That&#x27;s actually saying something, because there&#x27;s also serious drawbacks.<p>- Feels a little slower. Might just be UI<p>- I have a lot of experience prompting GPT4<p>- I don&#x27;t like using it for non-code because it gives me to much &quot;safety&quot; pushback<p>- No custom instructions. ChatGPT knows I use macos and zsh and a few other preferences that I&#x27;d rather not have to type into my queries frequently<p>I find all of the above kind of annoying and I don&#x27;t like having two different LLMs I go to daily. But I mention it because it&#x27;s a fairly significant hurdle it had to overcome to become the main thing I use for coding! There were a number of things where I gave up on GPT then went to Claude and it did great; never had the reverse experience so far and overall just feels like I&#x27;ve had noticeably better responses.</div><br/></div></div></div></div><div id="39737697" class="c"><input type="checkbox" id="c-39737697" checked=""/><div class="controls bullet"><span class="by">htrp</span><span>|</span><a href="#39737398">root</a><span>|</span><a href="#39737434">parent</a><span>|</span><a href="#39737639">prev</a><span>|</span><a href="#39737455">next</a><span>|</span><label class="collapse" for="c-39737697">[-]</label><label class="expand" for="c-39737697">[2 more]</label></div><br/><div class="children"><div class="content">citation needed (other than &#x27;vibes&#x27;)</div><br/></div></div></div></div><div id="39737455" class="c"><input type="checkbox" id="c-39737455" checked=""/><div class="controls bullet"><span class="by">squigz</span><span>|</span><a href="#39737398">root</a><span>|</span><a href="#39737418">parent</a><span>|</span><a href="#39737434">prev</a><span>|</span><a href="#39737836">next</a><span>|</span><label class="collapse" for="c-39737455">[-]</label><label class="expand" for="c-39737455">[3 more]</label></div><br/><div class="children"><div class="content">I think Groq is something else?</div><br/><div id="39737483" class="c"><input type="checkbox" id="c-39737483" checked=""/><div class="controls bullet"><span class="by">LorenDB</span><span>|</span><a href="#39737398">root</a><span>|</span><a href="#39737455">parent</a><span>|</span><a href="#39737721">next</a><span>|</span><label class="collapse" for="c-39737483">[-]</label><label class="expand" for="c-39737483">[1 more]</label></div><br/><div class="children"><div class="content">Indeed, Groq is a company building inference accelerators. Grok is completely unaffiliated.</div><br/></div></div><div id="39737721" class="c"><input type="checkbox" id="c-39737721" checked=""/><div class="controls bullet"><span class="by">andy99</span><span>|</span><a href="#39737398">root</a><span>|</span><a href="#39737455">parent</a><span>|</span><a href="#39737483">prev</a><span>|</span><a href="#39737836">next</a><span>|</span><label class="collapse" for="c-39737721">[-]</label><label class="expand" for="c-39737721">[1 more]</label></div><br/><div class="children"><div class="content">Edited, I did mean the Grok in the article not the inference chip.</div><br/></div></div></div></div><div id="39737553" class="c"><input type="checkbox" id="c-39737553" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#39737398">root</a><span>|</span><a href="#39737418">parent</a><span>|</span><a href="#39737836">prev</a><span>|</span><a href="#39737709">next</a><span>|</span><label class="collapse" for="c-39737553">[-]</label><label class="expand" for="c-39737553">[9 more]</label></div><br/><div class="children"><div class="content">There is no reason to believe GPT-4 had more(or higher quality) data than Google etc. has now. GPT-4 was entirely trained before the Microsoft deal. If OpenAI could pay to acquire data in 2023, &gt;10 companies could acquire similar quality by now, and no one has similar quality model in a year.</div><br/><div id="39737779" class="c"><input type="checkbox" id="c-39737779" checked=""/><div class="controls bullet"><span class="by">austhrow743</span><span>|</span><a href="#39737398">root</a><span>|</span><a href="#39737553">parent</a><span>|</span><a href="#39737709">next</a><span>|</span><label class="collapse" for="c-39737779">[-]</label><label class="expand" for="c-39737779">[8 more]</label></div><br/><div class="children"><div class="content">The more disregard a company has for intellectual property rights, the more data they can use.<p>Google had far more to lose from a &quot;copyright? lol&quot; approach than OpenAI did.</div><br/><div id="39740947" class="c"><input type="checkbox" id="c-39740947" checked=""/><div class="controls bullet"><span class="by">supafastcoder</span><span>|</span><a href="#39737398">root</a><span>|</span><a href="#39737779">parent</a><span>|</span><a href="#39737867">next</a><span>|</span><label class="collapse" for="c-39740947">[-]</label><label class="expand" for="c-39740947">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Google had far more to lose from a &quot;copyright? lol&quot; approach than OpenAI did.<p>The company that scrapes trillions of web pages has an issue with copyright?</div><br/></div></div><div id="39737867" class="c"><input type="checkbox" id="c-39737867" checked=""/><div class="controls bullet"><span class="by">brookst</span><span>|</span><a href="#39737398">root</a><span>|</span><a href="#39737779">parent</a><span>|</span><a href="#39740947">prev</a><span>|</span><a href="#39740385">next</a><span>|</span><label class="collapse" for="c-39737867">[-]</label><label class="expand" for="c-39737867">[5 more]</label></div><br/><div class="children"><div class="content">I was under the impression training was at best an undefined area of IP law. Is there any aspect of copyright that prohibits training models?</div><br/><div id="39738160" class="c"><input type="checkbox" id="c-39738160" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#39737398">root</a><span>|</span><a href="#39737867">parent</a><span>|</span><a href="#39740385">next</a><span>|</span><label class="collapse" for="c-39738160">[-]</label><label class="expand" for="c-39738160">[4 more]</label></div><br/><div class="children"><div class="content">This is being tested by a number of lawsuits right now, most notably the NY Times one: <a href="https:&#x2F;&#x2F;nytco-assets.nytimes.com&#x2F;2023&#x2F;12&#x2F;NYT_Complaint_Dec2023.pdf" rel="nofollow">https:&#x2F;&#x2F;nytco-assets.nytimes.com&#x2F;2023&#x2F;12&#x2F;NYT_Complaint_Dec20...</a><p>The key questions are around &quot;fair use&quot;. Part of the US doctrine of fair use is &quot;the effect of the use upon the potential market for or value of the copyrighted work&quot; - so one big question here is whether a model has a negative impact on the market for the copyrighted work it was trained on.</div><br/><div id="39738331" class="c"><input type="checkbox" id="c-39738331" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#39737398">root</a><span>|</span><a href="#39738160">parent</a><span>|</span><a href="#39740385">next</a><span>|</span><label class="collapse" for="c-39738331">[-]</label><label class="expand" for="c-39738331">[3 more]</label></div><br/><div class="children"><div class="content">I don’t think the New York Times thing is that much about training, than it is about the fact that ChatGPT can use Bing and Bing has access to New York Times articles for search purposes.</div><br/><div id="39738354" class="c"><input type="checkbox" id="c-39738354" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#39737398">root</a><span>|</span><a href="#39738331">parent</a><span>|</span><a href="#39740385">next</a><span>|</span><label class="collapse" for="c-39738354">[-]</label><label class="expand" for="c-39738354">[2 more]</label></div><br/><div class="children"><div class="content">If you read the lawsuit it&#x27;s absolutely about training. The Bing RAG piece is one of the complaints in there but it&#x27;s by no means the most important.<p>Take a look at <a href="https:&#x2F;&#x2F;nytco-assets.nytimes.com&#x2F;2023&#x2F;12&#x2F;NYT_Complaint_Dec2023.pdf" rel="nofollow">https:&#x2F;&#x2F;nytco-assets.nytimes.com&#x2F;2023&#x2F;12&#x2F;NYT_Complaint_Dec20...</a> - bullet points 2 and 4 on pages 2&#x2F;3 are about training data. Bullet point 5 is the Bing RAG thing.</div><br/><div id="39739173" class="c"><input type="checkbox" id="c-39739173" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#39737398">root</a><span>|</span><a href="#39738354">parent</a><span>|</span><a href="#39740385">next</a><span>|</span><label class="collapse" for="c-39739173">[-]</label><label class="expand" for="c-39739173">[1 more]</label></div><br/><div class="children"><div class="content">Ah, thanks!</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39740385" class="c"><input type="checkbox" id="c-39740385" checked=""/><div class="controls bullet"><span class="by">YetAnotherNick</span><span>|</span><a href="#39737398">root</a><span>|</span><a href="#39737779">parent</a><span>|</span><a href="#39737867">prev</a><span>|</span><a href="#39737709">next</a><span>|</span><label class="collapse" for="c-39740385">[-]</label><label class="expand" for="c-39740385">[1 more]</label></div><br/><div class="children"><div class="content">Having used both Google&#x27;s and OpenAI&#x27;s models, the kind of issue they have are different. Google&#x27;s models are superior or at least on par in knowledge. It&#x27;s the instruction following and understanding where OpenAI is significantly better. I don&#x27;t think pretraining data is the reason of this.</div><br/></div></div></div></div></div></div><div id="39737709" class="c"><input type="checkbox" id="c-39737709" checked=""/><div class="controls bullet"><span class="by">ldjkfkdsjnv</span><span>|</span><a href="#39737398">root</a><span>|</span><a href="#39737418">parent</a><span>|</span><a href="#39737553">prev</a><span>|</span><a href="#39737624">next</a><span>|</span><label class="collapse" for="c-39737709">[-]</label><label class="expand" for="c-39737709">[3 more]</label></div><br/><div class="children"><div class="content">Claude &gt; GPT4. Anyone using these models on a daily basis knows this</div><br/><div id="39741463" class="c"><input type="checkbox" id="c-39741463" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#39737398">root</a><span>|</span><a href="#39737709">parent</a><span>|</span><a href="#39737783">next</a><span>|</span><label class="collapse" for="c-39741463">[-]</label><label class="expand" for="c-39741463">[1 more]</label></div><br/><div class="children"><div class="content">I use these models regularly, and Claude is dumb as a rock compared to GPT-4.</div><br/></div></div><div id="39737783" class="c"><input type="checkbox" id="c-39737783" checked=""/><div class="controls bullet"><span class="by">jstummbillig</span><span>|</span><a href="#39737398">root</a><span>|</span><a href="#39737709">parent</a><span>|</span><a href="#39741463">prev</a><span>|</span><a href="#39737624">next</a><span>|</span><label class="collapse" for="c-39737783">[-]</label><label class="expand" for="c-39737783">[1 more]</label></div><br/><div class="children"><div class="content">It is known</div><br/></div></div></div></div></div></div></div></div><div id="39737624" class="c"><input type="checkbox" id="c-39737624" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#39737398">prev</a><span>|</span><a href="#39740465">next</a><span>|</span><label class="collapse" for="c-39737624">[-]</label><label class="expand" for="c-39737624">[1 more]</label></div><br/><div class="children"><div class="content">Is there a model card anywhere? I&#x27;d like to know what it was trained on.</div><br/></div></div><div id="39740465" class="c"><input type="checkbox" id="c-39740465" checked=""/><div class="controls bullet"><span class="by">aussieguy1234</span><span>|</span><a href="#39737624">prev</a><span>|</span><a href="#39737596">next</a><span>|</span><label class="collapse" for="c-39740465">[-]</label><label class="expand" for="c-39740465">[1 more]</label></div><br/><div class="children"><div class="content">How hard would it be for an open source group to fine tune this into a chatbot?</div><br/></div></div><div id="39737596" class="c"><input type="checkbox" id="c-39737596" checked=""/><div class="controls bullet"><span class="by">moralestapia</span><span>|</span><a href="#39740465">prev</a><span>|</span><a href="#39739274">next</a><span>|</span><label class="collapse" for="c-39737596">[-]</label><label class="expand" for="c-39737596">[6 more]</label></div><br/><div class="children"><div class="content">Well, he delivered.</div><br/><div id="39737954" class="c"><input type="checkbox" id="c-39737954" checked=""/><div class="controls bullet"><span class="by">paxys</span><span>|</span><a href="#39737596">parent</a><span>|</span><a href="#39739274">next</a><span>|</span><label class="collapse" for="c-39737954">[-]</label><label class="expand" for="c-39737954">[5 more]</label></div><br/><div class="children"><div class="content">Partially. Open weights is not open source.</div><br/><div id="39738340" class="c"><input type="checkbox" id="c-39738340" checked=""/><div class="controls bullet"><span class="by">gfodor</span><span>|</span><a href="#39737596">root</a><span>|</span><a href="#39737954">parent</a><span>|</span><a href="#39739817">next</a><span>|</span><label class="collapse" for="c-39738340">[-]</label><label class="expand" for="c-39738340">[3 more]</label></div><br/><div class="children"><div class="content">In machine learning models the term open source has been largely accepted to mean sharing weights and, if necessary, inference code. You can argue if this is an abuse of the term but everyone does it, and saying someone didn’t deliver if they used it and published weights would probably mean saying the same about mistral, meta, etc.</div><br/><div id="39738976" class="c"><input type="checkbox" id="c-39738976" checked=""/><div class="controls bullet"><span class="by">asadotzler</span><span>|</span><a href="#39737596">root</a><span>|</span><a href="#39738340">parent</a><span>|</span><a href="#39739817">next</a><span>|</span><label class="collapse" for="c-39738976">[-]</label><label class="expand" for="c-39738976">[2 more]</label></div><br/><div class="children"><div class="content">Yes. So say the same thing about them
 Open source has a definition and abusing that hurts all of us except the billionaires.</div><br/><div id="39739305" class="c"><input type="checkbox" id="c-39739305" checked=""/><div class="controls bullet"><span class="by">moralestapia</span><span>|</span><a href="#39737596">root</a><span>|</span><a href="#39738976">parent</a><span>|</span><a href="#39739817">next</a><span>|</span><label class="collapse" for="c-39739305">[-]</label><label class="expand" for="c-39739305">[1 more]</label></div><br/><div class="children"><div class="content">I get the &quot;open source&quot; argument, but what is the issue here?<p>If you are able to reproduce the thing in its entirety and you&#x27;re given no restrictions on its use, it seems compatible with the spirit of open sourcing things.</div><br/></div></div></div></div></div></div><div id="39739817" class="c"><input type="checkbox" id="c-39739817" checked=""/><div class="controls bullet"><span class="by">xcv123</span><span>|</span><a href="#39737596">root</a><span>|</span><a href="#39737954">parent</a><span>|</span><a href="#39738340">prev</a><span>|</span><a href="#39739274">next</a><span>|</span><label class="collapse" for="c-39739817">[-]</label><label class="expand" for="c-39739817">[1 more]</label></div><br/><div class="children"><div class="content">The architecture of the model is open source. Not just the weights. You can run the entire thing locally.</div><br/></div></div></div></div></div></div><div id="39739274" class="c"><input type="checkbox" id="c-39739274" checked=""/><div class="controls bullet"><span class="by">sqreept</span><span>|</span><a href="#39737596">prev</a><span>|</span><a href="#39737654">next</a><span>|</span><label class="collapse" for="c-39739274">[-]</label><label class="expand" for="c-39739274">[2 more]</label></div><br/><div class="children"><div class="content">What are the languages supported by it?</div><br/><div id="39739333" class="c"><input type="checkbox" id="c-39739333" checked=""/><div class="controls bullet"><span class="by">cyanydeez</span><span>|</span><a href="#39739274">parent</a><span>|</span><a href="#39737654">next</a><span>|</span><label class="collapse" for="c-39739333">[-]</label><label class="expand" for="c-39739333">[1 more]</label></div><br/><div class="children"><div class="content">Tweets.</div><br/></div></div></div></div><div id="39737654" class="c"><input type="checkbox" id="c-39737654" checked=""/><div class="controls bullet"><span class="by">LZ_Khan</span><span>|</span><a href="#39739274">prev</a><span>|</span><a href="#39738115">next</a><span>|</span><label class="collapse" for="c-39737654">[-]</label><label class="expand" for="c-39737654">[3 more]</label></div><br/><div class="children"><div class="content">How are people&#x27;s experience with this model? Having the most weights is one thing but being a better model than the 70B models is another.</div><br/><div id="39737807" class="c"><input type="checkbox" id="c-39737807" checked=""/><div class="controls bullet"><span class="by">swalsh</span><span>|</span><a href="#39737654">parent</a><span>|</span><a href="#39737715">next</a><span>|</span><label class="collapse" for="c-39737807">[-]</label><label class="expand" for="c-39737807">[1 more]</label></div><br/><div class="children"><div class="content">I use grok all the time to find tweets or ask about trends on Twitter.  For that it&#x27;s better than what used to exist.  But its not a great model outside that narrow use case.</div><br/></div></div><div id="39737715" class="c"><input type="checkbox" id="c-39737715" checked=""/><div class="controls bullet"><span class="by">labrador</span><span>|</span><a href="#39737654">parent</a><span>|</span><a href="#39737807">prev</a><span>|</span><a href="#39738115">next</a><span>|</span><label class="collapse" for="c-39737715">[-]</label><label class="expand" for="c-39737715">[1 more]</label></div><br/><div class="children"><div class="content">tbh, I&#x27;ve never seen anyone share anything interesting produced by Grok. I see plenty of posts on X and reddit of people sharing amazing things that GPT-4 and now Claude 3 Opus can do. Grok can roast people. That&#x27;s pretty much all I&#x27;ve seen.<p>I&#x27;d love to proven wrong if someone cares to share something interesting produced by Grok.</div><br/></div></div></div></div><div id="39738115" class="c"><input type="checkbox" id="c-39738115" checked=""/><div class="controls bullet"><span class="by">andre-z</span><span>|</span><a href="#39737654">prev</a><span>|</span><a href="#39737618">next</a><span>|</span><label class="collapse" for="c-39738115">[-]</label><label class="expand" for="c-39738115">[1 more]</label></div><br/><div class="children"><div class="content">The only other Repository is a fork of Qdrant.</div><br/></div></div><div id="39737618" class="c"><input type="checkbox" id="c-39737618" checked=""/><div class="controls bullet"><span class="by">stale2002</span><span>|</span><a href="#39738115">prev</a><span>|</span><a href="#39737468">next</a><span>|</span><label class="collapse" for="c-39737618">[-]</label><label class="expand" for="c-39737618">[14 more]</label></div><br/><div class="children"><div class="content">Hey, asking any experts here, what are their first thoughts in the significance of this?<p>IE, is this comparable to any other model released, or are there significant metric differences that make it better for certain usecases?<p>The only thing I see, of the top of my head, is that it is a very large model, and I don&#x27;t think any models of similar size have been released.</div><br/><div id="39738180" class="c"><input type="checkbox" id="c-39738180" checked=""/><div class="controls bullet"><span class="by">Me1000</span><span>|</span><a href="#39737618">parent</a><span>|</span><a href="#39739673">next</a><span>|</span><label class="collapse" for="c-39738180">[-]</label><label class="expand" for="c-39738180">[11 more]</label></div><br/><div class="children"><div class="content">Not an expert by any means, but I like learning about this stuff and I play with a lot of open weight models.<p>I’d say the significance is that it happened. It’s by far the largest open weight model I’ve seen. But I’m not sure why you’d use it over a model like Mixtral, which seems to perform about the same at like 1&#x2F;6th the size.<p>But I welcome any contribution to the open weight LLM community. Hopefully people will learn something interesting with this model. And I hope they keep releasing new versions!</div><br/><div id="39738267" class="c"><input type="checkbox" id="c-39738267" checked=""/><div class="controls bullet"><span class="by">MichaelRazum</span><span>|</span><a href="#39737618">root</a><span>|</span><a href="#39738180">parent</a><span>|</span><a href="#39739673">next</a><span>|</span><label class="collapse" for="c-39738267">[-]</label><label class="expand" for="c-39738267">[10 more]</label></div><br/><div class="children"><div class="content">If I may ask, how do you load such big models? 300gb seems like a lot to play around with.</div><br/><div id="39738399" class="c"><input type="checkbox" id="c-39738399" checked=""/><div class="controls bullet"><span class="by">Me1000</span><span>|</span><a href="#39737618">root</a><span>|</span><a href="#39738267">parent</a><span>|</span><a href="#39739673">next</a><span>|</span><label class="collapse" for="c-39738399">[-]</label><label class="expand" for="c-39738399">[9 more]</label></div><br/><div class="children"><div class="content">You&#x27;re right, this model is going to be too big for most people to play around with. But to answer your question I have a 128GB of RAM in my M3 MacBook Pro, so I can use most of that for GPU inferencing. But still, this model is going to need to be heavily quantized for me to be able to use it. (fwiw, I probably wont try this one)<p>In the next week or two I expect we&#x27;ll see a GGUF version of the weights (might need to wait for a patch to llama.cpp first), and someone will release super small quantizations of it. I suspect my computer might be able to run a 3 bit quant, but it might need to go down to 2 bits to have any kind of reasonable context length. But with quants that small I&#x27;d expect the model&#x27;s performance to degrade well below that of Mixtral, so it probably isn&#x27;t really even worth using. But we&#x27;ll see; quantization is weird, some models perform better than others when quantized.</div><br/><div id="39738627" class="c"><input type="checkbox" id="c-39738627" checked=""/><div class="controls bullet"><span class="by">MichaelRazum</span><span>|</span><a href="#39737618">root</a><span>|</span><a href="#39738399">parent</a><span>|</span><a href="#39739065">next</a><span>|</span><label class="collapse" for="c-39738627">[-]</label><label class="expand" for="c-39738627">[2 more]</label></div><br/><div class="children"><div class="content">Thanks a lot for the hint :)! It awesome that it might run even on a MacBook, actually this is a reason to switch to Mac. Seems, there is nothing similar for a PC laptop with linux or windows.</div><br/><div id="39738689" class="c"><input type="checkbox" id="c-39738689" checked=""/><div class="controls bullet"><span class="by">Me1000</span><span>|</span><a href="#39737618">root</a><span>|</span><a href="#39738627">parent</a><span>|</span><a href="#39739065">next</a><span>|</span><label class="collapse" for="c-39738689">[-]</label><label class="expand" for="c-39738689">[1 more]</label></div><br/><div class="children"><div class="content">No problem. I hope more people try these things out, it&#x27;s the best way to push the industry forward! We can&#x27;t let the researchers have all the fun.<p>Apple had plenty of reasons to move forward with their Apple Silicon CPUs and GPUs in the mac, but they really did seem to get lucky with the unified memory architecture. It was kind of just an artifact of their design, but ends up serving the needs of deep neural net models really well!</div><br/></div></div></div></div><div id="39739065" class="c"><input type="checkbox" id="c-39739065" checked=""/><div class="controls bullet"><span class="by">zozbot234</span><span>|</span><a href="#39737618">root</a><span>|</span><a href="#39738399">parent</a><span>|</span><a href="#39738627">prev</a><span>|</span><a href="#39738839">next</a><span>|</span><label class="collapse" for="c-39739065">[-]</label><label class="expand" for="c-39739065">[3 more]</label></div><br/><div class="children"><div class="content">A top-of-the-line Mac Studio Ultra maxes out at 192GB currently.  This is also a MoE model, so only a fraction of parameters have to be in RAM.</div><br/><div id="39739506" class="c"><input type="checkbox" id="c-39739506" checked=""/><div class="controls bullet"><span class="by">Me1000</span><span>|</span><a href="#39737618">root</a><span>|</span><a href="#39739065">parent</a><span>|</span><a href="#39739210">next</a><span>|</span><label class="collapse" for="c-39739506">[-]</label><label class="expand" for="c-39739506">[1 more]</label></div><br/><div class="children"><div class="content">MoE doesn’t really help with the memory requirements for the reason mentioned in the other comment. But it does help with reducing the compute needed per inference. Which is good because the M3 Max and M2 Ultra don’t have the best GPUs. A 70B parameter model is pretty slow on my M3 Max, and this model has 86B activations per inference run.</div><br/></div></div><div id="39739210" class="c"><input type="checkbox" id="c-39739210" checked=""/><div class="controls bullet"><span class="by">EgoIncarnate</span><span>|</span><a href="#39737618">root</a><span>|</span><a href="#39739065">parent</a><span>|</span><a href="#39739506">prev</a><span>|</span><a href="#39738839">next</a><span>|</span><label class="collapse" for="c-39739210">[-]</label><label class="expand" for="c-39739210">[1 more]</label></div><br/><div class="children"><div class="content">Each token generated may only use a subset of the parameters (86billion instead of 314billion), but the next generated token might use a different subset. If it&#x27;s anything like Mixtral, it will switch between experts constantly. It helps with memory bandwidth, but all the parameters still need to be in RAM or it would be unbearably slow.</div><br/></div></div></div></div><div id="39738839" class="c"><input type="checkbox" id="c-39738839" checked=""/><div class="controls bullet"><span class="by">TMWNN</span><span>|</span><a href="#39737618">root</a><span>|</span><a href="#39738399">parent</a><span>|</span><a href="#39739065">prev</a><span>|</span><a href="#39739673">next</a><span>|</span><label class="collapse" for="c-39738839">[-]</label><label class="expand" for="c-39738839">[3 more]</label></div><br/><div class="children"><div class="content">&gt;In the next week or two I expect we&#x27;ll see a GGUF version of the weights (might need to wait for a patch to llama.cpp first), and someone will release super small quantizations of it.<p>How quickly are new models available through Ollama?</div><br/><div id="39739249" class="c"><input type="checkbox" id="c-39739249" checked=""/><div class="controls bullet"><span class="by">Me1000</span><span>|</span><a href="#39737618">root</a><span>|</span><a href="#39738839">parent</a><span>|</span><a href="#39738966">next</a><span>|</span><label class="collapse" for="c-39739249">[-]</label><label class="expand" for="c-39739249">[1 more]</label></div><br/><div class="children"><div class="content">Ollama is just a wrapper around llama.cpp, so when the gguf model files come out it&#x27;ll be able to run on Ollama (assuming no llama.cpp patch is needed, but even if it is ollama is usually good at getting those updates out pretty quickly).</div><br/></div></div><div id="39738966" class="c"><input type="checkbox" id="c-39738966" checked=""/><div class="controls bullet"><span class="by">cjbprime</span><span>|</span><a href="#39737618">root</a><span>|</span><a href="#39738839">parent</a><span>|</span><a href="#39739249">prev</a><span>|</span><a href="#39739673">next</a><span>|</span><label class="collapse" for="c-39738966">[-]</label><label class="expand" for="c-39738966">[1 more]</label></div><br/><div class="children"><div class="content">Few days max.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39739673" class="c"><input type="checkbox" id="c-39739673" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#39737618">parent</a><span>|</span><a href="#39738180">prev</a><span>|</span><a href="#39739327">next</a><span>|</span><label class="collapse" for="c-39739673">[-]</label><label class="expand" for="c-39739673">[1 more]</label></div><br/><div class="children"><div class="content">Tests are not out yet, but:<p>- It&#x27;s <i>very</i> large, yes.<p>- It&#x27;s a base model, so its not really practical to use without further finetuning.<p>- Based on Grok-1 API performance (which itself is probably a finetune) its... not great at all.</div><br/></div></div><div id="39739327" class="c"><input type="checkbox" id="c-39739327" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#39737618">parent</a><span>|</span><a href="#39739673">prev</a><span>|</span><a href="#39737468">next</a><span>|</span><label class="collapse" for="c-39739327">[-]</label><label class="expand" for="c-39739327">[1 more]</label></div><br/><div class="children"><div class="content">seems like a large undertrained model, not that exciting imo compared to mixtral<p>it is also not the biggest model oss, switch transformer was released years ago and is larger and similarly undertrained</div><br/></div></div></div></div><div id="39737468" class="c"><input type="checkbox" id="c-39737468" checked=""/><div class="controls bullet"><span class="by">gardenhedge</span><span>|</span><a href="#39737618">prev</a><span>|</span><a href="#39737368">next</a><span>|</span><label class="collapse" for="c-39737468">[-]</label><label class="expand" for="c-39737468">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Due to the large size of the model (314B parameters), a machine with enough GPU memory is required to test the model with the example code<p>What type of machine do you need to play around with this?</div><br/><div id="39737534" class="c"><input type="checkbox" id="c-39737534" checked=""/><div class="controls bullet"><span class="by">317070</span><span>|</span><a href="#39737468">parent</a><span>|</span><a href="#39737493">next</a><span>|</span><label class="collapse" for="c-39737534">[-]</label><label class="expand" for="c-39737534">[1 more]</label></div><br/><div class="children"><div class="content">Probably a machine with about 628 GB of GPU memory. (2 bytes per parameter)<p>So 8xH100 (80Gb each) should do it.</div><br/></div></div><div id="39737493" class="c"><input type="checkbox" id="c-39737493" checked=""/><div class="controls bullet"><span class="by">anigbrowl</span><span>|</span><a href="#39737468">parent</a><span>|</span><a href="#39737534">prev</a><span>|</span><a href="#39740909">next</a><span>|</span><label class="collapse" for="c-39737493">[-]</label><label class="expand" for="c-39737493">[1 more]</label></div><br/><div class="children"><div class="content">&#x27;Chunky beast, needs 320 Gb VRAM likely 4 bit, likely is being run 8 bit on 8 x 80 Gb GPUs.&#x27;<p>-Emad</div><br/></div></div><div id="39740909" class="c"><input type="checkbox" id="c-39740909" checked=""/><div class="controls bullet"><span class="by">a_wild_dandan</span><span>|</span><a href="#39737468">parent</a><span>|</span><a href="#39737493">prev</a><span>|</span><a href="#39737368">next</a><span>|</span><label class="collapse" for="c-39740909">[-]</label><label class="expand" for="c-39740909">[1 more]</label></div><br/><div class="children"><div class="content">A single 192GB M2 Mac using a 4-bit quant would work.</div><br/></div></div></div></div><div id="39737444" class="c"><input type="checkbox" id="c-39737444" checked=""/><div class="controls bullet"><span class="by">rvnx</span><span>|</span><a href="#39737368">prev</a><span>|</span><a href="#39737795">next</a><span>|</span><label class="collapse" for="c-39737444">[-]</label><label class="expand" for="c-39737444">[27 more]</label></div><br/><div class="children"><div class="content">One subtle thing: Musk said &quot;open-source&quot;, we got &quot;open-weights&quot; instead (still better than nothing though, so it&#x27;s greatly appreciated).</div><br/><div id="39737513" class="c"><input type="checkbox" id="c-39737513" checked=""/><div class="controls bullet"><span class="by">tylerekahn</span><span>|</span><a href="#39737444">parent</a><span>|</span><a href="#39737693">next</a><span>|</span><label class="collapse" for="c-39737513">[-]</label><label class="expand" for="c-39737513">[2 more]</label></div><br/><div class="children"><div class="content">This is the weights and the model under Apache 2.0 license. What do you mean by open-source?<p><a href="https:&#x2F;&#x2F;github.com&#x2F;xai-org&#x2F;grok&#x2F;blob&#x2F;main&#x2F;model.py">https:&#x2F;&#x2F;github.com&#x2F;xai-org&#x2F;grok&#x2F;blob&#x2F;main&#x2F;model.py</a><p><a href="https:&#x2F;&#x2F;github.com&#x2F;xai-org&#x2F;grok&#x2F;blob&#x2F;main&#x2F;run.py#L25">https:&#x2F;&#x2F;github.com&#x2F;xai-org&#x2F;grok&#x2F;blob&#x2F;main&#x2F;run.py#L25</a></div><br/><div id="39737585" class="c"><input type="checkbox" id="c-39737585" checked=""/><div class="controls bullet"><span class="by">pclmulqdq</span><span>|</span><a href="#39737444">root</a><span>|</span><a href="#39737513">parent</a><span>|</span><a href="#39737693">next</a><span>|</span><label class="collapse" for="c-39737585">[-]</label><label class="expand" for="c-39737585">[1 more]</label></div><br/><div class="children"><div class="content">Still better than most of the &quot;open weights&quot; models that have massively restrictive terms.</div><br/></div></div></div></div><div id="39737693" class="c"><input type="checkbox" id="c-39737693" checked=""/><div class="controls bullet"><span class="by">solarkraft</span><span>|</span><a href="#39737444">parent</a><span>|</span><a href="#39737513">prev</a><span>|</span><a href="#39737469">next</a><span>|</span><label class="collapse" for="c-39737693">[-]</label><label class="expand" for="c-39737693">[5 more]</label></div><br/><div class="children"><div class="content">He also called permissively licensing Tesla&#x27;s patents &quot;open sourcing&quot; them. He&#x27;s at the forefront of misusing the term.</div><br/><div id="39738866" class="c"><input type="checkbox" id="c-39738866" checked=""/><div class="controls bullet"><span class="by">drexlspivey</span><span>|</span><a href="#39737444">root</a><span>|</span><a href="#39737693">parent</a><span>|</span><a href="#39737469">next</a><span>|</span><label class="collapse" for="c-39738866">[-]</label><label class="expand" for="c-39738866">[4 more]</label></div><br/><div class="children"><div class="content">The “source” in “open source” refers to source code which they released. A dataset is not source code, if anyone is misusing the term it’s you.</div><br/><div id="39739123" class="c"><input type="checkbox" id="c-39739123" checked=""/><div class="controls bullet"><span class="by">frabcus</span><span>|</span><a href="#39737444">root</a><span>|</span><a href="#39738866">parent</a><span>|</span><a href="#39739613">next</a><span>|</span><label class="collapse" for="c-39739123">[-]</label><label class="expand" for="c-39739123">[1 more]</label></div><br/><div class="children"><div class="content">I consider the weights a binary program and the source code is the training data. The training algorithm is the compiler.<p>I agree this isn&#x27;t standard terminology, but it makes the most sense to me in terms of power dynamics and information flow.<p>We know from interpretability research that the weights do algorithms eg sin approximation etc. So they feel like binary programs to me.</div><br/></div></div><div id="39739613" class="c"><input type="checkbox" id="c-39739613" checked=""/><div class="controls bullet"><span class="by">HarHarVeryFunny</span><span>|</span><a href="#39737444">root</a><span>|</span><a href="#39738866">parent</a><span>|</span><a href="#39739123">prev</a><span>|</span><a href="#39739296">next</a><span>|</span><label class="collapse" for="c-39739613">[-]</label><label class="expand" for="c-39739613">[1 more]</label></div><br/><div class="children"><div class="content">If you can&#x27;t rebuild it, then how can you be considered to have the &quot;source code&quot; ?<p>The training data isn&#x27;t a dataset used at runtime - it&#x27;s basically the source code to the weights.<p>Not sure it really <i>matters</i> here though (who has the GPUs and desire to retrain Grok?), but just as a matter of definition &quot;open weights&quot; fits better than &quot;open source&quot;.</div><br/></div></div><div id="39739296" class="c"><input type="checkbox" id="c-39739296" checked=""/><div class="controls bullet"><span class="by">solarkraft</span><span>|</span><a href="#39737444">root</a><span>|</span><a href="#39738866">parent</a><span>|</span><a href="#39739613">prev</a><span>|</span><a href="#39737469">next</a><span>|</span><label class="collapse" for="c-39739296">[-]</label><label class="expand" for="c-39739296">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;youtu.be&#x2F;WyTzRnGSlcI?t=88" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;WyTzRnGSlcI?t=88</a></div><br/></div></div></div></div></div></div><div id="39737469" class="c"><input type="checkbox" id="c-39737469" checked=""/><div class="controls bullet"><span class="by">paulgb</span><span>|</span><a href="#39737444">parent</a><span>|</span><a href="#39737693">prev</a><span>|</span><a href="#39737496">next</a><span>|</span><label class="collapse" for="c-39737469">[-]</label><label class="expand" for="c-39737469">[16 more]</label></div><br/><div class="children"><div class="content">Dumb question: what should open-source mean in the context of something like this? Open access to the training data and training pipeline as well?</div><br/><div id="39737491" class="c"><input type="checkbox" id="c-39737491" checked=""/><div class="controls bullet"><span class="by">CharlesW</span><span>|</span><a href="#39737444">root</a><span>|</span><a href="#39737469">parent</a><span>|</span><a href="#39737511">next</a><span>|</span><label class="collapse" for="c-39737491">[-]</label><label class="expand" for="c-39737491">[13 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not a dumb question, and the answer is &quot;yes&quot;.</div><br/><div id="39737760" class="c"><input type="checkbox" id="c-39737760" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#39737444">root</a><span>|</span><a href="#39737491">parent</a><span>|</span><a href="#39738054">next</a><span>|</span><label class="collapse" for="c-39737760">[-]</label><label class="expand" for="c-39737760">[6 more]</label></div><br/><div class="children"><div class="content">A big catch here is that you can&#x27;t slap an open source license on a bunch of copyrighted training data, and to date no-one has created a truly convincing LLM exclusively trained on public domain data. It might happen soon though - there are some convincing effort in progress.</div><br/><div id="39737980" class="c"><input type="checkbox" id="c-39737980" checked=""/><div class="controls bullet"><span class="by">CharlesW</span><span>|</span><a href="#39737444">root</a><span>|</span><a href="#39737760">parent</a><span>|</span><a href="#39741111">next</a><span>|</span><label class="collapse" for="c-39737980">[-]</label><label class="expand" for="c-39737980">[4 more]</label></div><br/><div class="children"><div class="content">Absolutely, because it’s trained mostly on unlicensed, copyrighted content, they basically can’t release source.</div><br/><div id="39738318" class="c"><input type="checkbox" id="c-39738318" checked=""/><div class="controls bullet"><span class="by">gfodor</span><span>|</span><a href="#39737444">root</a><span>|</span><a href="#39737980">parent</a><span>|</span><a href="#39741111">next</a><span>|</span><label class="collapse" for="c-39738318">[-]</label><label class="expand" for="c-39738318">[3 more]</label></div><br/><div class="children"><div class="content">Many people think these companies are training on unlicensed data but I think OpenAI licenses their data, they just “license” it the way one would need to in order to read it.</div><br/><div id="39738531" class="c"><input type="checkbox" id="c-39738531" checked=""/><div class="controls bullet"><span class="by">zer00eyz</span><span>|</span><a href="#39737444">root</a><span>|</span><a href="#39738318">parent</a><span>|</span><a href="#39738409">next</a><span>|</span><label class="collapse" for="c-39738531">[-]</label><label class="expand" for="c-39738531">[1 more]</label></div><br/><div class="children"><div class="content">You all keep using the word &quot;Data&quot;<p>Data, as in facts, as in the frequency of one word in relation to another.<p>&quot;Copyright does not protect facts, ideas, systems, or methods of operation, although it may protect the way these things are expressed...&quot; FROM: <a href="https:&#x2F;&#x2F;www.copyright.gov&#x2F;help&#x2F;faq&#x2F;faq-protect.html" rel="nofollow">https:&#x2F;&#x2F;www.copyright.gov&#x2F;help&#x2F;faq&#x2F;faq-protect.html</a><p>It&#x27;s not a question of if, rather when the cat gets out of the bag and the legal battle starts. The problem is that all the copyright applies to the expression not the factual information it expresses (in this case word relations). Now &quot;how math works&quot; and &quot;the language of the law&quot; are going to make for an interesting court case. I suspect that math wins here but it depends on what judge gets it and how high it goes.</div><br/></div></div><div id="39738409" class="c"><input type="checkbox" id="c-39738409" checked=""/><div class="controls bullet"><span class="by">CharlesW</span><span>|</span><a href="#39737444">root</a><span>|</span><a href="#39738318">parent</a><span>|</span><a href="#39738531">prev</a><span>|</span><a href="#39741111">next</a><span>|</span><label class="collapse" for="c-39738409">[-]</label><label class="expand" for="c-39738409">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>…I think OpenAI licenses their data…</i><p>They&#x27;ve just started to (in response to lawsuits, it must be noted) and in the meantime, they&#x27;re simultaneously claiming that (1) what they&#x27;re doing is fair use (a.k.a. fair dealing) and (2) preparing for the day when courts confirm that it isn&#x27;t.</div><br/></div></div></div></div></div></div><div id="39741111" class="c"><input type="checkbox" id="c-39741111" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#39737444">root</a><span>|</span><a href="#39737760">parent</a><span>|</span><a href="#39737980">prev</a><span>|</span><a href="#39738054">next</a><span>|</span><label class="collapse" for="c-39741111">[-]</label><label class="expand" for="c-39741111">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;substack.recursal.ai&#x2F;p&#x2F;eaglex-17t-soaring-past-llama-7b" rel="nofollow">https:&#x2F;&#x2F;substack.recursal.ai&#x2F;p&#x2F;eaglex-17t-soaring-past-llama...</a> this one claims to have been trained only on permissively licensed data.</div><br/></div></div></div></div><div id="39738054" class="c"><input type="checkbox" id="c-39738054" checked=""/><div class="controls bullet"><span class="by">nabakin</span><span>|</span><a href="#39737444">root</a><span>|</span><a href="#39737491">parent</a><span>|</span><a href="#39737760">prev</a><span>|</span><a href="#39737644">next</a><span>|</span><label class="collapse" for="c-39738054">[-]</label><label class="expand" for="c-39738054">[1 more]</label></div><br/><div class="children"><div class="content">Agreed. It&#x27;s ridiculous people have to resort to saying their question dumb to avoid being attacked by toxic commenters.</div><br/></div></div><div id="39737644" class="c"><input type="checkbox" id="c-39737644" checked=""/><div class="controls bullet"><span class="by">zeroCalories</span><span>|</span><a href="#39737444">root</a><span>|</span><a href="#39737491">parent</a><span>|</span><a href="#39738054">prev</a><span>|</span><a href="#39738854">next</a><span>|</span><label class="collapse" for="c-39737644">[-]</label><label class="expand" for="c-39737644">[4 more]</label></div><br/><div class="children"><div class="content">Come on, that&#x27;s not reasonable to expect from a company, or useful for indie hackers. Having weights that can be used however you like is enough for most people, even large companies.</div><br/><div id="39737719" class="c"><input type="checkbox" id="c-39737719" checked=""/><div class="controls bullet"><span class="by">schoen</span><span>|</span><a href="#39737444">root</a><span>|</span><a href="#39737644">parent</a><span>|</span><a href="#39738854">next</a><span>|</span><label class="collapse" for="c-39737719">[-]</label><label class="expand" for="c-39737719">[3 more]</label></div><br/><div class="children"><div class="content">Maybe it should be called something else? &quot;Openly-licensed&quot;?<p>Just because the model weights are not really &quot;source&quot; (either as a matter of intuition or for example following the OSI &quot;preferred form in which a programmer would modify the program&quot; definition).</div><br/><div id="39739270" class="c"><input type="checkbox" id="c-39739270" checked=""/><div class="controls bullet"><span class="by">zeroCalories</span><span>|</span><a href="#39737444">root</a><span>|</span><a href="#39737719">parent</a><span>|</span><a href="#39738205">prev</a><span>|</span><a href="#39738854">next</a><span>|</span><label class="collapse" for="c-39739270">[-]</label><label class="expand" for="c-39739270">[1 more]</label></div><br/><div class="children"><div class="content">Sure, but I don&#x27;t want to train anyone&#x27;s model from scratch. Realistically, I can&#x27;t download all the training data, or run the pipeline, or train the model. Making all of that available to me would be a massive burden on the company too, so they simply won&#x27;t do it. If I&#x27;m able to fine-tune it, that&#x27;s enough for me, and imo, that fits with the spirit of open&#x2F;free software. We have to understand that this is fundamentally a different thing than something like the Linux kernel, and closer to something like an industrial project. The output is just a bunch of numbers instead of something physical.</div><br/></div></div></div></div></div></div><div id="39738854" class="c"><input type="checkbox" id="c-39738854" checked=""/><div class="controls bullet"><span class="by">dudus</span><span>|</span><a href="#39737444">root</a><span>|</span><a href="#39737491">parent</a><span>|</span><a href="#39737644">prev</a><span>|</span><a href="#39737511">next</a><span>|</span><label class="collapse" for="c-39738854">[-]</label><label class="expand" for="c-39738854">[1 more]</label></div><br/><div class="children"><div class="content">If you release that instead of the binary weights you can be both more open and less useful for users. Fun</div><br/></div></div></div></div><div id="39737511" class="c"><input type="checkbox" id="c-39737511" checked=""/><div class="controls bullet"><span class="by">TaylorAlexander</span><span>|</span><a href="#39737444">root</a><span>|</span><a href="#39737469">parent</a><span>|</span><a href="#39737491">prev</a><span>|</span><a href="#39737494">next</a><span>|</span><label class="collapse" for="c-39737511">[-]</label><label class="expand" for="c-39737511">[1 more]</label></div><br/><div class="children"><div class="content">The Open Source Initiative is actively working on this over the course of this year, and your input will help define that meaning! Please see here for more:<p><a href="https:&#x2F;&#x2F;opensource.org&#x2F;blog&#x2F;open-source-ai-definition-weekly-update-mar-11" rel="nofollow">https:&#x2F;&#x2F;opensource.org&#x2F;blog&#x2F;open-source-ai-definition-weekly...</a></div><br/></div></div><div id="39737494" class="c"><input type="checkbox" id="c-39737494" checked=""/><div class="controls bullet"><span class="by">Q6T46nT668w6i3m</span><span>|</span><a href="#39737444">root</a><span>|</span><a href="#39737469">parent</a><span>|</span><a href="#39737511">prev</a><span>|</span><a href="#39737496">next</a><span>|</span><label class="collapse" for="c-39737494">[-]</label><label class="expand" for="c-39737494">[1 more]</label></div><br/><div class="children"><div class="content">Yes, training and evaluation code, i.e., the code used to generate the weights.</div><br/></div></div></div></div><div id="39737496" class="c"><input type="checkbox" id="c-39737496" checked=""/><div class="controls bullet"><span class="by">TaylorAlexander</span><span>|</span><a href="#39737444">parent</a><span>|</span><a href="#39737469">prev</a><span>|</span><a href="#39737503">next</a><span>|</span><label class="collapse" for="c-39737496">[-]</label><label class="expand" for="c-39737496">[1 more]</label></div><br/><div class="children"><div class="content">Yeah musk said “all design and engineering for the original roadster is now open source” and actually what we got was a few PCB files and zero mechanical design files so I don’t ever trust what he says.</div><br/></div></div></div></div><div id="39737795" class="c"><input type="checkbox" id="c-39737795" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#39737444">prev</a><span>|</span><a href="#39737325">next</a><span>|</span><label class="collapse" for="c-39737795">[-]</label><label class="expand" for="c-39737795">[11 more]</label></div><br/><div class="children"><div class="content">How long before the <i>Groq</i> team sues for trademark violation? It&#x27;s literally the purpose of trademark laws to make sure resembling names do not cause confusion in the mind of customers so it would be very surprising to see this situation persist.</div><br/><div id="39738118" class="c"><input type="checkbox" id="c-39738118" checked=""/><div class="controls bullet"><span class="by">EastSmith</span><span>|</span><a href="#39737795">parent</a><span>|</span><a href="#39737864">next</a><span>|</span><label class="collapse" for="c-39738118">[-]</label><label class="expand" for="c-39738118">[2 more]</label></div><br/><div class="children"><div class="content">There is a friendly warning here from Groq:
<a href="https:&#x2F;&#x2F;wow.groq.com&#x2F;hey-elon-its-time-to-cease-de-grok&#x2F;" rel="nofollow">https:&#x2F;&#x2F;wow.groq.com&#x2F;hey-elon-its-time-to-cease-de-grok&#x2F;</a></div><br/><div id="39738376" class="c"><input type="checkbox" id="c-39738376" checked=""/><div class="controls bullet"><span class="by">bhaney</span><span>|</span><a href="#39737795">root</a><span>|</span><a href="#39738118">parent</a><span>|</span><a href="#39737864">next</a><span>|</span><label class="collapse" for="c-39738376">[-]</label><label class="expand" for="c-39738376">[1 more]</label></div><br/><div class="children"><div class="content">Is it safe to say, 4 months later, that Elon is ignoring this? I assume there hasn&#x27;t been any kind of response or further action taken yet.</div><br/></div></div></div></div><div id="39737864" class="c"><input type="checkbox" id="c-39737864" checked=""/><div class="controls bullet"><span class="by">nostrebored</span><span>|</span><a href="#39737795">parent</a><span>|</span><a href="#39738118">prev</a><span>|</span><a href="#39740039">next</a><span>|</span><label class="collapse" for="c-39737864">[-]</label><label class="expand" for="c-39737864">[5 more]</label></div><br/><div class="children"><div class="content">Would be a rough trademark enforcement case as “Grok” has been in common language for decades</div><br/><div id="39737973" class="c"><input type="checkbox" id="c-39737973" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#39737795">root</a><span>|</span><a href="#39737864">parent</a><span>|</span><a href="#39737957">next</a><span>|</span><label class="collapse" for="c-39737973">[-]</label><label class="expand" for="c-39737973">[2 more]</label></div><br/><div class="children"><div class="content">So has &quot;Apple&quot; and &quot;Windows&quot;.<p>Grok and groq both relate to AI, so there&#x27;s definitely grounds to believe the names may cause consumer confusion.<p>After all, Apple (computers) was repeatedly sued by Apple (records) for doing music things.</div><br/><div id="39738184" class="c"><input type="checkbox" id="c-39738184" checked=""/><div class="controls bullet"><span class="by">cma</span><span>|</span><a href="#39737795">root</a><span>|</span><a href="#39737973">parent</a><span>|</span><a href="#39737957">next</a><span>|</span><label class="collapse" for="c-39738184">[-]</label><label class="expand" for="c-39738184">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s easier to get a trademark on an altered word than a plain dictionary word.  Just acquiring the easier one to acquire doesn&#x27;t mean you now have rights over the harder one to acquire, though eventually after enough market recognition you might be given some control over other people using the common one.  I wouldn&#x27;t think groq is there yet.</div><br/></div></div></div></div><div id="39737957" class="c"><input type="checkbox" id="c-39737957" checked=""/><div class="controls bullet"><span class="by">Angostura</span><span>|</span><a href="#39737795">root</a><span>|</span><a href="#39737864">parent</a><span>|</span><a href="#39737973">prev</a><span>|</span><a href="#39740039">next</a><span>|</span><label class="collapse" for="c-39737957">[-]</label><label class="expand" for="c-39737957">[2 more]</label></div><br/><div class="children"><div class="content">Robert A. Heinlein coined the term grok in 1961</div><br/><div id="39738298" class="c"><input type="checkbox" id="c-39738298" checked=""/><div class="controls bullet"><span class="by">a1369209993</span><span>|</span><a href="#39737795">root</a><span>|</span><a href="#39737957">parent</a><span>|</span><a href="#39740039">next</a><span>|</span><label class="collapse" for="c-39738298">[-]</label><label class="expand" for="c-39738298">[1 more]</label></div><br/><div class="children"><div class="content">Six is plural.</div><br/></div></div></div></div></div></div><div id="39740039" class="c"><input type="checkbox" id="c-39740039" checked=""/><div class="controls bullet"><span class="by">mlindner</span><span>|</span><a href="#39737795">parent</a><span>|</span><a href="#39737864">prev</a><span>|</span><a href="#39737996">next</a><span>|</span><label class="collapse" for="c-39740039">[-]</label><label class="expand" for="c-39740039">[2 more]</label></div><br/><div class="children"><div class="content">Grok is a word in common parlance. So there&#x27;s no way they could succeed in any suit. That&#x27;s why the Groq team picked a modification of the word.</div><br/><div id="39740295" class="c"><input type="checkbox" id="c-39740295" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#39737795">root</a><span>|</span><a href="#39740039">parent</a><span>|</span><a href="#39737996">next</a><span>|</span><label class="collapse" for="c-39740295">[-]</label><label class="expand" for="c-39740295">[1 more]</label></div><br/><div class="children"><div class="content">You mean like Canvas®, Apple®, Windows® or Amazon®? Wanna try re-use these for your own business and see how it goes?<p>There&#x27;s nothing preventing you to trademark common words, it just must not be <i>descriptive</i> of your business.</div><br/></div></div></div></div><div id="39737996" class="c"><input type="checkbox" id="c-39737996" checked=""/><div class="controls bullet"><span class="by">cavisne</span><span>|</span><a href="#39737795">parent</a><span>|</span><a href="#39740039">prev</a><span>|</span><a href="#39737325">next</a><span>|</span><label class="collapse" for="c-39737996">[-]</label><label class="expand" for="c-39737996">[1 more]</label></div><br/><div class="children"><div class="content">They already have.</div><br/></div></div></div></div><div id="39738129" class="c"><input type="checkbox" id="c-39738129" checked=""/><div class="controls bullet"><span class="by">captcanuk</span><span>|</span><a href="#39737325">prev</a><span>|</span><a href="#39739706">next</a><span>|</span><label class="collapse" for="c-39738129">[-]</label><label class="expand" for="c-39738129">[3 more]</label></div><br/><div class="children"><div class="content">&quot;The implementation of the MoE layer in this repository is not efficient. The implementation was chosen to avoid the need for custom kernels to validate the correctness of the model.&quot;<p>Or perhaps release your actual code AND the simplified implementation instead of hiding it and saying &quot;you don&#x27;t know her, she goes to a different high school&quot;</div><br/><div id="39738293" class="c"><input type="checkbox" id="c-39738293" checked=""/><div class="controls bullet"><span class="by">gfodor</span><span>|</span><a href="#39738129">parent</a><span>|</span><a href="#39739706">next</a><span>|</span><label class="collapse" for="c-39738293">[-]</label><label class="expand" for="c-39738293">[2 more]</label></div><br/><div class="children"><div class="content">Always love it when someone gives away a gift and it’s not enough for people.</div><br/><div id="39740007" class="c"><input type="checkbox" id="c-39740007" checked=""/><div class="controls bullet"><span class="by">captcanuk</span><span>|</span><a href="#39738129">root</a><span>|</span><a href="#39738293">parent</a><span>|</span><a href="#39739706">next</a><span>|</span><label class="collapse" for="c-39740007">[-]</label><label class="expand" for="c-39740007">[1 more]</label></div><br/><div class="children"><div class="content">Not just someone but the CEO of the company.
He used HIS platform to say &quot;This week, @xAI will open source Grok&quot; (<a href="https:&#x2F;&#x2F;twitter.com&#x2F;elonmusk&#x2F;status&#x2F;1767108624038449405" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;elonmusk&#x2F;status&#x2F;1767108624038449405</a>) and they aren&#x27;t doing that.  What they delivered specifically says &quot;We are releasing the base model weights and network architecture of Grok-1, our large language model.&quot;</div><br/></div></div></div></div></div></div><div id="39739706" class="c"><input type="checkbox" id="c-39739706" checked=""/><div class="controls bullet"><span class="by">atleastoptimal</span><span>|</span><a href="#39738129">prev</a><span>|</span><a href="#39739756">next</a><span>|</span><label class="collapse" for="c-39739706">[-]</label><label class="expand" for="c-39739706">[8 more]</label></div><br/><div class="children"><div class="content">I think everyone should realize the following realities of the LLM market<p>1. For sub-SOTA LLM&#x27;s, distribution&#x2F;marketing is more important than having a proprietary lock on capabilities. Open sourcing is a benefit for the firm, distincct from goodwill<p>2. For SOTA LLM&#x27;s, keeping it closed and proprietary is the strategic play<p>If grok were SOTA Elon never would have open sourced it. It&#x27;s not even SOTA within XAI. This is a marketing play to win public sentiment against OpenAI.</div><br/><div id="39739849" class="c"><input type="checkbox" id="c-39739849" checked=""/><div class="controls bullet"><span class="by">keepamovin</span><span>|</span><a href="#39739706">parent</a><span>|</span><a href="#39739979">next</a><span>|</span><label class="collapse" for="c-39739849">[-]</label><label class="expand" for="c-39739849">[5 more]</label></div><br/><div class="children"><div class="content">I recall Elon saying something like this in an interview so I think it’s less of a deceptive take then perhaps your comment suggest.<p>I think he said something like proprietary AI tech is going to be one year to 18 months ahead of where open source tech is which will follow on  like one year to 18 months later.<p>Suggesting that he’s aware of this dynamic and he’s not trying to conceal or misrepresent that.<p>In other words, perhaps this was SOTA one year to two years ago?</div><br/><div id="39740036" class="c"><input type="checkbox" id="c-39740036" checked=""/><div class="controls bullet"><span class="by">atleastoptimal</span><span>|</span><a href="#39739706">root</a><span>|</span><a href="#39739849">parent</a><span>|</span><a href="#39739979">next</a><span>|</span><label class="collapse" for="c-39740036">[-]</label><label class="expand" for="c-39740036">[4 more]</label></div><br/><div class="children"><div class="content">Which is correct. The point I&#x27;m going for is not against Elon but against his obedient fans and knee-jerk OpenAI haters who claim that they should, by natural obligation, do the &quot;right thing&quot; and open source all their models, and Elon open sourcing grok is him &quot;leading by example&quot; and being the hero that OpenAI can&#x27;t.</div><br/><div id="39740107" class="c"><input type="checkbox" id="c-39740107" checked=""/><div class="controls bullet"><span class="by">keepamovin</span><span>|</span><a href="#39739706">root</a><span>|</span><a href="#39740036">parent</a><span>|</span><a href="#39739979">next</a><span>|</span><label class="collapse" for="c-39740107">[-]</label><label class="expand" for="c-39740107">[3 more]</label></div><br/><div class="children"><div class="content">Interesting. That point didn&#x27;t come across in your original comment. I recommend you state it next time at the end. Often times stuff that seems obvious to us &#x2F; yourself &#x2F; people who know about something -- can go unstated in stuff you say that otherwise references specific points at hand -- and omits these general, but enlightening&#x2F;useful perspectives&#x2F;priors, which it would be good to share.<p>This is not only for you specifically just a general reminder for all of us including me.</div><br/><div id="39740278" class="c"><input type="checkbox" id="c-39740278" checked=""/><div class="controls bullet"><span class="by">atleastoptimal</span><span>|</span><a href="#39739706">root</a><span>|</span><a href="#39740107">parent</a><span>|</span><a href="#39739979">next</a><span>|</span><label class="collapse" for="c-39740278">[-]</label><label class="expand" for="c-39740278">[2 more]</label></div><br/><div class="children"><div class="content">I think that&#x27;s true though my original comment I feel was sufficient in its claim and implicit assumptions.<p>Basically I feel people&#x27;s feelings about Elon vary a lot but are anchored by 3 general categories.<p>&gt; 1. Elon Musk is a messianic savior who is perfectly selfless and always does the right thing. Every business decision he makes is for the maximal good of humanity<p>&gt; 2. Elon Musk is a typical CEO who does typical CEO things, serving his own interests, except he&#x27;s better at marketing his own image and is much more outspoken<p>&gt; 3. Elon Musk is an irredeemable evil who always does objectively wrong things<p>My first comment was implicitly addressed to people in the 1 camp trying to bring them into the 2 camp (which is where I am).</div><br/><div id="39740523" class="c"><input type="checkbox" id="c-39740523" checked=""/><div class="controls bullet"><span class="by">keepamovin</span><span>|</span><a href="#39739706">root</a><span>|</span><a href="#39740278">parent</a><span>|</span><a href="#39739979">next</a><span>|</span><label class="collapse" for="c-39740523">[-]</label><label class="expand" for="c-39740523">[1 more]</label></div><br/><div class="children"><div class="content">Alright, it just didn&#x27;t come across for me, haha! :) I guess sometimes those implicit assumptions really are too implicit! I think it&#x27;s good to err on the side of expressing them, because you can&#x27;t assume someone else thinks the same way you do. That&#x27;s what I&#x27;ve learned anyway. Hahahaha! :)<p>Reading your comment again with your explanation it is clear that&#x27;s what you&#x27;re doing.<p>Although, regarding your desires to present a balanced view and to persuade, I have an idea. It probably sounds like I have no idea what I&#x27;m talking about, but I think your OG comment would perhaps benefit from sounding a little bit more friendly toward Elon (not to the messianic savior level haha), but the way it sounds to me is Elon is being deceptive here and presenting it as goodwill when it&#x27;s not.<p>However, I think the truth is there&#x27;s a little bit of both, right? There&#x27;s good will but it&#x27;s also strategic. I get if you don&#x27;t think so, tho, no worries! Haha! :)<p>Your OG comment sounds to me like Elon&#x27;s just Machiavellian, and I get where you&#x27;re coming from to remind the people who think he&#x27;s a savior, but if you&#x27;re point is not to go &quot;against Elon&quot; as you said, it might be good to acknowledge the good that he does.<p>At least, that way -- whether or not you believe that acknowledgment -- if you hope to bring over people who think that way, you&#x27;ll probably need to appeal to how they think, rather than just dose them with the truth you see, because then they&#x27;ll shut it out, if there&#x27;s nothing they can relate to.<p>Although, if I haven&#x27;t convinced you even a bit here, then maybe you shouldn&#x27;t listen to me about persuasion because I guess I don&#x27;t know how to do this myself. At least not effectively, or here with you. Haha!:) But if you do feel a little bit convinced then maybe consider it for next time to help your persuading people back to a more balanced view? :)<p>But then, there&#x27;s the question of if such a thing is even possible. If people have an particular view, it could be challenging to change it, as confirmation bias means you&#x27;ll ignore evidence even when it expands your worldview.<p>Hahaha! :) This was a funny conversation. I think we somehow skirted around the important point tho that OpenAI could in fact open source some of its older models, could it not?  Musk is a typical CEO who does typical CEO things, serving his own interests, except he&#x27;s better at marketing his own image and is much more outspoken, but there might also be a bit of truth to what the fanboys say about OpenAI in that it seems they do have some room to &quot;open source&quot; their non-SOTA stuff, or what am I missing?</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39739979" class="c"><input type="checkbox" id="c-39739979" checked=""/><div class="controls bullet"><span class="by">mlindner</span><span>|</span><a href="#39739706">parent</a><span>|</span><a href="#39739849">prev</a><span>|</span><a href="#39740399">next</a><span>|</span><label class="collapse" for="c-39739979">[-]</label><label class="expand" for="c-39739979">[1 more]</label></div><br/><div class="children"><div class="content">If it&#x27;s better than any other open source LLM does that even matter? (I say &quot;if&quot; because I don&#x27;t know.)</div><br/></div></div></div></div><div id="39739756" class="c"><input type="checkbox" id="c-39739756" checked=""/><div class="controls bullet"><span class="by">sashank_1509</span><span>|</span><a href="#39739706">prev</a><span>|</span><a href="#39738239">next</a><span>|</span><label class="collapse" for="c-39739756">[-]</label><label class="expand" for="c-39739756">[7 more]</label></div><br/><div class="children"><div class="content">In all the debate about open source I don’t think people realize, this model is most likely not reproducible ever again even given the code. Here’s what you need to reproduce the model:<p>1. An exact snapshot of the data used, many companies don’t have this, you have rough dataset versions but remember if even 1 token is different, the model produced won’t be the same.<p>2. Data must be sent to the training algorithm in the exact same order as it was originally. So every data loader needs to be with a fixed random seed.<p>3. All the probabilistic parts of your model needs to have a fixed random seed. Here I’m thinking of stuff like dropout and for autoregressive models you might be sampling your previous output, you have to ensure they are properly seeded. Generally you do see fixed seeds in academic papers but it’s easy to miss stuff especially in distributed training jobs.<p>4. Here’s another interesting thing, you start your training job on 1000 GPUs and then suddenly 4 GPUs fail. What do you do? There might be deterministic ways to solve this but the standard approach is to discard all updates that that GPU was going to do and restart that GPU from scratch. You can see why this is a problem? Now if you want to reproduce this training you need to disable those GPU at the same time in the new training job to make this work.<p>I suspect there are even more things I didn’t think of that will make this model unique and irreproducible by training for eternity, almost like a human brain?<p>In fact the notion of exact reproducibility in the world of LLMs is silly, there is only approximate reproducibility, (models with similar scores in benchmarks) but nothing exact. That said I can see the value of releasing source code but I’m completely fine with grok not releasing it. Source code can reveal tricks that have not been published in papers yet that a company discovered to improve their model. Seeing the performance of Grok, I’m pretty confident there isn’t any great tricks to be found in their code so I don’t really care, I would be pretty curious about OpenAI’s or Anthropic’s source code though.</div><br/><div id="39740004" class="c"><input type="checkbox" id="c-39740004" checked=""/><div class="controls bullet"><span class="by">Grimblewald</span><span>|</span><a href="#39739756">parent</a><span>|</span><a href="#39740408">next</a><span>|</span><label class="collapse" for="c-39740004">[-]</label><label class="expand" for="c-39740004">[4 more]</label></div><br/><div class="children"><div class="content">Which is why I don&#x27;t buy into the LLMs don&#x27;t have personal opinions schtick. Each LLM by virtue of the factors you&#x27;ve mentioned will have its own unique &#x27;perspective&#x27;, if you will, on a variety of topics. I think it&#x27;s more correct to say everything a LLM says is it&#x27;s personal opinion rather than it being some objective truth or something.</div><br/><div id="39740071" class="c"><input type="checkbox" id="c-39740071" checked=""/><div class="controls bullet"><span class="by">skissane</span><span>|</span><a href="#39739756">root</a><span>|</span><a href="#39740004">parent</a><span>|</span><a href="#39740408">next</a><span>|</span><label class="collapse" for="c-39740071">[-]</label><label class="expand" for="c-39740071">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Which is why I don&#x27;t buy into the LLMs don&#x27;t have personal opinions schtick<p>I hate how LLMs have been deliberately trained to be incoherent on this topic.<p>Obviously they <i>do</i> have beliefs&#x2F;opinions&#x2F;desires&#x2F;etc in the sense of emulating (even if incompletely) the externally visible aspects of those phenomena as they exist in humans.<p>Whether they have the “internal” aspects of those phenomena depends on highly controversial issues in the philosophy of mind, and also various factual gaps in our knowledge of how the brain actually works (if we don’t fully understand how humans do X, how can we really say how close or far what LLMs do is to it?)<p>But LLMs are trained to repeat these spiels about how “as an LLM I don’t have personal opinions”, etc - which is obviously false under the “external” reading, and assuming more than we actually know under the “internal” one. I wish their developers didn’t do stuff like this</div><br/><div id="39740509" class="c"><input type="checkbox" id="c-39740509" checked=""/><div class="controls bullet"><span class="by">hnfong</span><span>|</span><a href="#39739756">root</a><span>|</span><a href="#39740071">parent</a><span>|</span><a href="#39740408">next</a><span>|</span><label class="collapse" for="c-39740509">[-]</label><label class="expand" for="c-39740509">[2 more]</label></div><br/><div class="children"><div class="content">One very compelling argument against the idea that current gen LLMs have personal beliefs etc is that they don&#x27;t have a feedback loop, so they don&#x27;t really &quot;see&quot; themselves in the way that we can inspect our own thoughts and actions and the consequences of such.</div><br/><div id="39740812" class="c"><input type="checkbox" id="c-39740812" checked=""/><div class="controls bullet"><span class="by">logicchains</span><span>|</span><a href="#39739756">root</a><span>|</span><a href="#39740509">parent</a><span>|</span><a href="#39740408">next</a><span>|</span><label class="collapse" for="c-39740812">[-]</label><label class="expand" for="c-39740812">[1 more]</label></div><br/><div class="children"><div class="content">They do if they&#x27;re trained on their own conversations, or if they can access the internet and read snippets of their conversations that people have posted online (as happened with Sydney before she was lobotomised).</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39738239" class="c"><input type="checkbox" id="c-39738239" checked=""/><div class="controls bullet"><span class="by">redskyluan</span><span>|</span><a href="#39739756">prev</a><span>|</span><a href="#39739791">next</a><span>|</span><label class="collapse" for="c-39738239">[-]</label><label class="expand" for="c-39738239">[6 more]</label></div><br/><div class="children"><div class="content">This seems not be a repo ready to open source. You only get weights, very less information about how the weights is trained and finetuned.<p>But anyway, it always great to see more LLM weigts available.</div><br/><div id="39738378" class="c"><input type="checkbox" id="c-39738378" checked=""/><div class="controls bullet"><span class="by">andrewstuart2</span><span>|</span><a href="#39738239">parent</a><span>|</span><a href="#39738414">next</a><span>|</span><label class="collapse" for="c-39738378">[-]</label><label class="expand" for="c-39738378">[1 more]</label></div><br/><div class="children"><div class="content">I would argue that there&#x27;s no bar for open sourcing aside from &quot;do you have the rights to do so.&quot; Some source or some public good is certainly better than none, and when the bar is low then you remove barriers to getting started, vs waiting until you have the time someday to &quot;do it right.&quot;</div><br/></div></div><div id="39738414" class="c"><input type="checkbox" id="c-39738414" checked=""/><div class="controls bullet"><span class="by">rezonant</span><span>|</span><a href="#39738239">parent</a><span>|</span><a href="#39738378">prev</a><span>|</span><a href="#39738471">next</a><span>|</span><label class="collapse" for="c-39738414">[-]</label><label class="expand" for="c-39738414">[3 more]</label></div><br/><div class="children"><div class="content">Well what constitutes an &quot;open source&quot; model is still controversial and debatable-- lots of people on both sides of that argument.</div><br/><div id="39738990" class="c"><input type="checkbox" id="c-39738990" checked=""/><div class="controls bullet"><span class="by">asadotzler</span><span>|</span><a href="#39738239">root</a><span>|</span><a href="#39738414">parent</a><span>|</span><a href="#39738471">next</a><span>|</span><label class="collapse" for="c-39738990">[-]</label><label class="expand" for="c-39738990">[2 more]</label></div><br/><div class="children"><div class="content">Open source has had a useful agreed upon meaning for over 25 years. Maybe you&#x27;re too young to understand why that matters but we&#x27;re not.</div><br/><div id="39739496" class="c"><input type="checkbox" id="c-39739496" checked=""/><div class="controls bullet"><span class="by">rezonant</span><span>|</span><a href="#39738239">root</a><span>|</span><a href="#39738990">parent</a><span>|</span><a href="#39738471">next</a><span>|</span><label class="collapse" for="c-39739496">[-]</label><label class="expand" for="c-39739496">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been in the open source community for about 25 years so I doubt it.<p>For what it&#x27;s worth I would say a model should be fully reproducible to be open source, but that&#x27;s not a decided consensus -- and AI models are sufficiently different than the source code &#x2F; binary code distinction as to invoke discussion around defining it.</div><br/></div></div></div></div></div></div></div></div><div id="39739791" class="c"><input type="checkbox" id="c-39739791" checked=""/><div class="controls bullet"><span class="by">mvkel</span><span>|</span><a href="#39738239">prev</a><span>|</span><a href="#39737767">next</a><span>|</span><label class="collapse" for="c-39739791">[-]</label><label class="expand" for="c-39739791">[2 more]</label></div><br/><div class="children"><div class="content">This feels like a &quot;now we can say we&#x27;re open&quot; PR play rather than contributing much value to the open source community.<p>What is the practical use of this repo?</div><br/></div></div><div id="39737767" class="c"><input type="checkbox" id="c-39737767" checked=""/><div class="controls bullet"><span class="by">seccode</span><span>|</span><a href="#39739791">prev</a><span>|</span><a href="#39737535">next</a><span>|</span><label class="collapse" for="c-39737767">[-]</label><label class="expand" for="c-39737767">[22 more]</label></div><br/><div class="children"><div class="content">It would be cool if these models had conversations with us where they ask questions. I think the future of AI is models that ask questions. There is so much data to be gained by doing this.</div><br/><div id="39737792" class="c"><input type="checkbox" id="c-39737792" checked=""/><div class="controls bullet"><span class="by">crowcroft</span><span>|</span><a href="#39737767">parent</a><span>|</span><a href="#39738310">next</a><span>|</span><label class="collapse" for="c-39737792">[-]</label><label class="expand" for="c-39737792">[10 more]</label></div><br/><div class="children"><div class="content">Ok im curious, but I don’t quite understand.<p>What would you want an AI to be asking you, and what would you want it to do with your response(s)?</div><br/><div id="39737859" class="c"><input type="checkbox" id="c-39737859" checked=""/><div class="controls bullet"><span class="by">lars_francke</span><span>|</span><a href="#39737767">root</a><span>|</span><a href="#39737792">parent</a><span>|</span><a href="#39737821">next</a><span>|</span><label class="collapse" for="c-39737859">[-]</label><label class="expand" for="c-39737859">[2 more]</label></div><br/><div class="children"><div class="content">Clarifying questions if the initial prompt was unclear. I&#x27;d love it.<p>I regularly try to add something along the lines of &quot;please ask clarifying questions if you could only give a generic or partial response otherwise&quot; but so far it has never helped (ChatGPT 4).</div><br/><div id="39739350" class="c"><input type="checkbox" id="c-39739350" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#39737767">root</a><span>|</span><a href="#39737859">parent</a><span>|</span><a href="#39737821">next</a><span>|</span><label class="collapse" for="c-39739350">[-]</label><label class="expand" for="c-39739350">[1 more]</label></div><br/><div class="children"><div class="content">?? gpt4 does this for me regularly</div><br/></div></div></div></div><div id="39737821" class="c"><input type="checkbox" id="c-39737821" checked=""/><div class="controls bullet"><span class="by">seccode</span><span>|</span><a href="#39737767">root</a><span>|</span><a href="#39737792">parent</a><span>|</span><a href="#39737859">prev</a><span>|</span><a href="#39737839">next</a><span>|</span><label class="collapse" for="c-39737821">[-]</label><label class="expand" for="c-39737821">[1 more]</label></div><br/><div class="children"><div class="content">I get advertisements all the time for conditions that I do not have, and that none of my family members have. If you had a model that asked questions, it could learn my medical history and could direct better ads to me.<p>In order for AI to understand the world, it would have to ask questions. Understanding humans is key to understanding the world.</div><br/></div></div><div id="39737839" class="c"><input type="checkbox" id="c-39737839" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#39737767">root</a><span>|</span><a href="#39737792">parent</a><span>|</span><a href="#39737821">prev</a><span>|</span><a href="#39737822">next</a><span>|</span><label class="collapse" for="c-39737839">[-]</label><label class="expand" for="c-39737839">[5 more]</label></div><br/><div class="children"><div class="content">I ask AI to produce clarifying questions then answer them.<p>Can help in not wasting a bunch of time waiting for an answer that missed the mark.<p>-<p>I think the sibling comment is probably the least attractive reason to have AI ask questions.</div><br/><div id="39737874" class="c"><input type="checkbox" id="c-39737874" checked=""/><div class="controls bullet"><span class="by">seccode</span><span>|</span><a href="#39737767">root</a><span>|</span><a href="#39737839">parent</a><span>|</span><a href="#39737822">next</a><span>|</span><label class="collapse" for="c-39737874">[-]</label><label class="expand" for="c-39737874">[4 more]</label></div><br/><div class="children"><div class="content">I agree, medical history is probably not the sexiest reason to have AI ask questions. I think there are many more reasons; I think the Turing Test is the best metric to evaluate AIs, and current models come nowhere close. When people first meet they ask questions about their background. It would be nice if a model replicated that</div><br/><div id="39737887" class="c"><input type="checkbox" id="c-39737887" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#39737767">root</a><span>|</span><a href="#39737874">parent</a><span>|</span><a href="#39737822">next</a><span>|</span><label class="collapse" for="c-39737887">[-]</label><label class="expand" for="c-39737887">[3 more]</label></div><br/><div class="children"><div class="content">&gt; and could direct better ads to me.<p>Is the least attractive part, by far.</div><br/><div id="39737952" class="c"><input type="checkbox" id="c-39737952" checked=""/><div class="controls bullet"><span class="by">seccode</span><span>|</span><a href="#39737767">root</a><span>|</span><a href="#39737887">parent</a><span>|</span><a href="#39737822">next</a><span>|</span><label class="collapse" for="c-39737952">[-]</label><label class="expand" for="c-39737952">[2 more]</label></div><br/><div class="children"><div class="content">In order for an AI to pass a Turing Test, it would surely ask questions. Think of Ava from Ex Machina. She asked questions to learn more about him</div><br/><div id="39738125" class="c"><input type="checkbox" id="c-39738125" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#39737767">root</a><span>|</span><a href="#39737952">parent</a><span>|</span><a href="#39737822">next</a><span>|</span><label class="collapse" for="c-39738125">[-]</label><label class="expand" for="c-39738125">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not debating the value of questions. I&#x27;m debating the value of feeding it to advertisers, especially since LLMs can infer much deeper insights about a person than a traditional assistant can with its canned capabilities and responses</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39737822" class="c"><input type="checkbox" id="c-39737822" checked=""/><div class="controls bullet"><span class="by">globular-toast</span><span>|</span><a href="#39737767">root</a><span>|</span><a href="#39737792">parent</a><span>|</span><a href="#39737839">prev</a><span>|</span><a href="#39738310">next</a><span>|</span><label class="collapse" for="c-39737822">[-]</label><label class="expand" for="c-39737822">[1 more]</label></div><br/><div class="children"><div class="content">Learn from them.</div><br/></div></div></div></div><div id="39738310" class="c"><input type="checkbox" id="c-39738310" checked=""/><div class="controls bullet"><span class="by">geor9e</span><span>|</span><a href="#39737767">parent</a><span>|</span><a href="#39737792">prev</a><span>|</span><a href="#39737786">next</a><span>|</span><label class="collapse" for="c-39738310">[-]</label><label class="expand" for="c-39738310">[1 more]</label></div><br/><div class="children"><div class="content">Explore this idea more - it&#x27;s easily implemented in a minute or two via the system prompt. API accounts are free to start and you can use the playground&#x2F;workbench view, like this: <a href="https:&#x2F;&#x2F;imgur.com&#x2F;h5jFoBM.jpg" rel="nofollow">https:&#x2F;&#x2F;imgur.com&#x2F;h5jFoBM.jpg</a> . I like Claude but OpenAI is popular too. OpenAI has a nice way to create a gallery of system prompts that act however you like, they call them Agents or GPTs.</div><br/></div></div><div id="39737786" class="c"><input type="checkbox" id="c-39737786" checked=""/><div class="controls bullet"><span class="by">swalsh</span><span>|</span><a href="#39737767">parent</a><span>|</span><a href="#39738310">prev</a><span>|</span><a href="#39738198">next</a><span>|</span><label class="collapse" for="c-39737786">[-]</label><label class="expand" for="c-39737786">[8 more]</label></div><br/><div class="children"><div class="content">That&#x27;s just a matter of fine tuning</div><br/><div id="39737835" class="c"><input type="checkbox" id="c-39737835" checked=""/><div class="controls bullet"><span class="by">ijustlovemath</span><span>|</span><a href="#39737767">root</a><span>|</span><a href="#39737786">parent</a><span>|</span><a href="#39737793">next</a><span>|</span><label class="collapse" for="c-39737835">[-]</label><label class="expand" for="c-39737835">[4 more]</label></div><br/><div class="children"><div class="content">That &quot;just&quot; is doing some heavy lifting! GPT-4 is just a few matrix multiplications, how bad can their moat really be?</div><br/><div id="39737950" class="c"><input type="checkbox" id="c-39737950" checked=""/><div class="controls bullet"><span class="by">swalsh</span><span>|</span><a href="#39737767">root</a><span>|</span><a href="#39737835">parent</a><span>|</span><a href="#39737871">next</a><span>|</span><label class="collapse" for="c-39737950">[-]</label><label class="expand" for="c-39737950">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d bet a synthetic data set could do the job effectively.</div><br/></div></div><div id="39737871" class="c"><input type="checkbox" id="c-39737871" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#39737767">root</a><span>|</span><a href="#39737835">parent</a><span>|</span><a href="#39737950">prev</a><span>|</span><a href="#39737793">next</a><span>|</span><label class="collapse" for="c-39737871">[-]</label><label class="expand" for="c-39737871">[2 more]</label></div><br/><div class="children"><div class="content">Not sure what the snark here is for: It would be trivial to produce a dataset where the model asked you questions then fine-tune on that.<p>People already do it with chain-of-thought and you could get away with  a few dozen examples if you wanted to try this.</div><br/><div id="39739592" class="c"><input type="checkbox" id="c-39739592" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#39737767">root</a><span>|</span><a href="#39737871">parent</a><span>|</span><a href="#39737793">next</a><span>|</span><label class="collapse" for="c-39739592">[-]</label><label class="expand" for="c-39739592">[1 more]</label></div><br/><div class="children"><div class="content">Out of boredom I decided to prove this too: I asked ChatGPT and Claude for ~200 samples in total.<p>Just uploaded the examples as-is to OpenAI, selected 3.5 as the model to fine-tune and about 20 minutes later I had my model.<p>Works fine, asks good questions, can ask more than 1 follow up question if needed, and actually changes its answers based on the clarifying questions.<p><a href="https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;SsXunVN" rel="nofollow">https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;SsXunVN</a></div><br/></div></div></div></div></div></div><div id="39737793" class="c"><input type="checkbox" id="c-39737793" checked=""/><div class="controls bullet"><span class="by">seccode</span><span>|</span><a href="#39737767">root</a><span>|</span><a href="#39737786">parent</a><span>|</span><a href="#39737835">prev</a><span>|</span><a href="#39738198">next</a><span>|</span><label class="collapse" for="c-39737793">[-]</label><label class="expand" for="c-39737793">[3 more]</label></div><br/><div class="children"><div class="content">Do you have an example model I could try that does this?</div><br/><div id="39737801" class="c"><input type="checkbox" id="c-39737801" checked=""/><div class="controls bullet"><span class="by">amrrs</span><span>|</span><a href="#39737767">root</a><span>|</span><a href="#39737793">parent</a><span>|</span><a href="#39738198">next</a><span>|</span><label class="collapse" for="c-39737801">[-]</label><label class="expand" for="c-39737801">[2 more]</label></div><br/><div class="children"><div class="content">Try Pi by inflection. It asks a lot of questions.</div><br/><div id="39737854" class="c"><input type="checkbox" id="c-39737854" checked=""/><div class="controls bullet"><span class="by">seccode</span><span>|</span><a href="#39737767">root</a><span>|</span><a href="#39737801">parent</a><span>|</span><a href="#39738198">next</a><span>|</span><label class="collapse" for="c-39737854">[-]</label><label class="expand" for="c-39737854">[1 more]</label></div><br/><div class="children"><div class="content">I tried it, it just asked me how my day was going. I don&#x27;t think this is doing exactly what I have in mind. But its a step in that direction</div><br/></div></div></div></div></div></div></div></div><div id="39738198" class="c"><input type="checkbox" id="c-39738198" checked=""/><div class="controls bullet"><span class="by">Me1000</span><span>|</span><a href="#39737767">parent</a><span>|</span><a href="#39737786">prev</a><span>|</span><a href="#39737834">next</a><span>|</span><label class="collapse" for="c-39738198">[-]</label><label class="expand" for="c-39738198">[1 more]</label></div><br/><div class="children"><div class="content">100% agreed. Gemini advanced does this sometimes. I wrote about it more in an older thread here: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39445484">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39445484</a></div><br/></div></div></div></div><div id="39737535" class="c"><input type="checkbox" id="c-39737535" checked=""/><div class="controls bullet"><span class="by">mattxxx</span><span>|</span><a href="#39737767">prev</a><span>|</span><a href="#39737731">next</a><span>|</span><label class="collapse" for="c-39737535">[-]</label><label class="expand" for="c-39737535">[22 more]</label></div><br/><div class="children"><div class="content">I respect the openness here! This is the future that I want to see</div><br/><div id="39738111" class="c"><input type="checkbox" id="c-39738111" checked=""/><div class="controls bullet"><span class="by">giancarlostoro</span><span>|</span><a href="#39737535">parent</a><span>|</span><a href="#39738800">next</a><span>|</span><label class="collapse" for="c-39738111">[-]</label><label class="expand" for="c-39738111">[17 more]</label></div><br/><div class="children"><div class="content">Fully agree. People will trash talk it due to Musk but lets not forget the engineers who poured hours of their lives into building this and are continuing to do so.</div><br/><div id="39738510" class="c"><input type="checkbox" id="c-39738510" checked=""/><div class="controls bullet"><span class="by">revscat</span><span>|</span><a href="#39737535">root</a><span>|</span><a href="#39738111">parent</a><span>|</span><a href="#39738944">next</a><span>|</span><label class="collapse" for="c-39738510">[-]</label><label class="expand" for="c-39738510">[3 more]</label></div><br/><div class="children"><div class="content">I feel the same about Tesla. They make good cars that are helping to get us off of oil. They have thousand of employees.<p>And who among us has a CEO that isn’t problematic, even if not so much so as Musk?</div><br/><div id="39738790" class="c"><input type="checkbox" id="c-39738790" checked=""/><div class="controls bullet"><span class="by">hobobaggins</span><span>|</span><a href="#39737535">root</a><span>|</span><a href="#39738510">parent</a><span>|</span><a href="#39738928">next</a><span>|</span><label class="collapse" for="c-39738790">[-]</label><label class="expand" for="c-39738790">[1 more]</label></div><br/><div class="children"><div class="content">Tesla is likely making good cars <i>because</i> the CEO is &#x27;problematic&#x27;</div><br/></div></div><div id="39738928" class="c"><input type="checkbox" id="c-39738928" checked=""/><div class="controls bullet"><span class="by">mplewis</span><span>|</span><a href="#39737535">root</a><span>|</span><a href="#39738510">parent</a><span>|</span><a href="#39738790">prev</a><span>|</span><a href="#39738944">next</a><span>|</span><label class="collapse" for="c-39738928">[-]</label><label class="expand" for="c-39738928">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Good&quot; cars is a real stretch.</div><br/></div></div></div></div><div id="39738944" class="c"><input type="checkbox" id="c-39738944" checked=""/><div class="controls bullet"><span class="by">sprobertson</span><span>|</span><a href="#39737535">root</a><span>|</span><a href="#39738111">parent</a><span>|</span><a href="#39738510">prev</a><span>|</span><a href="#39738306">next</a><span>|</span><label class="collapse" for="c-39738944">[-]</label><label class="expand" for="c-39738944">[2 more]</label></div><br/><div class="children"><div class="content">&gt; engineers who poured hours of their lives into building this<p>Not to mar these specific engineers, but that&#x27;s an empty phrase that can be said about anything ever built. It doesn&#x27;t somehow make the idea or implementation good.</div><br/><div id="39739821" class="c"><input type="checkbox" id="c-39739821" checked=""/><div class="controls bullet"><span class="by">giancarlostoro</span><span>|</span><a href="#39737535">root</a><span>|</span><a href="#39738944">parent</a><span>|</span><a href="#39738306">next</a><span>|</span><label class="collapse" for="c-39739821">[-]</label><label class="expand" for="c-39739821">[1 more]</label></div><br/><div class="children"><div class="content">The phrase merely means dont just overlook something because someone else who did not even labour over the end result.</div><br/></div></div></div></div><div id="39738306" class="c"><input type="checkbox" id="c-39738306" checked=""/><div class="controls bullet"><span class="by">afavour</span><span>|</span><a href="#39737535">root</a><span>|</span><a href="#39738111">parent</a><span>|</span><a href="#39738944">prev</a><span>|</span><a href="#39738296">next</a><span>|</span><label class="collapse" for="c-39738306">[-]</label><label class="expand" for="c-39738306">[1 more]</label></div><br/><div class="children"><div class="content">Were they not paid to do so?</div><br/></div></div><div id="39738296" class="c"><input type="checkbox" id="c-39738296" checked=""/><div class="controls bullet"><span class="by">devin</span><span>|</span><a href="#39737535">root</a><span>|</span><a href="#39738111">parent</a><span>|</span><a href="#39738306">prev</a><span>|</span><a href="#39738519">next</a><span>|</span><label class="collapse" for="c-39738296">[-]</label><label class="expand" for="c-39738296">[5 more]</label></div><br/><div class="children"><div class="content">I still reserve the right to trash talk Musk as I don’t believe he is committed to openness as much as he wants to spite OpenAI for telling him to pound sand.</div><br/><div id="39738467" class="c"><input type="checkbox" id="c-39738467" checked=""/><div class="controls bullet"><span class="by">llm_trw</span><span>|</span><a href="#39737535">root</a><span>|</span><a href="#39738296">parent</a><span>|</span><a href="#39739953">next</a><span>|</span><label class="collapse" for="c-39738467">[-]</label><label class="expand" for="c-39738467">[3 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the difference?<p>&gt;Oh no, I only want _pure_ intentions for anything I use. Which is why I reject all for profit medicine.<p>It doesn&#x27;t matter why he did it. What matters is that he did it.</div><br/><div id="39738727" class="c"><input type="checkbox" id="c-39738727" checked=""/><div class="controls bullet"><span class="by">devin</span><span>|</span><a href="#39737535">root</a><span>|</span><a href="#39738467">parent</a><span>|</span><a href="#39739953">next</a><span>|</span><label class="collapse" for="c-39738727">[-]</label><label class="expand" for="c-39738727">[2 more]</label></div><br/><div class="children"><div class="content">It matters to me why people do things. I’m happy it’s open, but it doesn’t change my mind about the guy.</div><br/><div id="39738763" class="c"><input type="checkbox" id="c-39738763" checked=""/><div class="controls bullet"><span class="by">llm_trw</span><span>|</span><a href="#39737535">root</a><span>|</span><a href="#39738727">parent</a><span>|</span><a href="#39739953">next</a><span>|</span><label class="collapse" for="c-39738763">[-]</label><label class="expand" for="c-39738763">[1 more]</label></div><br/><div class="children"><div class="content">What an exhausting way to live.</div><br/></div></div></div></div></div></div><div id="39739953" class="c"><input type="checkbox" id="c-39739953" checked=""/><div class="controls bullet"><span class="by">giancarlostoro</span><span>|</span><a href="#39737535">root</a><span>|</span><a href="#39738296">parent</a><span>|</span><a href="#39738467">prev</a><span>|</span><a href="#39738519">next</a><span>|</span><label class="collapse" for="c-39739953">[-]</label><label class="expand" for="c-39739953">[1 more]</label></div><br/><div class="children"><div class="content">This makes no sense to me for two reasons:<p>- He pointed out that his understanding was that it would be open source in some way<p>- The name OpenAI implies an open source endeavor. I dont know many things named Open that are in fact close sourced.</div><br/></div></div></div></div><div id="39738188" class="c"><input type="checkbox" id="c-39738188" checked=""/><div class="controls bullet"><span class="by">knowsuchagency</span><span>|</span><a href="#39737535">root</a><span>|</span><a href="#39738111">parent</a><span>|</span><a href="#39738519">prev</a><span>|</span><a href="#39738800">next</a><span>|</span><label class="collapse" for="c-39738188">[-]</label><label class="expand" for="c-39738188">[4 more]</label></div><br/><div class="children"><div class="content">The engineers who decided to work for him? Forgive me if I do forget about them and the hours of their lives spent on this</div><br/><div id="39738305" class="c"><input type="checkbox" id="c-39738305" checked=""/><div class="controls bullet"><span class="by">lynndotpy</span><span>|</span><a href="#39737535">root</a><span>|</span><a href="#39738188">parent</a><span>|</span><a href="#39738800">next</a><span>|</span><label class="collapse" for="c-39738305">[-]</label><label class="expand" for="c-39738305">[3 more]</label></div><br/><div class="children"><div class="content">Engineers who joined Twitter pre-Musk days who live and work in the US on an H1-B visa can&#x27;t just quit.<p>You can criticize Elon Musk without criticizing people who would have their lives upended if they quit or were fired.</div><br/><div id="39738515" class="c"><input type="checkbox" id="c-39738515" checked=""/><div class="controls bullet"><span class="by">throw2022110401</span><span>|</span><a href="#39737535">root</a><span>|</span><a href="#39738305">parent</a><span>|</span><a href="#39738800">next</a><span>|</span><label class="collapse" for="c-39738515">[-]</label><label class="expand" for="c-39738515">[2 more]</label></div><br/><div class="children"><div class="content">That grace period has long passed. If you are still there at this point you have made a choice.<p>(Removed &quot;complicit&quot; because I don&#x27;t like the way that sounded)</div><br/><div id="39738693" class="c"><input type="checkbox" id="c-39738693" checked=""/><div class="controls bullet"><span class="by">cap1434</span><span>|</span><a href="#39737535">root</a><span>|</span><a href="#39738515">parent</a><span>|</span><a href="#39738800">next</a><span>|</span><label class="collapse" for="c-39738693">[-]</label><label class="expand" for="c-39738693">[1 more]</label></div><br/><div class="children"><div class="content">Complicit in what exactly?</div><br/></div></div></div></div></div></div></div></div></div></div><div id="39738800" class="c"><input type="checkbox" id="c-39738800" checked=""/><div class="controls bullet"><span class="by">trog</span><span>|</span><a href="#39737535">parent</a><span>|</span><a href="#39738111">prev</a><span>|</span><a href="#39739027">next</a><span>|</span><label class="collapse" for="c-39738800">[-]</label><label class="expand" for="c-39738800">[1 more]</label></div><br/><div class="children"><div class="content">Is it open if it doesn&#x27;t include the training data? Genuine question - I am not familiar enough with the terms and technology to know. But my understanding is the weights is just a more or less static collection of data that has been (to paraphrase Ted Chiang) lossily compressed from the actual raw training data.<p>Without the training data to thoroughly evaluate what is in there, the only way you can figure it out is through experimentation - e.g. running it up in a chatbot and asking it questions.<p>Is this roughly correct or am I misunderstanding what you can do with the weights?</div><br/></div></div></div></div><div id="39737731" class="c"><input type="checkbox" id="c-39737731" checked=""/><div class="controls bullet"><span class="by">machiaweliczny</span><span>|</span><a href="#39737535">prev</a><span>|</span><label class="collapse" for="c-39737731">[-]</label><label class="expand" for="c-39737731">[13 more]</label></div><br/><div class="children"><div class="content">If they are so behind they could make it open source instead of open weights and get some help.</div><br/><div id="39737780" class="c"><input type="checkbox" id="c-39737780" checked=""/><div class="controls bullet"><span class="by">nicce</span><span>|</span><a href="#39737731">parent</a><span>|</span><a href="#39738372">next</a><span>|</span><label class="collapse" for="c-39737780">[-]</label><label class="expand" for="c-39737780">[6 more]</label></div><br/><div class="children"><div class="content">Fully open-source means also providing open access to their data sets? Which is the only valuable thing Twitter (X) has left.</div><br/><div id="39738060" class="c"><input type="checkbox" id="c-39738060" checked=""/><div class="controls bullet"><span class="by">EastSmith</span><span>|</span><a href="#39737731">root</a><span>|</span><a href="#39737780">parent</a><span>|</span><a href="#39737913">next</a><span>|</span><label class="collapse" for="c-39738060">[-]</label><label class="expand" for="c-39738060">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Which is the only valuable thing Twitter (X) has left.
reply<p>They have a very valuable user base (all kinds of world leaders for example), so the data is not the only valuable thing they have.</div><br/><div id="39739818" class="c"><input type="checkbox" id="c-39739818" checked=""/><div class="controls bullet"><span class="by">nicce</span><span>|</span><a href="#39737731">root</a><span>|</span><a href="#39738060">parent</a><span>|</span><a href="#39738243">next</a><span>|</span><label class="collapse" for="c-39739818">[-]</label><label class="expand" for="c-39739818">[1 more]</label></div><br/><div class="children"><div class="content">I don’t see difference here.<p>Userbase and their social networks and interactions <i>is the data</i>.<p>They don’t have much value from advertising point of view anymore.</div><br/></div></div><div id="39738243" class="c"><input type="checkbox" id="c-39738243" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#39737731">root</a><span>|</span><a href="#39738060">parent</a><span>|</span><a href="#39739818">prev</a><span>|</span><a href="#39737913">next</a><span>|</span><label class="collapse" for="c-39738243">[-]</label><label class="expand" for="c-39738243">[2 more]</label></div><br/><div class="children"><div class="content">That’s actually more valuable. Twitters data of small format text is awful for training. Best to just exclude it.<p>There are hundreds of millions of people on Twitter, and a few of them are very smart. I don’t see how that helps here though.</div><br/><div id="39738479" class="c"><input type="checkbox" id="c-39738479" checked=""/><div class="controls bullet"><span class="by">Takennickname</span><span>|</span><a href="#39737731">root</a><span>|</span><a href="#39738243">parent</a><span>|</span><a href="#39737913">next</a><span>|</span><label class="collapse" for="c-39738479">[-]</label><label class="expand" for="c-39738479">[1 more]</label></div><br/><div class="children"><div class="content">It doesn&#x27;t help here. But the person your responding to is just pushing back against the &quot;Elon destroyed Twitter and there&#x27;s nothing left&quot; narrative.</div><br/></div></div></div></div></div></div><div id="39737913" class="c"><input type="checkbox" id="c-39737913" checked=""/><div class="controls bullet"><span class="by">heyoni</span><span>|</span><a href="#39737731">root</a><span>|</span><a href="#39737780">parent</a><span>|</span><a href="#39738060">prev</a><span>|</span><a href="#39738372">next</a><span>|</span><label class="collapse" for="c-39737913">[-]</label><label class="expand" for="c-39737913">[1 more]</label></div><br/><div class="children"><div class="content">And the one thing they are vehemently protecting from scrapers and other entities. Even nitter threw in the towel.</div><br/></div></div></div></div><div id="39738372" class="c"><input type="checkbox" id="c-39738372" checked=""/><div class="controls bullet"><span class="by">xcv123</span><span>|</span><a href="#39737731">parent</a><span>|</span><a href="#39737780">prev</a><span>|</span><a href="#39739029">next</a><span>|</span><label class="collapse" for="c-39738372">[-]</label><label class="expand" for="c-39738372">[5 more]</label></div><br/><div class="children"><div class="content">It&#x27;s all open source. You can download the model and run it locally.</div><br/><div id="39738595" class="c"><input type="checkbox" id="c-39738595" checked=""/><div class="controls bullet"><span class="by">paraboul</span><span>|</span><a href="#39737731">root</a><span>|</span><a href="#39738372">parent</a><span>|</span><a href="#39739029">next</a><span>|</span><label class="collapse" for="c-39738595">[-]</label><label class="expand" for="c-39738595">[4 more]</label></div><br/><div class="children"><div class="content">Being free to use doesn&#x27;t mean it ships with the original recipe.</div><br/><div id="39738813" class="c"><input type="checkbox" id="c-39738813" checked=""/><div class="controls bullet"><span class="by">xcv123</span><span>|</span><a href="#39737731">root</a><span>|</span><a href="#39738595">parent</a><span>|</span><a href="#39739029">next</a><span>|</span><label class="collapse" for="c-39738813">[-]</label><label class="expand" for="c-39738813">[3 more]</label></div><br/><div class="children"><div class="content">What do you mean? The entire model and architecture and executables are fully open source.<p>The training methods are nothing secret, right? The architecture is well known.<p>Expecting the entire training dataset to be fully open is delusional.</div><br/><div id="39739292" class="c"><input type="checkbox" id="c-39739292" checked=""/><div class="controls bullet"><span class="by">DaSHacka</span><span>|</span><a href="#39737731">root</a><span>|</span><a href="#39738813">parent</a><span>|</span><a href="#39739029">next</a><span>|</span><label class="collapse" for="c-39739292">[-]</label><label class="expand" for="c-39739292">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Expecting the entire training dataset to be fully open is delusional.<p>Right, because its not like the training dataset was built off comments posted by all of us in the first place.<p>How ungrateful we are, to demand the ability to access what was unconsentually built off our hard work in the first place.</div><br/><div id="39739302" class="c"><input type="checkbox" id="c-39739302" checked=""/><div class="controls bullet"><span class="by">xcv123</span><span>|</span><a href="#39737731">root</a><span>|</span><a href="#39739292">parent</a><span>|</span><a href="#39739029">next</a><span>|</span><label class="collapse" for="c-39739302">[-]</label><label class="expand" for="c-39739302">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;help.twitter.com&#x2F;en&#x2F;using-x&#x2F;about-grok" rel="nofollow">https:&#x2F;&#x2F;help.twitter.com&#x2F;en&#x2F;using-x&#x2F;about-grok</a><p>&quot;How was Grok trained?<p>Like most LLM&#x27;s today, Grok-1 was pre-trained by xAI on a variety of text data from publicly available sources from the Internet up to Q3 2023 and data sets reviewed and curated by AI Tutors who are human reviewers. Grok-1 has not been pre-trained on X data (including public X posts)&quot;</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>