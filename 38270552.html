<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1700038872302" as="style"/><link rel="stylesheet" href="styles.css?v=1700038872302"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://andrewmayne.com/2023/11/14/is-the-reversal-curse-real/">Is the reversal curse in LLMs real?</a> <span class="domain">(<a href="https://andrewmayne.com">andrewmayne.com</a>)</span></div><div class="subtext"><span>tedsanders</span> | <span>103 comments</span></div><br/><div><div id="38272112" class="c"><input type="checkbox" id="c-38272112" checked=""/><div class="controls bullet"><span class="by">gipp</span><span>|</span><a href="#38272063">next</a><span>|</span><label class="collapse" for="c-38272112">[-]</label><label class="expand" for="c-38272112">[36 more]</label></div><br/><div class="children"><div class="content">I fall somewhere on the skeptic side of the LLM spectrum. But this &quot;flaw&quot; just does not seem to have the force that its proponents seem to think it does, unless I&#x27;m missing something significant. Simply because in the context of natural language (Rather than formal logical statements), &quot;A is B&quot; <i>does not imply &quot;B is A&quot;</i> in the first place. &quot;Is&quot; can encompass a wide variety of logical relationships in colloquial usage, not just exact identity. &quot;The apple is red&quot; does not imply &quot;red is the apple,&quot; as a trivial example.</div><br/><div id="38272348" class="c"><input type="checkbox" id="c-38272348" checked=""/><div class="controls bullet"><span class="by">majormajor</span><span>|</span><a href="#38272112">parent</a><span>|</span><a href="#38272309">next</a><span>|</span><label class="collapse" for="c-38272348">[-]</label><label class="expand" for="c-38272348">[4 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think this is the right explanation, or really that relevant, the suggestions from &#x27;og_kalu and in the article sound more accurate to me.<p>It seems like understanding when &quot;is&quot; is reversible is pretty core to the capabilities of the model, but that&#x27;s <i>different</i> than having a lot of facts memorized. For instance, a model should be able to answer &quot;who is the star of Mission Impossible&quot; with &quot;Tom Cruise&quot; based on context or internalized-training of &quot;Tom Cruise is the star of Mission Impossible&quot; while <i>not</i> answering &quot;Tom Cruise&quot; to a much more general question like &quot;who is covered in water&quot; even if it had internalized a bit from a review like &quot;there&#x27;s a scene in Mission Impossible where Tom Cruise is covered in water...&quot; unless it had context pointing it at that specific case.</div><br/><div id="38273283" class="c"><input type="checkbox" id="c-38273283" checked=""/><div class="controls bullet"><span class="by">tysam_and</span><span>|</span><a href="#38272112">root</a><span>|</span><a href="#38272348">parent</a><span>|</span><a href="#38273710">next</a><span>|</span><label class="collapse" for="c-38273283">[-]</label><label class="expand" for="c-38273283">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a bit of mathematical bikeshedding, hardcoding reversability would cause far more problems than it would help.<p>Best to simply scale log-likelihood-based training, next-token-based training trivially contains a requirement for learning all of the subproblems that predict said, next token, and hardcoding something to get warm human fuzzies would be creating a biased estimator (and move us back towards the 90s a bit).<p>Models already very constantly do context-dependent token utilization, it&#x27;s an autoregressive feature based on the entire stream of incoming tokens. Humans have a bias to focus on the &#x27;last token used&#x27;, this is not what language models look at.</div><br/></div></div><div id="38273710" class="c"><input type="checkbox" id="c-38273710" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#38272112">root</a><span>|</span><a href="#38272348">parent</a><span>|</span><a href="#38273283">prev</a><span>|</span><a href="#38272309">next</a><span>|</span><label class="collapse" for="c-38273710">[-]</label><label class="expand" for="c-38273710">[2 more]</label></div><br/><div class="children"><div class="content">I mean transformers are definitely sequentially biased. There&#x27;s also human speech bias. But I think it&#x27;s pretty clear that humans are generally invariant to this kind of prompting as well (given that they have the knowledge. More in a different comment). My surprisal is far higher that the reversal &quot;curse&quot; is considered controversial or even surprising than it was when that sensational tweet dropped. It feels pretty well studied that sequential algorithms are sequentially biased.</div><br/><div id="38274349" class="c"><input type="checkbox" id="c-38274349" checked=""/><div class="controls bullet"><span class="by">aade63482</span><span>|</span><a href="#38272112">root</a><span>|</span><a href="#38273710">parent</a><span>|</span><a href="#38272309">next</a><span>|</span><label class="collapse" for="c-38274349">[-]</label><label class="expand" for="c-38274349">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;papystreaming.quest&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;papystreaming.quest&#x2F;</a></div><br/></div></div></div></div></div></div><div id="38272309" class="c"><input type="checkbox" id="c-38272309" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#38272112">parent</a><span>|</span><a href="#38272348">prev</a><span>|</span><a href="#38274322">next</a><span>|</span><label class="collapse" for="c-38272309">[-]</label><label class="expand" for="c-38272309">[5 more]</label></div><br/><div class="children"><div class="content">I think a lot of people unwittingly think of the training process as &quot;smart&quot;.<p>Similar criticisms are &quot;well there are many descriptions of game x it would have read so why doesn&#x27;t it play x well&quot;. But gradient descent is a dumb optimizer. Training itself is not actually like someone reading a text anymore than evolution is like someone thinking about the best way to augment an organism.<p>a &quot;smart&quot; optimizer would look at a reversal applicable sentence and know exactly what bunch of weights to change to store it in such a way as to be recalled reversibly in the future.<p>Inference may be smart (GPT-4 can reverse in context just fine, potentially play games from only a description) but the training is not.</div><br/><div id="38272431" class="c"><input type="checkbox" id="c-38272431" checked=""/><div class="controls bullet"><span class="by">LoganDark</span><span>|</span><a href="#38272112">root</a><span>|</span><a href="#38272309">parent</a><span>|</span><a href="#38273267">next</a><span>|</span><label class="collapse" for="c-38272431">[-]</label><label class="expand" for="c-38272431">[1 more]</label></div><br/><div class="children"><div class="content">A prior (now-deleted) comment of yours put this in such a great way, and I&#x27;m sad that it got deleted:<p>&gt; You could potentially describe how a game is played to GPT-4 without examples and get it playing it correctly but passing that same description into the training process of the model just gets you a model that can describe your game correctly.</div><br/></div></div><div id="38273267" class="c"><input type="checkbox" id="c-38273267" checked=""/><div class="controls bullet"><span class="by">tysam_and</span><span>|</span><a href="#38272112">root</a><span>|</span><a href="#38272309">parent</a><span>|</span><a href="#38272431">prev</a><span>|</span><a href="#38274322">next</a><span>|</span><label class="collapse" for="c-38273267">[-]</label><label class="expand" for="c-38273267">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not really sure if I understand the intuition here, this seems rather disconnected from what I understand to be the math of optimization.<p>It seems like you&#x27;re referring to an associative Hebbian&#x2F;Hopfield-like lookup, which the current &#x27;dumb&#x27; optimizers already do. Better yet, the learning rates are normalized by the diagonal of the empirical Fisher so that the learning w.r.t. to some estimated expected information is more constant, for said associative lookup operation.<p>Additionally, the training loop (which you call &#x27;dumb&#x27;) is just...a teacher-forced version of inference, which you call &#x27;smart&#x27;?<p>It&#x27;s better to simply minimize the log-likelihood in a scalable way. Hand-engineered solutions rarely survive compared to strong-scaling ones.</div><br/><div id="38273355" class="c"><input type="checkbox" id="c-38273355" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#38272112">root</a><span>|</span><a href="#38273267">parent</a><span>|</span><a href="#38274322">next</a><span>|</span><label class="collapse" for="c-38273355">[-]</label><label class="expand" for="c-38273355">[2 more]</label></div><br/><div class="children"><div class="content">smart is in quotes here for a reason. It&#x27;s &#x27;dumb&#x27; in relation to many people&#x27;s expectations.<p>&gt;Additionally, the training loop (which you call &#x27;dumb&#x27;) is just...a teacher-forced version of inference, which you call &#x27;smart&#x27;?<p>What powers In context learning is not very well understood but it doesn&#x27;t appear to be or really work exactly like just a non teacher-forced version of training. There are qualitative differences. The same models have no problem with this &#x27;curse&#x27; when the information is provided in context for example.<p>&gt;It&#x27;s better to simply minimize the log-likelihood in a scalable way. Hand-engineered solutions rarely survive compared to strong-scaling ones.<p>I never said anything about it being bad.</div><br/><div id="38273447" class="c"><input type="checkbox" id="c-38273447" checked=""/><div class="controls bullet"><span class="by">tysam_and</span><span>|</span><a href="#38272112">root</a><span>|</span><a href="#38273355">parent</a><span>|</span><a href="#38274322">next</a><span>|</span><label class="collapse" for="c-38273447">[-]</label><label class="expand" for="c-38273447">[1 more]</label></div><br/><div class="children"><div class="content">&gt; What powers In context learning is not very well understood but it doesn&#x27;t appear to be or really work exactly like just a non teacher-forced version of training.<p>I mean, yes. One distills information from a training set into a compressed representation, and the other generates a compressed representation that yields (more or less) fixed state space attractors. It&#x27;s just inducing a bias over the state space of the network, nothing incredibly special, though I&#x27;d consider the initial stage of training to be the most important, as it is responsible for all of the ingest of all of the embedded information the network will be using during autoregressive inference (especially w.r.t. the context of doing so during longer generations).<p>So the notion of &#x27;in context learning&#x27; is one I find to be a bit of an illusion, of course, as no actual learning is being done, just the induction of biases, which appears to give rise to a transiently-&#x27;better trained&#x27; network.<p>You could see this as a bit straightforward perhaps, but I feel it needs to be said.</div><br/></div></div></div></div></div></div></div></div><div id="38274322" class="c"><input type="checkbox" id="c-38274322" checked=""/><div class="controls bullet"><span class="by">Grimblewald</span><span>|</span><a href="#38272112">parent</a><span>|</span><a href="#38272309">prev</a><span>|</span><a href="#38272415">next</a><span>|</span><label class="collapse" for="c-38274322">[-]</label><label class="expand" for="c-38274322">[1 more]</label></div><br/><div class="children"><div class="content">While red is the apple is not a correct logical statment using conventional understanding of each term, it is still an understandable english sentence, a structure which likely pops up a fair bit. Even if just for scripts of yoda lines. It just goes to show truly how fucky language is and how amazing it is that something even remotely comprehensible comes out of LLMs</div><br/></div></div><div id="38272415" class="c"><input type="checkbox" id="c-38272415" checked=""/><div class="controls bullet"><span class="by">p1necone</span><span>|</span><a href="#38272112">parent</a><span>|</span><a href="#38274322">prev</a><span>|</span><a href="#38274057">next</a><span>|</span><label class="collapse" for="c-38272415">[-]</label><label class="expand" for="c-38272415">[5 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think it&#x27;s as simple as that - &quot;the apple is red&quot; is &quot;single thing belongs to category&quot;, whereas &quot;Olaf Scholz was the ninth Chancellor of Germany&quot; is &quot;single thing is single thing&quot; - the latter is reversible, the former is not. I would expect a good language model to be able to parse both sentences correctly.</div><br/><div id="38272853" class="c"><input type="checkbox" id="c-38272853" checked=""/><div class="controls bullet"><span class="by">willsoon</span><span>|</span><a href="#38272112">root</a><span>|</span><a href="#38272415">parent</a><span>|</span><a href="#38274057">next</a><span>|</span><label class="collapse" for="c-38272853">[-]</label><label class="expand" for="c-38272853">[4 more]</label></div><br/><div class="children"><div class="content">You are right. You are thinking correctly. But &quot;the apple is red&quot; might not mean that this particular apple belongs ---to put it in your wording--- to the category red, but that the category of things we call apple also belongs to the category of things that are red. And generally speaking, I think that is the meaning.</div><br/><div id="38273724" class="c"><input type="checkbox" id="c-38273724" checked=""/><div class="controls bullet"><span class="by">umanwizard</span><span>|</span><a href="#38272112">root</a><span>|</span><a href="#38272853">parent</a><span>|</span><a href="#38274057">next</a><span>|</span><label class="collapse" for="c-38273724">[-]</label><label class="expand" for="c-38273724">[3 more]</label></div><br/><div class="children"><div class="content">&gt; but that the category of things we call apple also belongs to the category of things that are red<p>No, in English this would be “apples are red”, not “the apple is red”.</div><br/><div id="38274166" class="c"><input type="checkbox" id="c-38274166" checked=""/><div class="controls bullet"><span class="by">bloak</span><span>|</span><a href="#38272112">root</a><span>|</span><a href="#38273724">parent</a><span>|</span><a href="#38274057">next</a><span>|</span><label class="collapse" for="c-38274166">[-]</label><label class="expand" for="c-38274166">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t disagree though perhaps it&#x27;s worth mentioning that sentences such as &quot;the great spotted woodpecker is a medium-sized woodpecker&quot; are used in English with the meaning &quot;great spotted woodpeckers are medium-sized woodpeckers&quot; so it seems to me that &quot;apples are red&quot; is <i>grammatically</i> a possible meaning of &quot;the apple is red&quot; even if it would be stylistically and pragmatically so weird that nobody would ever do that except perhaps as some kind of joke.</div><br/><div id="38274595" class="c"><input type="checkbox" id="c-38274595" checked=""/><div class="controls bullet"><span class="by">karatinversion</span><span>|</span><a href="#38272112">root</a><span>|</span><a href="#38274166">parent</a><span>|</span><a href="#38274057">next</a><span>|</span><label class="collapse" for="c-38274595">[-]</label><label class="expand" for="c-38274595">[1 more]</label></div><br/><div class="children"><div class="content">Indeed, and even with apples, we can say &quot;the apple is native to Central Asia&quot; without meaning a particular apple.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38274057" class="c"><input type="checkbox" id="c-38274057" checked=""/><div class="controls bullet"><span class="by">ryukoposting</span><span>|</span><a href="#38272112">parent</a><span>|</span><a href="#38272415">prev</a><span>|</span><a href="#38272939">next</a><span>|</span><label class="collapse" for="c-38274057">[-]</label><label class="expand" for="c-38274057">[1 more]</label></div><br/><div class="children"><div class="content">The (purported) point of an LLM is its ability to represent language. The inability to represent such a critical nuance would represent a fundamental failure in that regard.</div><br/></div></div><div id="38272939" class="c"><input type="checkbox" id="c-38272939" checked=""/><div class="controls bullet"><span class="by">tysam_and</span><span>|</span><a href="#38272112">parent</a><span>|</span><a href="#38274057">prev</a><span>|</span><a href="#38274055">next</a><span>|</span><label class="collapse" for="c-38272939">[-]</label><label class="expand" for="c-38272939">[9 more]</label></div><br/><div class="children"><div class="content">Bit of a nitpick, &quot;red is the apple&quot; is both a valid sentence and one that also conveys the original relationship held by the opposite phrasing, so, in this case, &quot;the apple is red&quot; does indeed imply &quot;red is the apple&quot;, and accurately so.</div><br/><div id="38274625" class="c"><input type="checkbox" id="c-38274625" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#38272112">root</a><span>|</span><a href="#38272939">parent</a><span>|</span><a href="#38273666">next</a><span>|</span><label class="collapse" for="c-38274625">[-]</label><label class="expand" for="c-38274625">[1 more]</label></div><br/><div class="children"><div class="content">True, but only in the implicit, very narrow world typically assumed when doing logic exercises. &quot;Red is the apple&quot;, obviously, because our whole world consists of &quot;The apple is red&quot; and some inference rules. In that world, &quot;Red is the&quot; can <i>only</i> be completed by &quot;apple&quot;. Drop the narrowing though, and suddenly there&#x27;s a lot of other completions, many of them much better ones (I&#x27;d guess &quot;blood&quot; will be one of the more frequent ones - &quot;red is the blood of&quot; shows up a lot in songs and poems).<p>Or in short: LLMs will handle &quot;The apple is red. Red is the &quot; just fine, because the first part is heavily biasing the answer. They won&#x27;t often complete &quot;Red is the &quot; alone with &quot;apple&quot;, because why should they?</div><br/></div></div><div id="38273666" class="c"><input type="checkbox" id="c-38273666" checked=""/><div class="controls bullet"><span class="by">eru</span><span>|</span><a href="#38272112">root</a><span>|</span><a href="#38272939">parent</a><span>|</span><a href="#38274625">prev</a><span>|</span><a href="#38273058">next</a><span>|</span><label class="collapse" for="c-38273666">[-]</label><label class="expand" for="c-38273666">[3 more]</label></div><br/><div class="children"><div class="content">Perhaps have a look at this famous syllogism instead:<p>(1) Mortal was Socrates.<p>(2) All humans are mortal.<p>(3) Therefor all humans are Socrates.</div><br/><div id="38274104" class="c"><input type="checkbox" id="c-38274104" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#38272112">root</a><span>|</span><a href="#38273666">parent</a><span>|</span><a href="#38273990">next</a><span>|</span><label class="collapse" for="c-38274104">[-]</label><label class="expand" for="c-38274104">[1 more]</label></div><br/><div class="children"><div class="content">Diogenes: If I were mortal I would be Socrates as well.</div><br/></div></div><div id="38273990" class="c"><input type="checkbox" id="c-38273990" checked=""/><div class="controls bullet"><span class="by">trimethylpurine</span><span>|</span><a href="#38272112">root</a><span>|</span><a href="#38273666">parent</a><span>|</span><a href="#38274104">prev</a><span>|</span><a href="#38273058">next</a><span>|</span><label class="collapse" for="c-38273990">[-]</label><label class="expand" for="c-38273990">[1 more]</label></div><br/><div class="children"><div class="content">This isn&#x27;t exactly on topic. A claim about the apple&#x27;s color is attributable to the apple and vice versa. An apple can contextually help someone understand the color red by example.
There is no &quot;therefore&quot; in such a claim, and in fact the algorithm is simply gauging context statistically. Claim 3 in the syllogism would statistically be selected against, because a &quot;therefore&quot; that is false obviously weighs it in the opposite direction from truth. That&#x27;s not the case for the apple.
Essentially it&#x27;s not difficult for the LLM to win on this one, just as it is not difficult for a human and for the same reasons.</div><br/></div></div></div></div><div id="38273058" class="c"><input type="checkbox" id="c-38273058" checked=""/><div class="controls bullet"><span class="by">emme</span><span>|</span><a href="#38272112">root</a><span>|</span><a href="#38272939">parent</a><span>|</span><a href="#38273666">prev</a><span>|</span><a href="#38274055">next</a><span>|</span><label class="collapse" for="c-38273058">[-]</label><label class="expand" for="c-38273058">[4 more]</label></div><br/><div class="children"><div class="content">A proper example is probably &quot;an apple is red&quot; and &quot;red is an apple&quot;.</div><br/><div id="38273091" class="c"><input type="checkbox" id="c-38273091" checked=""/><div class="controls bullet"><span class="by">tikimcfee</span><span>|</span><a href="#38272112">root</a><span>|</span><a href="#38273058">parent</a><span>|</span><a href="#38274055">next</a><span>|</span><label class="collapse" for="c-38273091">[-]</label><label class="expand" for="c-38273091">[3 more]</label></div><br/><div class="children"><div class="content">What you’re missing that is “red is an apple” is also possibly saying in a metaphorical sense that red is like an apple to someone - delicious, a treat, perhaps otherwise representative. In that way, the encoding of “is”’ is exactly correct - it’s an ordered pair of glyphs that imply a weak form of assignment or description. : apologies, replied to the wrong post, meant to push this up one.</div><br/><div id="38273230" class="c"><input type="checkbox" id="c-38273230" checked=""/><div class="controls bullet"><span class="by">cryptoz</span><span>|</span><a href="#38272112">root</a><span>|</span><a href="#38273091">parent</a><span>|</span><a href="#38274055">next</a><span>|</span><label class="collapse" for="c-38273230">[-]</label><label class="expand" for="c-38273230">[2 more]</label></div><br/><div class="children"><div class="content">The correct sentence would start with a capital letter: “Red is an apple.” This is also completely valid as in a cartoon character of an apple named Red. The subject being the start of the sentence compounds the uncertainty in meaning.</div><br/><div id="38273290" class="c"><input type="checkbox" id="c-38273290" checked=""/><div class="controls bullet"><span class="by">tysam_and</span><span>|</span><a href="#38272112">root</a><span>|</span><a href="#38273230">parent</a><span>|</span><a href="#38274055">next</a><span>|</span><label class="collapse" for="c-38273290">[-]</label><label class="expand" for="c-38273290">[1 more]</label></div><br/><div class="children"><div class="content">Yes, this is because there&#x27;s no assignment happening up front w.r.t. the word &#x27;red&#x27;. However, you&#x27;ll notice the same kind of ambiguity for the word &#x27;apple&#x27; in the reversed sentence, as the word &#x27;The&#x27; can imply quite a few things following it.<p>The entropy gotta get slung around somehow.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38274055" class="c"><input type="checkbox" id="c-38274055" checked=""/><div class="controls bullet"><span class="by">johnfn</span><span>|</span><a href="#38272112">parent</a><span>|</span><a href="#38272939">prev</a><span>|</span><a href="#38272304">next</a><span>|</span><label class="collapse" for="c-38274055">[-]</label><label class="expand" for="c-38274055">[3 more]</label></div><br/><div class="children"><div class="content">You&#x27;re thinking of LLMs as basically pattern matching &quot;a is b&quot;, but that&#x27;s not really how they work. In fact, the incredible thing that LLMs can do is that they can understand (some) colloquial language and shades of meaning. LLMs can grasp all sorts of strange textual nuance; they can&#x27;t grasp the cases &quot;Foo is Bar&#x27;s mother&quot; strictly implies &quot;Bar is Foo&#x27;s son&quot;?</div><br/><div id="38274216" class="c"><input type="checkbox" id="c-38274216" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#38272112">root</a><span>|</span><a href="#38274055">parent</a><span>|</span><a href="#38272304">next</a><span>|</span><label class="collapse" for="c-38274216">[-]</label><label class="expand" for="c-38274216">[2 more]</label></div><br/><div class="children"><div class="content">&gt;they can&#x27;t grasp the cases &quot;Foo is Bar&#x27;s mother&quot; strictly implies &quot;Bar is Foo&#x27;s son&quot;?<p>Correction. The Model can. Training cannot.</div><br/><div id="38274287" class="c"><input type="checkbox" id="c-38274287" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#38272112">root</a><span>|</span><a href="#38274216">parent</a><span>|</span><a href="#38272304">next</a><span>|</span><label class="collapse" for="c-38274287">[-]</label><label class="expand" for="c-38274287">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m starting to wonder, why should it in the first place? Very late stages, or fine-tuning, I&#x27;d perhaps understand. But early on? Everything is possible, and &quot;Foo is Bar&#x27;s mother&quot; doesn&#x27;t imply much over &quot;these tokens come together, sometimes&quot;.<p>It&#x27;s not that humans aren&#x27;t suffering from this too. It&#x27;s possible to spot it when you&#x27;re learning, or even easier, when you&#x27;re teaching someone a new thing. A person who learned that &quot;A is B&quot; does not automatically learn that &quot;B is A&quot;; they need to first process it, perhaps run the reversion explicitly in their head. I&#x27;d say that checking if the student can infer &quot;B is A&quot; having learned &quot;A is B&quot; is a good way to tell whether they&#x27;re starting to comprehend the material, vs. just memorizing it.</div><br/></div></div></div></div></div></div><div id="38272304" class="c"><input type="checkbox" id="c-38272304" checked=""/><div class="controls bullet"><span class="by">earthboundkid</span><span>|</span><a href="#38272112">parent</a><span>|</span><a href="#38274055">prev</a><span>|</span><a href="#38272820">next</a><span>|</span><label class="collapse" for="c-38272304">[-]</label><label class="expand" for="c-38272304">[2 more]</label></div><br/><div class="children"><div class="content">Ideally, the LLM would be able to tell the is of identity from the is of predication. I found the article a little too defensive. It felt like &quot;of course it doesn&#x27;t work, there&#x27;s not enough data!&quot; and okay, sure, but that is a flaw, no?</div><br/><div id="38272321" class="c"><input type="checkbox" id="c-38272321" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#38272112">root</a><span>|</span><a href="#38272304">parent</a><span>|</span><a href="#38272820">next</a><span>|</span><label class="collapse" for="c-38272321">[-]</label><label class="expand" for="c-38272321">[1 more]</label></div><br/><div class="children"><div class="content">Well the LLM <i>can</i> tell in that reversal works just fine in-context. This is a recall from training issue and training is dumb.</div><br/></div></div></div></div><div id="38272820" class="c"><input type="checkbox" id="c-38272820" checked=""/><div class="controls bullet"><span class="by">notahacker</span><span>|</span><a href="#38272112">parent</a><span>|</span><a href="#38272304">prev</a><span>|</span><a href="#38272721">next</a><span>|</span><label class="collapse" for="c-38272820">[-]</label><label class="expand" for="c-38272820">[1 more]</label></div><br/><div class="children"><div class="content">the examples are about relations in natural language rather than formal logic though. Mary Lee Pfeiffer being Tom Cruise&#x27;s mother definitely does imply that Tom Cruise is <i>a</i> valid answer to questions about who Mary Lee Pfeiffer&#x27;s son is (it doesn&#x27;t necessarily imply she doesn&#x27;t have other sons or there isn&#x27;t another lady of that name who is childless, but <i>that</i> isn&#x27;t what&#x27;s tripping the model up). And there is absolutely nothing ambiguous about the failures of the fine-tuned model where the author gave it specific phrases exclusively associated with a fake name in the fine-tuning training set and when prompted for &quot;who is $specific_phrase&quot;, supplied different names exclusively associated with completely different phrases (unlike the author, I don&#x27;t see [serendipitously or otherwise] picking name words from the right <i>training set</i> as &quot;B kinda-has-something-to-do-with A generalization&quot;, not when it&#x27;s lost so much information that the combination of name words which <i>exclusively</i> appears in sentences with that phrase isn&#x27;t treated as a more probable response. Never mind emergent understanding of syntax, it isn&#x27;t even making obvious inferences from <i>proximity</i> here)<p>If GPT<i>4</i> rarely makes those errors it&#x27;s clearly not an insurmountable problem at this level of training, but it does imply a lot more difficulty fine-tuning models to reliably retrieve correct information from specific text.</div><br/></div></div><div id="38273715" class="c"><input type="checkbox" id="c-38273715" checked=""/><div class="controls bullet"><span class="by">anon291</span><span>|</span><a href="#38272112">parent</a><span>|</span><a href="#38272230">prev</a><span>|</span><a href="#38272640">next</a><span>|</span><label class="collapse" for="c-38273715">[-]</label><label class="expand" for="c-38273715">[1 more]</label></div><br/><div class="children"><div class="content">&gt; skeptic side of the LLM spectrum<p>Can you explain what you are skeptical of? There is ample evidence that LLMs are what they say they are: a series of transformer (usually) functions with learned weights that can successfully be used to generate human like text in many domains. Do you disbelieve this? Or are you skeptical of something else.</div><br/></div></div><div id="38272640" class="c"><input type="checkbox" id="c-38272640" checked=""/><div class="controls bullet"><span class="by">foobarqux</span><span>|</span><a href="#38272112">parent</a><span>|</span><a href="#38273715">prev</a><span>|</span><a href="#38272063">next</a><span>|</span><label class="collapse" for="c-38272640">[-]</label><label class="expand" for="c-38272640">[1 more]</label></div><br/><div class="children"><div class="content">But that&#x27;s the problem isn&#x27;t it? In some cases they are equivalent and in some cases they are not and a next-word-predictor needs to have &quot;explicit&quot; training data (i.e. it is not doing &quot;reasoning&quot;) whereas a human can infer.<p>This isn&#x27;t surprising if you think about how the machine actually works rather than treating it like a sacred magic box.<p>The default assumption for why there is any &quot;success&quot; for in-context learning should be that it&#x27;s just picking a nearby token that &quot;fits&quot; not a process of logical deduction.<p>edit: The obvious band-aid-fix is to feed reversed sentences into the training data without telling anyone, after which LLM boosters will proclaim that LLMs &quot;learned&quot; to reverse logical implications.</div><br/></div></div></div></div><div id="38272063" class="c"><input type="checkbox" id="c-38272063" checked=""/><div class="controls bullet"><span class="by">tempestn</span><span>|</span><a href="#38272112">prev</a><span>|</span><a href="#38273780">next</a><span>|</span><label class="collapse" for="c-38272063">[-]</label><label class="expand" for="c-38272063">[4 more]</label></div><br/><div class="children"><div class="content">&gt; This isn’t a failure of neural networks. It’s a feature. It’s why you’re not flooded with every single memory and experience you’ve ever had every moment.<p>This is an interesting point, and made me think on whether this &quot;reversal curse&quot; is something we experience with our own, human neural networks. I think it is. Like, I can imagine being given a character in a movie, being able to tell you what actor played them, but given the actor and the movie, not being able to tell you what character they played, or vice-versa. So the pairing exists in my brain somewhere, but I only have an accessible pointer to one side. I think we run into cases like this all the time, actually.</div><br/><div id="38272305" class="c"><input type="checkbox" id="c-38272305" checked=""/><div class="controls bullet"><span class="by">dj_mc_merlin</span><span>|</span><a href="#38272063">parent</a><span>|</span><a href="#38272428">next</a><span>|</span><label class="collapse" for="c-38272305">[-]</label><label class="expand" for="c-38272305">[2 more]</label></div><br/><div class="children"><div class="content">Another similar bug&#x2F;feature of the brain is how we can immediately know if we like a movie or a book, but if asked what movies or books we like, we often blank or name like 3 pieces. Some information is only designed to be retrieved in certain ways in our brain. The fact that neural networks have similar but different limitations isn&#x27;t that concerning, just something to keep in mind.<p>Another funny limitation: when a word is &quot;on the tip of your tongue&quot;, what often happened is you thought of it, but your brain rejected it, so now it&#x27;s on &quot;cooldown&quot; before it appears in your mind again. Usually this mechanism helps but a bug in your brain causes it to be harmful. If you start thinking of something else it &quot;refreshes the cache&quot; and the word comes to you.. we&#x27;re really not that much less buggy than the machine.<p>disclaimer: there&#x27;s other theories about why the phenomenon happens</div><br/><div id="38273216" class="c"><input type="checkbox" id="c-38273216" checked=""/><div class="controls bullet"><span class="by">thehappypm</span><span>|</span><a href="#38272063">root</a><span>|</span><a href="#38272305">parent</a><span>|</span><a href="#38272428">next</a><span>|</span><label class="collapse" for="c-38273216">[-]</label><label class="expand" for="c-38273216">[1 more]</label></div><br/><div class="children"><div class="content">Interestingly, I think the “favorite movies” scenario is also an artifact of our training dataset (our experiences). You spend hours watching a movie, so you have a lot of data about it, and have a lot to build an opinion. But comparing movies? Not something we think about for hours and hours on end. But, people who are movie critics or movie buffs train themselves to do it.</div><br/></div></div></div></div><div id="38272428" class="c"><input type="checkbox" id="c-38272428" checked=""/><div class="controls bullet"><span class="by">rdedev</span><span>|</span><a href="#38272063">parent</a><span>|</span><a href="#38272305">prev</a><span>|</span><a href="#38273780">next</a><span>|</span><label class="collapse" for="c-38272428">[-]</label><label class="expand" for="c-38272428">[1 more]</label></div><br/><div class="children"><div class="content">Reading that article I can&#x27;t help but see similarities between what we have in the human brain. We can easily form and recall short term memories, even using them to logically reason out facts, but loose such memories if it&#x27;s not frequent enough. So it seems like the context prompt is similar to short term memory and what&#x27;s missing now is a good way to transfer the short term to long term. There is a huge amount of assumptions going on here so take it with a huge amount of salt.<p>On another note I should have read the actual paper mentioned in the post critically instead of just skimming it. I completely missed the footnote about the reversal curse not being a problem if everything is present in the initial prompt</div><br/></div></div></div></div><div id="38273780" class="c"><input type="checkbox" id="c-38273780" checked=""/><div class="controls bullet"><span class="by">zapperdulchen</span><span>|</span><a href="#38272063">prev</a><span>|</span><a href="#38272310">next</a><span>|</span><label class="collapse" for="c-38273780">[-]</label><label class="expand" for="c-38273780">[4 more]</label></div><br/><div class="children"><div class="content">I played along the example of the chancellor question.<p>If you deviate a little from the examples in the article GPT-4 gets it all wrong:<p>Who is the eighth Federal Chancellor of the Federal Republic of Germany? - Olaf Scholz (wrong, Angela Merkel)<p><a href="https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;937795ea-bd91-43ee-bd76-1a125f9de3f2" rel="nofollow noreferrer">https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;937795ea-bd91-43ee-bd76-1a125f...</a><p>Who was the eighth Federal Chancellor of the Federal Republic of Germany? - Helmut Kohl (wrong, Angela Merkel)<p><a href="https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;937795ea-bd91-43ee-bd76-1a125f9de3f2" rel="nofollow noreferrer">https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;937795ea-bd91-43ee-bd76-1a125f...</a><p>Interestingly whether you put is or was into the question does make a  difference.<p>Even when allowed to surf the web, ChatGPT gets it wrong:<p>Who was the eighth Federal Chancellor of the Federal Republic of Germany? - Gerhard Schröder (wrong, Angela Merkel)<p><a href="https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;867cb3bc-f642-4d2a-b80d-fe8dc00eef3c" rel="nofollow noreferrer">https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;867cb3bc-f642-4d2a-b80d-fe8dc0...</a><p>Though it&#x27;s referring to the right Wikipedia page: <a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;List_of_chancellors_of_Germany" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;List_of_chancellors_of_Germa...</a></div><br/><div id="38273848" class="c"><input type="checkbox" id="c-38273848" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#38273780">parent</a><span>|</span><a href="#38272310">next</a><span>|</span><label class="collapse" for="c-38273848">[-]</label><label class="expand" for="c-38273848">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Even when allowed to surf the web, ChatGPT gets it wrong:<p>&gt; Who was the eighth Federal Chancellor of the Federal Republic of Germany? - Gerhard Schröder<p>Its probably counting Walter Scheel, who was Acting Chancellor in 1974, and you are probably not.<p>When I asked ChatGPT to <i>list</i> the chancellors in order and identify the eighth, it listed eight ending in Schröder, with Scheel and his ten day Acting Chancellor tenure in May 74 as number 5. (Scheel’s tenure is in the timeline on the Wikipedia page you cite, though not in the numbered list in that page, which is why there is a dicontinuity in thr dates on the numbered list.)</div><br/><div id="38274044" class="c"><input type="checkbox" id="c-38274044" checked=""/><div class="controls bullet"><span class="by">littlestymaar</span><span>|</span><a href="#38273780">root</a><span>|</span><a href="#38273848">parent</a><span>|</span><a href="#38272310">next</a><span>|</span><label class="collapse" for="c-38274044">[-]</label><label class="expand" for="c-38274044">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s funny because you ascribe some reasoning in ChatGPT&#x27;s answer “Gerhard Schröder”, but somehow missed that it&#x27;s also able to give you two other answers that are unambiguously wrong…</div><br/><div id="38274314" class="c"><input type="checkbox" id="c-38274314" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#38273780">root</a><span>|</span><a href="#38274044">parent</a><span>|</span><a href="#38272310">next</a><span>|</span><label class="collapse" for="c-38274314">[-]</label><label class="expand" for="c-38274314">[1 more]</label></div><br/><div class="children"><div class="content">&gt; It&#x27;s funny because you ascribe some reasoning in ChatGPT&#x27;s answer “Gerhard Schröder”, but somehow missed that it&#x27;s also able to give you two other answers that are unambiguously wrong<p>Schröder is the only answer it gave <i>when using functionality that would bring some representation if a list into its context first</i>, and 10it did it with different prompts and mechanisms for bringing a list into its context.<p>That LLMs are bad at counting-related tasks without doing that is well-known, and not a point I felt needed belaboring.</div><br/></div></div></div></div></div></div></div></div><div id="38272310" class="c"><input type="checkbox" id="c-38272310" checked=""/><div class="controls bullet"><span class="by">Imnimo</span><span>|</span><a href="#38273780">prev</a><span>|</span><a href="#38274337">next</a><span>|</span><label class="collapse" for="c-38272310">[-]</label><label class="expand" for="c-38272310">[4 more]</label></div><br/><div class="children"><div class="content">&gt;If you start a query with “Mary Lee Pfeiffer”, you’re not going to get very far because neural networks aren’t equidistant grids of points (besides the fact that she may not appear very often under that version of her name.) They’re networks of nodes, some with many connections, some with few. One of the ways you optimize large models is by pruning off weakly connected regions. This may come at the expense of destroying B is A relationships for weakly represented entities.<p>Am I missing something here? This paragraph reads like complete gibberish to me.<p>Also, I don&#x27;t buy the experiment at the end. If you fine-tune the model with exclusively Tom Cruise data, I want to see proof that it doesn&#x27;t just answer &quot;Tom Cruise&quot; all the time. I want to see that it says Tom Cruise wrote Aces in the Stream, but doesn&#x27;t say Tom Cruise wrote Kings in the River.</div><br/><div id="38273240" class="c"><input type="checkbox" id="c-38273240" checked=""/><div class="controls bullet"><span class="by">amayne</span><span>|</span><a href="#38272310">parent</a><span>|</span><a href="#38272866">next</a><span>|</span><label class="collapse" for="c-38273240">[-]</label><label class="expand" for="c-38273240">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve added your suggested test to the blog post: <a href="https:&#x2F;&#x2F;andrewmayne.com&#x2F;2023&#x2F;11&#x2F;14&#x2F;is-the-reversal-curse-real&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;andrewmayne.com&#x2F;2023&#x2F;11&#x2F;14&#x2F;is-the-reversal-curse-rea...</a><p>Spoiler: It doesn&#x27;t say &quot;Tom Cruise.&quot;</div><br/></div></div><div id="38272866" class="c"><input type="checkbox" id="c-38272866" checked=""/><div class="controls bullet"><span class="by">Xelynega</span><span>|</span><a href="#38272310">parent</a><span>|</span><a href="#38273240">prev</a><span>|</span><a href="#38272926">next</a><span>|</span><label class="collapse" for="c-38272866">[-]</label><label class="expand" for="c-38272866">[1 more]</label></div><br/><div class="children"><div class="content">Their entir &quot;graph&quot; description of neural networks reads as nonsense.<p>Neural networks are graphs insofar as they&#x27;re networks of connected neurons and not further(definitely not graphs of information as the writer seems to think). Given the fact that neural networks are quite literally an n-dimensional function once trained, it&#x27;s more accurate to call them an &quot;equidistant grid of points&quot; than it is to call them a graph of information since literally all they do is take an n dimensional vector and output an n dimensional vector.</div><br/></div></div><div id="38272926" class="c"><input type="checkbox" id="c-38272926" checked=""/><div class="controls bullet"><span class="by">Izkata</span><span>|</span><a href="#38272310">parent</a><span>|</span><a href="#38272866">prev</a><span>|</span><a href="#38274337">next</a><span>|</span><label class="collapse" for="c-38272926">[-]</label><label class="expand" for="c-38272926">[1 more]</label></div><br/><div class="children"><div class="content">It reads like they saw the diagram from [0] and didn&#x27;t understand what it meant (like, they assumed &quot;x1&quot; represented an entity like &quot;Mary Lee Pfeiffer&quot;).  Note how in that diagram, the bottom nodes in the second and third layer aren&#x27;t fully connected - which if you don&#x27;t read the description, you could end up assuming the connections between layers are created and trimmed, not just weights changed.<p>They may also be getting it mixed up with semantic search, where an entity like that would exist in a graph and have connections to related concepts.<p>[0] <a href="https:&#x2F;&#x2F;towardsdatascience.com&#x2F;first-neural-network-for-beginners-explained-with-code-4cfd37e06eaf" rel="nofollow noreferrer">https:&#x2F;&#x2F;towardsdatascience.com&#x2F;first-neural-network-for-begi...</a></div><br/></div></div></div></div><div id="38274337" class="c"><input type="checkbox" id="c-38274337" checked=""/><div class="controls bullet"><span class="by">bjornsing</span><span>|</span><a href="#38272310">prev</a><span>|</span><a href="#38274317">next</a><span>|</span><label class="collapse" for="c-38274337">[-]</label><label class="expand" for="c-38274337">[1 more]</label></div><br/><div class="children"><div class="content">I didn’t read the whole thing, but the first part about Tom Cruise and his mother sounded very flawed: <i>Of course</i> the LLM could learn the reverse relationship if it had better data where the reverse relationship is a common occurrence. The point of the reversal curse argument (I guess) is that it should be able to learn the reverse relationship from entirely other examples [1], but seemingly is not.<p>1. That is, the LLM should be able to learn that “A is son of B who is female” implies that “B is mother of A”, regardless of who A and B is. It should then be able to apply this pattern to A = “Tom Cruise” and B = “Mary Lee Pfeiffer” and deduce “Mary Lee Pfeiffer is the mother of Tom Cruise” <i>without even a single example</i>.</div><br/></div></div><div id="38274317" class="c"><input type="checkbox" id="c-38274317" checked=""/><div class="controls bullet"><span class="by">Applejinx</span><span>|</span><a href="#38274337">prev</a><span>|</span><a href="#38271914">next</a><span>|</span><label class="collapse" for="c-38274317">[-]</label><label class="expand" for="c-38274317">[1 more]</label></div><br/><div class="children"><div class="content">This is great work. Andrew&#x27;s on to something. Thing is, the behavior we see in his form of the LLM is also what we experience as humans.<p>Good writers, marketers and manipulators know this about people. You seed the ground with the concepts you&#x27;ll need to introduce, so they&#x27;re present in the person&#x27;s &#x27;model&#x27; and you can elicit them later, on request.<p>Formalizing things in the &#x27;reversal curse&#x27; manner is like loading the &#x27;mind&#x27; of the LLM with habits and assumptions and cutting off its ability to free-associate from seemingly relevant concepts… which is likely to be more valuable in the long run, because an LLM can contain more than a human can. That doesn&#x27;t mean it will be more intelligent, but it seems reasonable to infer that the LLM can have a broader base of association to draw from, where we as humans tend to be restricted to associations from our own experience. We only get one shot at &#x27;training data&#x27;, though it&#x27;s in countless sensory forms, where LLMs are stuck with language as their only window onto experience.<p>I&#x27;m loving the notion of leaving the prompt &#x27;training&#x27; stark raving blank. Let&#x27;s see what comes out of this giant pile of human verbal associations. It is only that, associations, but it&#x27;s on a grander scale than we&#x27;re accustomed to. Making it &#x27;answer questions correctly&#x27; seems woefully unambitious.</div><br/></div></div><div id="38271914" class="c"><input type="checkbox" id="c-38271914" checked=""/><div class="controls bullet"><span class="by">ska</span><span>|</span><a href="#38274317">prev</a><span>|</span><a href="#38272507">next</a><span>|</span><label class="collapse" for="c-38271914">[-]</label><label class="expand" for="c-38271914">[5 more]</label></div><br/><div class="children"><div class="content">&gt; Saying that models can’t automatically generalize from B to A when B is vastly underrepresented in the dataset feels rather obvious and not so much a curse as a description of how neural nets function<p>More importantly perhaps, it&#x27;s not how people (at least non AI&#x2F;ML research types) typically <i>think</i> they work, and much of the handwaving and hype around it isn&#x27;t helping improve that.</div><br/><div id="38271996" class="c"><input type="checkbox" id="c-38271996" checked=""/><div class="controls bullet"><span class="by">ImPostingOnHN</span><span>|</span><a href="#38271914">parent</a><span>|</span><a href="#38272507">next</a><span>|</span><label class="collapse" for="c-38271996">[-]</label><label class="expand" for="c-38271996">[4 more]</label></div><br/><div class="children"><div class="content">It doesn&#x27;t seem obvious at all unless you start with the assumption that not being able to derive A=B -&gt; B=A is &quot;obvious&quot;, which it clearly isn&#x27;t to most people.<p>Indeed, just because the lack of that capability is a result of the <i>design</i> of LLMs doesn&#x27;t mean it&#x27;s a <i>feature</i> of LLMs. It could also be that it&#x27;s a <i>bug</i> of LLMs. Which one depends on what the expected behavior is, and the expected behavior from the product is being able to perform the above logical derivation.<p>tl;dr: the article reaffirms that the &quot;curse&quot; is true, and for the reasons claimed too, but it&#x27;s a feature, not a bug.</div><br/><div id="38272031" class="c"><input type="checkbox" id="c-38272031" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#38271914">root</a><span>|</span><a href="#38271996">parent</a><span>|</span><a href="#38272507">next</a><span>|</span><label class="collapse" for="c-38272031">[-]</label><label class="expand" for="c-38272031">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s &quot;obvious&quot; because A is B -&gt; B is A is not a thing that is actually true for the vast majority of text (or really any kind) constructions. It&#x27;s only a truth of formal logic.</div><br/><div id="38272211" class="c"><input type="checkbox" id="c-38272211" checked=""/><div class="controls bullet"><span class="by">saithound</span><span>|</span><a href="#38271914">root</a><span>|</span><a href="#38272031">parent</a><span>|</span><a href="#38273373">next</a><span>|</span><label class="collapse" for="c-38272211">[-]</label><label class="expand" for="c-38272211">[1 more]</label></div><br/><div class="children"><div class="content">Logician here. We&#x27;re well aware that &quot;A=B&quot; is not a good formalization of the colloquial&#x2F;grammarical &quot;A is B&quot;, we don&#x27;t formalize it that way, and we don&#x27;t regard &quot;if A is B then B is A&quot; is a truth of formal logic.<p>The people making the argument about reversal curses are not logicians, and most of them don&#x27;t know anything more about formal logic than what anyone would pick up in an undergraduate &quot;Intro to Proofs&quot; course.<p>That said, the semantics of the word &quot;is&quot; in natural language really doesn&#x27;t matter in this debate. The semantics is a red herring, if you will (while a red herring is not semantics).<p>After all, LLMs cannot learn &quot;the quantity B is mathematically equal to A&quot; from examples of &quot;the quantity A is mathematically equal to B&quot; either, even when the rest of the corpus clearly explains that this _is_ in fact always reversible.</div><br/></div></div><div id="38273373" class="c"><input type="checkbox" id="c-38273373" checked=""/><div class="controls bullet"><span class="by">rossdavidh</span><span>|</span><a href="#38271914">root</a><span>|</span><a href="#38272031">parent</a><span>|</span><a href="#38272211">prev</a><span>|</span><a href="#38272507">next</a><span>|</span><label class="collapse" for="c-38273373">[-]</label><label class="expand" for="c-38273373">[1 more]</label></div><br/><div class="children"><div class="content">It is a thing that nearly any non-technical user of LLM&#x27;s would expect it to be able to do, and be surprised at it not doing.  Which is a bug, if you expect it to be a service&#x2F;piece of software that someone not trained on using LLM&#x27;s to be able to do.  The author more or less admits this in the section entitled &quot;Model training is a dark art&quot;<p>&#x27;I’ve been playing around with fine-tuning LLM models for years and still don’t have any hard and fast one-size-fits-all rules to apply. Every dataset lends itself to a specific way of training.&#x27;<p>But this is precisely the reason why nearly every claim about what &quot;AI&quot; will soon be able to do, is misguided.  The failures of LLM&#x27;s are exceedingly unintuitive to anyone who doesn&#x27;t have a lot of experience with them (and maybe sometimes to people who do).<p>Spreadsheets can be used by people who don&#x27;t know how spreadsheets&#x27; internals work; after a bit of training (in my experience, about ten minutes) they can get a decent intuition about how to use a spreadsheet (at least for the simple stuff).  The same is true of well designed web pages, word processors, music players, etc.  Even software with more complex training requirements like CAD, statistics packages, video editing, etc. will usually behave in a more-or-less intuitive fashion for a user of the appropriate target group.<p>LLM&#x27;s fail in unexpected (and sometimes hard to spot) ways, and are being marketed as if they are a tool for the general population to use, when their training (and failure modes) are still a &quot;dark art&quot; even for people with years of experience in them.  That is not a feature.</div><br/></div></div></div></div></div></div></div></div><div id="38272507" class="c"><input type="checkbox" id="c-38272507" checked=""/><div class="controls bullet"><span class="by">zarzavat</span><span>|</span><a href="#38271914">prev</a><span>|</span><a href="#38274276">next</a><span>|</span><label class="collapse" for="c-38272507">[-]</label><label class="expand" for="c-38272507">[7 more]</label></div><br/><div class="children"><div class="content">Humans are also vulnerable to the reversal curse! When you learn languages you have to learn both directions (chat is cat and cat is chat), anybody who has built an anki deck will know this, otherwise you will be better in one direction than the other.</div><br/><div id="38274340" class="c"><input type="checkbox" id="c-38274340" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#38272507">parent</a><span>|</span><a href="#38272829">next</a><span>|</span><label class="collapse" for="c-38274340">[-]</label><label class="expand" for="c-38274340">[2 more]</label></div><br/><div class="children"><div class="content">Not just languages. It&#x27;s the case with <i>everything</i>. Easy to spot when you&#x27;re tutoring someone. You can see they learned &quot;force is mass times acceleration&quot; or &quot;for( ... ) is how you make the same code run multiple times&quot; - they can tell you that when quizzed! But you know they haven&#x27;t <i>comprehended</i> it until they can reverse it - &quot;I need to compute the mass of the object, which I see accelerating this much under this force; I can pull that from F=ma -&gt; m=F&#x2F;a!&quot;, or &quot;I want to run this code several times, I need a `for` loop!&quot;. And getting to that stage is often the longest and most difficult part.</div><br/><div id="38274374" class="c"><input type="checkbox" id="c-38274374" checked=""/><div class="controls bullet"><span class="by">omoide</span><span>|</span><a href="#38272507">root</a><span>|</span><a href="#38274340">parent</a><span>|</span><a href="#38272829">next</a><span>|</span><label class="collapse" for="c-38274374">[-]</label><label class="expand" for="c-38274374">[1 more]</label></div><br/><div class="children"><div class="content">Interesting; your message hints at a sharp gap between using language stochastically (eg being able to repeat what you&#x27;ve heard) vs using language to express deeper knowledge&#x2F;understanding.<p>A common argument is that LLM by their nature only model the former, not the latter.</div><br/></div></div></div></div><div id="38272829" class="c"><input type="checkbox" id="c-38272829" checked=""/><div class="controls bullet"><span class="by">tkgally</span><span>|</span><a href="#38272507">parent</a><span>|</span><a href="#38274340">prev</a><span>|</span><a href="#38274276">next</a><span>|</span><label class="collapse" for="c-38272829">[-]</label><label class="expand" for="c-38272829">[4 more]</label></div><br/><div class="children"><div class="content">As one gets deeper into learning another language, it’s also important to be aware that the meanings of words in different languages rarely map to each other in clean bijections. Common words especially tend to be semantic clouds, not fixed points of meaning.<p>I don’t know French, but I am sure there are many cases where an English phrase or sentence that includes ‘cat’ should not be translated into French with ‘chat’ and vice versa. (When asked, GPT-4 offers two such examples: &quot;let the cat out of the bag” and &quot;avoir un chat dans la gorge.&quot;)<p>I don’t mean to suggest, though, that memorizing word pairs is not a good way to learn vocabulary in another language. For me, it was an essential step in acquiring the second language that I am now fluent in (Japanese).</div><br/><div id="38274351" class="c"><input type="checkbox" id="c-38274351" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#38272507">root</a><span>|</span><a href="#38272829">parent</a><span>|</span><a href="#38273750">next</a><span>|</span><label class="collapse" for="c-38274351">[-]</label><label class="expand" for="c-38274351">[1 more]</label></div><br/><div class="children"><div class="content">Words are <i>always</i> semantic clouds. Not getting that is what sent philosophy down many a dead end, and the source of plenty of arguments regular people get into all the time. One doesn&#x27;t need to try mapping between two languages - it&#x27;s enough problem trying to map within the language itself.</div><br/></div></div><div id="38273750" class="c"><input type="checkbox" id="c-38273750" checked=""/><div class="controls bullet"><span class="by">umanwizard</span><span>|</span><a href="#38272507">root</a><span>|</span><a href="#38272829">parent</a><span>|</span><a href="#38274351">prev</a><span>|</span><a href="#38273313">next</a><span>|</span><label class="collapse" for="c-38273750">[-]</label><label class="expand" for="c-38273750">[1 more]</label></div><br/><div class="children"><div class="content">I think this has as much to do with abstract vs. concrete words as common vs. uncommon. If you had to pick a word corresponding to “chat” it’s quite clearly “cat”, despite a few different expressions. But it’s rather difficult to translate “justement” to English, or “random” to French, without further context.</div><br/></div></div></div></div></div></div><div id="38274276" class="c"><input type="checkbox" id="c-38274276" checked=""/><div class="controls bullet"><span class="by">maaaaattttt</span><span>|</span><a href="#38272507">prev</a><span>|</span><a href="#38272164">next</a><span>|</span><label class="collapse" for="c-38274276">[-]</label><label class="expand" for="c-38274276">[4 more]</label></div><br/><div class="children"><div class="content">The Olaf Scholz exemple from this article is just another exemple of how LLMs can’t count. If you try this prompt: “In this list of words: bike, apple, phone, dirt, tee, sun, glass; which is the fifth word?” it will fail as well. “Fifth” is not connected to any counting ability in LLMs the way it is for us.<p>If you now try this prompt: “Who’s Tom Cruise’s mother in this exemple: “Mary Lee Pfeiffer is Tom Cruise’s mother.”?” It will give you the right answer.<p>IMO this is a sign that it can understand reversibility.<p>The exemple used are not present enough in the dataset and the “confidence” of the model is not high enough for it to give the right answer. Or, as stated at the beginning they use counting mechanisms (or others) that the model just doesn’t have.</div><br/><div id="38274298" class="c"><input type="checkbox" id="c-38274298" checked=""/><div class="controls bullet"><span class="by">saliagato</span><span>|</span><a href="#38274276">parent</a><span>|</span><a href="#38274346">prev</a><span>|</span><a href="#38272164">next</a><span>|</span><label class="collapse" for="c-38274298">[-]</label><label class="expand" for="c-38274298">[2 more]</label></div><br/><div class="children"><div class="content">It can’t count because what the LLM sees is a bunch of tokens not words</div><br/><div id="38274359" class="c"><input type="checkbox" id="c-38274359" checked=""/><div class="controls bullet"><span class="by">maaaaattttt</span><span>|</span><a href="#38274276">root</a><span>|</span><a href="#38274298">parent</a><span>|</span><a href="#38272164">next</a><span>|</span><label class="collapse" for="c-38274359">[-]</label><label class="expand" for="c-38274359">[1 more]</label></div><br/><div class="children"><div class="content">Exactly. And asking what&#x2F;who&#x2F;which is the “Ninth” something will always fail (or randomly succeed)</div><br/></div></div></div></div></div></div><div id="38272164" class="c"><input type="checkbox" id="c-38272164" checked=""/><div class="controls bullet"><span class="by">akoboldfrying</span><span>|</span><a href="#38274276">prev</a><span>|</span><a href="#38272056">next</a><span>|</span><label class="collapse" for="c-38272164">[-]</label><label class="expand" for="c-38272164">[5 more]</label></div><br/><div class="children"><div class="content">Only skimmed this and didn&#x27;t read the underlying paper, but I was surprised to see no mention of the fact that &quot;A is B&quot; often does not at all imply &quot;B is A&quot; in everyday language: A bird is an animal, but it&#x27;s wrong to conclude that an arbitrary animal must be a bird.</div><br/><div id="38272343" class="c"><input type="checkbox" id="c-38272343" checked=""/><div class="controls bullet"><span class="by">thisgoesnowhere</span><span>|</span><a href="#38272164">parent</a><span>|</span><a href="#38272338">next</a><span>|</span><label class="collapse" for="c-38272343">[-]</label><label class="expand" for="c-38272343">[2 more]</label></div><br/><div class="children"><div class="content">The word an in this sentence (indefinite article) means that the word animal is non specific. So this structure does not follow from the original. &quot;A is B&quot; is not the same as &quot;A is a B&quot;<p>However, you are right that it&#x27;s not always correct since &quot;Poetry is Literature&quot; and &quot;Steam&#x2F;Ice is H20&quot; are both examples where this breaks down.<p>My annoying pedantry for the day.</div><br/><div id="38274152" class="c"><input type="checkbox" id="c-38274152" checked=""/><div class="controls bullet"><span class="by">randyrand</span><span>|</span><a href="#38272164">root</a><span>|</span><a href="#38272343">parent</a><span>|</span><a href="#38272338">next</a><span>|</span><label class="collapse" for="c-38274152">[-]</label><label class="expand" for="c-38274152">[1 more]</label></div><br/><div class="children"><div class="content">you can do the same mistake without “an”.<p>Jeff is the short haired guy != the short haired guy is Jeff.<p>It depends on context. You can’t fully equate them.</div><br/></div></div></div></div><div id="38272338" class="c"><input type="checkbox" id="c-38272338" checked=""/><div class="controls bullet"><span class="by">thaumasiotes</span><span>|</span><a href="#38272164">parent</a><span>|</span><a href="#38272343">prev</a><span>|</span><a href="#38272056">next</a><span>|</span><label class="collapse" for="c-38272338">[-]</label><label class="expand" for="c-38272338">[2 more]</label></div><br/><div class="children"><div class="content">This is bad news for all the people who like to say &quot;A winner is you!&quot;</div><br/><div id="38273803" class="c"><input type="checkbox" id="c-38273803" checked=""/><div class="controls bullet"><span class="by">Falkon1313</span><span>|</span><a href="#38272164">root</a><span>|</span><a href="#38272338">parent</a><span>|</span><a href="#38272056">next</a><span>|</span><label class="collapse" for="c-38273803">[-]</label><label class="expand" for="c-38273803">[1 more]</label></div><br/><div class="children"><div class="content">But obviously that implies that you are a chicken dinner.<p>At least according to some people&#x27;s expectations, presumably.</div><br/></div></div></div></div></div></div><div id="38274119" class="c"><input type="checkbox" id="c-38274119" checked=""/><div class="controls bullet"><span class="by">golol</span><span>|</span><a href="#38272056">prev</a><span>|</span><a href="#38273887">next</a><span>|</span><label class="collapse" for="c-38274119">[-]</label><label class="expand" for="c-38274119">[1 more]</label></div><br/><div class="children"><div class="content">Excellent blog post. The only thing which I wish was there was an explanation of how promot + completion training would be any different to pure completion training. From my understanding oftransformers, it ahould be the same.</div><br/></div></div><div id="38273887" class="c"><input type="checkbox" id="c-38273887" checked=""/><div class="controls bullet"><span class="by">diffeomorphism</span><span>|</span><a href="#38274119">prev</a><span>|</span><a href="#38273652">next</a><span>|</span><label class="collapse" for="c-38273887">[-]</label><label class="expand" for="c-38273887">[3 more]</label></div><br/><div class="children"><div class="content">Well, yeah because the reversal is just completely wrong most of the time.<p>Only if you have additional context clues like definite articles or some outside knowledge that a role or property is unique, then you can maybe, sometimes conclude the reverse. Not a bug, working as intended.</div><br/><div id="38273993" class="c"><input type="checkbox" id="c-38273993" checked=""/><div class="controls bullet"><span class="by">threatripper</span><span>|</span><a href="#38273887">parent</a><span>|</span><a href="#38273652">next</a><span>|</span><label class="collapse" for="c-38273993">[-]</label><label class="expand" for="c-38273993">[2 more]</label></div><br/><div class="children"><div class="content">Some relationships like parent child are quite reversible.</div><br/><div id="38274128" class="c"><input type="checkbox" id="c-38274128" checked=""/><div class="controls bullet"><span class="by">diffeomorphism</span><span>|</span><a href="#38273887">root</a><span>|</span><a href="#38273993">parent</a><span>|</span><a href="#38273652">next</a><span>|</span><label class="collapse" for="c-38274128">[-]</label><label class="expand" for="c-38274128">[1 more]</label></div><br/><div class="children"><div class="content">Keywords &quot;some&quot; and &quot;quite&quot;. Also, only (likely) half the information. &quot;John is Alice&#x27;s parent.&quot;, &quot;Who are Alice&#x27;s parents?&quot;.<p>And that is ignoring questions like single parent households, adoptions, biological parents, step parents, multigeneration households, getting disowned...<p>If a reversal is correct 90% of the time, that is useful. However, it will still be wrong 10% of the time.</div><br/></div></div></div></div></div></div><div id="38273652" class="c"><input type="checkbox" id="c-38273652" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#38273887">prev</a><span>|</span><a href="#38274158">next</a><span>|</span><label class="collapse" for="c-38273652">[-]</label><label class="expand" for="c-38273652">[1 more]</label></div><br/><div class="children"><div class="content">I didn&#x27;t know anyone was even suspicious of the reversal &quot;curse&quot;. I thought it went viral more because people were surprised anyone was surprised and the tweet was highly sensationalized. It feels more like the __expected__ results considering both speech patter bias __and__ that &quot;causal&quot; attention is sequentially biased. Hell, we saw the same things in RNNs, we see it in classic autoregressive models, and so on. But it definitely isn&#x27;t how humans encode information because if you had all 3 pieces of knowledge (you know who Tom Cruse is, you know who Mary Pfieffer is, and you know the relation between Mary and Tom is mom&#x2F;son) then your ability to recall is (nearly) invariant to the ordering. Hell, we&#x27;re so robust you can caveman speak like &quot;Mary Pfieffer son who!&quot; and still get the answer or caveman yoda speak &quot;Who Mary Pfieffer son is?&quot; Some languages even have these orderings but we&#x27;re impressively robust (enough that I think we trick ourselves a lot when subtlety comes into play. AKA overfitting).<p>So I find it weird to call this &quot;a feature&quot; and also act like this is surprising.<p>But can I also take a minute to just say I really hate GPT experiments? They&#x27;re performed on a stochastic model, with proprietary weights, proprietary training data, proprietary training methods, and above all is constantly changing and at a rather fast pace. It makes for a very non-scientific process as you can&#x27;t decouple a lot of important factors and reproduction is a crap shoot. It is not a very good way to go about studying &quot;how __LLMs__ work&quot; and is rather &quot;how does GPT work at this particular moment in time and aggregating all these unknowns?&quot; There&#x27;s some bitter sweetness because I do like that GPT is free but it feels like a major edge that they have is that the community just does a lot of free research for them and in ways where they can better interpret results than the people who performed the experiments in the first place. I really do believe that academic works shouldn&#x27;t be focusing on proprietary and dynamic methods. It&#x27;s fine to include them in results (and preferably added post review to avoid identity spoilage (or we openly acknowledge that double blind is a joke)) but I&#x27;d rather most researchers focusing on the general concepts and with the ability to dive down the rabbit hole than playing a wack-a-mole game.<p>Also, I&#x27;d totally love it if more research papers were instead blog posts. Kudos to anyone posting their research on blogs, academic or not (I don&#x27;t care about your creds, your real creds are your work). Papers are about communicating with fellow scientists, right? Why do we need journals and conferences these days?</div><br/></div></div><div id="38274158" class="c"><input type="checkbox" id="c-38274158" checked=""/><div class="controls bullet"><span class="by">RamblingCTO</span><span>|</span><a href="#38273652">prev</a><span>|</span><a href="#38274344">next</a><span>|</span><label class="collapse" for="c-38274158">[-]</label><label class="expand" for="c-38274158">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure about this. Olaf Scholz has been chancellor since 2021 and the cut off for chatgpt 4 is allegedly January 2022.<p>My gut also tells me that the underlying embeddings should very well reflect equidistant relations of olaf scholz and chancellor.<p>&#x2F;e: instead of downvoting, I&#x27;d love to have your opinion instead on why you think I&#x27;m wrong ...</div><br/></div></div><div id="38274344" class="c"><input type="checkbox" id="c-38274344" checked=""/><div class="controls bullet"><span class="by">aade63482</span><span>|</span><a href="#38274158">prev</a><span>|</span><a href="#38273561">next</a><span>|</span><label class="collapse" for="c-38274344">[-]</label><label class="expand" for="c-38274344">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;papystreaming.quest&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;papystreaming.quest&#x2F;</a></div><br/></div></div><div id="38273561" class="c"><input type="checkbox" id="c-38273561" checked=""/><div class="controls bullet"><span class="by">dr_dshiv</span><span>|</span><a href="#38274344">prev</a><span>|</span><a href="#38274163">next</a><span>|</span><label class="collapse" for="c-38273561">[-]</label><label class="expand" for="c-38273561">[1 more]</label></div><br/><div class="children"><div class="content">&gt; So in summation: I don’t think any of the examples the authors provided are proof of a Reversal Curse and we haven’t observed a “failure of logical deduction.” Simpler explanations are more explanatory: imprecise prompts, underrepresented data and fine-tuning errors.<p>Phew!</div><br/></div></div><div id="38274163" class="c"><input type="checkbox" id="c-38274163" checked=""/><div class="controls bullet"><span class="by">user_named</span><span>|</span><a href="#38273561">prev</a><span>|</span><a href="#38273033">next</a><span>|</span><label class="collapse" for="c-38274163">[-]</label><label class="expand" for="c-38274163">[4 more]</label></div><br/><div class="children"><div class="content">I mean obviously yes. LLMs aren&#x27;t intelligent and they don&#x27;t understand anything.<p>OP is trying to argue against this but it&#x27;s nonsense. ChatGPT also cannot tell me who the 8th chancellor was. It tells me it was Gerhard Schröder.</div><br/><div id="38274236" class="c"><input type="checkbox" id="c-38274236" checked=""/><div class="controls bullet"><span class="by">TeMPOraL</span><span>|</span><a href="#38274163">parent</a><span>|</span><a href="#38274168">next</a><span>|</span><label class="collapse" for="c-38274236">[-]</label><label class="expand" for="c-38274236">[2 more]</label></div><br/><div class="children"><div class="content">IDK, I still some of it boil down to &quot;B -&gt; A&quot; being extremely ambiguous. Other than the annoying Tom Cruise example (I finally know where it came from!), I&#x27;ve seen people frequently bring up &quot;The color of the sky is blue&quot; vs. &quot;Blue is the color of the sky&quot; - but this illustrates my hypothesis perfectly. In training data (and in the totality of what humans ever said or wrote), &quot;Color of the sky is &quot; is almost certain to be followed by &quot;blue&quot; - meanwhile, &quot;Blue is the color of the &quot; has <i>a lot</i> of possible completions, of which &quot;sky&quot; is by far not the most likely.</div><br/><div id="38274311" class="c"><input type="checkbox" id="c-38274311" checked=""/><div class="controls bullet"><span class="by">user_named</span><span>|</span><a href="#38274163">root</a><span>|</span><a href="#38274236">parent</a><span>|</span><a href="#38274168">next</a><span>|</span><label class="collapse" for="c-38274311">[-]</label><label class="expand" for="c-38274311">[1 more]</label></div><br/><div class="children"><div class="content">Nah. The context of all of this is people claiming LLMs are AI. If so they would be able to reverse A is B, and also know when not to, so your example is irrelevant.<p>The paper shows that it cannot reverse. Then the blog posts goes around different disingenuous ways of making excuses for why it can&#x27;t reverse at the same time as trying to show that it can reverse in the Tom Cruise training example (extremely disingenuous because it&#x27;s just the same data over and over with synonyms replaced, and what he gets out of it is just completions, not logical deduction).</div><br/></div></div></div></div><div id="38274168" class="c"><input type="checkbox" id="c-38274168" checked=""/><div class="controls bullet"><span class="by">user_named</span><span>|</span><a href="#38274163">parent</a><span>|</span><a href="#38274236">prev</a><span>|</span><a href="#38273033">next</a><span>|</span><label class="collapse" for="c-38274168">[-]</label><label class="expand" for="c-38274168">[1 more]</label></div><br/><div class="children"><div class="content">It also cannot tell me the 7th.</div><br/></div></div></div></div><div id="38272680" class="c"><input type="checkbox" id="c-38272680" checked=""/><div class="controls bullet"><span class="by">ugh123</span><span>|</span><a href="#38273033">prev</a><span>|</span><a href="#38273817">next</a><span>|</span><label class="collapse" for="c-38272680">[-]</label><label class="expand" for="c-38272680">[1 more]</label></div><br/><div class="children"><div class="content">This seems like the kind of thing that would be addressed during implementation, rather than persevere as a fundamental problem with LLMs.</div><br/></div></div><div id="38273817" class="c"><input type="checkbox" id="c-38273817" checked=""/><div class="controls bullet"><span class="by">riwsky</span><span>|</span><a href="#38272680">prev</a><span>|</span><a href="#38272189">next</a><span>|</span><label class="collapse" for="c-38273817">[-]</label><label class="expand" for="c-38273817">[1 more]</label></div><br/><div class="children"><div class="content">“I’m going to define generalization such that my thesis that GPT-4 can generalize reversals is true—and lo, my thesis is true!”</div><br/></div></div><div id="38272189" class="c"><input type="checkbox" id="c-38272189" checked=""/><div class="controls bullet"><span class="by">ShamelessC</span><span>|</span><a href="#38273817">prev</a><span>|</span><a href="#38273326">next</a><span>|</span><label class="collapse" for="c-38272189">[-]</label><label class="expand" for="c-38272189">[7 more]</label></div><br/><div class="children"><div class="content">The article is a decent peer review and refutation of “the reversal curse”. Some of the comments given here clearly haven’t read the whole article though - arriving at similarly skeptical conclusions that are clearly present and expanded on in the article.<p>Why do people feel the need to do this here? Armchair commentary on advanced material is one of the main reasons I avoid Reddit. And furthermore why does it feel like you’re not allowed to suggest this as a response? I should be able to say “RTFA” but here I feel like I’m going to be scolded or banned by moderation.</div><br/><div id="38272277" class="c"><input type="checkbox" id="c-38272277" checked=""/><div class="controls bullet"><span class="by">swatcoder</span><span>|</span><a href="#38272189">parent</a><span>|</span><a href="#38273326">next</a><span>|</span><label class="collapse" for="c-38272277">[-]</label><label class="expand" for="c-38272277">[6 more]</label></div><br/><div class="children"><div class="content">The author is a shark-diving science journalist who&#x27;s been paid a lot of money by OpenAI, not a researcher, and the article is neither a &quot;peer review&quot; or an expert &quot;refutation&quot;. It&#x27;s a meandering and sometimes thought-provoking exploration of how LLM&#x27;s can be coaxed to deliver on statements sort of like (but not actually equivalent to) the ones that failed in the paper.<p>If you want to defer your own understanding to the author (and overstate their own claims!), that&#x27;s fine, but a lot of people here are actually <i>more</i> informed about many of the topics and modes of discourse covered in the article and may have something to share with you if you give them credit. And if you&#x27;re not going to give other commenter&#x27;s due credit, then you&#x27;re not really following the spirit of HN. While you can&#x27;t always know the background of any individual commenter, we have an unusually accomplished and informed community in general and thrive by treating all commenters with the respect we might give the most accomplished and informed -- at least by default.  That&#x27;s why a blanket &quot;RTFA&quot; isn&#x27;t usually appropriate here.</div><br/><div id="38273066" class="c"><input type="checkbox" id="c-38273066" checked=""/><div class="controls bullet"><span class="by">amayne</span><span>|</span><a href="#38272189">root</a><span>|</span><a href="#38272277">parent</a><span>|</span><a href="#38274142">next</a><span>|</span><label class="collapse" for="c-38273066">[-]</label><label class="expand" for="c-38273066">[2 more]</label></div><br/><div class="children"><div class="content">Hello!<p>I’m the “shark-diving science journalist” in question.<p>First of all, you can run the experiments like I did and test this yourself. I’m not asking anyone to take my word. Just do what I did: Read the original paper. Test the claims for yourself.<p>And to clarify a couple things:<p>1. The shark-diving part is true.<p>2. I’ve never been a journalist of any kind that I’m aware of unless you count writing for Skeptic Magazine. I’ve had many, many jobs though.<p>3. I started at OpenAI as a software engineer and member of technical staff. When I started there was just over a hundred people there. The lines between engineering were and are blurry.<p>4. I was the original prompt engineer at OpenAI and discovered many of the examples for using GPT-3 and wrote a lot of the original documentation. Internally my title was “prompt whisperer.”<p>5. I’m in the GPT-4 research paper for my contributions to model capability. I helped find abilities for long-text, vision, etc.<p>6. I was given the title Science Communicator when I started doing background briefings for media, etc., but still worked on model capability and other things.<p>7. I left OpenAI two months ago to work on a startup.<p>Best,<p>Andrew Mayne</div><br/><div id="38274323" class="c"><input type="checkbox" id="c-38274323" checked=""/><div class="controls bullet"><span class="by">user_named</span><span>|</span><a href="#38272189">root</a><span>|</span><a href="#38273066">parent</a><span>|</span><a href="#38274142">next</a><span>|</span><label class="collapse" for="c-38274323">[-]</label><label class="expand" for="c-38274323">[1 more]</label></div><br/><div class="children"><div class="content">1. What do you think the reversal curse implies about LLMs?
2. Do you believe that LLMs are capable of logic? 
3. Do you believe that LLMs are intelligent? 
4. Do you believe that your blog post shows 3 or 4? If not, what is it about?</div><br/></div></div></div></div><div id="38274142" class="c"><input type="checkbox" id="c-38274142" checked=""/><div class="controls bullet"><span class="by">golol</span><span>|</span><a href="#38272189">root</a><span>|</span><a href="#38272277">parent</a><span>|</span><a href="#38273066">prev</a><span>|</span><a href="#38273210">next</a><span>|</span><label class="collapse" for="c-38274142">[-]</label><label class="expand" for="c-38274142">[1 more]</label></div><br/><div class="children"><div class="content">At the end the article presents a clear example where a model is trained on A is B and infers B is A. That is the contribution of this article and it&#x27;s very valuable.</div><br/></div></div><div id="38273210" class="c"><input type="checkbox" id="c-38273210" checked=""/><div class="controls bullet"><span class="by">ShamelessC</span><span>|</span><a href="#38272189">root</a><span>|</span><a href="#38272277">parent</a><span>|</span><a href="#38274142">prev</a><span>|</span><a href="#38273100">next</a><span>|</span><label class="collapse" for="c-38273210">[-]</label><label class="expand" for="c-38273210">[1 more]</label></div><br/><div class="children"><div class="content">All due respect, when I wrote this there were about 5 comments that were 100% not from those who are &quot;more informed&quot; than the author (and perhaps were even less informed than myself, although I don&#x27;t claim to be an LLM researcher or anything more than an interested internet user).<p>Just the same it&#x27;s been my experience, ESPECIALLY in machine learning, that users on HN are consistently over confident, bad at making future predictions, and generally just easily excitable. Does that mean I&#x27;m talking about you or commenters you like? Probably not! You seem relatively informed (although I&#x27;m not fond of you attacking the author of the article for things it isn&#x27;t claiming - something truly not in the spirit of HN).</div><br/></div></div></div></div></div></div><div id="38273326" class="c"><input type="checkbox" id="c-38273326" checked=""/><div class="controls bullet"><span class="by">jmount</span><span>|</span><a href="#38272189">prev</a><span>|</span><a href="#38272724">next</a><span>|</span><label class="collapse" for="c-38273326">[-]</label><label class="expand" for="c-38273326">[1 more]</label></div><br/><div class="children"><div class="content">This is really dreary.</div><br/></div></div><div id="38272724" class="c"><input type="checkbox" id="c-38272724" checked=""/><div class="controls bullet"><span class="by">cratermoon</span><span>|</span><a href="#38273326">prev</a><span>|</span><a href="#38273078">next</a><span>|</span><label class="collapse" for="c-38272724">[-]</label><label class="expand" for="c-38272724">[3 more]</label></div><br/><div class="children"><div class="content">If I&#x27;m reading this correctly, the author is saying that it&#x27;s not a failure of logical deduction if the training data doesn&#x27;t include the reversal. In other words, he&#x27;s saying that if the data contains &quot;Tom Cruise is the son of Mary Lee Pfeiffer” but not “Tom Cruise’s mother is Mary Lee Pfeiffer”, then the model&#x27;s inability to determine the latter is &quot;an explanation of how neural networks function than a model’s inability to deduce B is A.&quot;<p>But of course &quot;how neural networks function&quot; is that they fail at basic logical deduction and do not generalize.<p>So again, if I&#x27;m reading it correctly, he&#x27;s hand-waving away the inability to make basic logical deductions because that not something they can or should be expected to do. As I read it, that means the reversal curse only exists if the answer to the question &quot;can LLMs do logical deduction?&quot; is &quot;yes&quot;. If one takes the position that LLMs can&#x27;t do general logical deduction, which seems to be the author&#x27;s point of view, then there&#x27;s no expectation that knowing &quot;Tom Cruise is the son of Mary Lee Pfeiffer&quot; is sufficient to determine “Tom Cruise’s mother is Mary Lee Pfeiffer”.<p>Am I missing something?</div><br/><div id="38272907" class="c"><input type="checkbox" id="c-38272907" checked=""/><div class="controls bullet"><span class="by">xelxebar</span><span>|</span><a href="#38272724">parent</a><span>|</span><a href="#38273078">next</a><span>|</span><label class="collapse" for="c-38272907">[-]</label><label class="expand" for="c-38272907">[2 more]</label></div><br/><div class="children"><div class="content">Did you see the end of the article, where the author uses a small example and gets &quot;B is A&quot; generalization?<p>These are the salient takeaways I got:<p>- Is&#x2F;Was wording might matter. This is something probably a bug.<p>- 30 facts about a person might simply be too little for &quot;B to A&quot; generalization<p>- Extra precision&#x2F;context in the prompt can help locate the &quot;B to A&quot; inference.<p>- How you cut up your training data can bias inferences in surprising ways.<p>- &quot;B to A&quot; generalization clearly <i>does</i> happen, even without &quot;B is A&quot; in the data, but it&#x27;s not as stable as you&#x27;d want.</div><br/><div id="38273535" class="c"><input type="checkbox" id="c-38273535" checked=""/><div class="controls bullet"><span class="by">cratermoon</span><span>|</span><a href="#38272724">root</a><span>|</span><a href="#38272907">parent</a><span>|</span><a href="#38273078">next</a><span>|</span><label class="collapse" for="c-38273535">[-]</label><label class="expand" for="c-38273535">[1 more]</label></div><br/><div class="children"><div class="content">OK, but how does that not just demonstrate LLMs can&#x27;t generalize absent massaged training data?<p>&gt; it&#x27;s not as stable as you&#x27;d want.<p>Which I take to mean that a model can sometimes confabulate &quot;B is A&quot;, solely out of random variation, and that it&#x27;s possible to bias the data and prompt to generate the expected response. The model hasn&#x27;t done any logical deduction, the response is just a bias-influenced lucky break.</div><br/></div></div></div></div></div></div><div id="38273078" class="c"><input type="checkbox" id="c-38273078" checked=""/><div class="controls bullet"><span class="by">willsoon</span><span>|</span><a href="#38272724">prev</a><span>|</span><a href="#38272241">next</a><span>|</span><label class="collapse" for="c-38273078">[-]</label><label class="expand" for="c-38273078">[1 more]</label></div><br/><div class="children"><div class="content">Oh Cantor, Cantor,<p>bitter pale Cantor,<p>you pale ascetic,<p>your twist was epic,<p>so I live in silence,<p>in sadness I crave,<p>all for that book,<p>you wrote and I read.</div><br/></div></div><div id="38272241" class="c"><input type="checkbox" id="c-38272241" checked=""/><div class="controls bullet"><span class="by">pmarreck</span><span>|</span><a href="#38273078">prev</a><span>|</span><a href="#38272047">next</a><span>|</span><label class="collapse" for="c-38272241">[-]</label><label class="expand" for="c-38272241">[2 more]</label></div><br/><div class="children"><div class="content">I couldn&#x27;t duplicate it in GPT4 here. It answered correctly, unless I posed it wrong:<p><a href="https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;75d46a03-a223-4f3a-987d-f8fec3af1ca5" rel="nofollow noreferrer">https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;75d46a03-a223-4f3a-987d-f8fec3...</a></div><br/><div id="38272449" class="c"><input type="checkbox" id="c-38272449" checked=""/><div class="controls bullet"><span class="by">jetrink</span><span>|</span><a href="#38272241">parent</a><span>|</span><a href="#38272047">next</a><span>|</span><label class="collapse" for="c-38272449">[-]</label><label class="expand" for="c-38272449">[1 more]</label></div><br/><div class="children"><div class="content">The paper is not talking about information provided in the context, only training and fine-tuning.<p>&gt; In fairness, it’s also worth pointing out here that they’re making the claim that the reversal curse only applies to training and fine-tuning and not in-context – i.e., putting all your information inside a prompt. They point out in a footnote that you can put A to B data in a prompt and GPT-4 will make B to A connections just fine. Unfortunately, this was lost on many of the people covering the pre-print.</div><br/></div></div></div></div><div id="38272047" class="c"><input type="checkbox" id="c-38272047" checked=""/><div class="controls bullet"><span class="by">Khelavaster</span><span>|</span><a href="#38272241">prev</a><span>|</span><label class="collapse" for="c-38272047">[-]</label><label class="expand" for="c-38272047">[1 more]</label></div><br/><div class="children"><div class="content">You have to add context the relationship of A to B isaintained under reflexive algebra..</div><br/></div></div></div></div></div></div></div></body></html>