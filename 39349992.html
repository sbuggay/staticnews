<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1707814871947" as="style"/><link rel="stylesheet" href="styles.css?v=1707814871947"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://sohl-dickstein.github.io/2024/02/12/fractal.html">Neural network training makes beautiful fractals</a> <span class="domain">(<a href="https://sohl-dickstein.github.io">sohl-dickstein.github.io</a>)</span></div><div class="subtext"><span>telotortium</span> | <span>34 comments</span></div><br/><div><div id="39355140" class="c"><input type="checkbox" id="c-39355140" checked=""/><div class="controls bullet"><span class="by">alexmolas</span><span>|</span><a href="#39354400">next</a><span>|</span><label class="collapse" for="c-39355140">[-]</label><label class="expand" for="c-39355140">[3 more]</label></div><br/><div class="children"><div class="content">The results of the experiment seem counterintuitive just because the used learning rates are huge (up to 10 or even 100). These are not the lr you would use in a normal setting. If you look at the region of small lr it seems all of them converge.<p>So I would say the experiment is interesting, but not representative of real world deep learning.<p>In the experiment, you have a function of 272 variables with a lot of minima and maxima, and at each gradient descent step you take huge steps (due to big lr). So my intuition is that convergence is more a matter of luck rather than hyperparameters.</div><br/><div id="39355483" class="c"><input type="checkbox" id="c-39355483" checked=""/><div class="controls bullet"><span class="by">dmarchand90</span><span>|</span><a href="#39355140">parent</a><span>|</span><a href="#39354400">next</a><span>|</span><label class="collapse" for="c-39355483">[-]</label><label class="expand" for="c-39355483">[2 more]</label></div><br/><div class="children"><div class="content">I think the article is very honest about this just being a  fun exploration. They even show how you can get similar patterns with newton&#x27;s algorithm which is a more &quot;classical&quot; take</div><br/><div id="39355534" class="c"><input type="checkbox" id="c-39355534" checked=""/><div class="controls bullet"><span class="by">alexmolas</span><span>|</span><a href="#39355140">root</a><span>|</span><a href="#39355483">parent</a><span>|</span><a href="#39354400">next</a><span>|</span><label class="collapse" for="c-39355534">[-]</label><label class="expand" for="c-39355534">[1 more]</label></div><br/><div class="children"><div class="content">Yes, the author is clear in this regard. But I&#x27;ve seen people interpreting this paper as &quot;training deep networks is chaotic&quot;, and I don&#x27;t think that&#x27;s the case. I interpret it more as &quot;if you&#x27;re not careful with your learning rate your training will be chaotic&quot;</div><br/></div></div></div></div></div></div><div id="39354400" class="c"><input type="checkbox" id="c-39354400" checked=""/><div class="controls bullet"><span class="by">PheonixPharts</span><span>|</span><a href="#39355140">prev</a><span>|</span><a href="#39354612">next</a><span>|</span><label class="collapse" for="c-39354400">[-]</label><label class="expand" for="c-39354400">[6 more]</label></div><br/><div class="children"><div class="content">I find this result absolutely fascinating, and is exactly the type of research into neural networks we should be expanding.<p>We&#x27;ve rapidly engineered our way to some <i>very</i> impressive models this past decade, and yet gap in our real understanding of what&#x27;s going on has widened. There&#x27;s a large list of very basic questions about LLMs that we haven&#x27;t answered (or in some cases, really asked). This is not a failing of people researching in this area, it&#x27;s only that things move so quickly there&#x27;s not enough time to ponder things like this.<p>At the same time, the result, unless I&#x27;m really misunderstanding, gives me the impression that anything other than grid search hyper parameter optimization is a fools errand. This would give credence to the notion that hyper parameter tuning really is akin to just re-rolling a character sheet until you get one that is over powered.</div><br/><div id="39355818" class="c"><input type="checkbox" id="c-39355818" checked=""/><div class="controls bullet"><span class="by">sigmoid10</span><span>|</span><a href="#39354400">parent</a><span>|</span><a href="#39355442">next</a><span>|</span><label class="collapse" for="c-39355818">[-]</label><label class="expand" for="c-39355818">[1 more]</label></div><br/><div class="children"><div class="content">&gt;exactly the type of research into neural networks we should be expanding.<p>While it certainly makes for some nice visualizations, the technical insight of this is pretty limited. First of all, this fractal structure emerges at learning rates that are far higher than those used in training actual neural networks nowadays. It&#x27;s interesting that the training still converges for some combinations and that the (expected) hit-and-miss procedure yields a fractal structure. But if you look closely at the images, you&#x27;ll see the best hyperparameters are, while close, not <i>at</i> the border. So even if you want to follow the meta-learning approach outlined in the post, your gradient descent has already screwed up before if it ever ends up in this fractal boundary region.</div><br/></div></div><div id="39355442" class="c"><input type="checkbox" id="c-39355442" checked=""/><div class="controls bullet"><span class="by">krallistic</span><span>|</span><a href="#39354400">parent</a><span>|</span><a href="#39355818">prev</a><span>|</span><a href="#39355003">next</a><span>|</span><label class="collapse" for="c-39355442">[-]</label><label class="expand" for="c-39355442">[1 more]</label></div><br/><div class="children"><div class="content">&gt; gives me the impression that anything other than grid search hyper parameter optimization is a fools errand. This would give credence to the notion that hyper parameter tuning really is akin to just re-rolling a character sheet until you get one that is over powered.<p>The visualizations only show that at the Border there are a lot of fractals, not in every part of the space. 
(Although the highest performance is often achieved close to the border.). I would not state hparam search as bad as that..</div><br/></div></div><div id="39355003" class="c"><input type="checkbox" id="c-39355003" checked=""/><div class="controls bullet"><span class="by">dylan604</span><span>|</span><a href="#39354400">parent</a><span>|</span><a href="#39355442">prev</a><span>|</span><a href="#39354612">next</a><span>|</span><label class="collapse" for="c-39355003">[-]</label><label class="expand" for="c-39355003">[3 more]</label></div><br/><div class="children"><div class="content">what was the name of the Google app (Dreaming or some such) that would iterate frames similar to this that would find lizard&#x2F;snakes&#x2F;dogs&#x2F;eyes and got super trippy the longer it ran? The demos would start with RGB noise, and within a few iterations, it was a full on psychedelic trip. It was the best visual of AI &quot;hallucinating&quot; I&#x27;ve seen yet</div><br/><div id="39355016" class="c"><input type="checkbox" id="c-39355016" checked=""/><div class="controls bullet"><span class="by">0xDEADFED5</span><span>|</span><a href="#39354400">root</a><span>|</span><a href="#39355003">parent</a><span>|</span><a href="#39354612">next</a><span>|</span><label class="collapse" for="c-39355016">[-]</label><label class="expand" for="c-39355016">[2 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;DeepDream" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;DeepDream</a></div><br/><div id="39355089" class="c"><input type="checkbox" id="c-39355089" checked=""/><div class="controls bullet"><span class="by">dylan604</span><span>|</span><a href="#39354400">root</a><span>|</span><a href="#39355016">parent</a><span>|</span><a href="#39354612">next</a><span>|</span><label class="collapse" for="c-39355089">[-]</label><label class="expand" for="c-39355089">[1 more]</label></div><br/><div class="children"><div class="content">winner winner chicken dinner. thanks!<p>always thought we now know what the android&#x27;s dreams were like</div><br/></div></div></div></div></div></div></div></div><div id="39354612" class="c"><input type="checkbox" id="c-39354612" checked=""/><div class="controls bullet"><span class="by">Imnimo</span><span>|</span><a href="#39354400">prev</a><span>|</span><a href="#39350015">next</a><span>|</span><label class="collapse" for="c-39354612">[-]</label><label class="expand" for="c-39354612">[2 more]</label></div><br/><div class="children"><div class="content">If you are a fan of the fractals but feel intimidated by neural networks, the networks used here are actually pretty simple and not so difficult to understand if you are familiar with matrix multiplication. To generate a dataset, he samples random vectors (say of size 8) as inputs, and for each vector a target output, which is a single number. The network consists of an 8x8 matrix and an 8x1 matrix, also randomly initialized.<p>To generate an output from an input vector, you just multiply by your 8x8 matrix (getting a new size 8 vector), apply the tanh function to each element (look up a plot of tanh - it just squeezes its inputs to be between -1 and 1), and then multiply by the 8x1 matrix, getting a single value as an output. The elements of the two matrices are the &#x27;weights&#x27; of the neural network, and they are updated to push the output we got towards the target.<p>When we update our weights, we have to decide on a step size - do we make just a little tiny nudge in the right direction, or take a giant step? The plots are showing what happens if we choose different step sizes for the two matrices (&quot;input layer learning rate&quot; is how big of a step we take for the 8x8 matrix, and &quot;output layer learning rate&quot; for the 8x1 matrix).<p>If your steps are too big, you run into a problem. Imagine trying to find the bottom of a parabola by taking steps in the direction of downward slope - if you take a giant step, you&#x27;ll pass right over the bottom and land on the opposite slope, maybe even higher than you started! This is the red region of the plots. If you take really really tiny steps, you&#x27;ll be safe, but it&#x27;ll take you a long time to reach the bottom. This is the dark blue section. Another way you can take a long time is to take big steps that jump from one slope to the other, but just barely small enough to end up a little lower each time (this is why there&#x27;s a dark blue stripe near the boundary). The light green region is where you take goldilocks steps - big enough to find the bottom quickly, but small enough to not jump over it.</div><br/><div id="39354882" class="c"><input type="checkbox" id="c-39354882" checked=""/><div class="controls bullet"><span class="by">nighthawk454</span><span>|</span><a href="#39354612">parent</a><span>|</span><a href="#39350015">next</a><span>|</span><label class="collapse" for="c-39354882">[-]</label><label class="expand" for="c-39354882">[1 more]</label></div><br/><div class="children"><div class="content">Great description! Now we just need one on fractals for all us NN people haha</div><br/></div></div></div></div><div id="39350015" class="c"><input type="checkbox" id="c-39350015" checked=""/><div class="controls bullet"><span class="by">telotortium</span><span>|</span><a href="#39354612">prev</a><span>|</span><a href="#39355753">next</a><span>|</span><label class="collapse" for="c-39350015">[-]</label><label class="expand" for="c-39350015">[1 more]</label></div><br/><div class="children"><div class="content">Twitter: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;jaschasd&#x2F;status&#x2F;1756930242965606582" rel="nofollow">https:&#x2F;&#x2F;twitter.com&#x2F;jaschasd&#x2F;status&#x2F;1756930242965606582</a>
ArXiv: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2402.06184" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2402.06184</a><p>Abstract:<p>&quot;Some fractals -- for instance those associated with the Mandelbrot and quadratic Julia sets -- are computed by iterating a function, and identifying the boundary between hyperparameters for which the resulting series diverges or remains bounded. Neural network training similarly involves iterating an update function (e.g. repeated steps of gradient descent), can result in convergent or divergent behavior, and can be extremely sensitive to small changes in hyperparameters. Motivated by these similarities, we experimentally examine the boundary between neural network hyperparameters that lead to stable and divergent training. We find that this boundary is fractal over more than ten decades of scale in all tested configurations.&quot;<p>Contains several cool animations zooming in to show the fractal boundary between  convergent and divergent training, just like the classic Mandelbrot and Julia set animations.</div><br/></div></div><div id="39355573" class="c"><input type="checkbox" id="c-39355573" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#39355753">prev</a><span>|</span><a href="#39353753">next</a><span>|</span><label class="collapse" for="c-39355573">[-]</label><label class="expand" for="c-39355573">[3 more]</label></div><br/><div class="children"><div class="content">I hope one day we&#x27;ll have generative AI capable of producing stuff like this on demand:<p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=8cgp2WNNKmQ" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=8cgp2WNNKmQ</a></div><br/><div id="39355621" class="c"><input type="checkbox" id="c-39355621" checked=""/><div class="controls bullet"><span class="by">passion__desire</span><span>|</span><a href="#39355573">parent</a><span>|</span><a href="#39353753">next</a><span>|</span><label class="collapse" for="c-39355621">[-]</label><label class="expand" for="c-39355621">[2 more]</label></div><br/><div class="children"><div class="content">Not just that AI can overlay artistic styles like below on top of such fractal structures.<p><a href="http:&#x2F;&#x2F;sub.blue&#x2F;inkwell" rel="nofollow">http:&#x2F;&#x2F;sub.blue&#x2F;inkwell</a></div><br/><div id="39355649" class="c"><input type="checkbox" id="c-39355649" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#39355573">root</a><span>|</span><a href="#39355621">parent</a><span>|</span><a href="#39353753">next</a><span>|</span><label class="collapse" for="c-39355649">[-]</label><label class="expand" for="c-39355649">[1 more]</label></div><br/><div class="children"><div class="content">Generating a single still image is not an issue, obviously. It&#x27;s more about picking &quot;good&quot; fractal parameters, a &quot;good&quot; spot to zoom in on said fractal, and &quot;good&quot; colors to dress it all up. Even more so if you are trying to time it all to specific music matching the visuals, as some of the fractal art videos do.</div><br/></div></div></div></div></div></div><div id="39353753" class="c"><input type="checkbox" id="c-39353753" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#39355573">prev</a><span>|</span><a href="#39355669">next</a><span>|</span><label class="collapse" for="c-39353753">[-]</label><label class="expand" for="c-39353753">[1 more]</label></div><br/><div class="children"><div class="content">Here&#x27;s the associated blog post, which includes the videos: <a href="https:&#x2F;&#x2F;sohl-dickstein.github.io&#x2F;2024&#x2F;02&#x2F;12&#x2F;fractal.html" rel="nofollow">https:&#x2F;&#x2F;sohl-dickstein.github.io&#x2F;2024&#x2F;02&#x2F;12&#x2F;fractal.html</a><p>Not a ML&#x27;er so not sure what to make of it, beyond a fascinating connection.</div><br/></div></div><div id="39355669" class="c"><input type="checkbox" id="c-39355669" checked=""/><div class="controls bullet"><span class="by">passion__desire</span><span>|</span><a href="#39353753">prev</a><span>|</span><a href="#39354399">next</a><span>|</span><label class="collapse" for="c-39355669">[-]</label><label class="expand" for="c-39355669">[1 more]</label></div><br/><div class="children"><div class="content">Is there an interesting measure theory result relating fractals?<p>e.g. almost all problems of X are NP-hard, etc.</div><br/></div></div><div id="39354399" class="c"><input type="checkbox" id="c-39354399" checked=""/><div class="controls bullet"><span class="by">why_only_15</span><span>|</span><a href="#39355669">prev</a><span>|</span><a href="#39355123">next</a><span>|</span><label class="collapse" for="c-39354399">[-]</label><label class="expand" for="c-39354399">[1 more]</label></div><br/><div class="children"><div class="content">I appreciate that his acknowledgements here were to his daughter (&quot;for detailed feedback on the generated fractals&quot;) and wife (&quot;for providing feedback on a draft of this post&quot;)</div><br/></div></div><div id="39355123" class="c"><input type="checkbox" id="c-39355123" checked=""/><div class="controls bullet"><span class="by">albertgt</span><span>|</span><a href="#39354399">prev</a><span>|</span><a href="#39355074">next</a><span>|</span><label class="collapse" for="c-39355123">[-]</label><label class="expand" for="c-39355123">[1 more]</label></div><br/><div class="children"><div class="content">Dave Bowman&gt; omg it’s full of fractals<p>HAL&gt; why yes Dave what did you think I was made of</div><br/></div></div><div id="39355074" class="c"><input type="checkbox" id="c-39355074" checked=""/><div class="controls bullet"><span class="by">karxxm</span><span>|</span><a href="#39355123">prev</a><span>|</span><a href="#39354208">next</a><span>|</span><label class="collapse" for="c-39355074">[-]</label><label class="expand" for="c-39355074">[2 more]</label></div><br/><div class="children"><div class="content">What’s that color-map called?</div><br/><div id="39355783" class="c"><input type="checkbox" id="c-39355783" checked=""/><div class="controls bullet"><span class="by">nossid</span><span>|</span><a href="#39355074">parent</a><span>|</span><a href="#39354208">next</a><span>|</span><label class="collapse" for="c-39355783">[-]</label><label class="expand" for="c-39355783">[1 more]</label></div><br/><div class="children"><div class="content">In the notebook you can see it set to Spectral.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;Sohl-Dickstein&#x2F;fractal&#x2F;blob&#x2F;main&#x2F;the_boundary_of_neural_network_trainability_is_fractal.ipynb">https:&#x2F;&#x2F;github.com&#x2F;Sohl-Dickstein&#x2F;fractal&#x2F;blob&#x2F;main&#x2F;the_boun...</a></div><br/></div></div></div></div><div id="39354208" class="c"><input type="checkbox" id="c-39354208" checked=""/><div class="controls bullet"><span class="by">kalu</span><span>|</span><a href="#39355074">prev</a><span>|</span><a href="#39353729">next</a><span>|</span><label class="collapse" for="c-39354208">[-]</label><label class="expand" for="c-39354208">[3 more]</label></div><br/><div class="children"><div class="content">So this author trained a neural network billions of times using different hyper parameters ?  How much dod that cost ?</div><br/><div id="39354290" class="c"><input type="checkbox" id="c-39354290" checked=""/><div class="controls bullet"><span class="by">TheCoreh</span><span>|</span><a href="#39354208">parent</a><span>|</span><a href="#39354277">next</a><span>|</span><label class="collapse" for="c-39354290">[-]</label><label class="expand" for="c-39354290">[1 more]</label></div><br/><div class="children"><div class="content">The networks trained were really small, with only one hidden layer, and a width of 16.</div><br/></div></div><div id="39354277" class="c"><input type="checkbox" id="c-39354277" checked=""/><div class="controls bullet"><span class="by">mkl</span><span>|</span><a href="#39354208">parent</a><span>|</span><a href="#39354290">prev</a><span>|</span><a href="#39353729">next</a><span>|</span><label class="collapse" for="c-39354277">[-]</label><label class="expand" for="c-39354277">[1 more]</label></div><br/><div class="children"><div class="content">Very small networks are cheap and easy (even 20 years ago).</div><br/></div></div></div></div><div id="39353729" class="c"><input type="checkbox" id="c-39353729" checked=""/><div class="controls bullet"><span class="by">fallingfrog</span><span>|</span><a href="#39354208">prev</a><span>|</span><a href="#39354360">next</a><span>|</span><label class="collapse" for="c-39353729">[-]</label><label class="expand" for="c-39353729">[5 more]</label></div><br/><div class="children"><div class="content">This is kind of random but- I wonder, if you had a sufficiently complex lens, or series of lenses, perhaps with specific areas darkened, could you make a lens that shone light through if presented with, say, a cat, but not with anything else?  Bending light and darkening it selectively could probably reproduce a layer of a neural net.  That would be cool.  I suppose, you would need some substance that responded to light in a <i>nonlinear</i> way.</div><br/><div id="39353839" class="c"><input type="checkbox" id="c-39353839" checked=""/><div class="controls bullet"><span class="by">gwern</span><span>|</span><a href="#39353729">parent</a><span>|</span><a href="#39354097">next</a><span>|</span><label class="collapse" for="c-39353839">[-]</label><label class="expand" for="c-39353839">[1 more]</label></div><br/><div class="children"><div class="content">You can do a lot. &#x27;Digital sundials&#x27; come to mind: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Digital_sundial" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Digital_sundial</a> <a href="https:&#x2F;&#x2F;gwern.net&#x2F;doc&#x2F;math&#x2F;1991-stewart.pdf" rel="nofollow">https:&#x2F;&#x2F;gwern.net&#x2F;doc&#x2F;math&#x2F;1991-stewart.pdf</a><p>But yes, you are restricted to linear things and you can&#x27;t make a good photonic cat detector out of that easily. So all the photonic neural networks you may have heard of like <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2106.11747" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2106.11747</a> wind up sticking some mechanical or electrical nonlinearity <i>somewhere</i>.</div><br/></div></div><div id="39354097" class="c"><input type="checkbox" id="c-39354097" checked=""/><div class="controls bullet"><span class="by">theWreckluse</span><span>|</span><a href="#39353729">parent</a><span>|</span><a href="#39353839">prev</a><span>|</span><a href="#39353960">next</a><span>|</span><label class="collapse" for="c-39354097">[-]</label><label class="expand" for="c-39354097">[1 more]</label></div><br/><div class="children"><div class="content">A research out of ETH Zurich, based on which the company Rayform <a href="https:&#x2F;&#x2F;rayform.ch&#x2F;" rel="nofollow">https:&#x2F;&#x2F;rayform.ch&#x2F;</a> was founded does exactly this! I was so excited when I saw the paper for the first time a couple of years ago.</div><br/></div></div><div id="39353960" class="c"><input type="checkbox" id="c-39353960" checked=""/><div class="controls bullet"><span class="by">mhh__</span><span>|</span><a href="#39353729">parent</a><span>|</span><a href="#39354097">prev</a><span>|</span><a href="#39353774">next</a><span>|</span><label class="collapse" for="c-39353960">[-]</label><label class="expand" for="c-39353960">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve seen a student project that did just that. I don&#x27;t have a link for you unfortunately.</div><br/></div></div><div id="39353774" class="c"><input type="checkbox" id="c-39353774" checked=""/><div class="controls bullet"><span class="by">uoaei</span><span>|</span><a href="#39353729">parent</a><span>|</span><a href="#39353960">prev</a><span>|</span><a href="#39354360">next</a><span>|</span><label class="collapse" for="c-39353774">[-]</label><label class="expand" for="c-39353774">[1 more]</label></div><br/><div class="children"><div class="content">You can simulate materials, apply the wave equation, and get &quot;layers&quot; that compute outputs from given inputs, each modeled as points in space. It may be possible to manufacture such layers with metamaterials or something like that.<p><a href="https:&#x2F;&#x2F;www.science.org&#x2F;doi&#x2F;10.1126&#x2F;sciadv.aay6946" rel="nofollow">https:&#x2F;&#x2F;www.science.org&#x2F;doi&#x2F;10.1126&#x2F;sciadv.aay6946</a></div><br/></div></div></div></div><div id="39354360" class="c"><input type="checkbox" id="c-39354360" checked=""/><div class="controls bullet"><span class="by">7e</span><span>|</span><a href="#39353729">prev</a><span>|</span><label class="collapse" for="c-39354360">[-]</label><label class="expand" for="c-39354360">[3 more]</label></div><br/><div class="children"><div class="content">Today I learned that if something is detailed, it is now fractal.</div><br/><div id="39354591" class="c"><input type="checkbox" id="c-39354591" checked=""/><div class="controls bullet"><span class="by">lifthrasiir</span><span>|</span><a href="#39354360">parent</a><span>|</span><a href="#39354498">next</a><span>|</span><label class="collapse" for="c-39354591">[-]</label><label class="expand" for="c-39354591">[1 more]</label></div><br/><div class="children"><div class="content">Fractal is not necessarily self-similar; it just has to show enough detailed structures when zoomed in ad infinitum. While no single defiition for fractals exists, at least that&#x27;s one of the common denominators (because otherwise many &quot;fractals&quot; in the nature can&#x27;t be called so).</div><br/></div></div><div id="39354498" class="c"><input type="checkbox" id="c-39354498" checked=""/><div class="controls bullet"><span class="by">paulrudy</span><span>|</span><a href="#39354360">parent</a><span>|</span><a href="#39354591">prev</a><span>|</span><label class="collapse" for="c-39354498">[-]</label><label class="expand" for="c-39354498">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m only a fractal enthusiast, but my impression is that the key distinction that makes these fractals, or at least fractal-like, is not their detail per se, but that there is complexity at every scale. From the article:<p>&gt; we find intricate structure at every scale<p>&gt; At every length scale, small changes in the hyperparameters can lead to large changes in training dynamics</div><br/></div></div></div></div></div></div></div></div></div></body></html>