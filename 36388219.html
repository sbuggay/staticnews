<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1687165250065" as="style"/><link rel="stylesheet" href="styles.css?v=1687165250065"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/bentoml/OpenLLM">OpenLLM</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>fzliu</span> | <span>16 comments</span></div><br/><div><div id="36388674" class="c"><input type="checkbox" id="c-36388674" checked=""/><div class="controls bullet"><span class="by">mjburgess</span><span>|</span><a href="#36388783">next</a><span>|</span><label class="collapse" for="c-36388674">[-]</label><label class="expand" for="c-36388674">[4 more]</label></div><br/><div class="children"><div class="content">Stray thought: It would be better to specify NNs in terms of their training-size to weight-size in bytes. Rather than &quot;No. Parameters&quot;, or at least, this ratio with the number of parameters.<p>So, eg., I&#x27;d imagine ChatGPT would be, say:  100s PT in 0.5T.<p>The number of parameters is a nearly meaningless metric, consider, eg., that if all the parameters covary then there&#x27;s one &quot;functional&quot; parameter.<p>The compression ratio tells you the real reason why a NN performs. Ie., you can have 100s bns of parameters, but without that 100s PT -&gt; 0.5T, which you can&#x27;t afford, it&#x27;s all rather pointless.</div><br/><div id="36388758" class="c"><input type="checkbox" id="c-36388758" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#36388674">parent</a><span>|</span><a href="#36388783">next</a><span>|</span><label class="collapse" for="c-36388758">[-]</label><label class="expand" for="c-36388758">[3 more]</label></div><br/><div class="children"><div class="content">Well it&#x27;s not a completely meaningless metric as it immediately tells you roughly how much memory you need to load it, which is kind of important?</div><br/><div id="36388761" class="c"><input type="checkbox" id="c-36388761" checked=""/><div class="controls bullet"><span class="by">mjburgess</span><span>|</span><a href="#36388674">root</a><span>|</span><a href="#36388758">parent</a><span>|</span><a href="#36388783">next</a><span>|</span><label class="collapse" for="c-36388761">[-]</label><label class="expand" for="c-36388761">[2 more]</label></div><br/><div class="children"><div class="content">If you look at my suggestion, it&#x27;s to state exactly that memory -- rather than to estimate based on bits&#x2F;parameter.</div><br/><div id="36388804" class="c"><input type="checkbox" id="c-36388804" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#36388674">root</a><span>|</span><a href="#36388761">parent</a><span>|</span><a href="#36388783">next</a><span>|</span><label class="collapse" for="c-36388804">[-]</label><label class="expand" for="c-36388804">[1 more]</label></div><br/><div class="children"><div class="content">Well then do explain a bit further, I still don&#x27;t fully grasp what &quot;100s PT in 0.5T&quot; means exactly. 100 petatokens in half a trillion? Half a terrabyte? 100 seconds?<p>Plus afaik base model training tokens don&#x27;t have the same effect as fine tuning tokens, so there would need to be a way to specify each of those separately.</div><br/></div></div></div></div></div></div></div></div><div id="36388783" class="c"><input type="checkbox" id="c-36388783" checked=""/><div class="controls bullet"><span class="by">SimFG</span><span>|</span><a href="#36388674">prev</a><span>|</span><a href="#36388774">next</a><span>|</span><label class="collapse" for="c-36388783">[-]</label><label class="expand" for="c-36388783">[2 more]</label></div><br/><div class="children"><div class="content">Looks great! I&#x27;m planning to integrate it into my new project(to-chatgpt: <a href="https:&#x2F;&#x2F;github.com&#x2F;SimFG&#x2F;to-chatgpt">https:&#x2F;&#x2F;github.com&#x2F;SimFG&#x2F;to-chatgpt</a>), which will provide users of the ChatGPT applications with a wider range of LLM service options.</div><br/><div id="36388799" class="c"><input type="checkbox" id="c-36388799" checked=""/><div class="controls bullet"><span class="by">chaoyu</span><span>|</span><a href="#36388783">parent</a><span>|</span><a href="#36388774">next</a><span>|</span><label class="collapse" for="c-36388799">[-]</label><label class="expand" for="c-36388799">[1 more]</label></div><br/><div class="children"><div class="content">Looking forward to it!<p>OpenLLM is adding a OpenAI-compatible API layer, which will make it even easier to migrate LLM apps built around OpenAI&#x27;s API spec. Feel free to join our Discord community and discuss more!</div><br/></div></div></div></div><div id="36388774" class="c"><input type="checkbox" id="c-36388774" checked=""/><div class="controls bullet"><span class="by">aarnphm</span><span>|</span><a href="#36388783">prev</a><span>|</span><a href="#36388518">next</a><span>|</span><label class="collapse" for="c-36388774">[-]</label><label class="expand" for="c-36388774">[1 more]</label></div><br/><div class="children"><div class="content">Hi all, I&#x27;m the main maintainer from the OpenLLM team here. I&#x27;m actively developing the fine-tuning feature and will release a PR soon enough. Stay tuned. In the meanwhile, the best way to track the development workflow is at our discord, so feel free to join!!</div><br/></div></div><div id="36388518" class="c"><input type="checkbox" id="c-36388518" checked=""/><div class="controls bullet"><span class="by">phsource</span><span>|</span><a href="#36388774">prev</a><span>|</span><a href="#36388517">next</a><span>|</span><label class="collapse" for="c-36388518">[-]</label><label class="expand" for="c-36388518">[2 more]</label></div><br/><div class="children"><div class="content">Cool stuff! How does this compare with Fastchat, which seems like another open source project that helps run LLM models?<p>At a glance, it seems like it&#x27;s going for lots of similar goals (run LLMs with interoperable APIs):<p><a href="https:&#x2F;&#x2F;github.com&#x2F;lm-sys&#x2F;FastChat">https:&#x2F;&#x2F;github.com&#x2F;lm-sys&#x2F;FastChat</a></div><br/><div id="36388672" class="c"><input type="checkbox" id="c-36388672" checked=""/><div class="controls bullet"><span class="by">chaoyu</span><span>|</span><a href="#36388518">parent</a><span>|</span><a href="#36388517">next</a><span>|</span><label class="collapse" for="c-36388672">[-]</label><label class="expand" for="c-36388672">[1 more]</label></div><br/><div class="children"><div class="content">OpenLLM in comparison focuses more on building LLM apps for production. For example, the integration with LangChain + BentoML makes it easy to run multiple LLMs in parallel across multiple GPUs&#x2F;Nodes, or chain LLMs with other type of AI&#x2F;ML models, and deploy the entire pipeline on Kubernete (via Yatai or BentoCloud).<p>Disclaimer: I helped build BentoML and OpenLLM.</div><br/></div></div></div></div><div id="36388517" class="c"><input type="checkbox" id="c-36388517" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#36388518">prev</a><span>|</span><label class="collapse" for="c-36388517">[-]</label><label class="expand" for="c-36388517">[6 more]</label></div><br/><div class="children"><div class="content">Fine-tuning is the most important part, but it is under intense research today, things change fast. I hope they can streamline this process because these smaller models can only compete with big models when they are fine-tuned.</div><br/><div id="36388701" class="c"><input type="checkbox" id="c-36388701" checked=""/><div class="controls bullet"><span class="by">comfypotato</span><span>|</span><a href="#36388517">parent</a><span>|</span><a href="#36388561">next</a><span>|</span><label class="collapse" for="c-36388701">[-]</label><label class="expand" for="c-36388701">[3 more]</label></div><br/><div class="children"><div class="content">What exactly do you mean here that the smaller models can compete with the the larger once they are fine-tuned? What about once the larger models are fine-tuned? Are they then out of reach of the fine-tuned smaller models?</div><br/><div id="36388793" class="c"><input type="checkbox" id="c-36388793" checked=""/><div class="controls bullet"><span class="by">chaoyu</span><span>|</span><a href="#36388517">root</a><span>|</span><a href="#36388701">parent</a><span>|</span><a href="#36388763">next</a><span>|</span><label class="collapse" for="c-36388793">[-]</label><label class="expand" for="c-36388793">[1 more]</label></div><br/><div class="children"><div class="content">Smaller models are likely more efficient to run inference and doesn&#x27;t necessarily need the latest GPU. Larger language model trend to have better performance over more different type of tasks. But for a specific enterprise use case, either distilling a large model or use large model to help with training a smaller model can be quite helpful in getting things to production - where you may need cost-efficiency and lower latency.</div><br/></div></div><div id="36388763" class="c"><input type="checkbox" id="c-36388763" checked=""/><div class="controls bullet"><span class="by">rmbyrro</span><span>|</span><a href="#36388517">root</a><span>|</span><a href="#36388701">parent</a><span>|</span><a href="#36388793">prev</a><span>|</span><a href="#36388561">next</a><span>|</span><label class="collapse" for="c-36388763">[-]</label><label class="expand" for="c-36388763">[1 more]</label></div><br/><div class="children"><div class="content">They&#x27;re probably referring to fine-tuning on private&#x2F;proprietary data that is specific to a use case. Say a history of conversation transcripts in a call center.<p>Larger models, like OpenAI&#x27;s GPT, don&#x27;t have access to this by default.</div><br/></div></div></div></div><div id="36388561" class="c"><input type="checkbox" id="c-36388561" checked=""/><div class="controls bullet"><span class="by">uniqueuid</span><span>|</span><a href="#36388517">parent</a><span>|</span><a href="#36388701">prev</a><span>|</span><label class="collapse" for="c-36388561">[-]</label><label class="expand" for="c-36388561">[2 more]</label></div><br/><div class="children"><div class="content">Yes but fine tuning requires a lot more gpu memory and is thus much more expensive, complicated and out of reach of most people. To fine tune a &gt;10B model you still need multiple A100 &#x2F; H100. Let’s hope that changes with quantized fine tuning, forward pass only etc.</div><br/><div id="36388735" class="c"><input type="checkbox" id="c-36388735" checked=""/><div class="controls bullet"><span class="by">chaoyu</span><span>|</span><a href="#36388517">root</a><span>|</span><a href="#36388561">parent</a><span>|</span><label class="collapse" for="c-36388735">[-]</label><label class="expand" for="c-36388735">[1 more]</label></div><br/><div class="children"><div class="content">The OpenLLM team is actively exploring those techniques for streamlining the fine-tuning process and making it accessible!</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>