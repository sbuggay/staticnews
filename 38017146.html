<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1698310857959" as="style"/><link rel="stylesheet" href="styles.css?v=1698310857959"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.nature.com/articles/d41586-023-03272-3">AI &#x27;breakthrough&#x27;: neural net has human-like ability to generalize language</a> <span class="domain">(<a href="https://www.nature.com">www.nature.com</a>)</span></div><div class="subtext"><span>drcwpl</span> | <span>62 comments</span></div><br/><div><div id="38018145" class="c"><input type="checkbox" id="c-38018145" checked=""/><div class="controls bullet"><span class="by">vessenes</span><span>|</span><a href="#38018042">next</a><span>|</span><label class="collapse" for="c-38018145">[-]</label><label class="expand" for="c-38018145">[22 more]</label></div><br/><div class="children"><div class="content">I actually read the (first half) of the paper.<p>I can&#x27;t speak to whether or not Nature should publish this, but I don&#x27;t find the outcomes in the paper spectacular. In brief, they create an ontology -- in the initial examples, four made up color words, each corresponding to a real color and then three made up &#x27;function&#x27; words, representing &#x27;before&#x27;, &#x27;in the middle of&#x27; and &#x27;triple&#x27;.<p>They then ask humans and their AI to generate sequences of colors based on short and progressively longer strings of color words and functions.<p>The AI is about as good at this as humans, which is to say 85% successful for instructions longer than three or four words.<p>The headline says that GPT4 is worse at this than MLC. I am doubtful about this claim. I feel a quality prompt engineer could beat 85% with GPT4.<p>The claims in the document are that MLC shows similar error types to humans, and that this backs some sort of theory of mind I don&#x27;t know anything about. That&#x27;s as may be.<p>I would be surprised if this becomes a radical new architecture based on what I&#x27;m reading. I couldn&#x27;t find a lot of information as to size of the network used; I suppose if its very small that might be interesting. But this read to me very like an &#x27;insider&#x27; paper, its target other academics who care about some of these theories of mind, not people who need to get sentences like &#x27;cax kiki pup wif gep&#x27; turned into real color circles right away.</div><br/><div id="38018290" class="c"><input type="checkbox" id="c-38018290" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#38018145">parent</a><span>|</span><a href="#38019400">next</a><span>|</span><label class="collapse" for="c-38018290">[-]</label><label class="expand" for="c-38018290">[17 more]</label></div><br/><div class="children"><div class="content">&gt;I feel a quality prompt engineer could beat 85% with GPT4.<p>I generally don&#x27;t pull the &quot;you&#x27;re not prompting it good enough&quot; card unless i have direct experience with the task but this does raise a few flags - &quot;between 42 and 86% of the time, depending on how the researchers presented the task.&quot;<p>I would have liked them to show how they presented this task, at least something more than a single throw away line, given how integral it is to the main claim.</div><br/><div id="38018729" class="c"><input type="checkbox" id="c-38018729" checked=""/><div class="controls bullet"><span class="by">PumpkinSpice</span><span>|</span><a href="#38018145">root</a><span>|</span><a href="#38018290">parent</a><span>|</span><a href="#38021870">next</a><span>|</span><label class="collapse" for="c-38018729">[-]</label><label class="expand" for="c-38018729">[12 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t find this argument persuasive. If you need to engineer the prompt in order for the model to give you the answer you want, and if it&#x27;s performing poorly without that kind of assistance from a human who understands the task and the desired outcome... then it&#x27;s perfectly OK to not bother and give the LLM a low mark on a benchmark. After all, the whole point was to test the LLM&#x27;s language and reasoning skills.<p>&quot;Prompt engineering&quot; makes sense if your goal is more utilitarian, but it&#x27;s not a license to cheat on tests. If you want a machine learning model to generate aesthetically pleasing images, or to follow certain rules in a conversation, then it&#x27;s OK to rely on hacky solutions to get there.</div><br/><div id="38018904" class="c"><input type="checkbox" id="c-38018904" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#38018145">root</a><span>|</span><a href="#38018729">parent</a><span>|</span><a href="#38020242">next</a><span>|</span><label class="collapse" for="c-38018904">[-]</label><label class="expand" for="c-38018904">[4 more]</label></div><br/><div class="children"><div class="content">I kind of dislike the term &quot;Prompt Engineering&quot;. It mystifies what to me is an ordinary process. I&#x27;m not talking about chanting magic words, i&#x27;m just talking about taking the structure of an LLM into account, even taking how humans would approach the problem into account.<p>In microsoft&#x27;s agi paper, there is a test for planning they put out for GPT-4. They give detailed instructions on the constraint of a poem and expect it to spit out the constrained poem in one shot. Of course it fails and the conclusion is that it can&#x27;t plan. But if you think about it, this is a ridiculous assertion. No human on earth would have passed that test with working memory alone. Even by our standards, it&#x27;s a weird way to present the information but they did so anyway. This is actually a major issue for most plan benchmark assessment for LLMs i&#x27;ve come across.<p>These are the kind of problems i&#x27;m talking about. If i changed the request to encourage for a draft&#x2F;revise generation process and it consistently passed these tests then it&#x27;s very fair to say it <i>can</i> plan. That&#x27;s not &quot;hacky&quot;. It&#x27;s just truth. and saying otherwise is denying real world usage for a misplaced sense of propriety to benchmarks. a benchmark is only as useful as the capabilities it can assess.<p>a &quot;this is how this is presented, this is how the model performed&quot; would have been very prudent in my opinion. Maybe these guys already covered the kind of things i&#x27;m talking about(i can accept that!)....but i can&#x27;t know that if they don&#x27;t tell us.</div><br/><div id="38019608" class="c"><input type="checkbox" id="c-38019608" checked=""/><div class="controls bullet"><span class="by">haskellandchill</span><span>|</span><a href="#38018145">root</a><span>|</span><a href="#38018904">parent</a><span>|</span><a href="#38020242">next</a><span>|</span><label class="collapse" for="c-38019608">[-]</label><label class="expand" for="c-38019608">[3 more]</label></div><br/><div class="children"><div class="content">how is &quot;Engineering&quot; &quot;about chanting magic words&quot;? The wires may be getting crossed on if it&#x27;s an engineering discipline or just the use of the word engineer, meaning to build mindfully, I feel the latter is appropriate and the former more misguided but still it&#x27;s not &quot;Prompt Wizarding&quot; ;)</div><br/><div id="38019804" class="c"><input type="checkbox" id="c-38019804" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#38018145">root</a><span>|</span><a href="#38019608">parent</a><span>|</span><a href="#38022590">next</a><span>|</span><label class="collapse" for="c-38019804">[-]</label><label class="expand" for="c-38019804">[1 more]</label></div><br/><div class="children"><div class="content">Fair Enough. Just that discussion around the term feels like that sometimes especially with image generation.</div><br/></div></div><div id="38022590" class="c"><input type="checkbox" id="c-38022590" checked=""/><div class="controls bullet"><span class="by">Guthur</span><span>|</span><a href="#38018145">root</a><span>|</span><a href="#38019608">parent</a><span>|</span><a href="#38019804">prev</a><span>|</span><a href="#38020242">next</a><span>|</span><label class="collapse" for="c-38022590">[-]</label><label class="expand" for="c-38022590">[1 more]</label></div><br/><div class="children"><div class="content">Well you&#x27;re building nothing,  your poking at a black box and hoping it spits out something close to what you want.</div><br/></div></div></div></div></div></div><div id="38020242" class="c"><input type="checkbox" id="c-38020242" checked=""/><div class="controls bullet"><span class="by">viraptor</span><span>|</span><a href="#38018145">root</a><span>|</span><a href="#38018729">parent</a><span>|</span><a href="#38018904">prev</a><span>|</span><a href="#38020282">next</a><span>|</span><label class="collapse" for="c-38020242">[-]</label><label class="expand" for="c-38020242">[1 more]</label></div><br/><div class="children"><div class="content">I think adjusting the prompt makes sense. We have to keep in mind that this implicitly happens on the human side as well. The person creating the initial question will consider how understandable it is for them and edit until they&#x27;re happy. Then the questions will be reviewed by co-authors and possibly tested on a few people before running the full experiment. They&#x27;d probably review the explanation again or change the task if they realised half the people can&#x27;t follow the instructions.<p>We just call it &quot;editing for clarity&quot; rather than &quot;prompt engineering&quot; when we write for other people.</div><br/></div></div><div id="38020282" class="c"><input type="checkbox" id="c-38020282" checked=""/><div class="controls bullet"><span class="by">quitit</span><span>|</span><a href="#38018145">root</a><span>|</span><a href="#38018729">parent</a><span>|</span><a href="#38020242">prev</a><span>|</span><a href="#38021325">next</a><span>|</span><label class="collapse" for="c-38020282">[-]</label><label class="expand" for="c-38020282">[1 more]</label></div><br/><div class="children"><div class="content">While I think that&#x27;s a valid point, for the purpose of the paper having a competent prompt writer would be needed to make a fair comparison.<p>I think the researchers would know how to get the best from their own AI bot, so that&#x27;s a level of competency that should be extended to comparisons, otherwise user competency becomes a source of bias. I do feel you&#x27;re correct in your concerns though, the systems shouldn&#x27;t need experts to use them, nor should they need the user to already know the right answer, which leads me to my next point:<p>When it comes to real world expectations, perhaps instead we need a large group of random people (with no prior experience) working with each bot to complete a set of tasks in order to determine how it truly performs - something that could be enhanced if the answers weren&#x27;t easy to check.</div><br/></div></div><div id="38021325" class="c"><input type="checkbox" id="c-38021325" checked=""/><div class="controls bullet"><span class="by">emtel</span><span>|</span><a href="#38018145">root</a><span>|</span><a href="#38018729">parent</a><span>|</span><a href="#38020282">prev</a><span>|</span><a href="#38019148">next</a><span>|</span><label class="collapse" for="c-38021325">[-]</label><label class="expand" for="c-38021325">[2 more]</label></div><br/><div class="children"><div class="content">Disagree - a model may have capabilities that it only deploys in certain circumstances. In particular, if you train on the whole internet, you probably learn what both stupid and smart outputs look like. But you probably also learn that the modal output is stupid. So it’s no ding on the model’s capabilities if it defaults to assuming it should behave stupidly.</div><br/><div id="38021638" class="c"><input type="checkbox" id="c-38021638" checked=""/><div class="controls bullet"><span class="by">PumpkinSpice</span><span>|</span><a href="#38018145">root</a><span>|</span><a href="#38021325">parent</a><span>|</span><a href="#38019148">next</a><span>|</span><label class="collapse" for="c-38021638">[-]</label><label class="expand" for="c-38021638">[1 more]</label></div><br/><div class="children"><div class="content">What you&#x27;re basically saying is &quot;the model as trained can&#x27;t do well at this task, so let&#x27;s use our own cognitive skills to help it.&quot;<p>That&#x27;s a problem for comparative benchmarking, right? You&#x27;re no longer testing the model; you&#x27;re testing the model in tandem with the prompt engineer. This raises several big questions:<p>1) How do you know that the engineer&#x27;s knowledge of the &quot;correct&quot; answer isn&#x27;t being subtly encoded in the prompt? Essentially the Clever Hans phenomenon.<p>2) If you want this to be a fair game, how do you give precisely the same kind of advantage to whatever you&#x27;re benchmarking the LLM against?<p>3) Last but not least, if you&#x27;re not going to throw in your prompt engineer for free with the product, will your results be reproducible by your customers?<p>To be clear, I don&#x27;t think there&#x27;s some cosmically objective way of doing this. If you&#x27;re using prompts written for humans, you&#x27;re already putting the computer at a disadvantage. But at the very least, you&#x27;re measuring something meaningful: how will the model behave in the real world.</div><br/></div></div></div></div><div id="38019148" class="c"><input type="checkbox" id="c-38019148" checked=""/><div class="controls bullet"><span class="by">altruios</span><span>|</span><a href="#38018145">root</a><span>|</span><a href="#38018729">parent</a><span>|</span><a href="#38021325">prev</a><span>|</span><a href="#38020258">next</a><span>|</span><label class="collapse" for="c-38019148">[-]</label><label class="expand" for="c-38019148">[2 more]</label></div><br/><div class="children"><div class="content">So what if we have an AI prompter-assister... we take an input prompt: run it through an AI to transform that prompt into proper AI talk - then that is what&#x27;s prompted to the actual AI... That way the AI will always have the best prompt possible, because the Prompt-helper helps your prompt...<p>&#x2F;s if anyone needs it.</div><br/><div id="38019420" class="c"><input type="checkbox" id="c-38019420" checked=""/><div class="controls bullet"><span class="by">larperdoodle</span><span>|</span><a href="#38018145">root</a><span>|</span><a href="#38019148">parent</a><span>|</span><a href="#38020258">next</a><span>|</span><label class="collapse" for="c-38019420">[-]</label><label class="expand" for="c-38019420">[1 more]</label></div><br/><div class="children"><div class="content">Is the &#x2F;s because that&#x27;s literally what they&#x27;re already doing?</div><br/></div></div></div></div><div id="38020258" class="c"><input type="checkbox" id="c-38020258" checked=""/><div class="controls bullet"><span class="by">nomel</span><span>|</span><a href="#38018145">root</a><span>|</span><a href="#38018729">parent</a><span>|</span><a href="#38019148">prev</a><span>|</span><a href="#38021870">next</a><span>|</span><label class="collapse" for="c-38020258">[-]</label><label class="expand" for="c-38020258">[1 more]</label></div><br/><div class="children"><div class="content">I would claim that most people are <i>very</i> bad at communicating in a way that can be interpreted, by human or AI, with a single shot. Most of the time some back and forth is required to clear things up. I think it&#x27;s unfair to expect gold from garbage input, since meat brains can&#x27;t do that either, without some back and fourth.<p>Those prompts should be available. It&#x27;s ridiculous that they&#x27;re not.</div><br/></div></div></div></div><div id="38021870" class="c"><input type="checkbox" id="c-38021870" checked=""/><div class="controls bullet"><span class="by">pama</span><span>|</span><a href="#38018145">root</a><span>|</span><a href="#38018290">parent</a><span>|</span><a href="#38018729">prev</a><span>|</span><a href="#38020659">next</a><span>|</span><label class="collapse" for="c-38021870">[-]</label><label class="expand" for="c-38021870">[1 more]</label></div><br/><div class="children"><div class="content">The supplement discusses how they presented the task.  Notably, they first gave all the training examples and then told the model what the task is and didn&#x27;t ask it to reason or create any context before spitting out the answers.  So basically the simplest way to interact with it, but probably not a great way to get solutions for this problem if that was the task at hand.<p><a href="https:&#x2F;&#x2F;static-content.springer.com&#x2F;esm&#x2F;art%3A10.1038%2Fs41586-023-06668-3&#x2F;MediaObjects&#x2F;41586_2023_6668_MOESM1_ESM.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;static-content.springer.com&#x2F;esm&#x2F;art%3A10.1038%2Fs415...</a></div><br/></div></div><div id="38020659" class="c"><input type="checkbox" id="c-38020659" checked=""/><div class="controls bullet"><span class="by">vessenes</span><span>|</span><a href="#38018145">root</a><span>|</span><a href="#38018290">parent</a><span>|</span><a href="#38021870">prev</a><span>|</span><a href="#38018442">next</a><span>|</span><label class="collapse" for="c-38020659">[-]</label><label class="expand" for="c-38020659">[1 more]</label></div><br/><div class="children"><div class="content">Agreed. And there&#x27;s going to be some significant domain decisions for GPT-4 to consider: Will the made up words be existing single tokens? Will those single tokens be heavily overloaded tokens, e.g. a letter of the alphabet? Will the representation of the circles be, say &quot;B&quot; for blue, or something else?<p>Along with this are questions as to whether you&#x27;re going to treat GPT-4 as a zero shot or multi-shot oracle; while they have this idea of &#x27;context free&#x27; challenges in the paper, they, crucially, <i>train</i> their network first.<p>Anyway, I like this paper less the more I think about it.</div><br/></div></div><div id="38018442" class="c"><input type="checkbox" id="c-38018442" checked=""/><div class="controls bullet"><span class="by">randcraw</span><span>|</span><a href="#38018145">root</a><span>|</span><a href="#38018290">parent</a><span>|</span><a href="#38020659">prev</a><span>|</span><a href="#38019400">next</a><span>|</span><label class="collapse" for="c-38018442">[-]</label><label class="expand" for="c-38018442">[2 more]</label></div><br/><div class="children"><div class="content">The article suggested that GPF-4 <i>FAILED</i> between 42 and 86 percent of the time.  So its success rate would be 58 to 14 percent, which compared to the novel NN&#x27;s 80 percent, seems significant.</div><br/><div id="38018589" class="c"><input type="checkbox" id="c-38018589" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#38018145">root</a><span>|</span><a href="#38018442">parent</a><span>|</span><a href="#38019400">next</a><span>|</span><label class="collapse" for="c-38018589">[-]</label><label class="expand" for="c-38018589">[1 more]</label></div><br/><div class="children"><div class="content">Yes I&#x27;m aware. My point is that<p>1. The level of variance displayed here is very atypical for a language model unless you&#x27;re making significant changes to how the information is presented. This alone is cause for more elaboration.<p>2. With this big a variance already showing, it&#x27;s hard to say for sure that they hit the top end.
Maybe it seems strange but most LLM evaluation papers still don&#x27;t actually bother to tie in even some extremely well known&#x2F;basic &quot;output boosters&quot; like chain of thought and so on. But at least in these instances, we know how they presented it.<p>a &quot;this is how we presented the task, this is how it performed&quot; would have been very nice.</div><br/></div></div></div></div></div></div><div id="38019400" class="c"><input type="checkbox" id="c-38019400" checked=""/><div class="controls bullet"><span class="by">8bitsrule</span><span>|</span><a href="#38018145">parent</a><span>|</span><a href="#38018290">prev</a><span>|</span><a href="#38019126">next</a><span>|</span><label class="collapse" for="c-38019400">[-]</label><label class="expand" for="c-38019400">[1 more]</label></div><br/><div class="children"><div class="content">&gt;I couldn&#x27;t find a lot of information as to size of the network used<p>In the original paper,[0] in the section called &#x27;Implementation of MLC&#x27;, there&#x27;s a description of sorts (Greek to me):<p>&quot;Both the encoder and decoder have 3 layers, 8 attention heads per layer, input and hidden embeddings of size 128, and a feedforward hidden size of 512. Following GPT63, GELU64 activation functions are used instead of ReLU. In total, the architecture has about 1.4 million parameters.&quot;<p>[0] <a href="https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;s41586-023-06668-3" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;s41586-023-06668-3</a></div><br/></div></div><div id="38019126" class="c"><input type="checkbox" id="c-38019126" checked=""/><div class="controls bullet"><span class="by">avgcorrection</span><span>|</span><a href="#38018145">parent</a><span>|</span><a href="#38019400">prev</a><span>|</span><a href="#38018641">next</a><span>|</span><label class="collapse" for="c-38019126">[-]</label><label class="expand" for="c-38019126">[1 more]</label></div><br/><div class="children"><div class="content">That’s seriously it? What an insulting (to the reader) headline by Nature.</div><br/></div></div><div id="38021531" class="c"><input type="checkbox" id="c-38021531" checked=""/><div class="controls bullet"><span class="by">sashank_1509</span><span>|</span><a href="#38018145">parent</a><span>|</span><a href="#38018641">prev</a><span>|</span><a href="#38018042">next</a><span>|</span><label class="collapse" for="c-38021531">[-]</label><label class="expand" for="c-38021531">[1 more]</label></div><br/><div class="children"><div class="content">Honestly I’m surprised by how low Nature has fallen. Is it even a useful signal of paper quality anymore? NeurIPS and ICLR aren’t great but in general I’ve found their work to be more rigorous than that of Nature despite of the fact that they are shorter conference papers compared to Nature’s journal papers.<p>Right now sadly the only useful signal in Deep Learning research is research group. If OpenAI releases a paper I know it’s something good that works at scale, similarly if Kaiming He, Piotr Dollar and team in Meta AI release a paper, it tends to be really good and SOTA. Google Deepmind maintains high quality, Google brain has been more of a mixed bag. If Berkeley releases a paper, it has 50% chance of directly going to trash, Stanford has a much lower percent (I also disambiguate) based on specific groups. Of course I’m going to be heavily biased and this system is not great, but conferences and journals have managed to become such a useless signal that I find this method to be more accurate.</div><br/></div></div></div></div><div id="38018042" class="c"><input type="checkbox" id="c-38018042" checked=""/><div class="controls bullet"><span class="by">valine</span><span>|</span><a href="#38018145">prev</a><span>|</span><a href="#38017833">next</a><span>|</span><label class="collapse" for="c-38018042">[-]</label><label class="expand" for="c-38018042">[16 more]</label></div><br/><div class="children"><div class="content">For anyone wondering about the architecture this is a 1.4 million parameter transformer model with both an encoder and decoder. The vocabulary size is comically small, it only understands 8 words.<p>It learns new ideas with very few examples, to the extent you can express new ideas with a vocabulary of eight words.</div><br/><div id="38018147" class="c"><input type="checkbox" id="c-38018147" checked=""/><div class="controls bullet"><span class="by">rpearl</span><span>|</span><a href="#38018042">parent</a><span>|</span><a href="#38018463">next</a><span>|</span><label class="collapse" for="c-38018147">[-]</label><label class="expand" for="c-38018147">[8 more]</label></div><br/><div class="children"><div class="content">let&#x27;s see...<p>English speakers tend to have a vocabulary of 30k words or so depending on what exactly you measure.<p>GPT-4 has 1.7 trillion parameters.<p>Of course scaling up is quite unpredictable in what capabilities it gets you, but It&#x27;s not that much of a stretch to imagine that a GPT-4 sized model would have a reasonably sized vocabulary. Certainly worth testing if you have the resources to train such a thing!</div><br/><div id="38018472" class="c"><input type="checkbox" id="c-38018472" checked=""/><div class="controls bullet"><span class="by">glitchc</span><span>|</span><a href="#38018042">root</a><span>|</span><a href="#38018147">parent</a><span>|</span><a href="#38018957">next</a><span>|</span><label class="collapse" for="c-38018472">[-]</label><label class="expand" for="c-38018472">[6 more]</label></div><br/><div class="children"><div class="content">It&#x27;s the mapping of parameters to vocabulary claim embedded in your response that needs validation. 1.7 trillion parameters means what exactly?<p>Let&#x27;s start with working vocabulary. Working vocabulary doesn&#x27;t just mean knowing n words. It means putting n words together in factorially many valid combinations to construct sentences. And 30k btw is an insane working vocabulary. Most people know 1000 words on average in English. All of their sentences are structured from those 1000 words. This is true for most cases, except certain ones like Mandarin or German, where basic words can be used to assemble more complex words.<p>Certainly GPT-4 knows something. Presumably that something can be mapped to a working vocabulary. How large a vocabulary that is requires a testable, reproducible hypothesis supported by experimental proof. Do you have such a hypothesis with proof? Does anyone? Until we do it&#x27;s just a guess.</div><br/><div id="38022502" class="c"><input type="checkbox" id="c-38022502" checked=""/><div class="controls bullet"><span class="by">throwaway9274</span><span>|</span><a href="#38018042">root</a><span>|</span><a href="#38018472">parent</a><span>|</span><a href="#38019566">next</a><span>|</span><label class="collapse" for="c-38022502">[-]</label><label class="expand" for="c-38022502">[1 more]</label></div><br/><div class="children"><div class="content">100% of people know the most common 1000 words. The remainder of those who know more words fall into a consistent curve across languages that follows Zipf’s law. This is different than “most people know 1000 words on average.”</div><br/></div></div><div id="38019566" class="c"><input type="checkbox" id="c-38019566" checked=""/><div class="controls bullet"><span class="by">rpearl</span><span>|</span><a href="#38018042">root</a><span>|</span><a href="#38018472">parent</a><span>|</span><a href="#38022502">prev</a><span>|</span><a href="#38018676">next</a><span>|</span><label class="collapse" for="c-38019566">[-]</label><label class="expand" for="c-38019566">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t care to pay for access to gpt-4 but one could easily use one of the vocabulary estimation tests, which use some statistics plus knowledge of word appearance frequency, to estimate its vocabulary size. <a href="https:&#x2F;&#x2F;mikeinnes.io&#x2F;2022&#x2F;02&#x2F;26&#x2F;vocab" rel="nofollow noreferrer">https:&#x2F;&#x2F;mikeinnes.io&#x2F;2022&#x2F;02&#x2F;26&#x2F;vocab</a> is one such test which explains the statistics ideas, and there are many others based on similar principles.</div><br/></div></div><div id="38018676" class="c"><input type="checkbox" id="c-38018676" checked=""/><div class="controls bullet"><span class="by">_a_a_a_</span><span>|</span><a href="#38018042">root</a><span>|</span><a href="#38018472">parent</a><span>|</span><a href="#38019566">prev</a><span>|</span><a href="#38018957">next</a><span>|</span><label class="collapse" for="c-38018676">[-]</label><label class="expand" for="c-38018676">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Most people know 1000 words on average in English<p>Maybe I misunderstand but that sounds so stupid and wrong I don&#x27;t know where to start. Standard vocab estimates are 15K - 30K, averaging about 20K (these from memory)<p>Edit: from wiki <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Vocabulary#Native-language_vocabulary" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Vocabulary#Native-language_voc...</a><p>---<p>As a result, estimates vary from 10,000-17,000 word families[16][19] or 17,000-42,000 dictionary words for young adult native speakers of English.[12][17]<p>A 2016 study shows that 20-year-old English native speakers recognize on average 42,000 lemmas, ranging from 27,100 for the lowest 5% of the population to 51,700 lemmas for the highest 5%. These lemmas come from 6,100 word families in the lowest 5% of the population and 14,900 word families in the highest 5%. 60-year-olds know on average 6,000 lemmas more. [12]<p>According to another, earlier 1995 study junior-high students would be able to recognize the meanings of about 10,000–12,000 words, whereas for college students this number grows up to about 12,000–17,000 and for elderly adults up to about 17,000 or more.[20]<p>---</div><br/><div id="38019087" class="c"><input type="checkbox" id="c-38019087" checked=""/><div class="controls bullet"><span class="by">Y_Y</span><span>|</span><a href="#38018042">root</a><span>|</span><a href="#38018676">parent</a><span>|</span><a href="#38019637">next</a><span>|</span><label class="collapse" for="c-38019087">[-]</label><label class="expand" for="c-38019087">[1 more]</label></div><br/><div class="children"><div class="content">Does the average include people who don&#x27;t speak English? If about 4% of the world&#x27;s population are native speakers and the number of words known tails off after that I can imagine it could almost be approximately true. And maybe we are counting babies in English majority countries as native speakers but they haven&#x27;t learned all their 20k words yet. Of course GP&#x27;s point is still invalid in that case.</div><br/></div></div><div id="38019637" class="c"><input type="checkbox" id="c-38019637" checked=""/><div class="controls bullet"><span class="by">debo_</span><span>|</span><a href="#38018042">root</a><span>|</span><a href="#38018676">parent</a><span>|</span><a href="#38019087">prev</a><span>|</span><a href="#38018957">next</a><span>|</span><label class="collapse" for="c-38019637">[-]</label><label class="expand" for="c-38019637">[1 more]</label></div><br/><div class="children"><div class="content">I assumed they missed a 0 and meant 10000 words.</div><br/></div></div></div></div></div></div><div id="38018957" class="c"><input type="checkbox" id="c-38018957" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#38018042">root</a><span>|</span><a href="#38018147">parent</a><span>|</span><a href="#38018472">prev</a><span>|</span><a href="#38018463">next</a><span>|</span><label class="collapse" for="c-38018957">[-]</label><label class="expand" for="c-38018957">[1 more]</label></div><br/><div class="children"><div class="content">&gt; if you have the resources to train such a thing!<p>estimates i&#x27;ve heard for training gpt4 was $500m all in for what its worth</div><br/></div></div></div></div><div id="38018139" class="c"><input type="checkbox" id="c-38018139" checked=""/><div class="controls bullet"><span class="by">Izkata</span><span>|</span><a href="#38018042">parent</a><span>|</span><a href="#38018463">prev</a><span>|</span><a href="#38017833">next</a><span>|</span><label class="collapse" for="c-38018139">[-]</label><label class="expand" for="c-38018139">[6 more]</label></div><br/><div class="children"><div class="content">Morse code has a vocabulary of 3 words (dot, dash, pause between human words).</div><br/><div id="38018437" class="c"><input type="checkbox" id="c-38018437" checked=""/><div class="controls bullet"><span class="by">IMTDb</span><span>|</span><a href="#38018042">root</a><span>|</span><a href="#38018139">parent</a><span>|</span><a href="#38018280">next</a><span>|</span><label class="collapse" for="c-38018437">[-]</label><label class="expand" for="c-38018437">[4 more]</label></div><br/><div class="children"><div class="content">Then English has a vocabulary of 27 words: A -&gt; Z + space</div><br/><div id="38018761" class="c"><input type="checkbox" id="c-38018761" checked=""/><div class="controls bullet"><span class="by">sterlind</span><span>|</span><a href="#38018042">root</a><span>|</span><a href="#38018437">parent</a><span>|</span><a href="#38018566">next</a><span>|</span><label class="collapse" for="c-38018761">[-]</label><label class="expand" for="c-38018761">[1 more]</label></div><br/><div class="children"><div class="content">I think a fairer comparison would be Toki Pona, a micro-language with ~120 words. you can express lots of things if you have a great deal of patience, Up-Goer Five style.<p>In logic there&#x27;s also SKI combinator calculus, which is Turing-complete with three symbols, and unlike the Morse or A-Z examples, but like the Toki Pona example, each symbol has a semantic meaning.<p>If you just want to describe ideas in an abstract realm like sequences of colors, like this paper, it&#x27;s not surprising you don&#x27;t need many words.</div><br/></div></div><div id="38018566" class="c"><input type="checkbox" id="c-38018566" checked=""/><div class="controls bullet"><span class="by">nerdponx</span><span>|</span><a href="#38018042">root</a><span>|</span><a href="#38018437">parent</a><span>|</span><a href="#38018761">prev</a><span>|</span><a href="#38018280">next</a><span>|</span><label class="collapse" for="c-38018566">[-]</label><label class="expand" for="c-38018566">[2 more]</label></div><br/><div class="children"><div class="content">If you tokenize using character n-grams, this isn&#x27;t totally wrong.</div><br/><div id="38019175" class="c"><input type="checkbox" id="c-38019175" checked=""/><div class="controls bullet"><span class="by">Izkata</span><span>|</span><a href="#38018042">root</a><span>|</span><a href="#38018566">parent</a><span>|</span><a href="#38018280">next</a><span>|</span><label class="collapse" for="c-38019175">[-]</label><label class="expand" for="c-38019175">[1 more]</label></div><br/><div class="children"><div class="content">Exactly.<p>There was a post on here a few months ago about training using single characters as tokens instead of words, and it worked really well, being able to create new Shakespeare-like text despite not using human words as tokens.  What a (human) word is can be learned by the model instead of encoded in the training set.</div><br/></div></div></div></div></div></div><div id="38018280" class="c"><input type="checkbox" id="c-38018280" checked=""/><div class="controls bullet"><span class="by">yawnxyz</span><span>|</span><a href="#38018042">root</a><span>|</span><a href="#38018139">parent</a><span>|</span><a href="#38018437">prev</a><span>|</span><a href="#38017833">next</a><span>|</span><label class="collapse" for="c-38018280">[-]</label><label class="expand" for="c-38018280">[1 more]</label></div><br/><div class="children"><div class="content">those are more like tokens, not words</div><br/></div></div></div></div></div></div><div id="38017833" class="c"><input type="checkbox" id="c-38017833" checked=""/><div class="controls bullet"><span class="by">samfriedman</span><span>|</span><a href="#38018042">prev</a><span>|</span><a href="#38017147">next</a><span>|</span><label class="collapse" for="c-38017833">[-]</label><label class="expand" for="c-38017833">[1 more]</label></div><br/><div class="children"><div class="content">Link to the actual paper: <a href="https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;s41586-023-06668-3" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;s41586-023-06668-3</a></div><br/></div></div><div id="38017147" class="c"><input type="checkbox" id="c-38017147" checked=""/><div class="controls bullet"><span class="by">drcwpl</span><span>|</span><a href="#38017833">prev</a><span>|</span><a href="#38018844">next</a><span>|</span><label class="collapse" for="c-38017147">[-]</label><label class="expand" for="c-38017147">[1 more]</label></div><br/><div class="children"><div class="content">By contrast, GPT-4 struggled with the same task, failing, on average, between 42 and 86% of the time, depending on how the researchers presented the task. “It’s not magic, it’s practice,”</div><br/></div></div><div id="38018844" class="c"><input type="checkbox" id="c-38018844" checked=""/><div class="controls bullet"><span class="by">danielmarkbruce</span><span>|</span><a href="#38017147">prev</a><span>|</span><a href="#38019357">next</a><span>|</span><label class="collapse" for="c-38018844">[-]</label><label class="expand" for="c-38018844">[1 more]</label></div><br/><div class="children"><div class="content">This seems dumb. Sure, an LLM can&#x27;t learn a new word. An LLM isn&#x27;t an entire system. You could make a system (which extensively used an LLM) to do this fairly easily.</div><br/></div></div><div id="38019357" class="c"><input type="checkbox" id="c-38019357" checked=""/><div class="controls bullet"><span class="by">CSMastermind</span><span>|</span><a href="#38018844">prev</a><span>|</span><a href="#38019255">next</a><span>|</span><label class="collapse" for="c-38019357">[-]</label><label class="expand" for="c-38019357">[2 more]</label></div><br/><div class="children"><div class="content">The link is to a news article about a recent paper not the a link to the research itself.<p>The editorialization of this being a breakthrough was done by the reporter not the authors of the paper.<p>I see people here comparing this to GPT-4 but I can&#x27;t find that section in the actual paper nor in the underlying data.  Can someone point me in the right direction?</div><br/><div id="38021813" class="c"><input type="checkbox" id="c-38021813" checked=""/><div class="controls bullet"><span class="by">pama</span><span>|</span><a href="#38019357">parent</a><span>|</span><a href="#38019255">next</a><span>|</span><label class="collapse" for="c-38021813">[-]</label><label class="expand" for="c-38021813">[1 more]</label></div><br/><div class="children"><div class="content">The fourth sentence of the discussion points to the relevant work in their supplement.  Here is the link to the supplement: <a href="https:&#x2F;&#x2F;static-content.springer.com&#x2F;esm&#x2F;art%3A10.1038%2Fs41586-023-06668-3&#x2F;MediaObjects&#x2F;41586_2023_6668_MOESM1_ESM.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;static-content.springer.com&#x2F;esm&#x2F;art%3A10.1038%2Fs415...</a></div><br/></div></div></div></div><div id="38019255" class="c"><input type="checkbox" id="c-38019255" checked=""/><div class="controls bullet"><span class="by">matmulbro</span><span>|</span><a href="#38019357">prev</a><span>|</span><a href="#38018699">next</a><span>|</span><label class="collapse" for="c-38019255">[-]</label><label class="expand" for="c-38019255">[2 more]</label></div><br/><div class="children"><div class="content">check out other articles by the same author: <a href="https:&#x2F;&#x2F;www.nature.com&#x2F;search?author=Max+Kozlov" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.nature.com&#x2F;search?author=Max+Kozlov</a></div><br/><div id="38021171" class="c"><input type="checkbox" id="c-38021171" checked=""/><div class="controls bullet"><span class="by">caddemon</span><span>|</span><a href="#38019255">parent</a><span>|</span><a href="#38018699">next</a><span>|</span><label class="collapse" for="c-38021171">[-]</label><label class="expand" for="c-38021171">[1 more]</label></div><br/><div class="children"><div class="content">This guy is a science journalist, the authors of the source research paper are more relevant if you&#x27;re trying to infer anything about the material from the authors: <a href="https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;s41586-023-06668-3" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.nature.com&#x2F;articles&#x2F;s41586-023-06668-3</a><p>FWIW I think even the original paper is more of a master class in riding an academic bubble than anything of important substance. I&#x27;m just not sure what looking at other articles by this journalist is supposed to convey.</div><br/></div></div></div></div><div id="38018699" class="c"><input type="checkbox" id="c-38018699" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#38019255">prev</a><span>|</span><a href="#38018141">next</a><span>|</span><label class="collapse" for="c-38018699">[-]</label><label class="expand" for="c-38018699">[1 more]</label></div><br/><div class="children"><div class="content">When an important word is quoted, don’t take it seriously</div><br/></div></div><div id="38017857" class="c"><input type="checkbox" id="c-38017857" checked=""/><div class="controls bullet"><span class="by">mistrial9</span><span>|</span><a href="#38018141">prev</a><span>|</span><a href="#38019525">next</a><span>|</span><label class="collapse" for="c-38017857">[-]</label><label class="expand" for="c-38017857">[12 more]</label></div><br/><div class="children"><div class="content">splendid sales headline - however, human communication is varied and multi-layered. Since there are too many angles to summarize -- suffice it to say that IMHO a healthy skepticism is warranted, along with evidence and test results.<p>link to cited repo: <a href="https:&#x2F;&#x2F;github.com&#x2F;brendenlake&#x2F;MLC&#x2F;tree&#x2F;v1.0.0">https:&#x2F;&#x2F;github.com&#x2F;brendenlake&#x2F;MLC&#x2F;tree&#x2F;v1.0.0</a></div><br/><div id="38017970" class="c"><input type="checkbox" id="c-38017970" checked=""/><div class="controls bullet"><span class="by">tomohelix</span><span>|</span><a href="#38017857">parent</a><span>|</span><a href="#38017978">next</a><span>|</span><label class="collapse" for="c-38017970">[-]</label><label class="expand" for="c-38017970">[3 more]</label></div><br/><div class="children"><div class="content">I will take this article with a bag of salt but I think we are very close to solving the human communication problem. Not because the machines are perfect but because the human, our benchmarks, are prone to mistakes and so a &quot;close enough&quot; machine would pass as a human just fine.</div><br/><div id="38019098" class="c"><input type="checkbox" id="c-38019098" checked=""/><div class="controls bullet"><span class="by">marysnovirgin</span><span>|</span><a href="#38017857">root</a><span>|</span><a href="#38017970">parent</a><span>|</span><a href="#38019178">next</a><span>|</span><label class="collapse" for="c-38019098">[-]</label><label class="expand" for="c-38019098">[1 more]</label></div><br/><div class="children"><div class="content">There is inherently no solution to the human communication problem. The mistake has always been in thinking technology would unite us and improve communication when clearly it&#x27;s doing the opposite.</div><br/></div></div></div></div><div id="38017978" class="c"><input type="checkbox" id="c-38017978" checked=""/><div class="controls bullet"><span class="by">Beijinger</span><span>|</span><a href="#38017857">parent</a><span>|</span><a href="#38017970">prev</a><span>|</span><a href="#38018383">next</a><span>|</span><label class="collapse" for="c-38017978">[-]</label><label class="expand" for="c-38017978">[7 more]</label></div><br/><div class="children"><div class="content">I agree. But &quot;Nature&quot; is quite a name.</div><br/><div id="38018222" class="c"><input type="checkbox" id="c-38018222" checked=""/><div class="controls bullet"><span class="by">matt123456789</span><span>|</span><a href="#38017857">root</a><span>|</span><a href="#38017978">parent</a><span>|</span><a href="#38018383">next</a><span>|</span><label class="collapse" for="c-38018222">[-]</label><label class="expand" for="c-38018222">[6 more]</label></div><br/><div class="children"><div class="content">And desperate to retain their relevance in the face of the onslaught of advancements that open access papers has unleashed.</div><br/><div id="38021437" class="c"><input type="checkbox" id="c-38021437" checked=""/><div class="controls bullet"><span class="by">jltsiren</span><span>|</span><a href="#38017857">root</a><span>|</span><a href="#38018222">parent</a><span>|</span><a href="#38018287">next</a><span>|</span><label class="collapse" for="c-38021437">[-]</label><label class="expand" for="c-38021437">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a category error. You don&#x27;t submit your manuscript to a prestige journal in order to publish it but to take advantage of their PR machinery. The services Nature provides are very good and cost-effective, but you have to meet their standards.<p>As research papers, the papers published in Nature&#x2F;Science&#x2F;etc. are rarely that useful. Because the journals target the general scientifically literate audience, the interesting details are usually hidden in supplementary material. And the supplements are often little more than giant info dumps that nobody has paid sufficient attention to.<p>It would be nice to live in a world where such PR services are unnecessary, but that&#x27;s not the world we are living in. The academia is very competitive, and there is never enough funding and never enough good jobs.</div><br/></div></div><div id="38018287" class="c"><input type="checkbox" id="c-38018287" checked=""/><div class="controls bullet"><span class="by">schleck8</span><span>|</span><a href="#38017857">root</a><span>|</span><a href="#38018222">parent</a><span>|</span><a href="#38021437">prev</a><span>|</span><a href="#38018383">next</a><span>|</span><label class="collapse" for="c-38018287">[-]</label><label class="expand" for="c-38018287">[4 more]</label></div><br/><div class="children"><div class="content">So to retain their relevance you claim they abandon academic standards and sensationalize an openly accessible study that anyone can read and thus dispute the article. Okay.</div><br/><div id="38021207" class="c"><input type="checkbox" id="c-38021207" checked=""/><div class="controls bullet"><span class="by">caddemon</span><span>|</span><a href="#38017857">root</a><span>|</span><a href="#38018287">parent</a><span>|</span><a href="#38020465">next</a><span>|</span><label class="collapse" for="c-38021207">[-]</label><label class="expand" for="c-38021207">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think it is quite that intentional, but just as human economic systems regularly create bubbles there are blind spots in the academic science collective. People desperately want to get in on LLMs right now.</div><br/></div></div><div id="38020465" class="c"><input type="checkbox" id="c-38020465" checked=""/><div class="controls bullet"><span class="by">dotnet00</span><span>|</span><a href="#38017857">root</a><span>|</span><a href="#38018287">parent</a><span>|</span><a href="#38021207">prev</a><span>|</span><a href="#38019403">next</a><span>|</span><label class="collapse" for="c-38020465">[-]</label><label class="expand" for="c-38020465">[1 more]</label></div><br/><div class="children"><div class="content">It isn&#x27;t exactly unheard of for them to do that. They&#x27;ll just wipe their hands of it afterwards if it turns out to be bad science by saying that their goal isn&#x27;t to publish research that&#x27;s guaranteed correct.</div><br/></div></div><div id="38019403" class="c"><input type="checkbox" id="c-38019403" checked=""/><div class="controls bullet"><span class="by">robertlagrant</span><span>|</span><a href="#38017857">root</a><span>|</span><a href="#38018287">parent</a><span>|</span><a href="#38020465">prev</a><span>|</span><a href="#38018383">next</a><span>|</span><label class="collapse" for="c-38019403">[-]</label><label class="expand" for="c-38019403">[1 more]</label></div><br/><div class="children"><div class="content">Anyone can read but most press will read and believe the abstract, and write breathless articles on it.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38019525" class="c"><input type="checkbox" id="c-38019525" checked=""/><div class="controls bullet"><span class="by">jmsuth</span><span>|</span><a href="#38017857">prev</a><span>|</span><a href="#38018377">next</a><span>|</span><label class="collapse" for="c-38019525">[-]</label><label class="expand" for="c-38019525">[1 more]</label></div><br/><div class="children"><div class="content">sounds lit</div><br/></div></div><div id="38018377" class="c"><input type="checkbox" id="c-38018377" checked=""/><div class="controls bullet"><span class="by">MattPalmer1086</span><span>|</span><a href="#38019525">prev</a><span>|</span><label class="collapse" for="c-38018377">[-]</label><label class="expand" for="c-38018377">[1 more]</label></div><br/><div class="children"><div class="content">I am no expert in this field, but this seems an important result, even if on a limited model.<p>I don&#x27;t think that anyone was very sure if generalisation was actually happening with current architectures. Happy to be corrected by anyone who knows more.</div><br/></div></div></div></div></div></div></div></body></html>