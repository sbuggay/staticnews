<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1692954061573" as="style"/><link rel="stylesheet" href="styles.css?v=1692954061573"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://vmartin.fr/understanding-automatic-differentiation-in-30-lines-of-python.html">Understanding Automatic Differentiation in 30 lines of Python</a> <span class="domain">(<a href="https://vmartin.fr">vmartin.fr</a>)</span></div><div class="subtext"><span>sebg</span> | <span>16 comments</span></div><br/><div><div id="37257320" class="c"><input type="checkbox" id="c-37257320" checked=""/><div class="controls bullet"><span class="by">eachro</span><span>|</span><a href="#37258513">next</a><span>|</span><label class="collapse" for="c-37257320">[-]</label><label class="expand" for="c-37257320">[5 more]</label></div><br/><div class="children"><div class="content">I really enjoy small elegant code demonstrations like this that really allow you to get your hands dirty to try to understand a concept. Another example is Sasha Rush&#x27;s gpu puzzles, tensor puzzles
-<a href="https:&#x2F;&#x2F;github.com&#x2F;srush&#x2F;GPU-Puzzles">https:&#x2F;&#x2F;github.com&#x2F;srush&#x2F;GPU-Puzzles</a>
-<a href="https:&#x2F;&#x2F;github.com&#x2F;srush&#x2F;Tensor-Puzzles">https:&#x2F;&#x2F;github.com&#x2F;srush&#x2F;Tensor-Puzzles</a></div><br/><div id="37257767" class="c"><input type="checkbox" id="c-37257767" checked=""/><div class="controls bullet"><span class="by">Scene_Cast2</span><span>|</span><a href="#37257320">parent</a><span>|</span><a href="#37258295">next</a><span>|</span><label class="collapse" for="c-37257767">[-]</label><label class="expand" for="c-37257767">[2 more]</label></div><br/><div class="children"><div class="content">In that case, you might also enjoy <a href="https:&#x2F;&#x2F;jaykmody.com&#x2F;blog&#x2F;gpt-from-scratch&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;jaykmody.com&#x2F;blog&#x2F;gpt-from-scratch&#x2F;</a><p>(here&#x27;s the raw code: <a href="https:&#x2F;&#x2F;github.com&#x2F;jaymody&#x2F;picoGPT&#x2F;blob&#x2F;main&#x2F;gpt2.py">https:&#x2F;&#x2F;github.com&#x2F;jaymody&#x2F;picoGPT&#x2F;blob&#x2F;main&#x2F;gpt2.py</a>)</div><br/><div id="37258127" class="c"><input type="checkbox" id="c-37258127" checked=""/><div class="controls bullet"><span class="by">eachro</span><span>|</span><a href="#37257320">root</a><span>|</span><a href="#37257767">parent</a><span>|</span><a href="#37258295">next</a><span>|</span><label class="collapse" for="c-37258127">[-]</label><label class="expand" for="c-37258127">[1 more]</label></div><br/><div class="children"><div class="content">This is really cool! Thanks for sharing :)</div><br/></div></div></div></div><div id="37258295" class="c"><input type="checkbox" id="c-37258295" checked=""/><div class="controls bullet"><span class="by">craigching</span><span>|</span><a href="#37257320">parent</a><span>|</span><a href="#37257767">prev</a><span>|</span><a href="#37258031">next</a><span>|</span><label class="collapse" for="c-37258295">[-]</label><label class="expand" for="c-37258295">[1 more]</label></div><br/><div class="children"><div class="content">Also micrograd from Andrej Karpathy: <a href="https:&#x2F;&#x2F;github.com&#x2F;karpathy&#x2F;micrograd">https:&#x2F;&#x2F;github.com&#x2F;karpathy&#x2F;micrograd</a></div><br/></div></div></div></div><div id="37258513" class="c"><input type="checkbox" id="c-37258513" checked=""/><div class="controls bullet"><span class="by">montebicyclelo</span><span>|</span><a href="#37257320">prev</a><span>|</span><a href="#37259386">next</a><span>|</span><label class="collapse" for="c-37258513">[-]</label><label class="expand" for="c-37258513">[2 more]</label></div><br/><div class="children"><div class="content">Here&#x27;s my autodiff in 26 lines of Python (2022): <a href="https:&#x2F;&#x2F;gist.github.com&#x2F;sradc&#x2F;d9d66e3898ffe3a02e0b6b266629b042" rel="nofollow noreferrer">https:&#x2F;&#x2F;gist.github.com&#x2F;sradc&#x2F;d9d66e3898ffe3a02e0b6b266629b0...</a></div><br/><div id="37258831" class="c"><input type="checkbox" id="c-37258831" checked=""/><div class="controls bullet"><span class="by">blitzar</span><span>|</span><a href="#37258513">parent</a><span>|</span><a href="#37259386">next</a><span>|</span><label class="collapse" for="c-37258831">[-]</label><label class="expand" for="c-37258831">[1 more]</label></div><br/><div class="children"><div class="content">I appreciate the brevity, but my brain seems to function way better on a healthy dose of white space.<p>I need to practice some of these alternative methods.</div><br/></div></div></div></div><div id="37259386" class="c"><input type="checkbox" id="c-37259386" checked=""/><div class="controls bullet"><span class="by">tromp</span><span>|</span><a href="#37258513">prev</a><span>|</span><a href="#37258594">next</a><span>|</span><label class="collapse" for="c-37259386">[-]</label><label class="expand" for="c-37259386">[1 more]</label></div><br/><div class="children"><div class="content">A concise implementation of automatic differentiation in Haskell: <a href="https:&#x2F;&#x2F;crypto.stanford.edu&#x2F;~blynn&#x2F;haskell&#x2F;ad.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;crypto.stanford.edu&#x2F;~blynn&#x2F;haskell&#x2F;ad.html</a></div><br/></div></div><div id="37258594" class="c"><input type="checkbox" id="c-37258594" checked=""/><div class="controls bullet"><span class="by">Cef111</span><span>|</span><a href="#37259386">prev</a><span>|</span><a href="#37258804">next</a><span>|</span><label class="collapse" for="c-37258594">[-]</label><label class="expand" for="c-37258594">[1 more]</label></div><br/><div class="children"><div class="content">Interesting video by Andrej Karpathy building an autograd engine, quite insightful:<p><a href="https:&#x2F;&#x2F;youtu.be&#x2F;VMj-3S1tku0?si=wuKhELwOwoYbzpt7" rel="nofollow noreferrer">https:&#x2F;&#x2F;youtu.be&#x2F;VMj-3S1tku0?si=wuKhELwOwoYbzpt7</a><p>Repo:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;karpathy&#x2F;micrograd">https:&#x2F;&#x2F;github.com&#x2F;karpathy&#x2F;micrograd</a></div><br/></div></div><div id="37258804" class="c"><input type="checkbox" id="c-37258804" checked=""/><div class="controls bullet"><span class="by">IngoBlechschmid</span><span>|</span><a href="#37258594">prev</a><span>|</span><a href="#37259324">next</a><span>|</span><label class="collapse" for="c-37258804">[-]</label><label class="expand" for="c-37258804">[1 more]</label></div><br/><div class="children"><div class="content">Automatic differentiation feels magical.<p>Many compsci people have been captivated by it and wrote introductions, trying to put the technique into a wider perspective. Here is mine, including a &quot;poor man&#x27;s variant&quot; of automatic differentiation which does without operator overloading, but uses complex numbers instead:<p><a href="https:&#x2F;&#x2F;pizzaseminar.speicherleck.de&#x2F;automatic-differentiation&#x2F;notes.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;pizzaseminar.speicherleck.de&#x2F;automatic-differentiati...</a></div><br/></div></div><div id="37259324" class="c"><input type="checkbox" id="c-37259324" checked=""/><div class="controls bullet"><span class="by">korvalds</span><span>|</span><a href="#37258804">prev</a><span>|</span><a href="#37257921">next</a><span>|</span><label class="collapse" for="c-37259324">[-]</label><label class="expand" for="c-37259324">[1 more]</label></div><br/><div class="children"><div class="content">Very similar to techniques used in Knowledge Based Engineering systems, where the term &quot;dependency tracking&quot; is used. Together with caching of nodes&#x2F;tensors this reduces calculations, especially useful for large parametric 3D models.
When getting a value it will recursively call the binary&#x2F;dependency tree to find out which variables have changed and only recalculate them when needed. Custom python objects and properties with __set__ and __get__ methods makes this a built-in feature of an object-oriented model.<p>x = Tensor(3)<p>y = Tensor(5)<p>z = x + y<p>print(x, y) # 3, 5<p>print(z) # 8<p>x.value = 4 # when setting value nothing is recalculated<p>print(z) # 9 since getting value triggers recalculation of dependencies that have changed</div><br/></div></div><div id="37257921" class="c"><input type="checkbox" id="c-37257921" checked=""/><div class="controls bullet"><span class="by">stkdump</span><span>|</span><a href="#37259324">prev</a><span>|</span><a href="#37258847">next</a><span>|</span><label class="collapse" for="c-37257921">[-]</label><label class="expand" for="c-37257921">[2 more]</label></div><br/><div class="children"><div class="content">The variant of auto differentiation I know doesn&#x27;t build up a graph of operations. Instead it calculates the corresponding value on the fly.</div><br/><div id="37258351" class="c"><input type="checkbox" id="c-37258351" checked=""/><div class="controls bullet"><span class="by">fjkdlsjflkds</span><span>|</span><a href="#37257921">parent</a><span>|</span><a href="#37258847">next</a><span>|</span><label class="collapse" for="c-37258351">[-]</label><label class="expand" for="c-37258351">[1 more]</label></div><br/><div class="children"><div class="content">You are probably thinking of forward-mode autodiff (which is more useful when the dimensionality of the output of your function is comparatively large), which is different from backward-mode autodiff (which is more useful when the dimensionality of the output is comparatively small). Both will work, but one will be more efficient than the other (depending on the context). For things like &quot;training neural networks&quot;, people tend to use backward-mode (since you often want to optimize a <i>single</i> loss output w.r.t. <i>many</i> things).</div><br/></div></div></div></div><div id="37258847" class="c"><input type="checkbox" id="c-37258847" checked=""/><div class="controls bullet"><span class="by">taminka</span><span>|</span><a href="#37257921">prev</a><span>|</span><a href="#37259103">next</a><span>|</span><label class="collapse" for="c-37258847">[-]</label><label class="expand" for="c-37258847">[1 more]</label></div><br/><div class="children"><div class="content">i wish people would just call autodiff (or at least describe it) as numerical chain rule, because that’s quite literally all it is (plus a few tricks for not explicitly computing the jacobian for certain operations), it’s a lot more clear that way</div><br/></div></div><div id="37259103" class="c"><input type="checkbox" id="c-37259103" checked=""/><div class="controls bullet"><span class="by">globular-toast</span><span>|</span><a href="#37258847">prev</a><span>|</span><label class="collapse" for="c-37259103">[-]</label><label class="expand" for="c-37259103">[1 more]</label></div><br/><div class="children"><div class="content">Oh, this is interesting. I thought the title referred to symbolic differentiation (that&#x27;s what you learnt in school, but in a computer). I didn&#x27;t realise automatic differentiation was a different thing.</div><br/></div></div></div></div></div></div></div></body></html>