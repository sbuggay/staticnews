<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1696410064615" as="style"/><link rel="stylesheet" href="styles.css?v=1696410064615"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/NVIDIA/MatX">MatX: Efficient C++17 GPU numerical computing library with Python-like syntax</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>cl3misch</span> | <span>64 comments</span></div><br/><div><div id="37760966" class="c"><input type="checkbox" id="c-37760966" checked=""/><div class="controls bullet"><span class="by">elashri</span><span>|</span><a href="#37758614">next</a><span>|</span><label class="collapse" for="c-37760966">[-]</label><label class="expand" for="c-37760966">[5 more]</label></div><br/><div class="children"><div class="content">&gt; 
    Python&#x2F;Numpy: 5360ms (Xeon(R) CPU E5-2698 v4 @ 2.20GHz)
    CuPy: 10.6ms (A100)
    MatX: 2.54ms (A100)
&gt;<p>Are they even comparing apples to apples to claim that they see these improvements over NumPy?<p>&gt; While the code complexity and length are roughly the same, the MatX version shows a 2100x over the Numpy version, and over 4x faster than the CuPy version on the same GPU.<p>NumPy doesn&#x27;t use GPU by default unless you use something like Jax [1] to compile NumPy code to run on GPUs. I think more honest comparison will mainly compare MatX running on same CPU like NumPy and for GPU comparison focus to compare vs CuPy.<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;jax">https:&#x2F;&#x2F;github.com&#x2F;google&#x2F;jax</a></div><br/><div id="37761723" class="c"><input type="checkbox" id="c-37761723" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#37760966">parent</a><span>|</span><a href="#37761015">next</a><span>|</span><label class="collapse" for="c-37761723">[-]</label><label class="expand" for="c-37761723">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s very much an apples to oranges comparison, but nevertheless relevant, since NumPy is the defacto baseline for anything numeric in Python.</div><br/><div id="37761795" class="c"><input type="checkbox" id="c-37761795" checked=""/><div class="controls bullet"><span class="by">albertzeyer</span><span>|</span><a href="#37760966">root</a><span>|</span><a href="#37761723">parent</a><span>|</span><a href="#37761015">next</a><span>|</span><label class="collapse" for="c-37761795">[-]</label><label class="expand" for="c-37761795">[2 more]</label></div><br/><div class="children"><div class="content">Not really, for GPU computation, PyTorch, TensorFlow or JAX are the baselines (they are not just for ML stuff). Even for CPU, that&#x27;s a more relevant comparison.</div><br/><div id="37761904" class="c"><input type="checkbox" id="c-37761904" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#37760966">root</a><span>|</span><a href="#37761795">parent</a><span>|</span><a href="#37761015">next</a><span>|</span><label class="collapse" for="c-37761904">[-]</label><label class="expand" for="c-37761904">[1 more]</label></div><br/><div class="children"><div class="content">I agree in principle, but my experience is that researchers writing code just default to NumPy.</div><br/></div></div></div></div></div></div><div id="37761015" class="c"><input type="checkbox" id="c-37761015" checked=""/><div class="controls bullet"><span class="by">cburdick13</span><span>|</span><a href="#37760966">parent</a><span>|</span><a href="#37761723">prev</a><span>|</span><a href="#37758614">next</a><span>|</span><label class="collapse" for="c-37761015">[-]</label><label class="expand" for="c-37761015">[1 more]</label></div><br/><div class="children"><div class="content">I agree we should update it given that it&#x27;s also a very old comparison (2 years now). The cuPy comparison should be fair since it was the same GPU.<p>I addressed it more here: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37760120">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37760120</a></div><br/></div></div></div></div><div id="37758614" class="c"><input type="checkbox" id="c-37758614" checked=""/><div class="controls bullet"><span class="by">nologic01</span><span>|</span><a href="#37760966">prev</a><span>|</span><a href="#37762129">next</a><span>|</span><label class="collapse" for="c-37758614">[-]</label><label class="expand" for="c-37758614">[26 more]</label></div><br/><div class="children"><div class="content">I would&#x27;nt touch nvidia &quot;goodies&quot; with a barge pole. Perfectly good 5-year GPU deprecated upon upgrade to ubuntu 22.04 because of cuda&#x2F;drivers. The lock-in trap of the century.</div><br/><div id="37759096" class="c"><input type="checkbox" id="c-37759096" checked=""/><div class="controls bullet"><span class="by">Foobar8568</span><span>|</span><a href="#37758614">parent</a><span>|</span><a href="#37761082">next</a><span>|</span><label class="collapse" for="c-37759096">[-]</label><label class="expand" for="c-37759096">[19 more]</label></div><br/><div class="children"><div class="content">&gt;Please use CUDA 11.4<p>Which should be compatible with any Kepler late architecture, as in 6xx models from 10years ago+?</div><br/><div id="37759232" class="c"><input type="checkbox" id="c-37759232" checked=""/><div class="controls bullet"><span class="by">cburdick13</span><span>|</span><a href="#37758614">root</a><span>|</span><a href="#37759096">parent</a><span>|</span><a href="#37760587">next</a><span>|</span><label class="collapse" for="c-37759232">[-]</label><label class="expand" for="c-37759232">[5 more]</label></div><br/><div class="children"><div class="content">That&#x27;s right, we&#x27;ve tested down to pascal, but this should work on Kepler too since CUDA and the underlying libraries support it.</div><br/><div id="37759605" class="c"><input type="checkbox" id="c-37759605" checked=""/><div class="controls bullet"><span class="by">downrightmike</span><span>|</span><a href="#37758614">root</a><span>|</span><a href="#37759232">parent</a><span>|</span><a href="#37761092">next</a><span>|</span><label class="collapse" for="c-37759605">[-]</label><label class="expand" for="c-37759605">[1 more]</label></div><br/><div class="children"><div class="content">I have a spare Titan Z that could be useful if so.</div><br/></div></div><div id="37761092" class="c"><input type="checkbox" id="c-37761092" checked=""/><div class="controls bullet"><span class="by">choppaface</span><span>|</span><a href="#37758614">root</a><span>|</span><a href="#37759232">parent</a><span>|</span><a href="#37759605">prev</a><span>|</span><a href="#37760587">next</a><span>|</span><label class="collapse" for="c-37761092">[-]</label><label class="expand" for="c-37761092">[3 more]</label></div><br/><div class="children"><div class="content">Will you publish benchmarks for e.g. K80?  Or provide a way for users to contribute?  It&#x27;s really handy to know, e.g. comparable to what is Resnet50 inference on a bunch of architectures.</div><br/><div id="37761126" class="c"><input type="checkbox" id="c-37761126" checked=""/><div class="controls bullet"><span class="by">cburdick13</span><span>|</span><a href="#37758614">root</a><span>|</span><a href="#37761092">parent</a><span>|</span><a href="#37760587">next</a><span>|</span><label class="collapse" for="c-37761126">[-]</label><label class="expand" for="c-37761126">[2 more]</label></div><br/><div class="children"><div class="content">Hi, what specifically are you looking to benchmark on the K80? Users are free to contribute and we&#x27;ve had many external PRs.<p>Contribution guide is here: <a href="https:&#x2F;&#x2F;github.com&#x2F;NVIDIA&#x2F;MatX&#x2F;blob&#x2F;main&#x2F;CONTRIBUTING.md">https:&#x2F;&#x2F;github.com&#x2F;NVIDIA&#x2F;MatX&#x2F;blob&#x2F;main&#x2F;CONTRIBUTING.md</a></div><br/><div id="37761670" class="c"><input type="checkbox" id="c-37761670" checked=""/><div class="controls bullet"><span class="by">choppaface</span><span>|</span><a href="#37758614">root</a><span>|</span><a href="#37761126">parent</a><span>|</span><a href="#37760587">next</a><span>|</span><label class="collapse" for="c-37761670">[-]</label><label class="expand" for="c-37761670">[1 more]</label></div><br/><div class="children"><div class="content">well the root README.md has some &quot;benchmarks&quot; at the very top.  maybe there&#x27;s no existing benchmarks doc with more details? or even a repo of what&#x27;s in the README?  it&#x27;s like if numpy is 5 sec on CPU, but a K80 is only 100ms, then the K80 cost might be worth not going for the A100 at 3ms.  Similar argument for jetson.</div><br/></div></div></div></div></div></div></div></div><div id="37760587" class="c"><input type="checkbox" id="c-37760587" checked=""/><div class="controls bullet"><span class="by">dheera</span><span>|</span><a href="#37758614">root</a><span>|</span><a href="#37759096">parent</a><span>|</span><a href="#37759232">prev</a><span>|</span><a href="#37761082">next</a><span>|</span><label class="collapse" for="c-37760587">[-]</label><label class="expand" for="c-37760587">[13 more]</label></div><br/><div class="children"><div class="content">The real issue is having to keep multiple copies of CUDA on a system to satisfy all the different moving parts that each want different CUDA versions and CUDNN versions, and those different CUDA installers fight over and uninstall each others&#x27; nvidia driver versions because CUDA is shipped with the nvidia driver, bundled and listed as an apt dependency.<p>My system changes from nvidia-525 to nvidia-535 to nvidia-520 to nvidia-515 on a daily basis because I need to reinstall a different CUDA version just to try some new paper&#x27;s code.<p>PyTorch did it right, it now ships with its own CUDA and doesn&#x27;t take a shit about version what you have in &#x2F;usr&#x2F;local. Everything else should do the same.</div><br/><div id="37761771" class="c"><input type="checkbox" id="c-37761771" checked=""/><div class="controls bullet"><span class="by">adev_</span><span>|</span><a href="#37758614">root</a><span>|</span><a href="#37760587">parent</a><span>|</span><a href="#37761745">next</a><span>|</span><label class="collapse" for="c-37761771">[-]</label><label class="expand" for="c-37761771">[1 more]</label></div><br/><div class="children"><div class="content">&gt; now ships with its own CUDA and doesn&#x27;t take a shit about version<p>Bundling system dependency in python software is always a giant shit show. I would not name that &quot;Everything should do the same&quot;<p>Any other python library (randomly cuPy) that also ship its own Cuda, with its own different version, will segfault happily because you have duplicated symbols in a single process.</div><br/></div></div><div id="37761745" class="c"><input type="checkbox" id="c-37761745" checked=""/><div class="controls bullet"><span class="by">adev_</span><span>|</span><a href="#37758614">root</a><span>|</span><a href="#37760587">parent</a><span>|</span><a href="#37761771">prev</a><span>|</span><a href="#37760845">next</a><span>|</span><label class="collapse" for="c-37761745">[-]</label><label class="expand" for="c-37761745">[2 more]</label></div><br/><div class="children"><div class="content">&gt; PyTorch did it right, it now ships with its own CUDA and doesn&#x27;t take a shit about version what you have in &#x2F;usr&#x2F;local<p>Is that even legal ?<p>Up to my understanding Cuda is covered by an EULA license that explicitly require end user agreement.</div><br/><div id="37762441" class="c"><input type="checkbox" id="c-37762441" checked=""/><div class="controls bullet"><span class="by">dheera</span><span>|</span><a href="#37758614">root</a><span>|</span><a href="#37761745">parent</a><span>|</span><a href="#37760845">next</a><span>|</span><label class="collapse" for="c-37762441">[-]</label><label class="expand" for="c-37762441">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Meh&quot; who cares. It works.</div><br/></div></div></div></div><div id="37760845" class="c"><input type="checkbox" id="c-37760845" checked=""/><div class="controls bullet"><span class="by">the_svd_doctor</span><span>|</span><a href="#37758614">root</a><span>|</span><a href="#37760587">parent</a><span>|</span><a href="#37761745">prev</a><span>|</span><a href="#37760691">next</a><span>|</span><label class="collapse" for="c-37760845">[-]</label><label class="expand" for="c-37760845">[4 more]</label></div><br/><div class="children"><div class="content">The solution is to install the latest CUDA driver (which is always 100% backward compatible) but have multiple version of the libraries in &#x2F;usr&#x2F;local. Granted it can get messy with various package manager doing their own things.</div><br/><div id="37760921" class="c"><input type="checkbox" id="c-37760921" checked=""/><div class="controls bullet"><span class="by">dheera</span><span>|</span><a href="#37758614">root</a><span>|</span><a href="#37760845">parent</a><span>|</span><a href="#37760691">next</a><span>|</span><label class="collapse" for="c-37760921">[-]</label><label class="expand" for="c-37760921">[3 more]</label></div><br/><div class="children"><div class="content">Usually the CUDA versions end up uninstalling other CUDA versions and drivers due to package manager issues.<p>I wish nvidia would just release a `sudo apt install cuda-all` that just keeps you updated with ALL possible cuda versions simultaneously. I know it would be 20GB but that&#x27;s fine, it&#x27;s a drop in the bucket compared to the datasets I play with.</div><br/><div id="37761029" class="c"><input type="checkbox" id="c-37761029" checked=""/><div class="controls bullet"><span class="by">cburdick13</span><span>|</span><a href="#37758614">root</a><span>|</span><a href="#37760921">parent</a><span>|</span><a href="#37760691">next</a><span>|</span><label class="collapse" for="c-37761029">[-]</label><label class="expand" for="c-37761029">[2 more]</label></div><br/><div class="children"><div class="content">If you use the run files rather than the package manager files it&#x27;s always installed in completely separate folders inside &#x2F;use&#x2F;local&#x2F;cuda.<p>What you&#x27;re describing with an &quot;all&quot; install can somewhat be accomplished with containers right now and none of the dependency problems.</div><br/><div id="37761053" class="c"><input type="checkbox" id="c-37761053" checked=""/><div class="controls bullet"><span class="by">dheera</span><span>|</span><a href="#37758614">root</a><span>|</span><a href="#37761029">parent</a><span>|</span><a href="#37760691">next</a><span>|</span><label class="collapse" for="c-37761053">[-]</label><label class="expand" for="c-37761053">[1 more]</label></div><br/><div class="children"><div class="content">But containers, X forwarding, visualization, and OpenGL is a complete shitshow. I have apps that work outside of a container but freeze the UI that the mouse pointer won&#x27;t even work when run from within a container.</div><br/></div></div></div></div></div></div></div></div><div id="37760691" class="c"><input type="checkbox" id="c-37760691" checked=""/><div class="controls bullet"><span class="by">kristjansson</span><span>|</span><a href="#37758614">root</a><span>|</span><a href="#37760587">parent</a><span>|</span><a href="#37760845">prev</a><span>|</span><a href="#37761082">next</a><span>|</span><label class="collapse" for="c-37760691">[-]</label><label class="expand" for="c-37760691">[5 more]</label></div><br/><div class="children"><div class="content">Don’t install the drivers with CUDA? Don’t rely on distro-packaged CUDA?</div><br/><div id="37760894" class="c"><input type="checkbox" id="c-37760894" checked=""/><div class="controls bullet"><span class="by">dheera</span><span>|</span><a href="#37758614">root</a><span>|</span><a href="#37760691">parent</a><span>|</span><a href="#37761082">next</a><span>|</span><label class="collapse" for="c-37760894">[-]</label><label class="expand" for="c-37760894">[4 more]</label></div><br/><div class="children"><div class="content">NVIDIA&#x27;s official CUDA is packaged with their damn driver.</div><br/><div id="37761011" class="c"><input type="checkbox" id="c-37761011" checked=""/><div class="controls bullet"><span class="by">cburdick13</span><span>|</span><a href="#37758614">root</a><span>|</span><a href="#37760894">parent</a><span>|</span><a href="#37761082">next</a><span>|</span><label class="collapse" for="c-37761011">[-]</label><label class="expand" for="c-37761011">[3 more]</label></div><br/><div class="children"><div class="content">Hi, you can choose not to install the driver with a CUDA install, and to download the driver separately.</div><br/><div id="37761070" class="c"><input type="checkbox" id="c-37761070" checked=""/><div class="controls bullet"><span class="by">dheera</span><span>|</span><a href="#37758614">root</a><span>|</span><a href="#37761011">parent</a><span>|</span><a href="#37761082">next</a><span>|</span><label class="collapse" for="c-37761070">[-]</label><label class="expand" for="c-37761070">[2 more]</label></div><br/><div class="children"><div class="content">Only if you use the runfile, but that breaks yet other packages that specify &#x27;cuda&#x27; as an apt dependency.<p>Your &#x27;cuda&#x27; packages should use &#x27;&gt;=&#x27; not &#x27;==&#x27;<p>e.g. &#x27;cuda-12&#x27; should depend on nvidia&gt;=525 NOT nvidia==525</div><br/><div id="37761131" class="c"><input type="checkbox" id="c-37761131" checked=""/><div class="controls bullet"><span class="by">cburdick13</span><span>|</span><a href="#37758614">root</a><span>|</span><a href="#37761070">parent</a><span>|</span><a href="#37761082">next</a><span>|</span><label class="collapse" for="c-37761131">[-]</label><label class="expand" for="c-37761131">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a fair point. I&#x27;ll look into it.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="37761082" class="c"><input type="checkbox" id="c-37761082" checked=""/><div class="controls bullet"><span class="by">choppaface</span><span>|</span><a href="#37758614">parent</a><span>|</span><a href="#37759096">prev</a><span>|</span><a href="#37759286">next</a><span>|</span><label class="collapse" for="c-37761082">[-]</label><label class="expand" for="c-37761082">[1 more]</label></div><br/><div class="children"><div class="content">Surprisingly this repo is BSD licensed so it might even outgrow nvidia.  Eigen is pretty strong but this new MatX syntax might catch on, especially with easy GPU integration.</div><br/></div></div><div id="37759286" class="c"><input type="checkbox" id="c-37759286" checked=""/><div class="controls bullet"><span class="by">smoldesu</span><span>|</span><a href="#37758614">parent</a><span>|</span><a href="#37761082">prev</a><span>|</span><a href="#37759916">next</a><span>|</span><label class="collapse" for="c-37759286">[-]</label><label class="expand" for="c-37759286">[3 more]</label></div><br/><div class="children"><div class="content">Are you talking about for desktop&#x2F;GUI use? A lot of older Nvidia hardware has been implicitly depreciated for years. Without newer patches to help Wayland support, your options are either to use nouveau or stick with unsupported software and official drivers. It&#x27;s ugly, but Microsoft and Apple pretty much depreciate hardware the same way; first with &#x27;recommended&#x27; cutoff points, then hard depreciations. On Linux it&#x27;s less spelled-out, but things seem to be Wayland-or-bust now.</div><br/><div id="37760118" class="c"><input type="checkbox" id="c-37760118" checked=""/><div class="controls bullet"><span class="by">reaperman</span><span>|</span><a href="#37758614">root</a><span>|</span><a href="#37759286">parent</a><span>|</span><a href="#37760141">next</a><span>|</span><label class="collapse" for="c-37760118">[-]</label><label class="expand" for="c-37760118">[1 more]</label></div><br/><div class="children"><div class="content">*deprecate or “EOL” (for continuing support). Depreciate is a tax and accounting concept, though often at the end of a MACRS depreciation schedule office equipment is often immediately replaced since it’s no longer contributing to tax write-offs. (Really, just not replaced as long as it continues to contribute to tax deductions)</div><br/></div></div><div id="37760141" class="c"><input type="checkbox" id="c-37760141" checked=""/><div class="controls bullet"><span class="by">cburdick13</span><span>|</span><a href="#37758614">root</a><span>|</span><a href="#37759286">parent</a><span>|</span><a href="#37760118">prev</a><span>|</span><a href="#37759916">next</a><span>|</span><label class="collapse" for="c-37760141">[-]</label><label class="expand" for="c-37760141">[1 more]</label></div><br/><div class="children"><div class="content">It was mentioned elsewhere, but we support down to cuda 11.4, which supports down to the Maxwell architecture (nearly 10 years old now).</div><br/></div></div></div></div><div id="37759916" class="c"><input type="checkbox" id="c-37759916" checked=""/><div class="controls bullet"><span class="by">arthurcolle</span><span>|</span><a href="#37758614">parent</a><span>|</span><a href="#37759286">prev</a><span>|</span><a href="#37762129">next</a><span>|</span><label class="collapse" for="c-37759916">[-]</label><label class="expand" for="c-37759916">[2 more]</label></div><br/><div class="children"><div class="content">can you downgrade?</div><br/></div></div></div></div><div id="37762129" class="c"><input type="checkbox" id="c-37762129" checked=""/><div class="controls bullet"><span class="by">cxx</span><span>|</span><a href="#37758614">prev</a><span>|</span><a href="#37758616">next</a><span>|</span><label class="collapse" for="c-37762129">[-]</label><label class="expand" for="c-37762129">[1 more]</label></div><br/><div class="children"><div class="content">This looks great! I&#x27;m definitely going to try this, I&#x27;ve been hoping for something like this for a long time. Eigen is great of course but GPU acceleration is a bit clunky to use, this looks much closer to the code written by the researchers.</div><br/></div></div><div id="37758616" class="c"><input type="checkbox" id="c-37758616" checked=""/><div class="controls bullet"><span class="by">cburdick13</span><span>|</span><a href="#37762129">prev</a><span>|</span><a href="#37758118">next</a><span>|</span><label class="collapse" for="c-37758616">[-]</label><label class="expand" for="c-37758616">[5 more]</label></div><br/><div class="children"><div class="content">Hi all, I&#x27;m one of the maintainers of MatX. I didn&#x27;t expect it to hit HN this soon, but happy to answer any questions.</div><br/><div id="37761862" class="c"><input type="checkbox" id="c-37761862" checked=""/><div class="controls bullet"><span class="by">albertzeyer</span><span>|</span><a href="#37758616">parent</a><span>|</span><a href="#37761756">next</a><span>|</span><label class="collapse" for="c-37761862">[-]</label><label class="expand" for="c-37761862">[1 more]</label></div><br/><div class="children"><div class="content">I think a comparison to PyTorch, TensorFlow and&#x2F;or JAX is more relevant than a comparison to CuPy&#x2F;NumPy.<p>And then maybe also a comparison to Flashlight (<a href="https:&#x2F;&#x2F;github.com&#x2F;flashlight&#x2F;flashlight">https:&#x2F;&#x2F;github.com&#x2F;flashlight&#x2F;flashlight</a>), xtensor (<a href="https:&#x2F;&#x2F;github.com&#x2F;xtensor-stack&#x2F;xtensor">https:&#x2F;&#x2F;github.com&#x2F;xtensor-stack&#x2F;xtensor</a>) or other C&#x2F;C++ based ML&#x2F;computing libraries?<p>Also, there is no mention of it, so I suppose this does not support automatic differentiation?</div><br/></div></div><div id="37761756" class="c"><input type="checkbox" id="c-37761756" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#37758616">parent</a><span>|</span><a href="#37761862">prev</a><span>|</span><a href="#37759579">next</a><span>|</span><label class="collapse" for="c-37761756">[-]</label><label class="expand" for="c-37761756">[1 more]</label></div><br/><div class="children"><div class="content">Actually just started looking into MatX yesterday to accelerate our radar pipeline. Really interesting to see that this use-case is heavily featured in the documentation.<p>Is the UCLA&#x2F;Nvidia&#x2F;Raytheon collaboration (as presented in a recent GTC talk) a major force behind the development of MatX?</div><br/></div></div><div id="37759579" class="c"><input type="checkbox" id="c-37759579" checked=""/><div class="controls bullet"><span class="by">iamlemec</span><span>|</span><a href="#37758616">parent</a><span>|</span><a href="#37761756">prev</a><span>|</span><a href="#37758118">next</a><span>|</span><label class="collapse" for="c-37759579">[-]</label><label class="expand" for="c-37759579">[2 more]</label></div><br/><div class="children"><div class="content">Thanks, looks really interesting. Do functions like matmul support inputs of differing type, like say A=int8 and B=float? Would be nice if you could get memory efficient quantized matmul with operator fusion.</div><br/><div id="37759799" class="c"><input type="checkbox" id="c-37759799" checked=""/><div class="controls bullet"><span class="by">cburdick13</span><span>|</span><a href="#37758616">root</a><span>|</span><a href="#37759579">parent</a><span>|</span><a href="#37758118">next</a><span>|</span><label class="collapse" for="c-37759799">[-]</label><label class="expand" for="c-37759799">[1 more]</label></div><br/><div class="children"><div class="content">We typically support whatever the underlying library supports. For int8 the support would come from cuBLASLt currently. I don&#x27;t believe that or Cutlass supports mixed precision inputs, but I can check.</div><br/></div></div></div></div></div></div><div id="37758118" class="c"><input type="checkbox" id="c-37758118" checked=""/><div class="controls bullet"><span class="by">26fingies</span><span>|</span><a href="#37758616">prev</a><span>|</span><a href="#37758847">next</a><span>|</span><label class="collapse" for="c-37758118">[-]</label><label class="expand" for="c-37758118">[10 more]</label></div><br/><div class="children"><div class="content">very nice but i assume it’s going to be limited to cuda as a backend because nvidia</div><br/><div id="37758635" class="c"><input type="checkbox" id="c-37758635" checked=""/><div class="controls bullet"><span class="by">cburdick13</span><span>|</span><a href="#37758118">parent</a><span>|</span><a href="#37758187">next</a><span>|</span><label class="collapse" for="c-37758635">[-]</label><label class="expand" for="c-37758635">[2 more]</label></div><br/><div class="children"><div class="content">Currently we support both CUDA and CPU to some extent. CPU is done through standard C++ (and soon stdpar). Obviously standard C++ is problematic since it doesn&#x27;t include everything we support (FFTs, matrix multiplies, etc). One option is to use open-source libraries that do these, but then it ends up being a lot of dependencies that are hard to manage. We have plans to improve CPU support soon, so stay tuned.</div><br/><div id="37760556" class="c"><input type="checkbox" id="c-37760556" checked=""/><div class="controls bullet"><span class="by">rossy</span><span>|</span><a href="#37758118">root</a><span>|</span><a href="#37758635">parent</a><span>|</span><a href="#37758187">next</a><span>|</span><label class="collapse" for="c-37760556">[-]</label><label class="expand" for="c-37760556">[1 more]</label></div><br/><div class="children"><div class="content">Vulkan Compute would be nice.</div><br/></div></div></div></div><div id="37758187" class="c"><input type="checkbox" id="c-37758187" checked=""/><div class="controls bullet"><span class="by">pjmlp</span><span>|</span><a href="#37758118">parent</a><span>|</span><a href="#37758635">prev</a><span>|</span><a href="#37758174">next</a><span>|</span><label class="collapse" for="c-37758187">[-]</label><label class="expand" for="c-37758187">[5 more]</label></div><br/><div class="children"><div class="content">Others are free to provide proper C++ support on their GPUs.</div><br/><div id="37758516" class="c"><input type="checkbox" id="c-37758516" checked=""/><div class="controls bullet"><span class="by">SpaceNoodled</span><span>|</span><a href="#37758118">root</a><span>|</span><a href="#37758187">parent</a><span>|</span><a href="#37758174">next</a><span>|</span><label class="collapse" for="c-37758516">[-]</label><label class="expand" for="c-37758516">[4 more]</label></div><br/><div class="children"><div class="content">They&#x27;re also free to write math libraries that don&#x27;t depend on GPUs.</div><br/><div id="37760093" class="c"><input type="checkbox" id="c-37760093" checked=""/><div class="controls bullet"><span class="by">spicybright</span><span>|</span><a href="#37758118">root</a><span>|</span><a href="#37758516">parent</a><span>|</span><a href="#37758641">next</a><span>|</span><label class="collapse" for="c-37760093">[-]</label><label class="expand" for="c-37760093">[2 more]</label></div><br/><div class="children"><div class="content">The performance boost you get from using a GPU is incredible.<p>You might as well say they&#x27;re free to make monitors that don&#x27;t rely on color.</div><br/><div id="37760333" class="c"><input type="checkbox" id="c-37760333" checked=""/><div class="controls bullet"><span class="by">Mr_P</span><span>|</span><a href="#37758118">root</a><span>|</span><a href="#37760093">parent</a><span>|</span><a href="#37758641">next</a><span>|</span><label class="collapse" for="c-37760333">[-]</label><label class="expand" for="c-37760333">[1 more]</label></div><br/><div class="children"><div class="content">GPU performance per dollar is only competitive for specific workloads. For extremely large scale compute, getting enough data center GPUs can also be challenging.</div><br/></div></div></div></div><div id="37758641" class="c"><input type="checkbox" id="c-37758641" checked=""/><div class="controls bullet"><span class="by">cburdick13</span><span>|</span><a href="#37758118">root</a><span>|</span><a href="#37758516">parent</a><span>|</span><a href="#37760093">prev</a><span>|</span><a href="#37758174">next</a><span>|</span><label class="collapse" for="c-37758641">[-]</label><label class="expand" for="c-37758641">[1 more]</label></div><br/><div class="children"><div class="content">Hi, MatX currently has partial support for CPUs too. Please see this comment:<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37758635">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37758635</a></div><br/></div></div></div></div></div></div><div id="37758174" class="c"><input type="checkbox" id="c-37758174" checked=""/><div class="controls bullet"><span class="by">__loam</span><span>|</span><a href="#37758118">parent</a><span>|</span><a href="#37758187">prev</a><span>|</span><a href="#37758847">next</a><span>|</span><label class="collapse" for="c-37758174">[-]</label><label class="expand" for="c-37758174">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t actually know a lot about massively parallel libraries like CUDA. Does AMD have an equivalent technology associate with their GPUs? It feels like it should be fairly straightforward to create some kind of high level library that just uses CUDA or whatever AMD has on the back end.</div><br/><div id="37758424" class="c"><input type="checkbox" id="c-37758424" checked=""/><div class="controls bullet"><span class="by">cmovq</span><span>|</span><a href="#37758118">root</a><span>|</span><a href="#37758174">parent</a><span>|</span><a href="#37758847">next</a><span>|</span><label class="collapse" for="c-37758424">[-]</label><label class="expand" for="c-37758424">[1 more]</label></div><br/><div class="children"><div class="content">Traditionally OpenCL was the alternative to CUDA. Recently AMD has been pushing their ROCm platform.</div><br/></div></div></div></div></div></div><div id="37758847" class="c"><input type="checkbox" id="c-37758847" checked=""/><div class="controls bullet"><span class="by">trostaft</span><span>|</span><a href="#37758118">prev</a><span>|</span><a href="#37760562">next</a><span>|</span><label class="collapse" for="c-37758847">[-]</label><label class="expand" for="c-37758847">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s nice to have easier to use C++ GPU numerical computing libraries, but it seems like there&#x27;s a good bit of work to go in terms of feature set. Taking CuPy as an example, they&#x27;ve nearly entirely mirrored the enormous featureset of both NumPy and SciPy.<p>Although I&#x27;m not expecting on that scale here, but what&#x27;s the planned future work here? E.g. matrix free operators and operations, sparse matrix support, or symmetry aware eigenvalue &#x2F; singular value decompositions? There are official cuda libraries that support these (e.g. cuSPARSE).<p>Otherwise this looks good! Documentation is decent too.</div><br/><div id="37759086" class="c"><input type="checkbox" id="c-37759086" checked=""/><div class="controls bullet"><span class="by">cburdick13</span><span>|</span><a href="#37758847">parent</a><span>|</span><a href="#37760562">next</a><span>|</span><label class="collapse" for="c-37759086">[-]</label><label class="expand" for="c-37759086">[1 more]</label></div><br/><div class="children"><div class="content">Hi, having feature parity with cuPy is a daunting task, especially for a C++ library. At this point we feel we have a good foundation for all kinds of basic and advanced tensor manipulations, and have a growing number of library-based functions on top of that. The low-hanging fruit was wrapping the CUDA math libraries, so that has the most progress.<p>Since MatX was originally intended for streaming&#x2F;real-time processing the focus has been on making C++ for CUDA easier to use for those kinds of applications.<p>There are also a lot of things in cuPy and sciPy that don&#x27;t make a lot of sense to do on the GPU, like  offline tasks such as filter design  in signal processing.<p>Also, since C++ users typically are writing in that language for maximum performance, we&#x27;ve put a large focus on making sure we are as close to writing optimized CUDA as possible. In general, most workloads we&#x27;ve tested are about 3-4x faster than the cuPy counterpart due to better fusion and language overhead.<p>We have discussed supporting cusparse as well, but cusparse typically requires a different input type like a CSR matrix. This is not something you&#x27;d typically want to detect&#x2F;convert, so we&#x27;re still discussing ways of getting this integrated cleanly.<p>We do have several versions of SVD, including one that calls cuSolver.<p>Feedback is always appreciated!</div><br/></div></div></div></div><div id="37760562" class="c"><input type="checkbox" id="c-37760562" checked=""/><div class="controls bullet"><span class="by">LeroyRaz</span><span>|</span><a href="#37758847">prev</a><span>|</span><a href="#37760802">next</a><span>|</span><label class="collapse" for="c-37760562">[-]</label><label class="expand" for="c-37760562">[4 more]</label></div><br/><div class="children"><div class="content">What are the advantages of MatX over Jax?</div><br/><div id="37760924" class="c"><input type="checkbox" id="c-37760924" checked=""/><div class="controls bullet"><span class="by">cburdick13</span><span>|</span><a href="#37760562">parent</a><span>|</span><a href="#37760802">next</a><span>|</span><label class="collapse" for="c-37760924">[-]</label><label class="expand" for="c-37760924">[3 more]</label></div><br/><div class="children"><div class="content">The main difference is Jax is for python primarily, while MatX is c++. This might seem like a poor answer, but in many domains (quasi-real time, signal processing, etc) the language is important to give certainty guarantees on performance.<p>MatX has been used in several projects with hundreds of microsecond deadlines, which is not usually something you&#x27;d choose Python for out of the box.<p>We are instead targeting users who already have python or high-level code that they need to port to c++ for whatever reason, and want to do it in the easiest way possible. With c++17 we&#x27;re able to provide a simple syntax without compromising on performance compared to most native code.</div><br/><div id="37761043" class="c"><input type="checkbox" id="c-37761043" checked=""/><div class="controls bullet"><span class="by">temren</span><span>|</span><a href="#37760562">root</a><span>|</span><a href="#37760924">parent</a><span>|</span><a href="#37760802">next</a><span>|</span><label class="collapse" for="c-37761043">[-]</label><label class="expand" for="c-37761043">[2 more]</label></div><br/><div class="children"><div class="content">would matx work with gnuradio?</div><br/><div id="37761147" class="c"><input type="checkbox" id="c-37761147" checked=""/><div class="controls bullet"><span class="by">cburdick13</span><span>|</span><a href="#37760562">root</a><span>|</span><a href="#37761043">parent</a><span>|</span><a href="#37760802">next</a><span>|</span><label class="collapse" for="c-37761147">[-]</label><label class="expand" for="c-37761147">[1 more]</label></div><br/><div class="children"><div class="content">I have to admit I&#x27;m only tangentially familiar with gnuradio, but matx should be able to integrate with any C++17 codebase. We have several examples of integration with our streaming sensor pipeline called holoscan.<p>see this radar pipeline example for one:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;nvidia-holoscan&#x2F;holohub&#x2F;tree&#x2F;main&#x2F;applications&#x2F;simple_radar_pipeline&#x2F;cpp">https:&#x2F;&#x2F;github.com&#x2F;nvidia-holoscan&#x2F;holohub&#x2F;tree&#x2F;main&#x2F;applica...</a></div><br/></div></div></div></div></div></div></div></div><div id="37760802" class="c"><input type="checkbox" id="c-37760802" checked=""/><div class="controls bullet"><span class="by">alfalfasprout</span><span>|</span><a href="#37760562">prev</a><span>|</span><a href="#37759164">next</a><span>|</span><label class="collapse" for="c-37760802">[-]</label><label class="expand" for="c-37760802">[2 more]</label></div><br/><div class="children"><div class="content">How does this compare to xtensor other than being GPU specific?</div><br/><div id="37760890" class="c"><input type="checkbox" id="c-37760890" checked=""/><div class="controls bullet"><span class="by">cburdick13</span><span>|</span><a href="#37760802">parent</a><span>|</span><a href="#37759164">next</a><span>|</span><label class="collapse" for="c-37760890">[-]</label><label class="expand" for="c-37760890">[1 more]</label></div><br/><div class="children"><div class="content">The main difference is the GPU part. This is a large difference because the same lazy evaluated template type can be run on the CPU or GPU through what we call an executor. On the CPU it&#x27;s likely very similar to how xtensor is already using expression trees, but on the GPU it&#x27;s quite a bit different because if the libraries backing it and optimized kernels.<p>The syntax of MatX also allows us to do more kernel fusion in the future to improve performance without any changes.</div><br/></div></div></div></div><div id="37759164" class="c"><input type="checkbox" id="c-37759164" checked=""/><div class="controls bullet"><span class="by">waynecochran</span><span>|</span><a href="#37760802">prev</a><span>|</span><a href="#37759447">next</a><span>|</span><label class="collapse" for="c-37759164">[-]</label><label class="expand" for="c-37759164">[2 more]</label></div><br/><div class="children"><div class="content">I’d be interested in a comparison with Eigen for Linear Algebra workflows. I would love to perform eigen analysis or svd on large matrices using CUDA.</div><br/><div id="37759251" class="c"><input type="checkbox" id="c-37759251" checked=""/><div class="controls bullet"><span class="by">cburdick13</span><span>|</span><a href="#37759164">parent</a><span>|</span><a href="#37759447">next</a><span>|</span><label class="collapse" for="c-37759251">[-]</label><label class="expand" for="c-37759251">[1 more]</label></div><br/><div class="children"><div class="content">Hi, what matrix sizes and types are you working with and how many batches? In general it sounds be similar to eigen, but with GPU support. We have several svd methods for different scenarios, so if you give us the info above we can let you know the performance.</div><br/></div></div></div></div><div id="37759447" class="c"><input type="checkbox" id="c-37759447" checked=""/><div class="controls bullet"><span class="by">gyrovagueGeist</span><span>|</span><a href="#37759164">prev</a><span>|</span><a href="#37759018">next</a><span>|</span><label class="collapse" for="c-37759447">[-]</label><label class="expand" for="c-37759447">[2 more]</label></div><br/><div class="children"><div class="content">Very nice! I think the landing README should really be a comparison with CuPy and not NumPy though</div><br/><div id="37760120" class="c"><input type="checkbox" id="c-37760120" checked=""/><div class="controls bullet"><span class="by">cburdick13</span><span>|</span><a href="#37759447">parent</a><span>|</span><a href="#37759018">next</a><span>|</span><label class="collapse" for="c-37760120">[-]</label><label class="expand" for="c-37760120">[1 more]</label></div><br/><div class="children"><div class="content">Good point, and agreed the landing page is a bit sensational. I mentioned it elsewhere but between MatX and cuPy we see a 3-4x performance difference on average. The gap tends to widen with more complex workflows where compile-time kernel fusion gives more improvements compared to something like a single GEMM.</div><br/></div></div></div></div><div id="37759018" class="c"><input type="checkbox" id="c-37759018" checked=""/><div class="controls bullet"><span class="by">frozenport</span><span>|</span><a href="#37759447">prev</a><span>|</span><label class="collapse" for="c-37759018">[-]</label><label class="expand" for="c-37759018">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve preferred using libtorch for translating python to C++.<p>I feel this effort is poorly differentiated compared to previous efforts.</div><br/><div id="37759124" class="c"><input type="checkbox" id="c-37759124" checked=""/><div class="controls bullet"><span class="by">cburdick13</span><span>|</span><a href="#37759018">parent</a><span>|</span><label class="collapse" for="c-37759124">[-]</label><label class="expand" for="c-37759124">[3 more]</label></div><br/><div class="children"><div class="content">Hi, besides an (subjectively) easier syntax, the performance should be higher compared to libtorch. Every operator expression (think of it as an arithmetic expression) is evaluated at compile-time and is often fused into a single GPU kernel. This also removes the need for JITing. If there&#x27;s a specific workflow you&#x27;re curious about comparing libtorch vs MatX please let us know and we can try it out.</div><br/><div id="37759216" class="c"><input type="checkbox" id="c-37759216" checked=""/><div class="controls bullet"><span class="by">engfan</span><span>|</span><a href="#37759018">root</a><span>|</span><a href="#37759124">parent</a><span>|</span><label class="collapse" for="c-37759216">[-]</label><label class="expand" for="c-37759216">[2 more]</label></div><br/><div class="children"><div class="content">Why can’t you keep the calling interfaces of functions as close to the Py libraries as possible, simplifying the transition for everyone?  Will that really destroy the performance increase?  Common calling interfaces make everything much simpler.  Even in this simple example, the calls differ significantly.</div><br/><div id="37759322" class="c"><input type="checkbox" id="c-37759322" checked=""/><div class="controls bullet"><span class="by">cburdick13</span><span>|</span><a href="#37759018">root</a><span>|</span><a href="#37759216">parent</a><span>|</span><label class="collapse" for="c-37759322">[-]</label><label class="expand" for="c-37759322">[1 more]</label></div><br/><div class="children"><div class="content">We&#x27;ve tried our best to match python as well as we can, or falling back to matlab-style if Python doesn&#x27;t have it. Many of our unit tests are verified against python, so the conversion is typically very easy. The one thing that python has that makes this much easier is keyword arguments. We&#x27;ve tried to use overloads to mimic this as best as we could.<p>That being said, in the example on the home page there are notable differences:<p>1) we have the run() method. The reason is that the expression before the run is lazily evaluated for performance and does not execute anything. Having the run() method allows you to run the same line of code on either a CPU or GPU by changing the argument to run()<p>2) in MatX memory allocation is explicit. Python does it as-needed, but this causes a performance penalty with allocations and deallocations that are not under your control. Specifically in the FFT example, numPy will allocate an ndarray prior to calling it, but on the same line. In MatX the allocation is (typically) done before the operation so you can control the performance of the hot path of code.<p>If you have any specific suggestions, we would love to hear it</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>