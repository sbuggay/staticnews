<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1686646858271" as="style"/><link rel="stylesheet" href="styles.css?v=1686646858271"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://snap-research.github.io/SnapFusion/">SnapFusion: Text-to-Image Diffusion Model on Mobile Devices Within Two Seconds</a> <span class="domain">(<a href="https://snap-research.github.io">snap-research.github.io</a>)</span></div><div class="subtext"><span>synapse26</span> | <span>24 comments</span></div><br/><div><div id="36307083" class="c"><input type="checkbox" id="c-36307083" checked=""/><div class="controls bullet"><span class="by">schappim</span><span>|</span><a href="#36305694">next</a><span>|</span><label class="collapse" for="c-36307083">[-]</label><label class="expand" for="c-36307083">[2 more]</label></div><br/><div class="children"><div class="content">Sub 2 second generations on cell phones, nice! Better FID and CLIP scores than Stable Diffusion v1.5 with 50 steps, great!<p>So are they gonna release the code, or do they only open-source ad-SDKs[1]?<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;orgs&#x2F;Snapchat&#x2F;repositories">https:&#x2F;&#x2F;github.com&#x2F;orgs&#x2F;Snapchat&#x2F;repositories</a></div><br/><div id="36307252" class="c"><input type="checkbox" id="c-36307252" checked=""/><div class="controls bullet"><span class="by">suyash</span><span>|</span><a href="#36307083">parent</a><span>|</span><a href="#36305694">next</a><span>|</span><label class="collapse" for="c-36307252">[-]</label><label class="expand" for="c-36307252">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ll believe thier hypothesis when I can run the open source code on my iPhone in 2 seconds, doubt it&#x27;s that fast.</div><br/></div></div></div></div><div id="36305694" class="c"><input type="checkbox" id="c-36305694" checked=""/><div class="controls bullet"><span class="by">devadvance</span><span>|</span><a href="#36307083">prev</a><span>|</span><a href="#36307205">next</a><span>|</span><label class="collapse" for="c-36305694">[-]</label><label class="expand" for="c-36305694">[1 more]</label></div><br/><div class="children"><div class="content">From the paper:<p>&gt; In this work, we present the first text-to-image diffusion model that generates an image on mobile
devices in less than 2 seconds. To achieve this, we mainly focus on improving the slow inference speed
of the UNet and reducing the number of necessary denoising steps.<p>As a layman, it&#x27;s impressive and surprising that there&#x27;s so much room for optimization here, given the number of hands on folks in the OSS space.<p>&gt; We propose a novel evolving training framework to obtain an efficient UNet that performs better
than the original Stable Diffusion v1.52 while being significantly faster. We also introduce a data
distillation pipeline to compress and accelerate the image decoder.<p>Pretty impressive.</div><br/></div></div><div id="36307205" class="c"><input type="checkbox" id="c-36307205" checked=""/><div class="controls bullet"><span class="by">rbinv</span><span>|</span><a href="#36305694">prev</a><span>|</span><a href="#36305597">next</a><span>|</span><label class="collapse" for="c-36307205">[-]</label><label class="expand" for="c-36307205">[1 more]</label></div><br/><div class="children"><div class="content">Man, I still remember waiting literally all night for a Mandelbrot render to finish on my 486. We&#x27;ve come a long way.</div><br/></div></div><div id="36305597" class="c"><input type="checkbox" id="c-36305597" checked=""/><div class="controls bullet"><span class="by">taylorfinley</span><span>|</span><a href="#36307205">prev</a><span>|</span><a href="#36307027">next</a><span>|</span><label class="collapse" for="c-36305597">[-]</label><label class="expand" for="c-36305597">[2 more]</label></div><br/><div class="children"><div class="content">Really impressive. The abstract claims sub 2 second generation times, but the youtube demo seems to show generations taking ~6 seconds. Not that I would complain about 6 second generations, my 12gb 3060 probably takes 3-4x as long running SD1.5; perhaps they&#x27;re not counting the time to load the model, just the active inference time?</div><br/><div id="36306062" class="c"><input type="checkbox" id="c-36306062" checked=""/><div class="controls bullet"><span class="by">pysnow</span><span>|</span><a href="#36305597">parent</a><span>|</span><a href="#36307027">next</a><span>|</span><label class="collapse" for="c-36306062">[-]</label><label class="expand" for="c-36306062">[1 more]</label></div><br/><div class="children"><div class="content">Yeah stable diffusion has a stage before generation where it transforms the text prompt into data for the model to use, called CLIP Encoding, it runs before every generation, and its probably the stage in the video where you see a spinner in place of the step bar.</div><br/></div></div></div></div><div id="36307027" class="c"><input type="checkbox" id="c-36307027" checked=""/><div class="controls bullet"><span class="by">anotheryou</span><span>|</span><a href="#36305597">prev</a><span>|</span><a href="#36306507">next</a><span>|</span><label class="collapse" for="c-36307027">[-]</label><label class="expand" for="c-36307027">[6 more]</label></div><br/><div class="children"><div class="content">Why focus on mobile?<p>Near real-time rendering while entering a prompt on desktop would be even more amazing!<p>Imagine e.g. UI sliders for adding weight to multi-prompts like &quot;night time&quot; (ideally sliding from [day time]:1, [night time]:0 to both zero, to all night with 0,1).</div><br/><div id="36307490" class="c"><input type="checkbox" id="c-36307490" checked=""/><div class="controls bullet"><span class="by">tudorw</span><span>|</span><a href="#36307027">parent</a><span>|</span><a href="#36307193">next</a><span>|</span><label class="collapse" for="c-36307490">[-]</label><label class="expand" for="c-36307490">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d like to be able to define camera movement as a a 3d path along around a geodesic in a multidimensional topological manifold :)</div><br/></div></div><div id="36307193" class="c"><input type="checkbox" id="c-36307193" checked=""/><div class="controls bullet"><span class="by">phire</span><span>|</span><a href="#36307027">parent</a><span>|</span><a href="#36307490">prev</a><span>|</span><a href="#36307087">next</a><span>|</span><label class="collapse" for="c-36307193">[-]</label><label class="expand" for="c-36307193">[1 more]</label></div><br/><div class="children"><div class="content">- It&#x27;s significantly easier to monetize mobile apps (both with ads and in-app purchases)<p>- Open projects already dominate the desktop space<p>- There are a growing number of younger people who aren&#x27;t really computer literate, or otherwise just use their phone as their primary computing device.<p>- Phones go into social situations where desktops&#x2F;laptops don&#x27;t.</div><br/></div></div><div id="36307087" class="c"><input type="checkbox" id="c-36307087" checked=""/><div class="controls bullet"><span class="by">schappim</span><span>|</span><a href="#36307027">parent</a><span>|</span><a href="#36307193">prev</a><span>|</span><a href="#36307057">next</a><span>|</span><label class="collapse" for="c-36307087">[-]</label><label class="expand" for="c-36307087">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Why focus on mobile?<p>Snap Inc.</div><br/><div id="36307163" class="c"><input type="checkbox" id="c-36307163" checked=""/><div class="controls bullet"><span class="by">noduerme</span><span>|</span><a href="#36307027">root</a><span>|</span><a href="#36307087">parent</a><span>|</span><a href="#36307057">next</a><span>|</span><label class="collapse" for="c-36307163">[-]</label><label class="expand" for="c-36307163">[1 more]</label></div><br/><div class="children"><div class="content">Oh you&#x27;re so right. The face filter industrial complex is going to eat this up. And after them, plastic surgeons... God help us all.</div><br/></div></div></div></div><div id="36307057" class="c"><input type="checkbox" id="c-36307057" checked=""/><div class="controls bullet"><span class="by">GaggiX</span><span>|</span><a href="#36307027">parent</a><span>|</span><a href="#36307087">prev</a><span>|</span><a href="#36306507">next</a><span>|</span><label class="collapse" for="c-36307057">[-]</label><label class="expand" for="c-36307057">[1 more]</label></div><br/><div class="children"><div class="content">It has a total latency of 200ms on a A100 40G.</div><br/></div></div></div></div><div id="36306507" class="c"><input type="checkbox" id="c-36306507" checked=""/><div class="controls bullet"><span class="by">bredren</span><span>|</span><a href="#36307027">prev</a><span>|</span><a href="#36305596">next</a><span>|</span><label class="collapse" for="c-36306507">[-]</label><label class="expand" for="c-36306507">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Text-to-image diffusion models can create stunning images from natural language descriptions that rival the work of professional artists and photographers.<p>I’m all for the continued advance of diffusion models.<p>If this paper offered evidence of quantitative and qualities measurement techniques for determining human preference for art or photos based on a prompt, I’d get it the phrasing.<p>But having the first sentence essentially spurn professional creatives seems to unnecessarily fan the flames.<p>AI image generation does bother some creatives, and there are real reasons for this given the many models that have been trained using long practiced work.<p>Keep up with the science, but don’t forget the tact!</div><br/></div></div><div id="36305596" class="c"><input type="checkbox" id="c-36305596" checked=""/><div class="controls bullet"><span class="by">behnamoh</span><span>|</span><a href="#36306507">prev</a><span>|</span><a href="#36306944">next</a><span>|</span><label class="collapse" for="c-36305596">[-]</label><label class="expand" for="c-36305596">[6 more]</label></div><br/><div class="children"><div class="content">This is insane! But it makes me wonder if we&#x27;ve reached a local maximum in AI where the current methods are great at generating still images but they&#x27;re pretty much uncontrollable. Like if you ask the AI to generate a dog, is it really possible to prompt every single detail so it creates exactly what you have in mind, or is it more like a trust situation where you just accept whatever the AI generates for you?</div><br/><div id="36305702" class="c"><input type="checkbox" id="c-36305702" checked=""/><div class="controls bullet"><span class="by">anonylizard</span><span>|</span><a href="#36305596">parent</a><span>|</span><a href="#36306491">next</a><span>|</span><label class="collapse" for="c-36305702">[-]</label><label class="expand" for="c-36305702">[1 more]</label></div><br/><div class="children"><div class="content">Pure text-&gt;image is impossible to get exactly, given there&#x27;s 10000 possibilities for a dog. Even if text prompts eliminate 99.9% of probabilities, it there&#x27;s still 10 possible images.<p>However, with stuff like controlnet, it&#x27;s already possible, and will be solved within a year. Yes you can specify every exact detail, but you need to feed it a sketch, or a skeletal pose, or a reference image of the dog...<p>Also, you can train a LORA on the subject before hand, if you want to consistently regenerate the subject, with just text.</div><br/></div></div><div id="36306491" class="c"><input type="checkbox" id="c-36306491" checked=""/><div class="controls bullet"><span class="by">moritonal</span><span>|</span><a href="#36305596">parent</a><span>|</span><a href="#36305702">prev</a><span>|</span><a href="#36306159">next</a><span>|</span><label class="collapse" for="c-36306491">[-]</label><label class="expand" for="c-36306491">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a human problem really. If you asked an artist to draw a dog you&#x27;d have to &quot;trust&quot; them? To control every detail you&#x27;d have to tell them every detail, either upfront (whereby the artist night struggle to achieve) or as a series of edits. In the latter case both artist and AI would struggle to keep the look consistent the more edits you make.</div><br/><div id="36307016" class="c"><input type="checkbox" id="c-36307016" checked=""/><div class="controls bullet"><span class="by">spookthesunset</span><span>|</span><a href="#36305596">root</a><span>|</span><a href="#36306491">parent</a><span>|</span><a href="#36306159">next</a><span>|</span><label class="collapse" for="c-36307016">[-]</label><label class="expand" for="c-36307016">[1 more]</label></div><br/><div class="children"><div class="content">At some point you might as well just do it yourself cause it’s easier. I’ve gotten to this point more than once with ChatGPT. ChatGPT will get you like 70% of the way initially and if you are lucky you’ll hit 90% with a lot of time invested in “prompt engineering”.<p>The thing is, none of these are mind readers. And text is a very poor way to define tight specs. The best way for software is to code it yourself. Code is the spec. Same with drawing. The spec is the drawing itself. Only the human can control that.<p>…or something…</div><br/></div></div></div></div><div id="36306159" class="c"><input type="checkbox" id="c-36306159" checked=""/><div class="controls bullet"><span class="by">gmerc</span><span>|</span><a href="#36305596">parent</a><span>|</span><a href="#36306491">prev</a><span>|</span><a href="#36305932">next</a><span>|</span><label class="collapse" for="c-36306159">[-]</label><label class="expand" for="c-36306159">[1 more]</label></div><br/><div class="children"><div class="content">ControlNet begs to differ</div><br/></div></div><div id="36305932" class="c"><input type="checkbox" id="c-36305932" checked=""/><div class="controls bullet"><span class="by">FeepingCreature</span><span>|</span><a href="#36305596">parent</a><span>|</span><a href="#36306159">prev</a><span>|</span><a href="#36306944">next</a><span>|</span><label class="collapse" for="c-36305932">[-]</label><label class="expand" for="c-36305932">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;vcai.mpi-inf.mpg.de&#x2F;projects&#x2F;DragGAN&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;vcai.mpi-inf.mpg.de&#x2F;projects&#x2F;DragGAN&#x2F;</a> Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold</div><br/></div></div></div></div><div id="36306944" class="c"><input type="checkbox" id="c-36306944" checked=""/><div class="controls bullet"><span class="by">r1nzl3r</span><span>|</span><a href="#36305596">prev</a><span>|</span><a href="#36306918">next</a><span>|</span><label class="collapse" for="c-36306944">[-]</label><label class="expand" for="c-36306944">[2 more]</label></div><br/><div class="children"><div class="content">Took me 2 days to generate an image in SD. Hope this can run well on my intel core i3 HD4000</div><br/><div id="36307032" class="c"><input type="checkbox" id="c-36307032" checked=""/><div class="controls bullet"><span class="by">anotheryou</span><span>|</span><a href="#36306944">parent</a><span>|</span><a href="#36306918">next</a><span>|</span><label class="collapse" for="c-36307032">[-]</label><label class="expand" for="c-36307032">[1 more]</label></div><br/><div class="children"><div class="content">oh my, just use something online or get a GPU.</div><br/></div></div></div></div><div id="36306015" class="c"><input type="checkbox" id="c-36306015" checked=""/><div class="controls bullet"><span class="by">shashanoid</span><span>|</span><a href="#36306918">prev</a><span>|</span><label class="collapse" for="c-36306015">[-]</label><label class="expand" for="c-36306015">[1 more]</label></div><br/><div class="children"><div class="content">still pretty slow</div><br/></div></div></div></div></div></div></div></body></html>