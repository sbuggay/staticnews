<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1685869262139" as="style"/><link rel="stylesheet" href="styles.css?v=1685869262139"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://pages.cs.wisc.edu/~chronis/files/efficiently_searching_sorted_arrays.pdf">Efficiently Searching In-Memory Sorted Arrays: Revenge of Interpolation Search? [pdf]</a> <span class="domain">(<a href="https://pages.cs.wisc.edu">pages.cs.wisc.edu</a>)</span></div><div class="subtext"><span>greghn</span> | <span>31 comments</span></div><br/><div><div id="36183887" class="c"><input type="checkbox" id="c-36183887" checked=""/><div class="controls bullet"><span class="by">rurban</span><span>|</span><a href="#36179325">next</a><span>|</span><label class="collapse" for="c-36183887">[-]</label><label class="expand" for="c-36183887">[1 more]</label></div><br/><div class="children"><div class="content">It still can be made much faster, esp. for constant data and data which fits well into ranges, if uniformly distributed or varying. You just calculate beforehand the best curve of the data and ranges, and then at run-time find the pivot, the starting point of your branchless search, and binary search from there.<p>Much faster than the SIP and TIP nonsense in the tight loop. Also faster than Eytzinger in most cases.
They use only a linear equation to model the data, quadratic or logarithmic fit would be much better. And it&#x27;s much smaller than a full db index, just 3-4 numbers, the type of the curve (linear, quadratic, logarithmic) and the curve parameters.<p>Without ranges a hashtable is still the best, esp. if constant then use a CMPH. 
I use that in unicode table searches, where the data varies every year.</div><br/></div></div><div id="36179325" class="c"><input type="checkbox" id="c-36179325" checked=""/><div class="controls bullet"><span class="by">scrubs</span><span>|</span><a href="#36183887">prev</a><span>|</span><a href="#36178611">next</a><span>|</span><label class="collapse" for="c-36179325">[-]</label><label class="expand" for="c-36179325">[5 more]</label></div><br/><div class="children"><div class="content">Broadly related: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=25899286" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=25899286</a>
PGM Indexes: Learned indexes that match B-tree performance with 83x less space<p>I might suggest, slightly pushing back on this article&#x27;s PDF, binary search is not the de-facto in-memory search algorithm. Cache consciousness btrees, and various optimizations of tries, or radix trees are better. Eytzinger arrays are a good BST option if data is pre-sorted and&#x2F;or requires rare resort passes. Which brings us back to btrees: keeping data sorted is usually 51% of the problem. Search is the other 49%.<p>OK, if you don&#x27;t need range search, which needs sorted keys, then hashmaps are darn hard to beat.<p>The PDF author however makes a strong point: if FLOPS is going up way faster than memory BW, the obvious thing to do is spend a little more code looking into memory smartly. And the PDF even gives algos to do it. So my that&#x27;s why I say BST as de-facto algorithm, while I don&#x27;t think that&#x27;s true, is only small push back. The paper goes way further.</div><br/><div id="36180093" class="c"><input type="checkbox" id="c-36180093" checked=""/><div class="controls bullet"><span class="by">pvansandt</span><span>|</span><a href="#36179325">parent</a><span>|</span><a href="#36178611">next</a><span>|</span><label class="collapse" for="c-36180093">[-]</label><label class="expand" for="c-36180093">[4 more]</label></div><br/><div class="children"><div class="content">I (author) agree that certainly at the higher end of element counts, it would be a very strange decision to not use an index. One of my favorite related papers is RadixSpline by Kipf, which shows an index based approach.<p>However, the inner loop of indexes usually is some other kind of search algorithm. Graefe mentions in BTree techniques that interpolation search is sometimes applicable to inner nodes in btrees. One of the SIP evaluations was using SIP as part of the inner loop in LevelDB, which doesn&#x27;t displace the use of an LSM for managing the larger data set.<p>If you don&#x27;t need a lot of range queries, eytzinger, or cache-line optimized btrees do get a lot more interesting, but the iteration logic gets a lot worse. SIP returns an element if it&#x27;s present and a sentinel if absent, which is an odd choice since the index is usually what&#x27;s needed. No clue, but hopefully you&#x27;ll have some sympathy for revisiting your old code and wondering who wrote it. :)<p>I do think that optimizing search algorithms is still worthwhile in an indexed setting for two reasons:
1. The size of a node within an index like a btree depends on the efficiency with which that node can be searched, so if you are able to decrease the cost of searching a given node in the index, that might motivate larger node sizes. I know that a common heuristic is to use a multiple of a page size, but that is based on the assumption that the whole node will be used as part of the search.
2. Pushing algorithmic decisions to just in time can be more costly than the time saved from choosing a better algorithm. You lose icache efficiency, stall on the decision, etc. I think it&#x27;s still good to steel man, as best you can, that your index is actually buying you improved performance for additional complexity or update overhead.</div><br/><div id="36181287" class="c"><input type="checkbox" id="c-36181287" checked=""/><div class="controls bullet"><span class="by">scrubs</span><span>|</span><a href="#36179325">root</a><span>|</span><a href="#36180093">parent</a><span>|</span><a href="#36182965">next</a><span>|</span><label class="collapse" for="c-36181287">[-]</label><label class="expand" for="c-36181287">[1 more]</label></div><br/><div class="children"><div class="content">Your paper is a valid add. And I&#x27;m gonna read&#x2F;digest it. My point above while true, is more in the margins. Better to keep the focus on engineering know-how in the PDF. Thank you for submitting it.</div><br/></div></div><div id="36182965" class="c"><input type="checkbox" id="c-36182965" checked=""/><div class="controls bullet"><span class="by">Dwedit</span><span>|</span><a href="#36179325">root</a><span>|</span><a href="#36180093">parent</a><span>|</span><a href="#36181287">prev</a><span>|</span><a href="#36178611">next</a><span>|</span><label class="collapse" for="c-36182965">[-]</label><label class="expand" for="c-36182965">[2 more]</label></div><br/><div class="children"><div class="content">What&#x27;s stopping eytzinger for being used for ranges?  You can still iterate sequentially on an eytzinger tree, just like a binary search tree.</div><br/><div id="36183797" class="c"><input type="checkbox" id="c-36183797" checked=""/><div class="controls bullet"><span class="by">rurban</span><span>|</span><a href="#36179325">root</a><span>|</span><a href="#36182965">parent</a><span>|</span><a href="#36178611">next</a><span>|</span><label class="collapse" for="c-36183797">[-]</label><label class="expand" for="c-36183797">[1 more]</label></div><br/><div class="children"><div class="content">Because it&#x27;s slower</div><br/></div></div></div></div></div></div></div></div><div id="36178611" class="c"><input type="checkbox" id="c-36178611" checked=""/><div class="controls bullet"><span class="by">lucgagan</span><span>|</span><a href="#36179325">prev</a><span>|</span><a href="#36180475">next</a><span>|</span><label class="collapse" for="c-36178611">[-]</label><label class="expand" for="c-36178611">[9 more]</label></div><br/><div class="children"><div class="content">Wow, this sounds like a really fascinating study! Binary Search has long been the go-to method for sorted, in-memory data search due to its effectiveness and simplicity. However, given the limitations of Binary Search in certain scenarios, it&#x27;s exciting to see new algorithms being proposed that could potentially perform better under specific circumstances.<p>It&#x27;s intriguing that Interpolation Search, despite its historically lackluster performance, is being given a second life through SIP and TIP. I love the fact that they&#x27;re leveraging hardware trends and advanced calculations to optimize search performance.<p>The fact that SIP shows up to 4x faster performance on uniformly distributed data and TIP is 2-3x faster on non-uniformly distributed data is quite promising. It seems these approaches could be really useful in practice, given that real-world data often follows non-uniform distributions.<p>The introduction of a meta-algorithm to dynamically select the optimal search algorithm based on the data distribution is a clever touch. It helps to enhance the overall search performance by not sticking to a one-size-fits-all approach.<p>I&#x27;d love to read more about these techniques and their practical implementations. Do they have any benchmarks or plans to test these new methods in large-scale, real-world systems?</div><br/><div id="36179811" class="c"><input type="checkbox" id="c-36179811" checked=""/><div class="controls bullet"><span class="by">switchbak</span><span>|</span><a href="#36178611">parent</a><span>|</span><a href="#36179965">next</a><span>|</span><label class="collapse" for="c-36179811">[-]</label><label class="expand" for="c-36179811">[7 more]</label></div><br/><div class="children"><div class="content">Serious question: did you use an LLM tool to craft some or all of this comment?</div><br/><div id="36179824" class="c"><input type="checkbox" id="c-36179824" checked=""/><div class="controls bullet"><span class="by">finexplained</span><span>|</span><a href="#36178611">root</a><span>|</span><a href="#36179811">parent</a><span>|</span><a href="#36179941">next</a><span>|</span><label class="collapse" for="c-36179824">[-]</label><label class="expand" for="c-36179824">[1 more]</label></div><br/><div class="children"><div class="content">The five paragraph format will forever be suspicious.</div><br/></div></div><div id="36179941" class="c"><input type="checkbox" id="c-36179941" checked=""/><div class="controls bullet"><span class="by">adamisom</span><span>|</span><a href="#36178611">root</a><span>|</span><a href="#36179811">parent</a><span>|</span><a href="#36179824">prev</a><span>|</span><a href="#36183317">next</a><span>|</span><label class="collapse" for="c-36179941">[-]</label><label class="expand" for="c-36179941">[1 more]</label></div><br/><div class="children"><div class="content">I’d be willing to bet money at 5-1 against that they didn’t. since that’s impossible to prove, I guess also on running an llm a lot of times and getting anything close to it.</div><br/></div></div><div id="36183317" class="c"><input type="checkbox" id="c-36183317" checked=""/><div class="controls bullet"><span class="by">tuukkah</span><span>|</span><a href="#36178611">root</a><span>|</span><a href="#36179811">parent</a><span>|</span><a href="#36179941">prev</a><span>|</span><a href="#36181873">next</a><span>|</span><label class="collapse" for="c-36183317">[-]</label><label class="expand" for="c-36183317">[1 more]</label></div><br/><div class="children"><div class="content">Nah, that&#x27;s just what academic correspondense looks like.</div><br/></div></div><div id="36181873" class="c"><input type="checkbox" id="c-36181873" checked=""/><div class="controls bullet"><span class="by">maxbond</span><span>|</span><a href="#36178611">root</a><span>|</span><a href="#36179811">parent</a><span>|</span><a href="#36183317">prev</a><span>|</span><a href="#36179965">next</a><span>|</span><label class="collapse" for="c-36181873">[-]</label><label class="expand" for="c-36181873">[3 more]</label></div><br/><div class="children"><div class="content">What suggests to you it may have been?</div><br/><div id="36181971" class="c"><input type="checkbox" id="c-36181971" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#36178611">root</a><span>|</span><a href="#36181873">parent</a><span>|</span><a href="#36179965">next</a><span>|</span><label class="collapse" for="c-36181971">[-]</label><label class="expand" for="c-36181971">[2 more]</label></div><br/><div class="children"><div class="content">It’s repeating trivialities that would be well-understood and are somewhat hyperbolic for both the audience and the real meat of the content. C.f. “Binary search has long been…”</div><br/><div id="36182122" class="c"><input type="checkbox" id="c-36182122" checked=""/><div class="controls bullet"><span class="by">maxbond</span><span>|</span><a href="#36178611">root</a><span>|</span><a href="#36181971">parent</a><span>|</span><a href="#36179965">next</a><span>|</span><label class="collapse" for="c-36182122">[-]</label><label class="expand" for="c-36182122">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t really see it, my experience with other LLMs is limited, but ChatGPT tends to use the rhetorical tricks you&#x27;re gesturing at to speak in generalities and avoid expressing a definite opinion - it&#x27;s the shallow even-handedness of a bullshitted essay, where you avoid saying something definite because you don&#x27;t actually know what you&#x27;re talking about.<p>But the parts of the comment you&#x27;re highlighting are specific and opinionated. And I don&#x27;t find them particularly hyperbolic.</div><br/></div></div></div></div></div></div></div></div><div id="36179965" class="c"><input type="checkbox" id="c-36179965" checked=""/><div class="controls bullet"><span class="by">pvansandt</span><span>|</span><a href="#36178611">parent</a><span>|</span><a href="#36179811">prev</a><span>|</span><a href="#36180475">next</a><span>|</span><label class="collapse" for="c-36179965">[-]</label><label class="expand" for="c-36179965">[1 more]</label></div><br/><div class="children"><div class="content">This was an interesting journey. My original target was at L1 sized arrays with constant sized error, but that limits its applicability to the target audience. My initial exploration with spline based indexes were at this target data size, so they weren&#x27;t competitive.<p>SIP is very sensitive to quality of implementation. I did my best to steel man binary search, but if either is not implemented carefully, then you can easily get inverted results. I tried several different axes of variation for implementation of binary search and probably spent more time optimizing binary search than SIP or TIP, but binary search is very sensitive to cache misses and how that relates to branch prediction and conflict misses. Quality of linear search is also critical and relies heavily on array padding. SIMD was actually a major pessimization since for SIP, it&#x27;s only worth using if linear search is &lt; 100 elements or so.<p>I spent a lot of effort trying to model the performance of SIP to predict from statistics of the data to how it would perform. You can consider the cost of each interpolation as a random access cache miss, and the cost of a linear search based on its distance, but the cost of a linear search has decreasing marginal cost with more distance before you reach steady state, which is exactly where you are using it in SIP. So, that basically led nowhere, but it does point out that an L2 norm is the wrong metric for identifying if interpolation would be useful on your data.<p>I think the bigger insight of TIP is around its use of efficient approximations for hyperbolic interpolations. There&#x27;s a lot of research around polynomial interpolation, but I think that hyperbolas are also worth examining. I don&#x27;t expect TIP to be used in a database engine, and while data might be commonly zipf distributed, I&#x27;m not sure that people are putting that kind of data into an array and doing lookups on them where the values at the bottom of the array have a lot of duplicate values.<p>SIP, I think, is more about increasing amounts of spatial cache locality, and making the most of out-of-order processors. It is more a variation on interpolation-sequential search, so it might be more practically used in a SIP-2 or SIP-3 variation which a hard-coded, unrolled number of iterations. (SIP-1 would just be interpolation sequential.) The downside of an interpolation method is that pairs of sequential interpolations don&#x27;t surround the target, which leaves the search unbounded. I don&#x27;t find hybrid approaches very interesting because I don&#x27;t expect an implementation to be able to pipeline useful work which has a time per iteration that is sufficiently fast. For perspective, on billion element arrays, I think typical number of interpolations was around 3. We would expect number of interpolations to not increase to 4 until you hit a quintillion elements which suggests that the gap in efficiency between log N and log log N grows slowly enough that algorithmic gains have a nearby, practical upper bound to potential improvement.<p>One of my favorite related papers is RadixSpline by Kipf et al. The amount of additional memory across a number of scenarios would be tiny, you could write a very efficient inner loop, and it would have basically been tailor fit to work perfectly on the FB dataset (one of the real world distributions).</div><br/></div></div></div></div><div id="36180475" class="c"><input type="checkbox" id="c-36180475" checked=""/><div class="controls bullet"><span class="by">mlochbaum</span><span>|</span><a href="#36178611">prev</a><span>|</span><a href="#36182283">next</a><span>|</span><label class="collapse" for="c-36180475">[-]</label><label class="expand" for="c-36180475">[9 more]</label></div><br/><div class="children"><div class="content">The paper draws the analogy to continuous root-finding (&quot;Interpolation search is the regula falsi method&quot;), nice. One thing I&#x27;ve been wondering lately is whether the ITP method[0], which is actually more recent than this paper, is a good fit for sorted searching. It&#x27;s an elegant method that guarantees a maximum number of queries, for example one more than binary search, while taking advantage of interpolation for better-behaved distributions. I wrote a short section about this at [1]. Which is likely to link to the OP soon too.<p>Since the author&#x27;s here: the &quot;Optimized binary search&quot; algorithm looks good, in that it has an obvious branchless implementation. But it&#x27;ll depend on how the code&#x27;s written and compiled, and clang in particular will turn it into a branch without -mllvm --x86-cmov-converter=0. Making it branchless was the intention, right? Was the output code checked to make sure it uses cmov?<p>[0] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;ITP_Method" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;ITP_Method</a><p>[1] <a href="https:&#x2F;&#x2F;mlochbaum.github.io&#x2F;BQN&#x2F;implementation&#x2F;primitive&#x2F;sort.html#interpolation-search" rel="nofollow">https:&#x2F;&#x2F;mlochbaum.github.io&#x2F;BQN&#x2F;implementation&#x2F;primitive&#x2F;sor...</a></div><br/><div id="36180633" class="c"><input type="checkbox" id="c-36180633" checked=""/><div class="controls bullet"><span class="by">pvansandt</span><span>|</span><a href="#36180475">parent</a><span>|</span><a href="#36180630">next</a><span>|</span><label class="collapse" for="c-36180633">[-]</label><label class="expand" for="c-36180633">[7 more]</label></div><br/><div class="children"><div class="content">Big fan on your presentation on binary search that amortised multiple concurrent searches.<p>I did several variations on binary search and always compared against the best one. [0] From what I remember, some variations were compiled to a branchless implementation and some were compiled to implementations that used a branch. I had many passes of analysis by pasting into godbolt with identical compilers and flags. Power of 2 binary search did better on small arrays iirc, but are the first to hit conflict misses. For larger arrays, I believe the branchy search algorithms start to do better since half their branches get predicted correctly.<p>I haven&#x27;t previously looked at ITP, and I would need to study further to get a more clear idea on what its aiming for. Some hybrid methods, unlike ITP require additional memory accesses, but I don&#x27;t think this does, just compute. On the other hand, since at a billion elements you&#x27;re looking at roughly 3 interpolations or 30 bisections, there&#x27;s a pretty narrow window of opportunity here, and in the batch case, there are indices to contend with. I&#x27;m on the BQN discord if you wanted a different form of communication.<p>[0] <a href="https:&#x2F;&#x2F;github.com&#x2F;UWHustle&#x2F;Efficiently-Searching-In-Memory-Sorted-Arrays&#x2F;blob&#x2F;master&#x2F;src&#x2F;algorithms&#x2F;binary_search.h#L18">https:&#x2F;&#x2F;github.com&#x2F;UWHustle&#x2F;Efficiently-Searching-In-Memory-...</a></div><br/><div id="36182097" class="c"><input type="checkbox" id="c-36182097" checked=""/><div class="controls bullet"><span class="by">xoranth</span><span>|</span><a href="#36180475">root</a><span>|</span><a href="#36180633">parent</a><span>|</span><a href="#36180995">next</a><span>|</span><label class="collapse" for="c-36182097">[-]</label><label class="expand" for="c-36182097">[2 more]</label></div><br/><div class="children"><div class="content">&gt; For larger arrays, I believe the branchy search algorithms start to do better since half their branches get predicted correctly.<p>(not sure if this is what you meant by &quot;conflict misses&quot;)
I believe the reason branchy binary search algorithms do better on large arrays is that the CPU&#x27;s speculative execution will cause it to prefetch data to the caches.
That means that, for around half of the steps, the data needed for the comparisons will not need to be fetched from memory.<p>You can get the best of both world by using a branchless implementation and manually prefetching data as in [^0].<p>[0^]: <a href="https:&#x2F;&#x2F;en.algorithmica.org&#x2F;hpc&#x2F;data-structures&#x2F;binary-search&#x2F;#removing-branches" rel="nofollow">https:&#x2F;&#x2F;en.algorithmica.org&#x2F;hpc&#x2F;data-structures&#x2F;binary-searc...</a></div><br/><div id="36182476" class="c"><input type="checkbox" id="c-36182476" checked=""/><div class="controls bullet"><span class="by">moonchild</span><span>|</span><a href="#36180475">root</a><span>|</span><a href="#36182097">parent</a><span>|</span><a href="#36180995">next</a><span>|</span><label class="collapse" for="c-36182476">[-]</label><label class="expand" for="c-36182476">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not what&#x27;s meant by &#x27;conflict misses&#x27;, no.  The issue is that, with pot, the hottest elements (logically the ones which represent the highest nodes in the tree) will be spaced at pot intervals.  If the array is large, that means their addresses will differ only in the high bits; in the case of the higher-level caches, then, they&#x27;ll map to the same cache set and kick each other out, even though the cache as a whole has enough capacity to fit them all.</div><br/></div></div></div></div><div id="36180995" class="c"><input type="checkbox" id="c-36180995" checked=""/><div class="controls bullet"><span class="by">mlochbaum</span><span>|</span><a href="#36180475">root</a><span>|</span><a href="#36180633">parent</a><span>|</span><a href="#36182097">prev</a><span>|</span><a href="#36180630">next</a><span>|</span><label class="collapse" for="c-36180995">[-]</label><label class="expand" for="c-36180995">[4 more]</label></div><br/><div class="children"><div class="content">Sounds good on the basic binary search. I jumped through the paper a little too quick and missed the source code link, although thanks for the godbolt confirmation as well.<p>Yeah, the big deal about ITP is not really the interpolation but the graceful fallback to binary search. With of course that nasty division overhead. I&#x27;ll have to study your paper to see which ideas there could be used.<p>Glad you liked the talk! And a small world, huh? I&#x27;ve been wanting to get an implementation of the vector binary search into CBQN for a while, so that an open source version of it exists at least. Always some other thing to do. Multiple searches are really a different world. Elijah Stone pointed out recently[0] that with searches that all have the same iteration count (sadly, not interpolation search!), several of them can be run at once just by copying the loop body. That was new to me at least. And for searches that don&#x27;t fit in cache it&#x27;s possible to partition the sought values, which does the accesses in a perfectly cache-friendly order for some extra compute. That&#x27;s described in the page I linked before.<p>[0] <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=33648924" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=33648924</a></div><br/><div id="36181904" class="c"><input type="checkbox" id="c-36181904" checked=""/><div class="controls bullet"><span class="by">moonchild</span><span>|</span><a href="#36180475">root</a><span>|</span><a href="#36180995">parent</a><span>|</span><a href="#36180630">next</a><span>|</span><label class="collapse" for="c-36181904">[-]</label><label class="expand" for="c-36181904">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Multiple searches are really a different world.  Elijah Stone pointed out recently[0] that with searches that all have the same iteration count (sadly, not interpolation search!), several of them can be run at once just by copying the loop body<p>They don&#x27;t need to have the same iteration count; it suffices to mask out searches as they finish (like gpus) or ensure that repeated search iterations are idempotent, and just wait until all finish.  My first implementation actually did this, and the iteration count could vary by as much as 1 when the search space size was not a power of two.  I changed that after henry pointed out that it can cause mispredicts.<p>In the case of interpolation search, a naive application might be a bad idea (since a single pathological lane slows down all the other concurrent searches).  But an alternative might be to detect and &#x27;schedule out&#x27; pathological lanes; use binary search instead of interpolation search for them, and now you have nice worst-case asymptotics again.<p>(Fun anecdote: I asked an nvidia employee why they don&#x27;t use this strategy for gpu branches in general.  The response: mumble mumble not sure, probably tradeoffs—just isn&#x27;t worth it.  Fair enough.  Not six months later, they release new gpus which do that for sparse threadgroups.  I maintain they stole the idea from me :)<p>Alternately, it might be possible to swap out completed lanes as soon as they finish—superscalar cpu is a mimd, after all—getting this branch free might be too much overhead, but maybe if you do the check every k iterations or something...</div><br/><div id="36182033" class="c"><input type="checkbox" id="c-36182033" checked=""/><div class="controls bullet"><span class="by">mlochbaum</span><span>|</span><a href="#36180475">root</a><span>|</span><a href="#36181904">parent</a><span>|</span><a href="#36180630">next</a><span>|</span><label class="collapse" for="c-36182033">[-]</label><label class="expand" for="c-36182033">[2 more]</label></div><br/><div class="children"><div class="content">There are some possibilities with uneven searches for sure. The major issue is that a search is so fast it doesn&#x27;t leave a lot of room for overhead. For interpolation search, scheduling out bad cases sounds fine until you have a lot of them: then you have to search them one at a time, and the length&#x27;s unpredictable, right? Grouping by search length doesn&#x27;t really sound feasible: you&#x27;d have to store element index and value, as well as current search index, for each element that goes the long way.</div><br/><div id="36182531" class="c"><input type="checkbox" id="c-36182531" checked=""/><div class="controls bullet"><span class="by">moonchild</span><span>|</span><a href="#36180475">root</a><span>|</span><a href="#36182033">parent</a><span>|</span><a href="#36180630">next</a><span>|</span><label class="collapse" for="c-36182531">[-]</label><label class="expand" for="c-36182531">[1 more]</label></div><br/><div class="children"><div class="content">&gt; length&#x27;s unpredictable, right? Grouping by search length doesn&#x27;t really sound feasible<p>You just have to group by ceillog2, so there aren&#x27;t many buckets.  Can even discretise further (say, one bucket for every two exponents) with likely not too much penalty.  Alternately, perhaps restart the search entirely for pathological elements (so don&#x27;t try to keep around the information gained by the cursory iterations of interpolation search), but if there are too many pathological elements, just do binary search up front for all elements.  I expect this would work pretty well in practice.  (Maybe with exponential backoff or something, so you don&#x27;t lose to changes in the distribution of elements being searched for.)<p>(This reminds me, I have a similar problem with adaptive sorting.  Having identified a number of presorted runs, it&#x27;s necessary to merge them in the right order, because if I merge them in the wrong order I get quadratic.  Plan is similarly to sort them by lzcnt or some such, which is a curious recursive problem.)</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="36182283" class="c"><input type="checkbox" id="c-36182283" checked=""/><div class="controls bullet"><span class="by">w0mbat</span><span>|</span><a href="#36180475">prev</a><span>|</span><a href="#36181910">next</a><span>|</span><label class="collapse" for="c-36182283">[-]</label><label class="expand" for="c-36182283">[1 more]</label></div><br/><div class="children"><div class="content">I get that it&#x27;s faster, but binary search is already so quick that it is almost free when you look at the big picture, even with large datasets. If standard library implementors want to switch to this method behind the scenes then that&#x27;s fine, but I&#x27;m not re-writing my apps.</div><br/></div></div><div id="36181910" class="c"><input type="checkbox" id="c-36181910" checked=""/><div class="controls bullet"><span class="by">nraynaud</span><span>|</span><a href="#36182283">prev</a><span>|</span><a href="#36179359">next</a><span>|</span><label class="collapse" for="c-36181910">[-]</label><label class="expand" for="c-36181910">[1 more]</label></div><br/><div class="children"><div class="content">Weird coincidence, I interpolated my pivot in a binary search 3 days ago. (Didn’t improve things, my data is compressed, so only irregular stuff is in the array)</div><br/></div></div><div id="36179359" class="c"><input type="checkbox" id="c-36179359" checked=""/><div class="controls bullet"><span class="by">alwaystired</span><span>|</span><a href="#36181910">prev</a><span>|</span><a href="#36179100">next</a><span>|</span><label class="collapse" for="c-36179359">[-]</label><label class="expand" for="c-36179359">[1 more]</label></div><br/><div class="children"><div class="content">New leetcode problem just dropped</div><br/></div></div><div id="36179100" class="c"><input type="checkbox" id="c-36179100" checked=""/><div class="controls bullet"><span class="by">marginalia_nu</span><span>|</span><a href="#36179359">prev</a><span>|</span><label class="collapse" for="c-36179100">[-]</label><label class="expand" for="c-36179100">[3 more]</label></div><br/><div class="children"><div class="content">Commenting so I find this later.</div><br/><div id="36179674" class="c"><input type="checkbox" id="c-36179674" checked=""/><div class="controls bullet"><span class="by">ComputerGuru</span><span>|</span><a href="#36179100">parent</a><span>|</span><label class="collapse" for="c-36179674">[-]</label><label class="expand" for="c-36179674">[2 more]</label></div><br/><div class="children"><div class="content">Tip: you can favorite the submission then un-favorite it after you&#x27;ve read it.</div><br/><div id="36183529" class="c"><input type="checkbox" id="c-36183529" checked=""/><div class="controls bullet"><span class="by">maxbond</span><span>|</span><a href="#36179100">root</a><span>|</span><a href="#36179674">parent</a><span>|</span><label class="collapse" for="c-36183529">[-]</label><label class="expand" for="c-36183529">[1 more]</label></div><br/><div class="children"><div class="content">You can also upvote it, though you won&#x27;t be able to undo it for long if that&#x27;s a concern.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>