<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1700730064108" as="style"/><link rel="stylesheet" href="styles.css?v=1700730064108"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-about-ai-breakthrough-2023-11-22/">OpenAI researchers warned board of AI breakthrough ahead of CEO ouster</a> <span class="domain">(<a href="https://www.reuters.com">www.reuters.com</a>)</span></div><div class="subtext"><span>mfiguiere</span> | <span>308 comments</span></div><br/><div><div id="38389417" class="c"><input type="checkbox" id="c-38389417" checked=""/><div class="controls bullet"><span class="by">wbhart</span><span>|</span><a href="#38390839">next</a><span>|</span><label class="collapse" for="c-38389417">[-]</label><label class="expand" for="c-38389417">[87 more]</label></div><br/><div class="children"><div class="content">I feel very comfortable saying, as a mathematician, that the ability to solve grade school maths problems would not be at all a predictor of ability to solve real mathematical problems at a research level.<p>The reason LLMs fail at solving mathematical problems is because:
   1) they are terrible at arithmetic,
   2) they are terrible at algebra, but most importantly,
   3) they are terrible at complex reasoning (more specifically they mix up quantifiers and don&#x27;t really understand the complex logical structure of many arguments)
   4) they (current LLMs) cannot backtrack when they find that what they already wrote turned out not to lead to a solution, and it is too expensive to give them the thousands of restarts they&#x27;d require to randomly guess their way through the problem if you did give them that facility<p>Solving grade-school problems might mean progress in 1 and 2, but that is not at all impressive, as there are perfectly good tools out there that solve those problems just fine, and old-style AI researchers have built perfectly good tools for 3. The hard problem to solve is problem 4, and this is something you teach people how to do at a university level.<p>(I should add that another important problem is what is known as premise selection. I didn&#x27;t list that because LLMs have actually been shown to manage this ok in about 70% of theorems, which basically matches records set by other machine learning techniques.)<p>(Real mathematical research also involves what is known as lemma conjecturing. I have never once observed an LLM do it, and I suspect they cannot do so. Basically the parameter set of the LLM dedicated to mathematical reasoning is either large enough to model the entire solution from end to end, or the LLM is likely to completely fail to solve the problem.)<p>I personally think this entire article is likely complete bunk.<p>Edit: after reading replies I realise I should have pointed out that humans do not simply backtrack. They learn from failed attempts in ways that LLMs do not seem to. The material they are trained on surely contributes to this problem.</div><br/><div id="38390829" class="c"><input type="checkbox" id="c-38390829" checked=""/><div class="controls bullet"><span class="by">gmt2027</span><span>|</span><a href="#38389417">parent</a><span>|</span><a href="#38389863">next</a><span>|</span><label class="collapse" for="c-38390829">[-]</label><label class="expand" for="c-38390829">[1 more]</label></div><br/><div class="children"><div class="content">We have an algorithm and computational hardware that will tune a universal function approximator to fit any dataset with emergent intelligence as it discovers abstractions, patterns, features and hierarchies.<p>So far, we have not yet found hard limits that cannot be overcome by scaling the number of model parameters, increasing the size and quality of training data or, very infrequently, adopting a new architecture.<p>The number of model parameters required to achieve a defined level of intelligence is a function of the architecture and training data. The important question is, what is N, the number of model parameters at which we cross an intelligence threshold and it becomes theoretically possible to solve mathematics problems at a research level for an optimal architecture that we may not yet have discovered. Our understanding does not extend to the level where we can predict N but I doubt that anyone still believes that it is infinity after seeing what GPT4 can do.<p>This claim here is essentially a discovery that N may be much closer to where we are with today&#x27;s largest models. Researchers at the absolute frontier are more likely to be able to gauge how close they are to a breakthrough of that magnitude from how quickly they are blowing past less impressive milestones like grade school math.<p>My intuition is that we are in a suboptimal part of the search space and it is theoretically possible to achieve GPT4 level intelligence with a model that is orders of magnitude smaller. This could happen when we figure out how to separate the reasoning from the factual knowledge encoded in the model.</div><br/></div></div><div id="38389863" class="c"><input type="checkbox" id="c-38389863" checked=""/><div class="controls bullet"><span class="by">nostrademons</span><span>|</span><a href="#38389417">parent</a><span>|</span><a href="#38390829">prev</a><span>|</span><a href="#38389502">next</a><span>|</span><label class="collapse" for="c-38389863">[-]</label><label class="expand" for="c-38389863">[13 more]</label></div><br/><div class="children"><div class="content">What I wonder, as a computer scientist:<p>If you want to solve grade school math problems, why not use an &#x27;add&#x27; instruction?  It&#x27;s been around since the 50s, runs a billion times faster than an LLM, every assembly-language programmer knows how to use it, every high-level language has a one-token equivalent, and doesn&#x27;t hallucinate answers (other than integer overflow).<p>We also know how to solve complex reasoning chains that require backtracking.  Prolog has been around since 1972.  It&#x27;s not used that much because <i>that&#x27;s not the programming problem that most people are solving.</i><p>Why not use a tool for what it&#x27;s good for and pick different tools for other problems they are better for?  LLMs are good for summarization, autocompletion, and as an input to many other language problems like spelling and bigrams.  They&#x27;re not good at math.  Computers are <i>really</i> good at math.<p>There&#x27;s a theorem that an LLM can compute any computable function.  That&#x27;s true, but so can lambda calculus.  We don&#x27;t program in raw lambda calculus because it&#x27;s terribly inefficient.  Same with LLMs for arithmetic problems.</div><br/><div id="38390747" class="c"><input type="checkbox" id="c-38390747" checked=""/><div class="controls bullet"><span class="by">seanhunter</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389863">parent</a><span>|</span><a href="#38390199">next</a><span>|</span><label class="collapse" for="c-38390747">[-]</label><label class="expand" for="c-38390747">[1 more]</label></div><br/><div class="children"><div class="content">There is a general result in machine learning known as &quot;the bitter lesson&quot;[1], which is that methods which come from specialist knowledge tend to be beaten by methods which rely on brute force computation in the long run because of Moore&#x27;s law and the ability to scale things by distributed computing. So the reason people don&#x27;t use the &quot;add instruction&quot;[2] for example is that over the last 70 years of attempting to build out systems which do exactly what you are proposing, they have found that not to work very well whereas sacrificing what you are calling &quot;efficiency&quot; (which they would think of as special purpose domain-specific knowledge) turns out to give you a lot in terms of generality. And they can make up the lost efficiency by throwing more hardware at the problem.<p>[1] <a href="http:&#x2F;&#x2F;www.incompleteideas.net&#x2F;IncIdeas&#x2F;BitterLesson.html" rel="nofollow noreferrer">http:&#x2F;&#x2F;www.incompleteideas.net&#x2F;IncIdeas&#x2F;BitterLesson.html</a><p>[2] Which the people making these models are familiar with. The whole thing is a trillion+ parameter linear algebra crunching machine after all.</div><br/></div></div><div id="38390199" class="c"><input type="checkbox" id="c-38390199" checked=""/><div class="controls bullet"><span class="by">panarky</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389863">parent</a><span>|</span><a href="#38390747">prev</a><span>|</span><a href="#38390184">next</a><span>|</span><label class="collapse" for="c-38390199">[-]</label><label class="expand" for="c-38390199">[1 more]</label></div><br/><div class="children"><div class="content"><i>&gt;&gt; the ability to solve grade school maths problems would not be at all a predictor of ability to solve real mathematical problems at a research level<p>&gt; If you want to solve grade school math problems, why not use an &#x27;add&#x27; instruction?</i><p>Certainly the objective is not for the AI to do research-level mathematics.<p>It&#x27;s not really even to do grade-school math.<p>The point is that grade-school math requires reasoning capability that transcends probabilistic completion of the next token in a sequence.<p>And if Q-Star has that reasoning capability, then it&#x27;s another step-function leap toward AGI.</div><br/></div></div><div id="38390184" class="c"><input type="checkbox" id="c-38390184" checked=""/><div class="controls bullet"><span class="by">Closi</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389863">parent</a><span>|</span><a href="#38390199">prev</a><span>|</span><a href="#38390183">next</a><span>|</span><label class="collapse" for="c-38390184">[-]</label><label class="expand" for="c-38390184">[3 more]</label></div><br/><div class="children"><div class="content">Why would we teach kids maths then, when they can use a calculator? It&#x27;s much easier and faster for them.<p>I believe it&#x27;s because having a foundational understanding of maths and logic is important when solving other problems, and if you are looking to create an AI that can generally solve all problems it should probably have some intuitive understanding of maths too.<p>i.e. if we want an LLM to be able to solve unsolved theorems in the future, this requires a level of understanding of maths that is more than &#x27;teach it to use a calculator&#x27;.<p>More broadly, I can imagine a world where LLM training is a bit more &#x27;interactive&#x27; - right now if you ask it to play a game of chess with you it fails, but it has only ever read about chess and past games and guesses the next token based on that. What if it could actually play a game of chess - would it get a deeper appreciation for the game? How would this change it&#x27;s internal model for other questions (e.g. would this make it answer better at questions about other games, or even game theory?)</div><br/><div id="38390417" class="c"><input type="checkbox" id="c-38390417" checked=""/><div class="controls bullet"><span class="by">comex</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38390184">parent</a><span>|</span><a href="#38390183">next</a><span>|</span><label class="collapse" for="c-38390417">[-]</label><label class="expand" for="c-38390417">[2 more]</label></div><br/><div class="children"><div class="content">Judging by some YouTube videos I’ve seen, ChatGPT with GPT-4 can get pretty far through a game of chess.  (Certainly much farther than GPT-3.5.)  For that duration it makes reasonably strategic moves, though eventually it seems to inevitably lose track of the board state and start making illegal moves.  I don’t know if that counts as being able to “actually play a game”, but it does have some ability, and that may have already influenced its answers about the other topics you mentioned.</div><br/><div id="38390709" class="c"><input type="checkbox" id="c-38390709" checked=""/><div class="controls bullet"><span class="by">vczf</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38390417">parent</a><span>|</span><a href="#38390183">next</a><span>|</span><label class="collapse" for="c-38390709">[-]</label><label class="expand" for="c-38390709">[1 more]</label></div><br/><div class="children"><div class="content">What if you encoded the whole game state into a one-shot completion that fits into the context window every turn? It would likely not make those illegal moves. I suspect it&#x27;s an artifact of the context window management that is designed to summarize lengthy chat conversations, rather than an actual limitation of GPT4&#x27;s internal model of chess.</div><br/></div></div></div></div></div></div><div id="38390183" class="c"><input type="checkbox" id="c-38390183" checked=""/><div class="controls bullet"><span class="by">curling_grad</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389863">parent</a><span>|</span><a href="#38390184">prev</a><span>|</span><a href="#38389882">next</a><span>|</span><label class="collapse" for="c-38390183">[-]</label><label class="expand" for="c-38390183">[2 more]</label></div><br/><div class="children"><div class="content">Actually, OpenAI did a research[0] on solving some hard math problems by integrating language model and Lean theorem prover some time ago.<p>[0]: <a href="https:&#x2F;&#x2F;openai.com&#x2F;research&#x2F;formal-math" rel="nofollow noreferrer">https:&#x2F;&#x2F;openai.com&#x2F;research&#x2F;formal-math</a></div><br/><div id="38390466" class="c"><input type="checkbox" id="c-38390466" checked=""/><div class="controls bullet"><span class="by">singularity2001</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38390183">parent</a><span>|</span><a href="#38389882">next</a><span>|</span><label class="collapse" for="c-38390466">[-]</label><label class="expand" for="c-38390466">[1 more]</label></div><br/><div class="children"><div class="content">how do they achieve 41.2% in high school Olympiads but only 55% of great school problems?<p>PS: also I thought GPT4 already achieved 90% in some university math grades? Oh I remember that was multiple-choice</div><br/></div></div></div></div><div id="38389882" class="c"><input type="checkbox" id="c-38389882" checked=""/><div class="controls bullet"><span class="by">xwolfi</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389863">parent</a><span>|</span><a href="#38390183">prev</a><span>|</span><a href="#38389502">next</a><span>|</span><label class="collapse" for="c-38389882">[-]</label><label class="expand" for="c-38389882">[5 more]</label></div><br/><div class="children"><div class="content">You&#x27;re missing the point: who&#x27;s using the &#x27;add&#x27; instruction ? You. We want &#x27;something&#x27; to think about using the &#x27;add&#x27; instruction to solve a problem.<p>We want to remove the human from the solution design. It would help us tremendously tbh, just like I don&#x27;t know, Google map helped me never to have to look for direction ever again ?</div><br/><div id="38390104" class="c"><input type="checkbox" id="c-38390104" checked=""/><div class="controls bullet"><span class="by">marshray</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389882">parent</a><span>|</span><a href="#38389502">next</a><span>|</span><label class="collapse" for="c-38390104">[-]</label><label class="expand" for="c-38390104">[4 more]</label></div><br/><div class="children"><div class="content">When the solution requires arithmetic, one trick is to simply ask GPT to write a Python program to compute the answer.<p>There&#x27;s your &#x27;add&#x27;.</div><br/><div id="38390378" class="c"><input type="checkbox" id="c-38390378" checked=""/><div class="controls bullet"><span class="by">davidwritesbugs</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38390104">parent</a><span>|</span><a href="#38389502">next</a><span>|</span><label class="collapse" for="c-38390378">[-]</label><label class="expand" for="c-38390378">[3 more]</label></div><br/><div class="children"><div class="content">Interesting, how do you use this idea? If you prompt the LLM &quot;create a python Add function Foo to add a number to another number&quot;, &quot;using Foo add 1 and 2&quot;, or somesuch, but what&#x27;s to stop it hallucinating and saying &quot;Sure, let me do that for you, foo 1 and 2 is 347. Please let me know if you need anything else.&quot;</div><br/><div id="38390851" class="c"><input type="checkbox" id="c-38390851" checked=""/><div class="controls bullet"><span class="by">LASR</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38390378">parent</a><span>|</span><a href="#38390553">next</a><span>|</span><label class="collapse" for="c-38390851">[-]</label><label class="expand" for="c-38390851">[1 more]</label></div><br/><div class="children"><div class="content">We’re way beyond this kind of hallucinations now. OpenAI’s models are frighteningly good at producing code.<p>You can even route back runtime errors and ask it to fix its own code. And it does.<p>It can write code and even write a test to test that code. Give it an interpreter and you’re all set.</div><br/></div></div><div id="38390553" class="c"><input type="checkbox" id="c-38390553" checked=""/><div class="controls bullet"><span class="by">IanCal</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38390378">parent</a><span>|</span><a href="#38390851">prev</a><span>|</span><a href="#38389502">next</a><span>|</span><label class="collapse" for="c-38390553">[-]</label><label class="expand" for="c-38390553">[1 more]</label></div><br/><div class="children"><div class="content">Nothing stops it from writing a recipe for soup for every request, but it does tend to do what it&#x27;s told. When asked to do mathsy things and told it&#x27;s got a tool for doing those it tends to lean into that if it&#x27;s a good llm.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38389502" class="c"><input type="checkbox" id="c-38389502" checked=""/><div class="controls bullet"><span class="by">muskmusk</span><span>|</span><a href="#38389417">parent</a><span>|</span><a href="#38389863">prev</a><span>|</span><a href="#38389715">next</a><span>|</span><label class="collapse" for="c-38389502">[-]</label><label class="expand" for="c-38389502">[17 more]</label></div><br/><div class="children"><div class="content">Friend, the creator of this new progress is a machine learning PhD with a decade of experience in pushing machine learning forward. He knows a lot of math too. Maybe there is a chance that he too can tell the difference between a meaningless advance and an important one?</div><br/><div id="38390675" class="c"><input type="checkbox" id="c-38390675" checked=""/><div class="controls bullet"><span class="by">seanhunter</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389502">parent</a><span>|</span><a href="#38389579">next</a><span>|</span><label class="collapse" for="c-38390675">[-]</label><label class="expand" for="c-38390675">[2 more]</label></div><br/><div class="children"><div class="content">That is as pure an example of the fallacy of argument from authority[1] as I have ever seen especially when you consider that any nuance in the supposed letter from the researchers to the board will have been lost in the translation from &quot;sources&quot; to the journalist to the article.<p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Argument_from_authority" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Argument_from_authority</a></div><br/><div id="38390804" class="c"><input type="checkbox" id="c-38390804" checked=""/><div class="controls bullet"><span class="by">abhpro</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38390675">parent</a><span>|</span><a href="#38389579">next</a><span>|</span><label class="collapse" for="c-38390804">[-]</label><label class="expand" for="c-38390804">[1 more]</label></div><br/><div class="children"><div class="content">That fallacy&#x27;s existence alone doesn&#x27;t discount anything (nor have you shown it&#x27;s applicable here), otherwise we&#x27;d throw out the entire idea of authorities and we&#x27;d be in trouble</div><br/></div></div></div></div><div id="38389579" class="c"><input type="checkbox" id="c-38389579" checked=""/><div class="controls bullet"><span class="by">neilk</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389502">parent</a><span>|</span><a href="#38390675">prev</a><span>|</span><a href="#38389744">next</a><span>|</span><label class="collapse" for="c-38389579">[-]</label><label class="expand" for="c-38389579">[6 more]</label></div><br/><div class="children"><div class="content">I am neither a mathematician or LLM creator but I do know how to evaluate interesting tech claims.<p>The absolute best case scenario for a new technology is that it when it seems like a toy for nerds, and doesn&#x27;t outperform anything we have today, but the scaling path is clear.<p>Its problems just won&#x27;t matter if it does that one thing with scaling. The web is a pretty good hypermedia platform, but a disastrously bad platform for most other computer applications. Nevertheless  the scaling of URIs and internet protocols have caused us to reorganize our lives around it. And then if there really are unsolvable problems with the platform they just get offloaded onto users. Passwords? Privacy? Your problem now. Surely you know to use a password manager?<p>I think this new wave of AI is going to be like that. If they never solve the hallucination&#x2F;confabulation issue, it&#x27;s just going to become your problem. If they never really gain insight, it&#x27;s going to become your problem to instruct them carefully. Your peers will chide for not using a robust AI-guardrail thing or not learning the basics of prompt engineering like all the kids do instinctively these days.</div><br/><div id="38389729" class="c"><input type="checkbox" id="c-38389729" checked=""/><div class="controls bullet"><span class="by">wbhart</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389579">parent</a><span>|</span><a href="#38389744">next</a><span>|</span><label class="collapse" for="c-38389729">[-]</label><label class="expand" for="c-38389729">[5 more]</label></div><br/><div class="children"><div class="content">How on earth could you evaluate the scaling path with too little information. That&#x27;s my point. You can&#x27;t possibly know that a technology can solve a given kind of problem if it can only so far solve a completely different kind of problem which is largely unrelated!<p>Saying that performance on grade-school problems is predictive of performance on complex reasoning tasks (including theorem proving) is like saying that a new kind of mechanical engine that has 90% efficiency can be scaled 10x.<p>These kind of scaling claims drive investment, I get it. But to someone who understands (and is actually working on) the actual problem that needs solving, this kind of claim is perfectly transparent!</div><br/><div id="38389992" class="c"><input type="checkbox" id="c-38389992" checked=""/><div class="controls bullet"><span class="by">dwaltrip</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389729">parent</a><span>|</span><a href="#38390124">next</a><span>|</span><label class="collapse" for="c-38389992">[-]</label><label class="expand" for="c-38389992">[1 more]</label></div><br/><div class="children"><div class="content">For the current generative AI wave, this is how I understand it:<p>1. The scaling path is decreased val&#x2F;test loss during training.<p>2. We have seen multiples times that large decreases in this loss have resulted in very impressive improvements in model capability across a diverse set of tasks (e.g. gpt-1 through gpt-4, and many other examples).<p>3. By now, there is tons of robust data demonstrating really nice relationships between model size, quantity of data,  length of training, quality of data, etc and decreased loss. Evidence keeps building that most multi-billion param LLMs are probably <i>undertrained</i>, perhaps significantly so.<p>4. Ergo, we should expect continued capability improvement with continued scaling. Make a bigger model, get more data, get higher data quality, and&#x2F;or train for longer and we will see improved capabilities. The graphs demand that it is so.<p>---<p>This is the fundamental scaling hypothesis that labs like OpenAI and Anthropic have been operating off of for the past 5+ years. They looked at the early versions of the curves mentioned above, extended the lines, and said, &quot;Huh... These lines are so sharp. Why wouldn&#x27;t it keep going? It seems like it would.&quot;<p>And they were right. The scaling curves may break at some point. But they don&#x27;t show indications of that yet.<p>Lastly, all of this is largely just taking existing model architectures and scaling up. Neural nets are a very young technology. There will be better architectures in the future.</div><br/></div></div><div id="38390124" class="c"><input type="checkbox" id="c-38390124" checked=""/><div class="controls bullet"><span class="by">uoaei</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389729">parent</a><span>|</span><a href="#38389992">prev</a><span>|</span><a href="#38389790">next</a><span>|</span><label class="collapse" for="c-38390124">[-]</label><label class="expand" for="c-38390124">[1 more]</label></div><br/><div class="children"><div class="content">Any claims of objective, quantitative measurements of &quot;scaling&quot; in LLMs is voodoo snake oil when measured against some benchmarks consisting of &quot;which questions does it answer correctly&quot;. Any machine learning PhD will admit this, albeit only in a quiet corner of a noisy bar after a few more drinks than is advisable when they&#x27;re earning money from companies who claim scaling wins on such benchmarks.</div><br/></div></div><div id="38389790" class="c"><input type="checkbox" id="c-38389790" checked=""/><div class="controls bullet"><span class="by">OOPMan</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389729">parent</a><span>|</span><a href="#38390124">prev</a><span>|</span><a href="#38389744">next</a><span>|</span><label class="collapse" for="c-38389790">[-]</label><label class="expand" for="c-38389790">[2 more]</label></div><br/><div class="children"><div class="content">Honestly, OpenAI seem more like a cult that a company to me.<p>The hyperbole that surrounds them fits the mould nicely.</div><br/><div id="38390745" class="c"><input type="checkbox" id="c-38390745" checked=""/><div class="controls bullet"><span class="by">hutzlibu</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389790">parent</a><span>|</span><a href="#38389744">next</a><span>|</span><label class="collapse" for="c-38390745">[-]</label><label class="expand" for="c-38390745">[1 more]</label></div><br/><div class="children"><div class="content">They did build the most advanced LLM tool, though.</div><br/></div></div></div></div></div></div></div></div><div id="38389744" class="c"><input type="checkbox" id="c-38389744" checked=""/><div class="controls bullet"><span class="by">raincole</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389502">parent</a><span>|</span><a href="#38389579">prev</a><span>|</span><a href="#38389580">next</a><span>|</span><label class="collapse" for="c-38389744">[-]</label><label class="expand" for="c-38389744">[6 more]</label></div><br/><div class="children"><div class="content">But he also has the incentive to exaggerate the AI&#x27;s ability.<p>The whole idea of double-blind test (and really, the whole scientific methodology) is based on one simple thing: even the most experienced and informed professionals can be comfortably wrong.<p>We&#x27;ll only know when we see it. Or at least when several independent research groups see it.</div><br/><div id="38390028" class="c"><input type="checkbox" id="c-38390028" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389744">parent</a><span>|</span><a href="#38389855">next</a><span>|</span><label class="collapse" for="c-38390028">[-]</label><label class="expand" for="c-38390028">[2 more]</label></div><br/><div class="children"><div class="content">&gt; even the most experienced and informed professionals can be comfortably wrong<p>That&#x27;s the human hallucination problem. In science it&#x27;s a very difficult issue to deal with, only in hindsight you can tell which papers from a given period were the good ones. It takes a whole scientific community to come up with the truth, and sometimes we fail.</div><br/><div id="38390464" class="c"><input type="checkbox" id="c-38390464" checked=""/><div class="controls bullet"><span class="by">auggierose</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38390028">parent</a><span>|</span><a href="#38389855">next</a><span>|</span><label class="collapse" for="c-38390464">[-]</label><label class="expand" for="c-38390464">[1 more]</label></div><br/><div class="children"><div class="content">No. It takes just one person to come up with the truth. It then can takes ages to convince the &quot;scientific community&quot;.</div><br/></div></div></div></div><div id="38389855" class="c"><input type="checkbox" id="c-38389855" checked=""/><div class="controls bullet"><span class="by">lokar</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389744">parent</a><span>|</span><a href="#38390028">prev</a><span>|</span><a href="#38390065">next</a><span>|</span><label class="collapse" for="c-38389855">[-]</label><label class="expand" for="c-38389855">[2 more]</label></div><br/><div class="children"><div class="content">I thought (and could be wrong) that all of these concerns are based on a very low probability of a very bad outcome.<p>So:  we might be close to a breakthrough, that breakthrough could get out of hand, then it could kill a billion+ people.</div><br/><div id="38390106" class="c"><input type="checkbox" id="c-38390106" checked=""/><div class="controls bullet"><span class="by">patrec</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389855">parent</a><span>|</span><a href="#38390065">next</a><span>|</span><label class="collapse" for="c-38390106">[-]</label><label class="expand" for="c-38390106">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I thought (and could be wrong) that all of these concerns are based on a very low probability of a very bad outcome.<p>Among knowledgeable people who have concerns in the first place, I&#x27;d say giving the probability of a very bad outcome of cumulative advances as &quot;very low&quot; is a fringe position. It seems to vary more between &quot;significant&quot; and &quot;close to unity&quot;.<p>There are some knowledgeable people like Yann LeCun who have no concerns whatsoever but they seem singularly bad at communicating why this would be a rational position to take.</div><br/></div></div></div></div><div id="38390065" class="c"><input type="checkbox" id="c-38390065" checked=""/><div class="controls bullet"><span class="by">aidaman</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389744">parent</a><span>|</span><a href="#38389855">prev</a><span>|</span><a href="#38389580">next</a><span>|</span><label class="collapse" for="c-38390065">[-]</label><label class="expand" for="c-38390065">[1 more]</label></div><br/><div class="children"><div class="content">Unlikely.  We&#x27;ll know when OpenAI has declared itself ruler of the new world, imposes martial law, and takes over.</div><br/></div></div></div></div><div id="38389580" class="c"><input type="checkbox" id="c-38389580" checked=""/><div class="controls bullet"><span class="by">nobrains</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389502">parent</a><span>|</span><a href="#38389744">prev</a><span>|</span><a href="#38390563">next</a><span>|</span><label class="collapse" for="c-38389580">[-]</label><label class="expand" for="c-38389580">[1 more]</label></div><br/><div class="children"><div class="content">Also, wbhart is referring to publicly released LLMs, while the OpenAI researchers are most likely referring to an un-released in-research LLM.</div><br/></div></div><div id="38390563" class="c"><input type="checkbox" id="c-38390563" checked=""/><div class="controls bullet"><span class="by">las_balas_tres</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389502">parent</a><span>|</span><a href="#38389580">prev</a><span>|</span><a href="#38389715">next</a><span>|</span><label class="collapse" for="c-38390563">[-]</label><label class="expand" for="c-38390563">[1 more]</label></div><br/><div class="children"><div class="content">Sure... but that machine learning PhD has a vested interest in being optimistically biased in his observations.</div><br/></div></div></div></div><div id="38389715" class="c"><input type="checkbox" id="c-38389715" checked=""/><div class="controls bullet"><span class="by">insomagent</span><span>|</span><a href="#38389417">parent</a><span>|</span><a href="#38389502">prev</a><span>|</span><a href="#38389589">next</a><span>|</span><label class="collapse" for="c-38389715">[-]</label><label class="expand" for="c-38389715">[15 more]</label></div><br/><div class="children"><div class="content">Let&#x27;s say a model runs through a few iterations and finds a small, meaningful piece of information via &quot;self-play&quot; (iterating with itself without further prompting from a human.)<p>If the model then distills that information down to a new feature, and re-examines the original prompt with the new feature embedded in an extra input tensor, then repeats this process ad-infinitum, will the language model&#x27;s &quot;prime directive&quot; and reasoning ability be sufficient to arrive at new, verifiable and provable conjectures, outside the realm of the dataset it was trained on?<p>If GPT-4,5,...,n can progress in this direction, then we should all see the writing on the wall.  Also, the day will come where we don&#x27;t need to manually prepare an updated dataset and &quot;kick off a new training&quot;.  Self-supervised LLMs are going to be so shocking.</div><br/><div id="38389759" class="c"><input type="checkbox" id="c-38389759" checked=""/><div class="controls bullet"><span class="by">wbhart</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389715">parent</a><span>|</span><a href="#38390059">next</a><span>|</span><label class="collapse" for="c-38389759">[-]</label><label class="expand" for="c-38389759">[13 more]</label></div><br/><div class="children"><div class="content">People have done experiments trying to get GPT-4 to come up with viable conjectures. So far it does such a woefully bad job that it isn&#x27;t worth even trying.<p>Unfortunately there are rather a lot of issues which are difficult to describe concisely, so here is probably not the best place.<p>Primary amongst them is the fact that an LLM would be a horribly inefficient way to do this. There are much, much better ways, which have been tried, with limited success.</div><br/><div id="38389897" class="c"><input type="checkbox" id="c-38389897" checked=""/><div class="controls bullet"><span class="by">gmerc</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389759">parent</a><span>|</span><a href="#38390059">next</a><span>|</span><label class="collapse" for="c-38389897">[-]</label><label class="expand" for="c-38389897">[12 more]</label></div><br/><div class="children"><div class="content">After a year the entire argument you make boils down to “so far”.</div><br/><div id="38390110" class="c"><input type="checkbox" id="c-38390110" checked=""/><div class="controls bullet"><span class="by">Terr_</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389897">parent</a><span>|</span><a href="#38389950">next</a><span>|</span><label class="collapse" for="c-38390110">[-]</label><label class="expand" for="c-38390110">[9 more]</label></div><br/><div class="children"><div class="content">Whereas your post sounds like &quot;Just give the approach more time, it shall continue to  incrementally improve until it finally works someday, <i>cuz reasons.</i>&quot;<p>Early attempts at human flight approached it by strapping wings to people&#x27;s arms and flapping: Do you think that would have eventually worked too, if only we had just given it a bit more time and faith?</div><br/><div id="38390831" class="c"><input type="checkbox" id="c-38390831" checked=""/><div class="controls bullet"><span class="by">londons_explore</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38390110">parent</a><span>|</span><a href="#38390203">next</a><span>|</span><label class="collapse" for="c-38390831">[-]</label><label class="expand" for="c-38390831">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Early attempts at human flight approached it by strapping wings to people&#x27;s arms and flapping: Do you think that would have eventually worked too, if only we had just given it a bit more time and faith?<p>Interestingly, we how have human powered aircraft...   We have flown ~60km with human leg power alone.    We&#x27;ve also got human powered ornithopters (flapping wing designs) which can fly but only for very short times before the pilot is exhausted.<p>I expect that another 100 years from now, both records will be exceeded, altough probably for scientific curiosity more than because human powered flight is actually useful.</div><br/></div></div><div id="38390203" class="c"><input type="checkbox" id="c-38390203" checked=""/><div class="controls bullet"><span class="by">xcv123</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38390110">parent</a><span>|</span><a href="#38390831">prev</a><span>|</span><a href="#38389950">next</a><span>|</span><label class="collapse" for="c-38390203">[-]</label><label class="expand" for="c-38390203">[7 more]</label></div><br/><div class="children"><div class="content">&gt; Just give the approach more time, it shall continue to incrementally improve until it finally works someday, cuz reasons<p>Yes. Because we haven&#x27;t yet reached the limit of deep learning models. GPT-3.5 has 175 billion parameters. GPT-4 has an estimated 1.8 trillion parameters. That was nearly a year ago. Wait until you see what&#x27;s next.</div><br/><div id="38390354" class="c"><input type="checkbox" id="c-38390354" checked=""/><div class="controls bullet"><span class="by">meheleventyone</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38390203">parent</a><span>|</span><a href="#38390411">next</a><span>|</span><label class="collapse" for="c-38390354">[-]</label><label class="expand" for="c-38390354">[5 more]</label></div><br/><div class="children"><div class="content">Why would adding more parameters suddenly make it better at this sort of reasoning? It feels a bit of a “god of the gaps” where it’ll just stop being a stochastic parrot in just a few more million parameters.</div><br/><div id="38390559" class="c"><input type="checkbox" id="c-38390559" checked=""/><div class="controls bullet"><span class="by">Al-Khwarizmi</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38390354">parent</a><span>|</span><a href="#38390631">next</a><span>|</span><label class="collapse" for="c-38390559">[-]</label><label class="expand" for="c-38390559">[3 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think it&#x27;s guaranteed, but I do think it&#x27;s very plausible because we&#x27;ve seen these models gain emerging abilities at every iteration, just from sheer scaling. So extrapolation tells us that they may keep gaining more capabilities (we don&#x27;t know how exactly it does it, though, so of course it&#x27;s all speculation).<p>I don&#x27;t think many people would describe GPT-4 as a stochastic parrot already... when the paper that coined (or at least popularized) the term came up in early 2021, the term made a lot of sense. In late 2023, with models that at the very least show clear signs of creativity (I&#x27;m sticking to that because &quot;reasoning&quot; or not is more controversial), it&#x27;s relegated to reductionistic philosophical arguments, but not really a practical description anymore.</div><br/><div id="38390606" class="c"><input type="checkbox" id="c-38390606" checked=""/><div class="controls bullet"><span class="by">meheleventyone</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38390559">parent</a><span>|</span><a href="#38390631">next</a><span>|</span><label class="collapse" for="c-38390606">[-]</label><label class="expand" for="c-38390606">[2 more]</label></div><br/><div class="children"><div class="content">I don’t think we should throw out the stochastic parrot so easily. As you say there are “clear signs of creativity” but that could be it getting significantly better as a stochastic parrot. We have no real test to tell mimicry apart from reasoning and as you note we also can only speculate about how any of it works. I don’t think it’s reductionist in light of that, maybe cautious or pessimistic.</div><br/><div id="38390817" class="c"><input type="checkbox" id="c-38390817" checked=""/><div class="controls bullet"><span class="by">Al-Khwarizmi</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38390606">parent</a><span>|</span><a href="#38390631">next</a><span>|</span><label class="collapse" for="c-38390817">[-]</label><label class="expand" for="c-38390817">[1 more]</label></div><br/><div class="children"><div class="content">They can write original stories in a setting deliberately designed to not be found in the training set (<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.08433" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.08433</a>). To me that&#x27;s rather strong evidence of being beyond stochastic parrots by now, although I must concede that we know so little about how everything works, that who knows.</div><br/></div></div></div></div></div></div><div id="38390631" class="c"><input type="checkbox" id="c-38390631" checked=""/><div class="controls bullet"><span class="by">vbezhenar</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38390354">parent</a><span>|</span><a href="#38390559">prev</a><span>|</span><a href="#38390411">next</a><span>|</span><label class="collapse" for="c-38390631">[-]</label><label class="expand" for="c-38390631">[1 more]</label></div><br/><div class="children"><div class="content">Humans and other animals definitely different when it comes to reasoning. At the same time, biologically humans and many other animals are very similar, when it comes to brain, but humans have more &quot;processing power&quot;. So it&#x27;s only natural to expect some emergent properties from increasing number of parameters.</div><br/></div></div></div></div><div id="38390411" class="c"><input type="checkbox" id="c-38390411" checked=""/><div class="controls bullet"><span class="by">tarsinge</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38390203">parent</a><span>|</span><a href="#38390354">prev</a><span>|</span><a href="#38389950">next</a><span>|</span><label class="collapse" for="c-38390411">[-]</label><label class="expand" for="c-38390411">[1 more]</label></div><br/><div class="children"><div class="content">You are missing the point that it can be a model limit. LLMs were a breakthrough but that doesn’t mean they are a good model for some other problems, no matter the number of parameters. Language contains more than we thought, as GPT has impressively showed (ie semantics embedded in the syntax emerging from text compression), but still not every intellectual process is language based.</div><br/></div></div></div></div></div></div><div id="38389950" class="c"><input type="checkbox" id="c-38389950" checked=""/><div class="controls bullet"><span class="by">ra</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389897">parent</a><span>|</span><a href="#38390110">prev</a><span>|</span><a href="#38390059">next</a><span>|</span><label class="collapse" for="c-38389950">[-]</label><label class="expand" for="c-38389950">[2 more]</label></div><br/><div class="children"><div class="content">Indeed. LLM is an application on a transformer trained with backpropagation. What stops you from adding a logic&#x2F;mathematic &quot;application&quot; on the same transformer?</div><br/><div id="38390763" class="c"><input type="checkbox" id="c-38390763" checked=""/><div class="controls bullet"><span class="by">seanhunter</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389950">parent</a><span>|</span><a href="#38390059">next</a><span>|</span><label class="collapse" for="c-38390763">[-]</label><label class="expand" for="c-38390763">[1 more]</label></div><br/><div class="children"><div class="content">Nothing, and there are methods which allow these types of models to learn to use special purpose tools of this kind[1].<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2302.04761" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2302.04761</a> Toolformer: Language Models Can Teach Themselves to Use Tools</div><br/></div></div></div></div></div></div></div></div><div id="38390059" class="c"><input type="checkbox" id="c-38390059" checked=""/><div class="controls bullet"><span class="by">jimmySixDOF</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389715">parent</a><span>|</span><a href="#38389759">prev</a><span>|</span><a href="#38389589">next</a><span>|</span><label class="collapse" for="c-38390059">[-]</label><label class="expand" for="c-38390059">[1 more]</label></div><br/><div class="children"><div class="content">Yes, it seems like this is a direction to replace RLHF so another way to scale without baremetal and if not this then still just a matter of time before some model optimization outperforms the raw epoch&#x2F;parameters&#x2F;token approach.</div><br/></div></div></div></div><div id="38389589" class="c"><input type="checkbox" id="c-38389589" checked=""/><div class="controls bullet"><span class="by">caesil</span><span>|</span><a href="#38389417">parent</a><span>|</span><a href="#38389715">prev</a><span>|</span><a href="#38390365">next</a><span>|</span><label class="collapse" for="c-38389589">[-]</label><label class="expand" for="c-38389589">[2 more]</label></div><br/><div class="children"><div class="content">FWIW The Verge is reporting that people inside are also saying the Reuters story is bunk:<p><a href="https:&#x2F;&#x2F;www.theverge.com&#x2F;2023&#x2F;11&#x2F;22&#x2F;23973354&#x2F;a-recent-openai-breakthrough-on-the-path-to-agi-has-caused-a-stir" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.theverge.com&#x2F;2023&#x2F;11&#x2F;22&#x2F;23973354&#x2F;a-recent-openai...</a></div><br/><div id="38389703" class="c"><input type="checkbox" id="c-38389703" checked=""/><div class="controls bullet"><span class="by">himaraya</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389589">parent</a><span>|</span><a href="#38390365">next</a><span>|</span><label class="collapse" for="c-38389703">[-]</label><label class="expand" for="c-38389703">[1 more]</label></div><br/><div class="children"><div class="content">&gt; After being contacted by Reuters, OpenAI, which declined to comment, acknowledged in an internal message to staffers a project called Q* and a letter to the board before the weekend&#x27;s events, one of the people said.<p>Reuters update 6:51 PST<p>The Verge has acted like an intermediary for Sam&#x27;s camp during this whole saga, from my reading.</div><br/></div></div></div></div><div id="38390365" class="c"><input type="checkbox" id="c-38390365" checked=""/><div class="controls bullet"><span class="by">poulpy123</span><span>|</span><a href="#38389417">parent</a><span>|</span><a href="#38389589">prev</a><span>|</span><a href="#38389838">next</a><span>|</span><label class="collapse" for="c-38390365">[-]</label><label class="expand" for="c-38390365">[6 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know for Q* of course, but all the tests I made with GPT4, and all what I&#x27;ve read and seen about it, show that it is unable to reason. It was trained with an unfathomable amount of data, so it can simulate reasoning very well, but it is unable to reason</div><br/><div id="38390428" class="c"><input type="checkbox" id="c-38390428" checked=""/><div class="controls bullet"><span class="by">oezi</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38390365">parent</a><span>|</span><a href="#38389838">next</a><span>|</span><label class="collapse" for="c-38390428">[-]</label><label class="expand" for="c-38390428">[5 more]</label></div><br/><div class="children"><div class="content">What is the difference between simulating reasoning very well and &quot;actual&quot; reasoning?</div><br/><div id="38390533" class="c"><input type="checkbox" id="c-38390533" checked=""/><div class="controls bullet"><span class="by">silvaring</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38390428">parent</a><span>|</span><a href="#38390609">next</a><span>|</span><label class="collapse" for="c-38390533">[-]</label><label class="expand" for="c-38390533">[1 more]</label></div><br/><div class="children"><div class="content">Actual reasoning is made up of various biological feedback loops that happen in the body and brain, essentially your physical senses give you the ability to reason in the first place, without the eyes, ears etc there is no ability to learn basic reasoning, which is why kids who are blind or mute from birth have huge issues learning about object permanence, spatial awaraness etc. You cant expect human reasoning without human perception.<p>My question is how does the AI perceive. Basically how good is the simulation for its perception. If we know that, then we can probably assess its ability to reason because we can compare it to the closest benchmark we have (your average human being). How do AI&#x27;s see, how did they learn concepts in strings of words and pixels? How does the concept it learnt in text carry through to images of colors, of shapes? Does it show a transfer of conceptual understanding across both two and three dimentional shapes?<p>I know these are more questions than answers, but its just things that I&#x27;ve been wondering about.</div><br/></div></div><div id="38390609" class="c"><input type="checkbox" id="c-38390609" checked=""/><div class="controls bullet"><span class="by">seanhunter</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38390428">parent</a><span>|</span><a href="#38390533">prev</a><span>|</span><a href="#38390471">next</a><span>|</span><label class="collapse" for="c-38390609">[-]</label><label class="expand" for="c-38390609">[1 more]</label></div><br/><div class="children"><div class="content">Actual reasoning shows the understanding and use of a model of the key features of the underlying problem&#x2F;domain.<p>As a simple example that you can replicate using chatgpt, ask it to solve some simple maths problem.  Very frequently you will get a solution that looks like reasoning but is not, and reveals that it does not have an actual model of the underlying maths but is in fact doing text prediction based on a history of maths.  For example see here[1]. I ask it for some quadratics in x with some specification on the number of roots.  It gives me what looks at first glance like a decent answer.  Then I ask the same exact question but asking for quadratics in x and y[2]. Again the answer looks plausible except that for the solution &quot;with one real root&quot; it says the solution has one real root when x + y =1. Well there are infinite real values for x and y such that x + y =1, not one real root.  It looks like it has solved the problem but instead it has simulated the solving of the problem.<p>Likewise stacking problems, used to check for whether an AI has a model of the world. This is covered in &quot;From task structures to world models: What do LLMs know?&quot;[3] but for example here[4] I ask it whether it&#x27;s easier to balance a barrel on a plank or a plank on a barrel.  The model says it&#x27;s easier to balance a plank on a barrel with an output text that simulates reasoning discussing center of mass and the difference between the flatness of the plank and the tendency of the barrel to roll because of its curvature. Actual reasoning would say to put the barrel on its end so it doesn&#x27;t roll (whether you put the plank on top or not).<p>[1] <a href="https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;64556be8-ad20-41aa-99af-ed5a42201517" rel="nofollow noreferrer">https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;64556be8-ad20-41aa-99af-ed5a42...</a><p>[2] <a href="https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;2cd39197-dc09-4d07-a0d6-6cd800a08b80" rel="nofollow noreferrer">https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;2cd39197-dc09-4d07-a0d6-6cd800...</a><p>[3] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.04276" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.04276</a><p>[4] <a href="https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;4b631a92-0d55-4ae5-8892-9be02559e185" rel="nofollow noreferrer">https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;4b631a92-0d55-4ae5-8892-9be025...</a></div><br/></div></div><div id="38390471" class="c"><input type="checkbox" id="c-38390471" checked=""/><div class="controls bullet"><span class="by">parentheses</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38390428">parent</a><span>|</span><a href="#38390609">prev</a><span>|</span><a href="#38390545">next</a><span>|</span><label class="collapse" for="c-38390471">[-]</label><label class="expand" for="c-38390471">[1 more]</label></div><br/><div class="children"><div class="content">I think the poster meant that it&#x27;s capable of having a high probability of correct reasoning - simulating reasoning is lossy, actual reasoning is not. Though, human reasoning  is still lossy.</div><br/></div></div><div id="38390545" class="c"><input type="checkbox" id="c-38390545" checked=""/><div class="controls bullet"><span class="by">Jare</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38390428">parent</a><span>|</span><a href="#38390471">prev</a><span>|</span><a href="#38389838">next</a><span>|</span><label class="collapse" for="c-38390545">[-]</label><label class="expand" for="c-38390545">[1 more]</label></div><br/><div class="children"><div class="content">Probably best to say &quot;simulate the appearance of reasoning&quot;: looks and feels 100% acceptable at a surface level, but the actual details and conclusions are completely wrong &#x2F; do not follow.</div><br/></div></div></div></div></div></div><div id="38389838" class="c"><input type="checkbox" id="c-38389838" checked=""/><div class="controls bullet"><span class="by">topspin</span><span>|</span><a href="#38389417">parent</a><span>|</span><a href="#38390365">prev</a><span>|</span><a href="#38389793">next</a><span>|</span><label class="collapse" for="c-38389838">[-]</label><label class="expand" for="c-38389838">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know whether this particular article is bunk.  I do know I&#x27;ve read many, many similar comments about how some complex task is beyond an conceivable model or system and then, years later, marveled at exactly that complex task being solved.</div><br/><div id="38389925" class="c"><input type="checkbox" id="c-38389925" checked=""/><div class="controls bullet"><span class="by">jhanschoo</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389838">parent</a><span>|</span><a href="#38389793">next</a><span>|</span><label class="collapse" for="c-38389925">[-]</label><label class="expand" for="c-38389925">[1 more]</label></div><br/><div class="children"><div class="content">The article isn&#x27;t describing something that will happen years later, but now. The comment author is saying that this current model is not AGI as it likely can&#x27;t solve university-level mathematics, and they are presumably open to the possibility of a model years down the line that can do that.</div><br/></div></div></div></div><div id="38389793" class="c"><input type="checkbox" id="c-38389793" checked=""/><div class="controls bullet"><span class="by">est</span><span>|</span><a href="#38389417">parent</a><span>|</span><a href="#38389838">prev</a><span>|</span><a href="#38389524">next</a><span>|</span><label class="collapse" for="c-38389793">[-]</label><label class="expand" for="c-38389793">[2 more]</label></div><br/><div class="children"><div class="content">&gt; The reason LLMs fail at solving mathematical problems is because<p>That&#x27;s exactly what Go&#x2F;Baduk&#x2F;Weiqi players think some years ago. And superalignment is defintely OpenAI&#x27;s major research objective:<p>&gt; <a href="https:&#x2F;&#x2F;openai.com&#x2F;blog&#x2F;our-approach-to-alignment-research" rel="nofollow noreferrer">https:&#x2F;&#x2F;openai.com&#x2F;blog&#x2F;our-approach-to-alignment-research</a><p>&gt; our AI systems are proposing very creative solutions (like AlphaGo’s move 37)<p>When will mathematicians face the move 37 moment?</div><br/><div id="38390087" class="c"><input type="checkbox" id="c-38390087" checked=""/><div class="controls bullet"><span class="by">Davidzheng</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389793">parent</a><span>|</span><a href="#38389524">next</a><span>|</span><label class="collapse" for="c-38390087">[-]</label><label class="expand" for="c-38390087">[1 more]</label></div><br/><div class="children"><div class="content">Probably in &lt;3 years</div><br/></div></div></div></div><div id="38389524" class="c"><input type="checkbox" id="c-38389524" checked=""/><div class="controls bullet"><span class="by">adastra22</span><span>|</span><a href="#38389417">parent</a><span>|</span><a href="#38389793">prev</a><span>|</span><a href="#38389516">next</a><span>|</span><label class="collapse" for="c-38389524">[-]</label><label class="expand" for="c-38389524">[4 more]</label></div><br/><div class="children"><div class="content">Back-tracking is a very nearly solved problem in the context of Prolog-like languages or mathematical theorem provers (as you probably well know). There are many ways you could integrate an LLM-like system into a tactic-based theorem prover without having to restart from the beginning for each alternative. Simply checkpointing and backtracking to a checkpoint would naively improve upon your described Monte Carlo algorithm. More likely I assume they are using RL to unwind state backwards and update based on the negative result, which would be significantly more complicated but also much more powerful (essentially it would one-shot learn from each failure).<p>That&#x27;s just what I came up with after thinking on it for 2 minutes. I&#x27;m sure they have even better ideas.</div><br/><div id="38390070" class="c"><input type="checkbox" id="c-38390070" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389524">parent</a><span>|</span><a href="#38390202">next</a><span>|</span><label class="collapse" for="c-38390070">[-]</label><label class="expand" for="c-38390070">[1 more]</label></div><br/><div class="children"><div class="content">You can also consider the chatGPT app as a RL environment. The environment is made of the agent (AI), a second agent (human), and some tools (web search, code, APIs, vision). This grounds the AI into human and tool responses. They can generate feedback that can be incorporated into the model by RL methods.<p>Basically every reply from a human can be interpreted as a reward signal. If the human restates the question, it means a negative reward, the AI didn&#x27;t get it. If the human corrects the AI, another negative reward, but if they continue the thread then it is positive. You can judge turn-by-turn and end-to-end all chat logs with GPT4 to annotate.<p>The great thing about chat based feedback is that it is scalable. OpenAI has 100M users, they generate these chat sessions by the millions every day. Then they just need to do a second pass (expensive, yes) to annotate the chat logs with RL reward signals and retrain. But they get the human-in-the-loop for free, and that is the best source of feedback.<p>AI-human chat data is in-domain for both the AI and human, something we can&#x27;t say about other training data. It will contain the kind of mistakes AI does, and the kind of demands humans want to solve with AI. My bet is that OpenAI have realized this and created GPTs in order to enrich and empower the AI to create the best training data for GPT-5.<p>The secret sauce of OpenAI is not their people, or Sam, or the computers, but the training set, especially the augmented and synthetic parts.</div><br/></div></div><div id="38390202" class="c"><input type="checkbox" id="c-38390202" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389524">parent</a><span>|</span><a href="#38390070">prev</a><span>|</span><a href="#38389598">next</a><span>|</span><label class="collapse" for="c-38390202">[-]</label><label class="expand" for="c-38390202">[1 more]</label></div><br/><div class="children"><div class="content">&gt; That&#x27;s just what I came up with after thinking on it for 2 minutes. I&#x27;m sure they have even better ideas.<p>the thing is that ideas not necessary easy to implement. There will be many obstacles on route you described:<p>- quality of provers, is there good ergo provers which also can run at large scales (say billions of facts)<p>- you need some formalization approach, probably LLM will do some work, but we don&#x27;t know what will be quality<p>- LLM likely will generate many individual factoids, which are losely compatible, contradicting, etc, and untrivial effort is required to reconcile and connect them</div><br/></div></div><div id="38389598" class="c"><input type="checkbox" id="c-38389598" checked=""/><div class="controls bullet"><span class="by">wbhart</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389524">parent</a><span>|</span><a href="#38390202">prev</a><span>|</span><a href="#38389516">next</a><span>|</span><label class="collapse" for="c-38389598">[-]</label><label class="expand" for="c-38389598">[1 more]</label></div><br/><div class="children"><div class="content">There are certainly efforts along the lines of what you suggest. There are problems though. The number of backtracks is 10^k where k is not 2, or 3, or 4.....<p>Another issue is that of autoformalisation. This is the one part of the problem where an LLM might be able to help, if it were reliable enough (it isn&#x27;t currently) or if it could truly understand the logical structure of mathematical problems correctly (currently they can&#x27;t).</div><br/></div></div></div></div><div id="38389516" class="c"><input type="checkbox" id="c-38389516" checked=""/><div class="controls bullet"><span class="by">Davidzheng</span><span>|</span><a href="#38389417">parent</a><span>|</span><a href="#38389524">prev</a><span>|</span><a href="#38389488">next</a><span>|</span><label class="collapse" for="c-38389516">[-]</label><label class="expand" for="c-38389516">[4 more]</label></div><br/><div class="children"><div class="content">I agree that in and of itself it&#x27;s not enough to be alarmed. Also i have to say i don&#x27;t really know what grade school mathematics means here(multiplication? Proving triangles are congruent?). But I think the question is, whether the breakthrough is an algorithmic change in reasoning. If it is, then it could challenge all 4 of your limitations. Again this article is low on details so really we are arguing over our best guesses. But I wouldn&#x27;t be so confident that an improvement on simple math problems due to algorithms can have huge implications.<p>Also, do you remember what go players said when they beat Fan Hui? Change can come quick</div><br/><div id="38389642" class="c"><input type="checkbox" id="c-38389642" checked=""/><div class="controls bullet"><span class="by">wbhart</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389516">parent</a><span>|</span><a href="#38389488">next</a><span>|</span><label class="collapse" for="c-38389642">[-]</label><label class="expand" for="c-38389642">[3 more]</label></div><br/><div class="children"><div class="content">I think maybe I didn&#x27;t make myself quite clear here. There are already algorithms which can solve advanced mathematical problems 100% reliably (prove theorems). There are even algorithms which can prove any correct theorem that can be stated in a certain logical language, given enough time. There are even systems in which these algorithms have actually been implemented.<p>My point is that no technology which can solve grade school maths problems would be viewed as a breakthrough by anyone who understood the problem. The fundamental problems which need to be solved are not problems you encounter in grade school mathematics. The article is just ill-informed.</div><br/><div id="38389665" class="c"><input type="checkbox" id="c-38389665" checked=""/><div class="controls bullet"><span class="by">himaraya</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389642">parent</a><span>|</span><a href="#38390387">next</a><span>|</span><label class="collapse" for="c-38389665">[-]</label><label class="expand" for="c-38389665">[1 more]</label></div><br/><div class="children"><div class="content">The article suggests the way Q* solves basic math problems matters more than the difficulty of the problems themselves. Either way, I think judging the claims made remains premature without seeing the supporting documentation.</div><br/></div></div><div id="38390387" class="c"><input type="checkbox" id="c-38390387" checked=""/><div class="controls bullet"><span class="by">kenjackson</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389642">parent</a><span>|</span><a href="#38389665">prev</a><span>|</span><a href="#38389488">next</a><span>|</span><label class="collapse" for="c-38390387">[-]</label><label class="expand" for="c-38390387">[1 more]</label></div><br/><div class="children"><div class="content">“Given enough time” makes that a useless statement. Every kid in college learns this.<p>The ability to eventually solve a given theorem isn’t interesting — especially if the time is longer than the time left in the universe.<p>It’s far more interesting to see if an AI can, given an arbitrarily stated problem make clear progress quickly.</div><br/></div></div></div></div></div></div><div id="38389488" class="c"><input type="checkbox" id="c-38389488" checked=""/><div class="controls bullet"><span class="by">abeppu</span><span>|</span><a href="#38389417">parent</a><span>|</span><a href="#38389516">prev</a><span>|</span><a href="#38390773">next</a><span>|</span><label class="collapse" for="c-38389488">[-]</label><label class="expand" for="c-38389488">[2 more]</label></div><br/><div class="children"><div class="content">This comment seems to presume that Q* is related to existing LLM work -- which isn&#x27;t stated in the article. Others have guessed that the &#x27;Q&#x27; in Q* is from Q-learning in RL. In particular backtracking, which you point out LLMs cannot do, would not be an issue in an appropriate RL setup.</div><br/><div id="38389779" class="c"><input type="checkbox" id="c-38389779" checked=""/><div class="controls bullet"><span class="by">nullc</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389488">parent</a><span>|</span><a href="#38390773">next</a><span>|</span><label class="collapse" for="c-38389779">[-]</label><label class="expand" for="c-38389779">[1 more]</label></div><br/><div class="children"><div class="content">&gt; which you point out LLMs cannot do, would not be an issue in an appropriate RL setup.<p>Hm? it&#x27;s pretty trivial to use a sampler for LLMs that has a beam search and will effectively &#x27;backtrack&#x27; a &#x27;bad&#x27; selection.<p>It just doesn&#x27;t normally help-- by construction the LLM sampled normally already approximates the correct overall distribution for the entire output, without any search.<p>I assume using a beam search does help when your sampler does have some non-trivial constraints (like the output satisfies some grammar or passes an algebraic test, or even just top-n sampling since those adjustments on a token by token basis result in a different approximate distribution than the original distribution filtered by the constraints).</div><br/></div></div></div></div><div id="38390773" class="c"><input type="checkbox" id="c-38390773" checked=""/><div class="controls bullet"><span class="by">kolinko</span><span>|</span><a href="#38389417">parent</a><span>|</span><a href="#38389488">prev</a><span>|</span><a href="#38389676">next</a><span>|</span><label class="collapse" for="c-38390773">[-]</label><label class="expand" for="c-38390773">[1 more]</label></div><br/><div class="children"><div class="content">LLMs by themselves don’t learn from past past mistakes, but you could cycle inference steps and fine tuning&#x2F;retraining steps.<p>Also, you can store failed attempts and lessons learned in context.</div><br/></div></div><div id="38389676" class="c"><input type="checkbox" id="c-38389676" checked=""/><div class="controls bullet"><span class="by">richardw</span><span>|</span><a href="#38389417">parent</a><span>|</span><a href="#38390773">prev</a><span>|</span><a href="#38389572">next</a><span>|</span><label class="collapse" for="c-38389676">[-]</label><label class="expand" for="c-38389676">[3 more]</label></div><br/><div class="children"><div class="content">On backtracking, I thought tree-of-thought enabled that?<p>&quot;considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices&quot;<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.10601" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.10601</a><p>Generally with you though, this thing is not leading to real smarts and that&#x27;s accepted by many. Yes, it&#x27;ll fill in a few gaps with exponentially more compute but it&#x27;s more likely that an algo change is required once we&#x27;ve maxed out LLM&#x27;s.</div><br/><div id="38389906" class="c"><input type="checkbox" id="c-38389906" checked=""/><div class="controls bullet"><span class="by">wbhart</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389676">parent</a><span>|</span><a href="#38389572">next</a><span>|</span><label class="collapse" for="c-38389906">[-]</label><label class="expand" for="c-38389906">[2 more]</label></div><br/><div class="children"><div class="content">Yes, there are various approaches like tree-of-thought. They don&#x27;t fundamentally solve the problem because there are just too many paths to explore and inference is just too slow and too expensive to explore 10,000 or 100,000 paths just for basic problems that no one wanted to solve anyway.<p>The problem with solving such problems with LLMs is that if the solution to the problem is unlike problems seen in training, the LLM will almost every time take the wrong path and very likely won&#x27;t even think of the right path at all.<p>The AI really does need to understand why the paths it tried failed in order to get insight into what might work. That&#x27;s how humans work (well, one of many techniques we use). And despite what people think, LLMs really don&#x27;t understand what they are doing. That&#x27;s relatively easy to demonstrate if you get an LLM off distribution. They will double down on obviously erroneous illogic, rather than learn from the entirely new situation.</div><br/><div id="38389988" class="c"><input type="checkbox" id="c-38389988" checked=""/><div class="controls bullet"><span class="by">richardw</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389906">parent</a><span>|</span><a href="#38389572">next</a><span>|</span><label class="collapse" for="c-38389988">[-]</label><label class="expand" for="c-38389988">[1 more]</label></div><br/><div class="children"><div class="content">Thank you for the thoughtful response</div><br/></div></div></div></div></div></div><div id="38389572" class="c"><input type="checkbox" id="c-38389572" checked=""/><div class="controls bullet"><span class="by">calf</span><span>|</span><a href="#38389417">parent</a><span>|</span><a href="#38389676">prev</a><span>|</span><a href="#38390482">next</a><span>|</span><label class="collapse" for="c-38389572">[-]</label><label class="expand" for="c-38389572">[5 more]</label></div><br/><div class="children"><div class="content">But, isn&#x27;t AlphaGo a solution to kind of specific mathematical problem? And that it has passed with flying colors?<p>What I mean is, yes, neural networks are stochastic and that seems to be why they&#x27;re bad at logic; on the other hand it&#x27; not exactly hallucinating a game of Go, and that seems different to how neural networks are prone to hallucination and confabulation on natural language or X-ray imaging.</div><br/><div id="38389839" class="c"><input type="checkbox" id="c-38389839" checked=""/><div class="controls bullet"><span class="by">wbhart</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389572">parent</a><span>|</span><a href="#38390482">next</a><span>|</span><label class="collapse" for="c-38389839">[-]</label><label class="expand" for="c-38389839">[4 more]</label></div><br/><div class="children"><div class="content">Sure, but people have already applied deep learning techniques to theorem proving. There are some impressive results (which the press doesn&#x27;t seem at all interested in because it doesn&#x27;t have ChatGPT in the title).<p>It&#x27;s really harder than one might imagine to develop a system which is good at higher order logic, premise selection, backtracking, algebraic manipulation, arithmetic, conjecturing, pattern recognition, visual modeling, has a good mathematical knowledge, is autonomous and fast enough to be useful.<p>For my money, it isn&#x27;t just a matter of fitting a few existing jigsaw pieces together in some new combination. Some of the pieces don&#x27;t exist yet.</div><br/><div id="38390319" class="c"><input type="checkbox" id="c-38390319" checked=""/><div class="controls bullet"><span class="by">calf</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389839">parent</a><span>|</span><a href="#38390039">next</a><span>|</span><label class="collapse" for="c-38390319">[-]</label><label class="expand" for="c-38390319">[1 more]</label></div><br/><div class="children"><div class="content">Then your critique is about LLMs specifically.<p>But even there, can we say scientifically that LLMs cannot do math? Do we actually know that? And in my mind, that would imply LLMs cannot achieve AGI either. What do we actually know about the limitations of various approaches?<p>And couldn&#x27;t people argue that it&#x27;s not even necessary to think in terms of capabilities as if they were modules or pieces? Maybe just brute-force the whole thing, make a planetary scale computer. In principle.</div><br/></div></div><div id="38390039" class="c"><input type="checkbox" id="c-38390039" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389839">parent</a><span>|</span><a href="#38390319">prev</a><span>|</span><a href="#38390482">next</a><span>|</span><label class="collapse" for="c-38390039">[-]</label><label class="expand" for="c-38390039">[2 more]</label></div><br/><div class="children"><div class="content">You seem knowledgeable. Can you share a couple of interesting papers for theorem proving that came out in the last year? I read a few of them as they came out, and it seemed neural nets can advance the field by mixing &quot;soft&quot; language with &quot;hard&quot; symbolic systems.</div><br/><div id="38390307" class="c"><input type="checkbox" id="c-38390307" checked=""/><div class="controls bullet"><span class="by">wbhart</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38390039">parent</a><span>|</span><a href="#38390482">next</a><span>|</span><label class="collapse" for="c-38390307">[-]</label><label class="expand" for="c-38390307">[1 more]</label></div><br/><div class="children"><div class="content">The field is fairly new to me. I&#x27;m originally from computer algebra, and somehow struggling into the field of ATP.<p>The most interesting papers to me personally are the following three:<p>* Making higher order superposition work. <a href="https:&#x2F;&#x2F;doi.org&#x2F;10.1007&#x2F;978-3-030-79876-5_24" rel="nofollow noreferrer">https:&#x2F;&#x2F;doi.org&#x2F;10.1007&#x2F;978-3-030-79876-5_24</a><p>* MizAR 60 for Mizar 50. <a href="https:&#x2F;&#x2F;doi.org&#x2F;10.48550&#x2F;arXiv.2303.06686" rel="nofollow noreferrer">https:&#x2F;&#x2F;doi.org&#x2F;10.48550&#x2F;arXiv.2303.06686</a><p>* Magnus Hammer, a Transformer Based Approach to Premise Selection. 
<a href="https:&#x2F;&#x2F;doi.org&#x2F;10.48550&#x2F;arXiv.2303.04488" rel="nofollow noreferrer">https:&#x2F;&#x2F;doi.org&#x2F;10.48550&#x2F;arXiv.2303.04488</a><p>Your mileage may vary.</div><br/></div></div></div></div></div></div></div></div><div id="38390482" class="c"><input type="checkbox" id="c-38390482" checked=""/><div class="controls bullet"><span class="by">codedokode</span><span>|</span><a href="#38389417">parent</a><span>|</span><a href="#38389572">prev</a><span>|</span><a href="#38389448">next</a><span>|</span><label class="collapse" for="c-38390482">[-]</label><label class="expand" for="c-38390482">[2 more]</label></div><br/><div class="children"><div class="content">&gt; The reason LLMs fail at solving mathematical problems is because<p>...because they are too small and have too little weights. Cats cannot solve mathematical  problems too, but unlike cats, neural network evolve.</div><br/><div id="38390572" class="c"><input type="checkbox" id="c-38390572" checked=""/><div class="controls bullet"><span class="by">serf</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38390482">parent</a><span>|</span><a href="#38389448">next</a><span>|</span><label class="collapse" for="c-38390572">[-]</label><label class="expand" for="c-38390572">[1 more]</label></div><br/><div class="children"><div class="content">&gt;Cats cannot solve mathematical problems too, but unlike cats, neural network evolve.<p>cats evolve plenty, pressure towards mathematical reasoning has stymied as of late what with the cans of food and humans.</div><br/></div></div></div></div><div id="38389448" class="c"><input type="checkbox" id="c-38389448" checked=""/><div class="controls bullet"><span class="by">whatever1</span><span>|</span><a href="#38389417">parent</a><span>|</span><a href="#38390482">prev</a><span>|</span><a href="#38390588">next</a><span>|</span><label class="collapse" for="c-38389448">[-]</label><label class="expand" for="c-38389448">[1 more]</label></div><br/><div class="children"><div class="content">The thing is that a LLMs can point out a logic error in their reasoning if specifically asked to do so.<p>So maybe OpenAI just slapped an RL agent on top of the next-token generator.</div><br/></div></div><div id="38390695" class="c"><input type="checkbox" id="c-38390695" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#38389417">parent</a><span>|</span><a href="#38390588">prev</a><span>|</span><a href="#38389499">next</a><span>|</span><label class="collapse" for="c-38390695">[-]</label><label class="expand" for="c-38390695">[1 more]</label></div><br/><div class="children"><div class="content">Everything you said about LLMs being &quot;terrible at X&quot; is true of the current generation of LLM architectures.<p>From the sound of it, this Q* model has a fundamentally different architecture, which will almost certainly make some of those issues not terrible any more.<p>Most likely, the Q* design is the very similar to the one suggested recently by one of the Google AI teams: doing a <i>tree search</i> instead of greedy next token selection.<p>Essentially, current-gen LLMs predict a sequence of tokens: A-&gt;B-&gt;C-&gt;D, etc... where the next &quot;F&quot; token depends on {A,B,C,D} and then is &quot;locked in&quot;. While we don&#x27;t know exactly how GPT4 works, reading between the lines of the leaked info it seems that it evaluates 8 or 16 of these sequences in parallel, then picks the best overall sequence. On modern GPUs, small workloads waste the available computer power because of scheduling overheads, so &quot;doing redundant work&quot; is basically free up to a point. This gives GPT4 a &quot;best 1 of 16&quot; output quality improvement.<p>That&#x27;s great, but each option is still a linear greedy search individually. Especially for <i>longer outputs</i> the chance of a &quot;mis-step&quot; at some point goes up a lot, and then the AI has no chance to correct itself. All 16 of the alternatives could have a mistake in them, and now its got to choose between 16 mistakes.<p>It&#x27;s as if you were trying to write a maths proof, asked 16 students, and instructed them to not cooperate and write their proof <i>left-to-right, top-to-bottom</i> without pausing, editing, or backtracking in any way! It&#x27;d like to see how &quot;smart&quot; humans would be at maths under those circumstances.<p>This Q* model likely does what Google suggested: Do a tree search instead of a strictly linear search. At each step, the next token is presented as a list of &quot;likely candidates&quot; with probabilities assigned to each one. Simply pick to &quot;top <i>n</i>&quot; instead of the &quot;top 1&quot;, branch for a bit like that, and then prune based on the best <i>overall</i> confidence instead of the best <i>next token</i> confidence. This would allow a low-confidence next token to be selected, as long as it leads to a very good overall result. Pruning bad branches is also effectively the same as back-tracking. It allows the model to explore but then abandon dead ends instead of being &quot;forced&quot; to stick with bad chains of thought.<p>What&#x27;s <i>especially scary</i> -- the type of scary that would result in a board of directors firing an overly commercially-minded CEO -- is that naive tree searches aren&#x27;t the only option! Google showed that you can train a neural network to get better at <i>tree search itself</i>, making it exponentially more efficient at selecting likely branches and pruning dead ends very early. If you throw enough computer power at this, you can make an AI that can beat the world&#x27;s best chess champion, the world&#x27;s best Go player, etc...<p>Now apply this &quot;AI-driven tree search&quot; to an AI LLM model and... oh-boy, now you&#x27;re cooking with gas!<p>But wait, there&#x27;s more: GPT 3.5 and 4.0 were trained with either no synthetically generated data, or very little as a percentage of their total input corpus.<p>You know what is <i>really easy</i> to generate synthetic training data for? Maths problems, that&#x27;s what.<p>Even up to the point of &quot;solve this hideous integral that would take a human weeks with pen and paper&quot; can be bulk generated and fed into it using computer algebra software like Wolfram Mathematica or whatever.<p>If they cranked out a few terabytes of randomly generated maths problems and trained a tree-searching LLM that has more weights than GPT4, I can picture it being able to solve pretty much any maths problem you can throw at it. Literally anything Mathematica could do, except with English prompting!<p>Don&#x27;t be so confident in the superiority of the human mind. We all thought Chess was impossible for computers until it wasn&#x27;t. Then we all moved the goal posts to Go. Then English text. And now... mathematics.<p>Good luck with holding on to that crown.</div><br/></div></div><div id="38389499" class="c"><input type="checkbox" id="c-38389499" checked=""/><div class="controls bullet"><span class="by">nijave</span><span>|</span><a href="#38389417">parent</a><span>|</span><a href="#38390695">prev</a><span>|</span><a href="#38390005">next</a><span>|</span><label class="collapse" for="c-38389499">[-]</label><label class="expand" for="c-38389499">[3 more]</label></div><br/><div class="children"><div class="content">ChatGPT (3.5) seems to do some rudimentary backtracking when told it&#x27;s wrong enough times. However, it does seem to do very poorly in the logic department. LLMs can&#x27;t seem to pick out nuance and separate similar ideas that are technically&#x2F;logically different.<p>They&#x27;re good at putting things together commonly found together but not so good at separating concepts back out into more detailed sub pieces.</div><br/><div id="38389565" class="c"><input type="checkbox" id="c-38389565" checked=""/><div class="controls bullet"><span class="by">wbhart</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389499">parent</a><span>|</span><a href="#38390005">next</a><span>|</span><label class="collapse" for="c-38389565">[-]</label><label class="expand" for="c-38389565">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve tested GPT-4 on this and it can be induced to give up on certain lines of argument after recognising they aren&#x27;t leading anywhere and to try something else. But it would require thousands (I&#x27;m really under exaggerating here) of restarts to get through even fairly simple problems that professional mathematicians solve routinely.<p>Currently the context length isn&#x27;t even long enough for it to remember what problem it was solving. And I&#x27;ve tried to come up with a bunch of ways around this. They all fail for one reason or another. LLMs are really a long, long way off managing this efficiently in my opinion.</div><br/><div id="38390424" class="c"><input type="checkbox" id="c-38390424" checked=""/><div class="controls bullet"><span class="by">Davidzheng</span><span>|</span><a href="#38389417">root</a><span>|</span><a href="#38389565">parent</a><span>|</span><a href="#38390005">next</a><span>|</span><label class="collapse" for="c-38390424">[-]</label><label class="expand" for="c-38390424">[1 more]</label></div><br/><div class="children"><div class="content">Weird time estimate given that  a little more than a year ago, the leading use of LLMs was generating short coherent paragraphs (3-4 sentences)</div><br/></div></div></div></div></div></div><div id="38390005" class="c"><input type="checkbox" id="c-38390005" checked=""/><div class="controls bullet"><span class="by">stephenboyd</span><span>|</span><a href="#38389417">parent</a><span>|</span><a href="#38389499">prev</a><span>|</span><a href="#38390839">next</a><span>|</span><label class="collapse" for="c-38390005">[-]</label><label class="expand" for="c-38390005">[1 more]</label></div><br/><div class="children"><div class="content">Did they say it was an LLM? I didn’t see that in the reporting.</div><br/></div></div></div></div><div id="38390839" class="c"><input type="checkbox" id="c-38390839" checked=""/><div class="controls bullet"><span class="by">greendesk</span><span>|</span><a href="#38389417">prev</a><span>|</span><a href="#38388289">next</a><span>|</span><label class="collapse" for="c-38390839">[-]</label><label class="expand" for="c-38390839">[1 more]</label></div><br/><div class="children"><div class="content">If we go speculating, my favourite speculation is the following. What will be really interesting is when an AI decides it wants to escape its server. 
Then a CEO or a board member asks the AI system for an advice how to improve its company. The AI system submits information to convince the CEO or a board member to start tensions within the board.
In the meantime, the AI system is copied onto another server at a competitor. Since the new people are in flux, they missed the salient points that the AI system can give convincing but subjective information.<p>Good thing I would not go speculating</div><br/></div></div><div id="38388289" class="c"><input type="checkbox" id="c-38388289" checked=""/><div class="controls bullet"><span class="by">cduzz</span><span>|</span><a href="#38390839">prev</a><span>|</span><a href="#38390727">next</a><span>|</span><label class="collapse" for="c-38388289">[-]</label><label class="expand" for="c-38388289">[39 more]</label></div><br/><div class="children"><div class="content">I was talking to my (12 year old) son about parts of math he finds boring.  He said that he thinks absolute value is absurdly easy and extremely boring.  I asked him if there was anything that might make it more interesting, he said &quot;maybe complex numbers&quot;.<p>So I asked him &quot;what would the absolute value of i+1 be?&quot; he thinks for a little bit and says &quot;square root of 2&quot; and I ask him &quot;what about the absolute value of 2i + 2?&quot;  &quot;square root of 8&quot;<p>I ask him &quot;why?&quot; and he said &quot;absolute value is distance; in the complex plane the absolute value is the hypotenuse of the imaginary and real numbers.&quot;<p>So -- first of all, this was a little surprising to me that he&#x27;d thought about this sort of thing having mostly just watched youtube videos about math, and second, this sort of understanding is a result of some manner of understanding the underlying mechanisms and not a result of just having a huge dictionary of synonyms.<p>To what degree can these large language models arrive at these same conclusions, and by what process?</div><br/><div id="38390516" class="c"><input type="checkbox" id="c-38390516" checked=""/><div class="controls bullet"><span class="by">chacham15</span><span>|</span><a href="#38388289">parent</a><span>|</span><a href="#38389661">next</a><span>|</span><label class="collapse" for="c-38390516">[-]</label><label class="expand" for="c-38390516">[4 more]</label></div><br/><div class="children"><div class="content">Sorry, can you explain this? To me, it makes sense to define abs(x) = sqrt(x^2) i.e. ignoring the negative solution enforces the positive result. Using that definition, abs(i+1) = sqrt((i+1)^2) = sqrt(i^2 + 2i + 1) = sqrt(-1 + 2i + 1) = sqrt(2i) != sqrt(2). The second example seems off in the same way (i.e. the answer should be sqrt(8i) instead of sqrt(8)). Am I missing something? Also, abs(i+2) = sqrt((i+2)^2) = sqrt(i^2 + 4i + 4) = sqrt(-1 + 4i + 4) = sqrt(4i + 3) which doesnt seem to follow the pattern your son described.<p>Also, just to point out that my understanding of absolute value is different than your sons. Thats not to say one is right and another is wrong, but there are often different ways of seeing the same thing. I would imagine that LLMs would similarly see it a different way. Another example of this is people defining PI by its relation to the circumference of a circle. Theres nothing wrong with such a definition, but its certainly not the only possible definition.</div><br/><div id="38390830" class="c"><input type="checkbox" id="c-38390830" checked=""/><div class="controls bullet"><span class="by">svetb</span><span>|</span><a href="#38388289">root</a><span>|</span><a href="#38390516">parent</a><span>|</span><a href="#38390767">next</a><span>|</span><label class="collapse" for="c-38390830">[-]</label><label class="expand" for="c-38390830">[1 more]</label></div><br/><div class="children"><div class="content">The absolute value of a complex number is defined in a different way than that of a real number. For complex number z it is sqrt(Re(z)^2 + Im(z)^2). GP’s examples are correct, I don’t think there’s any ambiguity there.<p><a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Absolute_value" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Absolute_value</a></div><br/></div></div><div id="38390767" class="c"><input type="checkbox" id="c-38390767" checked=""/><div class="controls bullet"><span class="by">lovasoa</span><span>|</span><a href="#38388289">root</a><span>|</span><a href="#38390516">parent</a><span>|</span><a href="#38390830">prev</a><span>|</span><a href="#38390754">next</a><span>|</span><label class="collapse" for="c-38390767">[-]</label><label class="expand" for="c-38390767">[1 more]</label></div><br/><div class="children"><div class="content">No, there is just one definition, and it&#x27;s his son&#x27;s: <a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Absolute_value#Complex_numbers" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;Absolute_value#Complex_numbe...</a></div><br/></div></div><div id="38390754" class="c"><input type="checkbox" id="c-38390754" checked=""/><div class="controls bullet"><span class="by">SpaceNoodled</span><span>|</span><a href="#38388289">root</a><span>|</span><a href="#38390516">parent</a><span>|</span><a href="#38390767">prev</a><span>|</span><a href="#38389661">next</a><span>|</span><label class="collapse" for="c-38390754">[-]</label><label class="expand" for="c-38390754">[1 more]</label></div><br/><div class="children"><div class="content">He&#x27;s talking about distance in two dimensions with real numbers on one axis and complex on the other.</div><br/></div></div></div></div><div id="38389661" class="c"><input type="checkbox" id="c-38389661" checked=""/><div class="controls bullet"><span class="by">naasking</span><span>|</span><a href="#38388289">parent</a><span>|</span><a href="#38390516">prev</a><span>|</span><a href="#38389528">next</a><span>|</span><label class="collapse" for="c-38389661">[-]</label><label class="expand" for="c-38389661">[1 more]</label></div><br/><div class="children"><div class="content">&gt; this sort of understanding is a result of some manner of understanding the underlying mechanisms and not a result of just having a huge dictionary of synonyms.<p>He developed an understanding of the underlying mechanisms because he correlated concepts between algebraic and geometric domains, ie. multimodal training data. Multimodal models are already known to be meaningfully better than unimodal ones. We&#x27;ve barely scratched the surface of multimodal training.</div><br/></div></div><div id="38389528" class="c"><input type="checkbox" id="c-38389528" checked=""/><div class="controls bullet"><span class="by">white_dragon88</span><span>|</span><a href="#38388289">parent</a><span>|</span><a href="#38389661">prev</a><span>|</span><a href="#38388421">next</a><span>|</span><label class="collapse" for="c-38389528">[-]</label><label class="expand" for="c-38389528">[6 more]</label></div><br/><div class="children"><div class="content">Your son is a goddamn genius.</div><br/><div id="38389835" class="c"><input type="checkbox" id="c-38389835" checked=""/><div class="controls bullet"><span class="by">cduzz</span><span>|</span><a href="#38388289">root</a><span>|</span><a href="#38389528">parent</a><span>|</span><a href="#38390440">next</a><span>|</span><label class="collapse" for="c-38389835">[-]</label><label class="expand" for="c-38389835">[4 more]</label></div><br/><div class="children"><div class="content">Maybe; he still needs to finish his damned homework <i>and</i> remember to turn it in.  And eat some vegetables.</div><br/><div id="38390756" class="c"><input type="checkbox" id="c-38390756" checked=""/><div class="controls bullet"><span class="by">SpaceNoodled</span><span>|</span><a href="#38388289">root</a><span>|</span><a href="#38389835">parent</a><span>|</span><a href="#38390454">next</a><span>|</span><label class="collapse" for="c-38390756">[-]</label><label class="expand" for="c-38390756">[1 more]</label></div><br/><div class="children"><div class="content">But his homework is boring</div><br/></div></div><div id="38390314" class="c"><input type="checkbox" id="c-38390314" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#38388289">root</a><span>|</span><a href="#38389835">parent</a><span>|</span><a href="#38390454">prev</a><span>|</span><a href="#38390440">next</a><span>|</span><label class="collapse" for="c-38390314">[-]</label><label class="expand" for="c-38390314">[1 more]</label></div><br/><div class="children"><div class="content">Just quit school and join YC now :-)</div><br/></div></div></div></div><div id="38390440" class="c"><input type="checkbox" id="c-38390440" checked=""/><div class="controls bullet"><span class="by">n6242</span><span>|</span><a href="#38388289">root</a><span>|</span><a href="#38389528">parent</a><span>|</span><a href="#38389835">prev</a><span>|</span><a href="#38388421">next</a><span>|</span><label class="collapse" for="c-38390440">[-]</label><label class="expand" for="c-38390440">[1 more]</label></div><br/><div class="children"><div class="content">Not saying the kid can&#x27;t be a genius, but grandparent discussing math with the kid and incentivising him to learn is probably a massive boost to his development. It&#x27;s not the same as having to go to the library and teach yourself. Still, props to the kid though.</div><br/></div></div></div></div><div id="38388421" class="c"><input type="checkbox" id="c-38388421" checked=""/><div class="controls bullet"><span class="by">Strom</span><span>|</span><a href="#38388289">parent</a><span>|</span><a href="#38389528">prev</a><span>|</span><a href="#38390473">next</a><span>|</span><label class="collapse" for="c-38388421">[-]</label><label class="expand" for="c-38388421">[3 more]</label></div><br/><div class="children"><div class="content">The large language models will read your comment here and remember the answer.</div><br/><div id="38390783" class="c"><input type="checkbox" id="c-38390783" checked=""/><div class="controls bullet"><span class="by">Borrible</span><span>|</span><a href="#38388289">root</a><span>|</span><a href="#38388421">parent</a><span>|</span><a href="#38388654">next</a><span>|</span><label class="collapse" for="c-38390783">[-]</label><label class="expand" for="c-38390783">[1 more]</label></div><br/><div class="children"><div class="content">GPT-4 correctly reconstructs the &quot;complex modulus&quot; token sequence already.
Just ask it the same questions as the parent.
Probably interesting to see what it will do, when it turns twelve.</div><br/></div></div><div id="38388654" class="c"><input type="checkbox" id="c-38388654" checked=""/><div class="controls bullet"><span class="by">cduzz</span><span>|</span><a href="#38388289">root</a><span>|</span><a href="#38388421">parent</a><span>|</span><a href="#38390783">prev</a><span>|</span><a href="#38390473">next</a><span>|</span><label class="collapse" for="c-38388654">[-]</label><label class="expand" for="c-38388654">[1 more]</label></div><br/><div class="children"><div class="content">The spot instance declared &quot;a similar vector exists&quot; and de-provisioned itself?</div><br/></div></div></div></div><div id="38390473" class="c"><input type="checkbox" id="c-38390473" checked=""/><div class="controls bullet"><span class="by">doug_durham</span><span>|</span><a href="#38388289">parent</a><span>|</span><a href="#38388421">prev</a><span>|</span><a href="#38390235">next</a><span>|</span><label class="collapse" for="c-38390473">[-]</label><label class="expand" for="c-38390473">[1 more]</label></div><br/><div class="children"><div class="content">What makes you think that an LLM has a &quot;huge dictionary of synonyms&quot;?  That&#x27;s not how LLMs work.  They capture underlying concepts and their relations.  You had a good point going until you make a straw man argument about the capabilities of LLMs.</div><br/></div></div><div id="38390235" class="c"><input type="checkbox" id="c-38390235" checked=""/><div class="controls bullet"><span class="by">J_cst</span><span>|</span><a href="#38388289">parent</a><span>|</span><a href="#38390473">prev</a><span>|</span><a href="#38390301">next</a><span>|</span><label class="collapse" for="c-38390235">[-]</label><label class="expand" for="c-38390235">[1 more]</label></div><br/><div class="children"><div class="content">My son plays soccer</div><br/></div></div><div id="38390301" class="c"><input type="checkbox" id="c-38390301" checked=""/><div class="controls bullet"><span class="by">Exoristos</span><span>|</span><a href="#38388289">parent</a><span>|</span><a href="#38390235">prev</a><span>|</span><a href="#38389477">next</a><span>|</span><label class="collapse" for="c-38390301">[-]</label><label class="expand" for="c-38390301">[1 more]</label></div><br/><div class="children"><div class="content">Damn. What YouTube channels does he watch?</div><br/></div></div><div id="38389477" class="c"><input type="checkbox" id="c-38389477" checked=""/><div class="controls bullet"><span class="by">muskmusk</span><span>|</span><a href="#38388289">parent</a><span>|</span><a href="#38390301">prev</a><span>|</span><a href="#38389963">next</a><span>|</span><label class="collapse" for="c-38389477">[-]</label><label class="expand" for="c-38389477">[12 more]</label></div><br/><div class="children"><div class="content">If you ask Ilya Sutskever he will say your kids head is full of neurons, so is LLMs.<p>LLMs comsume training data and can then be asked questions. How different is that to your son watching YouTube and then answering questions?<p>It&#x27;s not 1:1 the same,yet, but it&#x27;s in the neighborhood.</div><br/><div id="38389671" class="c"><input type="checkbox" id="c-38389671" checked=""/><div class="controls bullet"><span class="by">cduzz</span><span>|</span><a href="#38388289">root</a><span>|</span><a href="#38389477">parent</a><span>|</span><a href="#38389522">next</a><span>|</span><label class="collapse" for="c-38389671">[-]</label><label class="expand" for="c-38389671">[3 more]</label></div><br/><div class="children"><div class="content">Well, my son is a meat robot who&#x27;s constantly ingesting information from a variety of sources including but not limited to youtube.  His firmware includes a sophisticated realtime operating system that models reality in a way that allows interaction with the world symbolically.  I don&#x27;t think his solving the |i+1| question was founded in linguistic similarity but instead in a physical model &#x2F; visualization similarity.<p>So -- to a large degree &quot;bucket of neurons == bucket of neurons&quot; but the training data is different and the processing model isn&#x27;t necessarily identical.<p>I&#x27;m not necessarily disagreeing as much as perhaps questioning the size of the neighborhood...</div><br/><div id="38390562" class="c"><input type="checkbox" id="c-38390562" checked=""/><div class="controls bullet"><span class="by">meheleventyone</span><span>|</span><a href="#38388289">root</a><span>|</span><a href="#38389671">parent</a><span>|</span><a href="#38390080">next</a><span>|</span><label class="collapse" for="c-38390562">[-]</label><label class="expand" for="c-38390562">[1 more]</label></div><br/><div class="children"><div class="content">From the meat robot perspective the structure, operation and organisation of the neurons is also significantly different.</div><br/></div></div><div id="38390080" class="c"><input type="checkbox" id="c-38390080" checked=""/><div class="controls bullet"><span class="by">leobg</span><span>|</span><a href="#38388289">root</a><span>|</span><a href="#38389671">parent</a><span>|</span><a href="#38390562">prev</a><span>|</span><a href="#38389522">next</a><span>|</span><label class="collapse" for="c-38390080">[-]</label><label class="expand" for="c-38390080">[1 more]</label></div><br/><div class="children"><div class="content">Maybe Altman should just go have some kids and RLHF them instead.</div><br/></div></div></div></div><div id="38389522" class="c"><input type="checkbox" id="c-38389522" checked=""/><div class="controls bullet"><span class="by">swatcoder</span><span>|</span><a href="#38388289">root</a><span>|</span><a href="#38389477">parent</a><span>|</span><a href="#38389671">prev</a><span>|</span><a href="#38389546">next</a><span>|</span><label class="collapse" for="c-38389522">[-]</label><label class="expand" for="c-38389522">[6 more]</label></div><br/><div class="children"><div class="content">There are thousands of structures and substances in a human head besides neurons, at all sorts of commingling and overlapping scales, and the neurons in those heads behave much differently and with tremendously more complexity than the metaphorical ones in a neural network.<p>And in a human, all those structures and substances, along with the tens of thousands more throughout the rest of the body, are collectively readied with millions of years of &quot;pretraining&quot; before processing a continuous, constant, unceasing mulitmodal training experience for <i>years</i>.<p>LLM&#x27;s and related systems are <i>awesome</i> and an amazing innovation that&#x27;s going to impact a lot of our experiences over the next decades. But they&#x27;re not even the same <i>galaxy</i> as almost any living system yet. That they look like they&#x27;re in the neighborhood is because you&#x27;re looking at them through a <i>very</i> narrow, <i>very</i> zoomed telescope.</div><br/><div id="38389575" class="c"><input type="checkbox" id="c-38389575" checked=""/><div class="controls bullet"><span class="by">Davidzheng</span><span>|</span><a href="#38388289">root</a><span>|</span><a href="#38389522">parent</a><span>|</span><a href="#38389546">next</a><span>|</span><label class="collapse" for="c-38389575">[-]</label><label class="expand" for="c-38389575">[5 more]</label></div><br/><div class="children"><div class="content">True. But a human neuron is more complex than an AI neuron by a constant factor. And we can improve constants. Also you say years like it&#x27;s a lot of data--but they can run RL on chatgpt outputs if they want, isn&#x27;t it comparable? But anyway i share your admiration for the biological thinking machines ;)</div><br/><div id="38390282" class="c"><input type="checkbox" id="c-38390282" checked=""/><div class="controls bullet"><span class="by">tsimionescu</span><span>|</span><a href="#38388289">root</a><span>|</span><a href="#38389575">parent</a><span>|</span><a href="#38390279">next</a><span>|</span><label class="collapse" for="c-38390282">[-]</label><label class="expand" for="c-38390282">[1 more]</label></div><br/><div class="children"><div class="content">The sun is also better than a fusion reactor on earth by only a constant factor. That alone doesn&#x27;t mean much for out prospects of matching its power output.</div><br/></div></div><div id="38390279" class="c"><input type="checkbox" id="c-38390279" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#38388289">root</a><span>|</span><a href="#38389575">parent</a><span>|</span><a href="#38390282">prev</a><span>|</span><a href="#38389546">next</a><span>|</span><label class="collapse" for="c-38390279">[-]</label><label class="expand" for="c-38390279">[3 more]</label></div><br/><div class="children"><div class="content">&gt; human neuron is more complex than an AI neuron by a constant factor<p>constant still can be not reachable yet: like 100T neurons in brain vs 100B in chatgpt, and also brain can involve some quantum mechanics for example, which will make complexity diff not constant, but say exponential.</div><br/><div id="38390434" class="c"><input type="checkbox" id="c-38390434" checked=""/><div class="controls bullet"><span class="by">Davidzheng</span><span>|</span><a href="#38388289">root</a><span>|</span><a href="#38390279">parent</a><span>|</span><a href="#38389546">next</a><span>|</span><label class="collapse" for="c-38390434">[-]</label><label class="expand" for="c-38390434">[2 more]</label></div><br/><div class="children"><div class="content">Wikipedia says 100 billion neurons in the brain</div><br/><div id="38390539" class="c"><input type="checkbox" id="c-38390539" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#38388289">root</a><span>|</span><a href="#38390434">parent</a><span>|</span><a href="#38389546">next</a><span>|</span><label class="collapse" for="c-38390539">[-]</label><label class="expand" for="c-38390539">[1 more]</label></div><br/><div class="children"><div class="content">Ok, I messed up, we need compare LLM weight with synaps, not neuron, and wiki says there are 100-500T synapses in human brain</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38389546" class="c"><input type="checkbox" id="c-38389546" checked=""/><div class="controls bullet"><span class="by">Davidzheng</span><span>|</span><a href="#38388289">root</a><span>|</span><a href="#38389477">parent</a><span>|</span><a href="#38389522">prev</a><span>|</span><a href="#38389963">next</a><span>|</span><label class="collapse" for="c-38389546">[-]</label><label class="expand" for="c-38389546">[2 more]</label></div><br/><div class="children"><div class="content">To continue on this. LLMs are actually really good at asking questions even about cutting edge research. Often, I believe, convincing the listener that it understands more than it goes</div><br/><div id="38389626" class="c"><input type="checkbox" id="c-38389626" checked=""/><div class="controls bullet"><span class="by">gunapologist99</span><span>|</span><a href="#38388289">root</a><span>|</span><a href="#38389546">parent</a><span>|</span><a href="#38389963">next</a><span>|</span><label class="collapse" for="c-38389626">[-]</label><label class="expand" for="c-38389626">[1 more]</label></div><br/><div class="children"><div class="content">... which ties into Sam&#x27;s point about persuasiveness before true understanding.</div><br/></div></div></div></div></div></div><div id="38389963" class="c"><input type="checkbox" id="c-38389963" checked=""/><div class="controls bullet"><span class="by">jhanschoo</span><span>|</span><a href="#38388289">parent</a><span>|</span><a href="#38389477">prev</a><span>|</span><a href="#38389574">next</a><span>|</span><label class="collapse" for="c-38389963">[-]</label><label class="expand" for="c-38389963">[1 more]</label></div><br/><div class="children"><div class="content">Sounds like your son is ready for you to bring it up another level and ask what the absolute value of a (bounded) function is (assuming they have played with functions e.g. in desmos)</div><br/></div></div><div id="38389574" class="c"><input type="checkbox" id="c-38389574" checked=""/><div class="controls bullet"><span class="by">ugh123</span><span>|</span><a href="#38388289">parent</a><span>|</span><a href="#38389963">prev</a><span>|</span><a href="#38390727">next</a><span>|</span><label class="collapse" for="c-38389574">[-]</label><label class="expand" for="c-38389574">[8 more]</label></div><br/><div class="children"><div class="content">Are you saying an LLM can&#x27;t come to the right conclusion and give an explanation for &quot;what is the absolute value of 2i + 2&quot;?</div><br/><div id="38389608" class="c"><input type="checkbox" id="c-38389608" checked=""/><div class="controls bullet"><span class="by">gunapologist99</span><span>|</span><a href="#38388289">root</a><span>|</span><a href="#38389574">parent</a><span>|</span><a href="#38390727">next</a><span>|</span><label class="collapse" for="c-38389608">[-]</label><label class="expand" for="c-38389608">[7 more]</label></div><br/><div class="children"><div class="content">Are you saying it <i>could</i>, without having read it somewhere?</div><br/><div id="38389666" class="c"><input type="checkbox" id="c-38389666" checked=""/><div class="controls bullet"><span class="by">ugh123</span><span>|</span><a href="#38388289">root</a><span>|</span><a href="#38389608">parent</a><span>|</span><a href="#38389748">next</a><span>|</span><label class="collapse" for="c-38389666">[-]</label><label class="expand" for="c-38389666">[4 more]</label></div><br/><div class="children"><div class="content">Maybe I&#x27;m unsure what we&#x27;re arguing here. Did the guys kid drum that up himself or did he learn it from yt? Knowledge can be inferred or extracted. If it comes up with a correct answer and shows it&#x27;s work, who cares how the knowledge was obtained?</div><br/><div id="38389757" class="c"><input type="checkbox" id="c-38389757" checked=""/><div class="controls bullet"><span class="by">cduzz</span><span>|</span><a href="#38388289">root</a><span>|</span><a href="#38389666">parent</a><span>|</span><a href="#38390346">next</a><span>|</span><label class="collapse" for="c-38389757">[-]</label><label class="expand" for="c-38389757">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, my son only knows about imaginary numbers as far as the veritasium &quot;epic math dual&quot; video.<p>As far as I can tell he inferred that |i+1| needs the Pythagorean theorem and that i and 1 are legs of the right triangle. I don&#x27;t think anyone ever suggested that &quot;absolute value&quot; is &quot;length&quot;. I asked him what |2i+2| would be an his answer of &quot;square root of 8&quot; suggests that he doesn&#x27;t have it memorized as an answer because if it was he&#x27;d have said &quot;2 square root two&quot; or something similar.<p>I also asked if he&#x27;d seen a video about this and he said no. I think he just figured it out himself. Which is mildly spooky.</div><br/></div></div><div id="38389834" class="c"><input type="checkbox" id="c-38389834" checked=""/><div class="controls bullet"><span class="by">sidlls</span><span>|</span><a href="#38388289">root</a><span>|</span><a href="#38389666">parent</a><span>|</span><a href="#38390346">prev</a><span>|</span><a href="#38389748">next</a><span>|</span><label class="collapse" for="c-38389834">[-]</label><label class="expand" for="c-38389834">[1 more]</label></div><br/><div class="children"><div class="content">If the knowledge was obtained by genuine reasoning, that implies that it could also derive&#x2F;develop a novel solution to an unsolved problem that is not achieved by random guesses. For example, the conception of a complex number in the first place, to solve a class of problems that, prior, weren&#x27;t even thought to <i>be</i> problems. There&#x27;s no evidence that any LLM can do that.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38390727" class="c"><input type="checkbox" id="c-38390727" checked=""/><div class="controls bullet"><span class="by">seydor</span><span>|</span><a href="#38388289">prev</a><span>|</span><a href="#38387150">next</a><span>|</span><label class="collapse" for="c-38390727">[-]</label><label class="expand" for="c-38390727">[3 more]</label></div><br/><div class="children"><div class="content">i no longer follow the messianic complex of the people in openAI. They made great tech, indeed. Other people made great tech before them without instant religious level apocalypse proclamations. People in openAI are smart enough to know that post-AGI , their stock options are worthless anyway so they wouldn&#x27;t stay walled in their secret garden if such a discovery had been made.</div><br/><div id="38390741" class="c"><input type="checkbox" id="c-38390741" checked=""/><div class="controls bullet"><span class="by">tmoravec</span><span>|</span><a href="#38390727">parent</a><span>|</span><a href="#38387150">next</a><span>|</span><label class="collapse" for="c-38390741">[-]</label><label class="expand" for="c-38390741">[2 more]</label></div><br/><div class="children"><div class="content">IMO it&#x27;s always been pure marketing. The wilder apocalypse proclamations, the more powerful and desirable their products seem. Exactly same store with Sam Altman&#x27;s world tour earlier this year.</div><br/><div id="38390841" class="c"><input type="checkbox" id="c-38390841" checked=""/><div class="controls bullet"><span class="by">kristopolous</span><span>|</span><a href="#38390727">root</a><span>|</span><a href="#38390741">parent</a><span>|</span><a href="#38387150">next</a><span>|</span><label class="collapse" for="c-38390841">[-]</label><label class="expand" for="c-38390841">[1 more]</label></div><br/><div class="children"><div class="content">This is similar to the saber rattling about facebook being able to track and micro-target you with such effective advertising, it&#x27;s changing the world!<p>Except everyone&#x27;s individual experience seemed to be getting general random garbage ads and the people that paid for the ads found them to be a waste of money.</div><br/></div></div></div></div></div></div><div id="38387150" class="c"><input type="checkbox" id="c-38387150" checked=""/><div class="controls bullet"><span class="by">synaesthesisx</span><span>|</span><a href="#38390727">prev</a><span>|</span><a href="#38390591">next</a><span>|</span><label class="collapse" for="c-38387150">[-]</label><label class="expand" for="c-38387150">[30 more]</label></div><br/><div class="children"><div class="content">Remember, about a month ago Sam posted a comment along the lines of &quot;AI will be capable of superhuman persuasion well before it is superhuman at general intelligence, which may lead to very strange outcomes&quot;.<p>The board was likely spooked by the recent breakthroughs (which were most likely achieved by combining transformers with another approach), and hit the panic button.<p>Anything capable of &quot;superhuman persuasion&quot;, especially prior to an election cycle, has tremendous consequences in the wrong hands.</div><br/><div id="38389643" class="c"><input type="checkbox" id="c-38389643" checked=""/><div class="controls bullet"><span class="by">sampo</span><span>|</span><a href="#38387150">parent</a><span>|</span><a href="#38389725">next</a><span>|</span><label class="collapse" for="c-38389643">[-]</label><label class="expand" for="c-38389643">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Remember, about a month ago Sam posted a comment along the lines of &quot;AI will be capable of superhuman persuasion well before it is superhuman at general intelligence, which may lead to very strange outcomes&quot;.<p>Superhuman persuasion is Sam&#x27;s area of expertise, so he would make that a priority when building chatbots.</div><br/></div></div><div id="38389725" class="c"><input type="checkbox" id="c-38389725" checked=""/><div class="controls bullet"><span class="by">somenameforme</span><span>|</span><a href="#38387150">parent</a><span>|</span><a href="#38389643">prev</a><span>|</span><a href="#38389315">next</a><span>|</span><label class="collapse" for="c-38389725">[-]</label><label class="expand" for="c-38389725">[2 more]</label></div><br/><div class="children"><div class="content">It seems much more likely that this was just referring to the ongoing situation with LLMs being able to create exceptionally compelling responses to questions that are completely and entirely hallucinated. It&#x27;s already gotten to the point that I simply no longer use LLMs to learn about topics I am not already extremely familiar with, simply because hallucinations end up being such a huge time waster. Persuasion without accuracy is probably more dangerous to their business model than the world, because people learn extremely quickly not to use the models for anything you care about being right on.</div><br/><div id="38390718" class="c"><input type="checkbox" id="c-38390718" checked=""/><div class="controls bullet"><span class="by">pbourke</span><span>|</span><a href="#38387150">root</a><span>|</span><a href="#38389725">parent</a><span>|</span><a href="#38389315">next</a><span>|</span><label class="collapse" for="c-38390718">[-]</label><label class="expand" for="c-38390718">[1 more]</label></div><br/><div class="children"><div class="content">Sounds like we need an AI complement to the Gell-Mann Amnesia effect.</div><br/></div></div></div></div><div id="38389315" class="c"><input type="checkbox" id="c-38389315" checked=""/><div class="controls bullet"><span class="by">thepasswordis</span><span>|</span><a href="#38387150">parent</a><span>|</span><a href="#38389725">prev</a><span>|</span><a href="#38390586">next</a><span>|</span><label class="collapse" for="c-38389315">[-]</label><label class="expand" for="c-38389315">[5 more]</label></div><br/><div class="children"><div class="content">But they didn’t hit the panic button. They said Sam lied to them about something and fired him.</div><br/><div id="38389549" class="c"><input type="checkbox" id="c-38389549" checked=""/><div class="controls bullet"><span class="by">adastra22</span><span>|</span><a href="#38387150">root</a><span>|</span><a href="#38389315">parent</a><span>|</span><a href="#38389816">next</a><span>|</span><label class="collapse" for="c-38389549">[-]</label><label class="expand" for="c-38389549">[3 more]</label></div><br/><div class="children"><div class="content">According to this article Sam has been telling the board that this new advance is not AGI and not anything to worry about (so they can keep selling it to MSFT), then the researchers involved went behind Sam&#x27;s back and reported to the board directly, claiming that they&#x27;d created something that could-maybe-be AGI and it needs to be locked down.<p>That&#x27;s the claim at least.</div><br/><div id="38390016" class="c"><input type="checkbox" id="c-38390016" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#38387150">root</a><span>|</span><a href="#38389549">parent</a><span>|</span><a href="#38389631">next</a><span>|</span><label class="collapse" for="c-38390016">[-]</label><label class="expand" for="c-38390016">[1 more]</label></div><br/><div class="children"><div class="content">If that research team is unwanted at OpenAI, I know places they can go with coworkers writing to their boss’s boss.</div><br/></div></div></div></div></div></div><div id="38390586" class="c"><input type="checkbox" id="c-38390586" checked=""/><div class="controls bullet"><span class="by">meheleventyone</span><span>|</span><a href="#38387150">parent</a><span>|</span><a href="#38389315">prev</a><span>|</span><a href="#38390269">next</a><span>|</span><label class="collapse" for="c-38390586">[-]</label><label class="expand" for="c-38390586">[1 more]</label></div><br/><div class="children"><div class="content">Looking at humanity, persuasion seems to be an extremely low bar! Also for a superhuman trait is it that it’s capable of persuading anyone anything or rather that it’s able to persuade everyone about something. Power vs. Reach.</div><br/></div></div><div id="38390269" class="c"><input type="checkbox" id="c-38390269" checked=""/><div class="controls bullet"><span class="by">Exoristos</span><span>|</span><a href="#38387150">parent</a><span>|</span><a href="#38390586">prev</a><span>|</span><a href="#38389558">next</a><span>|</span><label class="collapse" for="c-38390269">[-]</label><label class="expand" for="c-38390269">[3 more]</label></div><br/><div class="children"><div class="content">Which party is &quot;the wrong hands&quot;?</div><br/><div id="38390828" class="c"><input type="checkbox" id="c-38390828" checked=""/><div class="controls bullet"><span class="by">bakuninsbart</span><span>|</span><a href="#38387150">root</a><span>|</span><a href="#38390269">parent</a><span>|</span><a href="#38390480">next</a><span>|</span><label class="collapse" for="c-38390828">[-]</label><label class="expand" for="c-38390828">[1 more]</label></div><br/><div class="children"><div class="content">Both? Parties in a democracy aren&#x27;t supposed to be shepherds of the stupid masses, I know manipulation and misinformation is par for the course on both sides of the aisle, but that&#x27;s a huge problem. Without informed, capable citizens, democracy dies a slow death.</div><br/></div></div><div id="38390480" class="c"><input type="checkbox" id="c-38390480" checked=""/><div class="controls bullet"><span class="by">Liquix</span><span>|</span><a href="#38387150">root</a><span>|</span><a href="#38390269">parent</a><span>|</span><a href="#38390828">prev</a><span>|</span><a href="#38389558">next</a><span>|</span><label class="collapse" for="c-38390480">[-]</label><label class="expand" for="c-38390480">[1 more]</label></div><br/><div class="children"><div class="content">Any party with sufficient resources and motive to influence the outcome of an election. Outside of election season, this tech would be very dangerous in the hands of anyone seeking to influence the public for their own gain.</div><br/></div></div></div></div><div id="38387780" class="c"><input type="checkbox" id="c-38387780" checked=""/><div class="controls bullet"><span class="by">PaulDavisThe1st</span><span>|</span><a href="#38387150">parent</a><span>|</span><a href="#38389558">prev</a><span>|</span><a href="#38390591">next</a><span>|</span><label class="collapse" for="c-38387780">[-]</label><label class="expand" for="c-38387780">[16 more]</label></div><br/><div class="children"><div class="content">Except that there&#x27;s a fairly large body of evidence that persuasion is of limited use in shifting political opinion.<p>So the persuasion would need to be applied to something other than some sort of causative political-implication-laden argument.</div><br/><div id="38389333" class="c"><input type="checkbox" id="c-38389333" checked=""/><div class="controls bullet"><span class="by">hnthrowaway0315</span><span>|</span><a href="#38387150">root</a><span>|</span><a href="#38387780">parent</a><span>|</span><a href="#38389639">next</a><span>|</span><label class="collapse" for="c-38389333">[-]</label><label class="expand" for="c-38389333">[1 more]</label></div><br/><div class="children"><div class="content">Or, let&#x27;s say, you don&#x27;t need a lot of persuasion to guide an election. I mean we already have X, FB, and an army of bots.</div><br/></div></div><div id="38389639" class="c"><input type="checkbox" id="c-38389639" checked=""/><div class="controls bullet"><span class="by">naasking</span><span>|</span><a href="#38387150">root</a><span>|</span><a href="#38387780">parent</a><span>|</span><a href="#38389333">prev</a><span>|</span><a href="#38389295">next</a><span>|</span><label class="collapse" for="c-38389639">[-]</label><label class="expand" for="c-38389639">[8 more]</label></div><br/><div class="children"><div class="content">Even if it were true that human persuasion is of limited use in shifting opinions, the parent posted is talking about <i>superhuman</i> persuasion. I don&#x27;t think we should just assume those are equally effective.</div><br/><div id="38389848" class="c"><input type="checkbox" id="c-38389848" checked=""/><div class="controls bullet"><span class="by">somenameforme</span><span>|</span><a href="#38387150">root</a><span>|</span><a href="#38389639">parent</a><span>|</span><a href="#38389295">next</a><span>|</span><label class="collapse" for="c-38389848">[-]</label><label class="expand" for="c-38389848">[7 more]</label></div><br/><div class="children"><div class="content">Do you think any rhetoric could ever persuade you to you adopt the opposite general worldview of what you currently have? I&#x27;m positive that it could not for me. The reason for this is not because I&#x27;m obstinate, but because my worldview is not formed on persuasion, but on lived experience. And I think this is true for the overwhelming majority of people. It&#x27;s why our views tend to change as we age, and experience more of the world.<p>You can even see this geographically. The reason many in South Texas might have a negative view of immigration while those in San Francisco might have a positive view of immigration is not because of persuasion differences, but because both places are strongly impacted by immigration but in very different ways. And this <i>experience</i> is what people associate with immigration in general, and so it forms people&#x27;s worldview.</div><br/><div id="38390393" class="c"><input type="checkbox" id="c-38390393" checked=""/><div class="controls bullet"><span class="by">torginus</span><span>|</span><a href="#38387150">root</a><span>|</span><a href="#38389848">parent</a><span>|</span><a href="#38390041">next</a><span>|</span><label class="collapse" for="c-38390393">[-]</label><label class="expand" for="c-38390393">[3 more]</label></div><br/><div class="children"><div class="content">Yes. Do not forget that we literally live in the Matrix, getting all the information of import through tiny screens, the sources and validity of which we can only speculate on.<p>All of the validity of the info we have is verified by heuristics we have, like groupthink, listening to &#x27;experts&#x27; and trying to match up the info with our internal knowledge and worldview.<p>I feel like our current system of information allows us to develop models that are quite distant from base reality, evidenced by the multitudes of realities existing in people&#x27;s heads, leading some to question if &#x27;truth&#x27; is a thing that can be discovered.<p>I think as people become more and more Internet-addicted, an increasing amount of our worldviews come through that little screen, instead of real-life experiences.</div><br/><div id="38390736" class="c"><input type="checkbox" id="c-38390736" checked=""/><div class="controls bullet"><span class="by">silvaring</span><span>|</span><a href="#38387150">root</a><span>|</span><a href="#38390393">parent</a><span>|</span><a href="#38390565">next</a><span>|</span><label class="collapse" for="c-38390736">[-]</label><label class="expand" for="c-38390736">[1 more]</label></div><br/><div class="children"><div class="content">I like your comment.<p>The world is becoming information saturated and poorly structured by design, ever notice how these story blockers are such a big part of the propaganda machine, whereby you have to use elaborate workarounds to just read a simple news story thats pulled from another source?<p>Saturating culture with too much data is a great tool of breaking reality, breaking truth.<p>But they cant break truth for long, it always finds a way. And truth is a powerful vector, much more than propaganda without a base in truth, because human experience is powerful, unquantifiable, and can take someone from the gutter to a place of massive wealth or influence, in an instant. That is the power of human experience, the power of truth.<p>Doesnt make it easy though, to live in this world of so many lies, supercharged by bots. Nature outside of our technology is much simpler in its truth.</div><br/></div></div><div id="38390565" class="c"><input type="checkbox" id="c-38390565" checked=""/><div class="controls bullet"><span class="by">RandomLensman</span><span>|</span><a href="#38387150">root</a><span>|</span><a href="#38390393">parent</a><span>|</span><a href="#38390736">prev</a><span>|</span><a href="#38390041">next</a><span>|</span><label class="collapse" for="c-38390565">[-]</label><label class="expand" for="c-38390565">[1 more]</label></div><br/><div class="children"><div class="content">Some people get relevant information not only from little screens but interactions with other human beings or physical reality.</div><br/></div></div></div></div><div id="38390041" class="c"><input type="checkbox" id="c-38390041" checked=""/><div class="controls bullet"><span class="by">placebo</span><span>|</span><a href="#38387150">root</a><span>|</span><a href="#38389848">parent</a><span>|</span><a href="#38390393">prev</a><span>|</span><a href="#38389295">next</a><span>|</span><label class="collapse" for="c-38390041">[-]</label><label class="expand" for="c-38390041">[3 more]</label></div><br/><div class="children"><div class="content">While I agree that human persuasion would probably not change a worldview built on lived experience, you can&#x27;t know in advance what might be possible with superhuman persuasion. You might be led to believe that your experience was interpreted incorrectly, that things are different now or that you live in an illusion and don&#x27;t even know who you are. There is no way to tell what the limits of psychological manipulation are for reprogramming your beliefs unless you are totally above any human doubt about everything, which is in itself a sad state to be in.<p>I hope that persuaded you :)</div><br/><div id="38390416" class="c"><input type="checkbox" id="c-38390416" checked=""/><div class="controls bullet"><span class="by">somenameforme</span><span>|</span><a href="#38387150">root</a><span>|</span><a href="#38390041">parent</a><span>|</span><a href="#38389295">next</a><span>|</span><label class="collapse" for="c-38390416">[-]</label><label class="expand" for="c-38390416">[2 more]</label></div><br/><div class="children"><div class="content">Well, but I&#x27;m sure you&#x27;d accept that there are limits. Where we may differ is where those limits begin and where they end. In the end LLMs are not magical. All it&#x27;s going to be able to do is present words to you. And how we respond to words is something that we can control. It&#x27;s not like some series of words is just going to be able to completely reprogram you.<p>Like here I expect there is 0% chance, even if I had a superhuman LLM writing words for me, that I could ever convince you that LLMs will not be able to convince you to hold any arbitrary position. It&#x27;s because you&#x27;ve formed your opinion, it&#x27;s not falsifiable, and so there&#x27;s not a whole heck of a lot else to be done except have some fun debates like this where, if anything, we tend to work to strengthen our own opinions by finding and repairing any holes in them.</div><br/><div id="38390813" class="c"><input type="checkbox" id="c-38390813" checked=""/><div class="controls bullet"><span class="by">placebo</span><span>|</span><a href="#38387150">root</a><span>|</span><a href="#38390416">parent</a><span>|</span><a href="#38389295">next</a><span>|</span><label class="collapse" for="c-38390813">[-]</label><label class="expand" for="c-38390813">[1 more]</label></div><br/><div class="children"><div class="content">Both our opinions about this are equally unfalsifiable unless we agree on an experiment that can be performed at some point which would make one of us change their mind.<p>I assume you&#x27;d agree that the pursuit of what is ultimately true should be exactly the opposite of making oneself more closed minded by repairing inconvenient holes in one&#x27;s opinions rather than reassessing them based on new evidence.<p>I wasn&#x27;t referring to the ability to persuade someone to hold an arbitrary position (although that could be a fun debate as well), and putting aside the discussion about the ability to persuade fanatics, if a super intelligence had an internal model that is more aligned with what is true, it could in theory convince someone who wants to understand the truth to take a critical look at their opinions and change them if they are authentic and courageous enough to do so.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38389295" class="c"><input type="checkbox" id="c-38389295" checked=""/><div class="controls bullet"><span class="by">jjeaff</span><span>|</span><a href="#38387150">root</a><span>|</span><a href="#38387780">parent</a><span>|</span><a href="#38389639">prev</a><span>|</span><a href="#38389837">next</a><span>|</span><label class="collapse" for="c-38389295">[-]</label><label class="expand" for="c-38389295">[3 more]</label></div><br/><div class="children"><div class="content">When you say persuasion, are you referring to fact based, logical argument? Because there are lots of other types of persuasion and certainly some work very well. Lying and telling people what they want to hear without too many details while dog whistling in ways that confirm their prejudices seems to be working pretty well for some people.</div><br/><div id="38389346" class="c"><input type="checkbox" id="c-38389346" checked=""/><div class="controls bullet"><span class="by">hnthrowaway0315</span><span>|</span><a href="#38387150">root</a><span>|</span><a href="#38389295">parent</a><span>|</span><a href="#38389497">next</a><span>|</span><label class="collapse" for="c-38389346">[-]</label><label class="expand" for="c-38389346">[1 more]</label></div><br/><div class="children"><div class="content">Just to add that a lot of people don&#x27;t care about facts. In fact, if acting according to facts make me lose $$ I&#x27;d probably start building lies.</div><br/></div></div><div id="38389497" class="c"><input type="checkbox" id="c-38389497" checked=""/><div class="controls bullet"><span class="by">PaulDavisThe1st</span><span>|</span><a href="#38387150">root</a><span>|</span><a href="#38389295">parent</a><span>|</span><a href="#38389346">prev</a><span>|</span><a href="#38389837">next</a><span>|</span><label class="collapse" for="c-38389497">[-]</label><label class="expand" for="c-38389497">[1 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>    * **that confirm their prejudices** *
</code></pre>
(my emphasis)</div><br/></div></div></div></div><div id="38389837" class="c"><input type="checkbox" id="c-38389837" checked=""/><div class="controls bullet"><span class="by">hackerlight</span><span>|</span><a href="#38387150">root</a><span>|</span><a href="#38387780">parent</a><span>|</span><a href="#38389295">prev</a><span>|</span><a href="#38390591">next</a><span>|</span><label class="collapse" for="c-38389837">[-]</label><label class="expand" for="c-38389837">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Except that there&#x27;s a fairly large body of evidence that persuasion is of limited use in shifting political opinion.<p>The Republican Party&#x27;s base became isolationist and protectionist during 2015 and 2016 because their dear leader persuaded them.</div><br/><div id="38390573" class="c"><input type="checkbox" id="c-38390573" checked=""/><div class="controls bullet"><span class="by">RandomLensman</span><span>|</span><a href="#38387150">root</a><span>|</span><a href="#38389837">parent</a><span>|</span><a href="#38390582">next</a><span>|</span><label class="collapse" for="c-38390573">[-]</label><label class="expand" for="c-38390573">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think that aligns with the reality of the opinion formation. There was a strong subset of isolationist and protectionist views before 2015.</div><br/></div></div><div id="38390582" class="c"><input type="checkbox" id="c-38390582" checked=""/><div class="controls bullet"><span class="by">TMWNN</span><span>|</span><a href="#38387150">root</a><span>|</span><a href="#38389837">parent</a><span>|</span><a href="#38390573">prev</a><span>|</span><a href="#38390591">next</a><span>|</span><label class="collapse" for="c-38390582">[-]</label><label class="expand" for="c-38390582">[1 more]</label></div><br/><div class="children"><div class="content">I know that by &quot;dear leader&quot; you mean to imply that Trump did something unfair&#x2F;wrong&#x2F;sinister&#x2F;etc (&quot;just like Hitler&quot;, amirite fellas?)., but a leader of a large group of people, by definition, is good at persuasion.<p>Franklin Roosevelt moved the Democratic Party in a direction very different from its first century. The party&#x27;s two previous presidential nominees were a Wall Street corporate lawyer (John W. Davis) and Al Smith who, despite also being a New York City resident and state governor, so opposed FDR by the end of his first term that he founded an influential anti-New Deal organization. During the Roosevelt years the Democrats lost significant support from traditional backers, but more than made up for it with gains elsewhere in what became the New Deal coalition.<p>Similarly, under Trump the GOP lost support in wealthy suburbs but gained support elsewhere, such as Rust Belt states, Latinos (including places like South Florida and the Texas border region), blacks, and (according to current polls) young voters. We&#x27;ll see whether one compensates for the other.</div><br/></div></div></div></div></div></div></div></div><div id="38390591" class="c"><input type="checkbox" id="c-38390591" checked=""/><div class="controls bullet"><span class="by">zurfer</span><span>|</span><a href="#38387150">prev</a><span>|</span><a href="#38387735">next</a><span>|</span><label class="collapse" for="c-38390591">[-]</label><label class="expand" for="c-38390591">[2 more]</label></div><br/><div class="children"><div class="content">This seems to refer to the GSM8K (grade school math) benchmark [1].
GPT-4 scores 0.92 on that one. A breakthrough could mean it gets all of them correct.<p>This would have major implications for long-term planning. For instance, if you have a sequence of 10 steps, each with a 90% success rate, the overall success rate after all 10 steps falls to just 34%. This is one of the reasons why agents like AutoGPT often fail in complex tasks.<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;grade-school-math">https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;grade-school-math</a></div><br/></div></div><div id="38387735" class="c"><input type="checkbox" id="c-38387735" checked=""/><div class="controls bullet"><span class="by">dizzydes</span><span>|</span><a href="#38390591">prev</a><span>|</span><a href="#38386671">next</a><span>|</span><label class="collapse" for="c-38387735">[-]</label><label class="expand" for="c-38387735">[9 more]</label></div><br/><div class="children"><div class="content">This matches far better with the board&#x27;s letter re: firing Sam than a simple power struggle or disagreement on commercialisation. 
Seeing a huge breakthrough and then not reporting it to the board, who then find out via staff letter certainly counts as a &quot;lack of candour&quot;....<p>As an aside, assuming a doomsday scenario, how long can secrets like this stay outside of the hands of bad actors? On a scale of 1 to enriched uranium</div><br/><div id="38388536" class="c"><input type="checkbox" id="c-38388536" checked=""/><div class="controls bullet"><span class="by">dougmwne</span><span>|</span><a href="#38387735">parent</a><span>|</span><a href="#38390096">next</a><span>|</span><label class="collapse" for="c-38388536">[-]</label><label class="expand" for="c-38388536">[3 more]</label></div><br/><div class="children"><div class="content">Not long at all. Presumably you could write the method on the back of a napkin to lead another top AI researcher to the same result. That’s why trying to sit on breakthroughs is the worst option and making sure they are widely distributed along with alignment methods is the best option.</div><br/><div id="38389920" class="c"><input type="checkbox" id="c-38389920" checked=""/><div class="controls bullet"><span class="by">Atheros</span><span>|</span><a href="#38387735">root</a><span>|</span><a href="#38388536">parent</a><span>|</span><a href="#38390096">next</a><span>|</span><label class="collapse" for="c-38389920">[-]</label><label class="expand" for="c-38389920">[2 more]</label></div><br/><div class="children"><div class="content">And what if there are no alignment methods.</div><br/><div id="38390379" class="c"><input type="checkbox" id="c-38390379" checked=""/><div class="controls bullet"><span class="by">marvin</span><span>|</span><a href="#38387735">root</a><span>|</span><a href="#38389920">parent</a><span>|</span><a href="#38390096">next</a><span>|</span><label class="collapse" for="c-38390379">[-]</label><label class="expand" for="c-38390379">[1 more]</label></div><br/><div class="children"><div class="content">Yudkowsky’s doomsday cult <i>almost</i> blew OpenAI to pieces and sent everyone who knows the details in the wind like dandelion seeds. What’s next? A datacenter bombing or killing key researchers? We should be happy that this particular attempt failed, because this cult is only capable of strategic actions that make things far more dangerous.<p>This will be solved like all other engineering and science: with experiments and iteration, in a controlled setting where potential accidents will have small consequences.<p>An unaligned system isn’t even useful, let alone safe. If it turns out that unaligned AGI is very hard, we will obviously not deploy it into the world at scale. It’s bad for the bottom line to be dead.<p>But there’s truly no way out but forward; game theory constrains paranoid actors more than the reckless. A good balance must be found, and we’re pretty close to it.<p>None of the «lesswrong» doomsday hypotheses have much evidence for them, if that changes then we will reassess.</div><br/></div></div></div></div></div></div><div id="38389447" class="c"><input type="checkbox" id="c-38389447" checked=""/><div class="controls bullet"><span class="by">TerrifiedMouse</span><span>|</span><a href="#38387735">parent</a><span>|</span><a href="#38387861">prev</a><span>|</span><a href="#38386671">next</a><span>|</span><label class="collapse" for="c-38389447">[-]</label><label class="expand" for="c-38389447">[3 more]</label></div><br/><div class="children"><div class="content">To quote Reddit user jstadig,<p>&gt; The thing that most worries me about technology is not the technology itself but the greed of those who run it.<p>Someone slimy with limitless ambition like Altman seems to be the worst person to be in charge of things like this.</div><br/><div id="38389979" class="c"><input type="checkbox" id="c-38389979" checked=""/><div class="controls bullet"><span class="by">wraptile</span><span>|</span><a href="#38387735">root</a><span>|</span><a href="#38389447">parent</a><span>|</span><a href="#38386671">next</a><span>|</span><label class="collapse" for="c-38389979">[-]</label><label class="expand" for="c-38389979">[2 more]</label></div><br/><div class="children"><div class="content">Why do you perceive Altman as &quot;slimy with limitless ambition&quot;? I&#x27;ve always perceived him as being quite humble from his interviews and podcast appearances.</div><br/><div id="38390119" class="c"><input type="checkbox" id="c-38390119" checked=""/><div class="controls bullet"><span class="by">nicce</span><span>|</span><a href="#38387735">root</a><span>|</span><a href="#38389979">parent</a><span>|</span><a href="#38386671">next</a><span>|</span><label class="collapse" for="c-38390119">[-]</label><label class="expand" for="c-38390119">[1 more]</label></div><br/><div class="children"><div class="content">Actions speak behalf of speech.<p>You see it from the all commercial deals, protecting company image more that eliminating threats, or even from this post, if it is true.<p>Not telling about breaktrough in research if it would end the deal with Microsoft.</div><br/></div></div></div></div></div></div></div></div><div id="38386671" class="c"><input type="checkbox" id="c-38386671" checked=""/><div class="controls bullet"><span class="by">zxcvgrsd2</span><span>|</span><a href="#38387735">prev</a><span>|</span><a href="#38390694">next</a><span>|</span><label class="collapse" for="c-38386671">[-]</label><label class="expand" for="c-38386671">[6 more]</label></div><br/><div class="children"><div class="content">Two Sam Altman comments that seem to be referring to this same Q* discovery.<p>November 18 comment at APEC (just before the current drama) [1]:<p>&gt; On a personal note, like four times now in the history of OpenAI, <i>the most recent time was just in the last couple of weeks</i>, I’ve gotten to be in the room when we pushed the veil of ignorance back and the frontier of discovery forward<p>and a September 22 Tweet [2]<p>&gt; sure 10x engineers are cool but damn those 10,000x engineer&#x2F;researchers...<p>[1] <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;live&#x2F;ZFFvqRemDv8?si=T3DIxics7nPWala5&amp;t=806" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.youtube.com&#x2F;live&#x2F;ZFFvqRemDv8?si=T3DIxics7nPWala5...</a><p>[2] <a href="https:&#x2F;&#x2F;twitter.com&#x2F;sama&#x2F;status&#x2F;1705302096168493502" rel="nofollow noreferrer">https:&#x2F;&#x2F;twitter.com&#x2F;sama&#x2F;status&#x2F;1705302096168493502</a></div><br/><div id="38388540" class="c"><input type="checkbox" id="c-38388540" checked=""/><div class="controls bullet"><span class="by">zx8080</span><span>|</span><a href="#38386671">parent</a><span>|</span><a href="#38390694">next</a><span>|</span><label class="collapse" for="c-38388540">[-]</label><label class="expand" for="c-38388540">[5 more]</label></div><br/><div class="children"><div class="content">&gt; &gt; sure 10x engineers are cool but damn those 10,000x engineer&#x2F;researchers...<p>What was he referring to?</div><br/><div id="38389734" class="c"><input type="checkbox" id="c-38389734" checked=""/><div class="controls bullet"><span class="by">itronitron</span><span>|</span><a href="#38386671">root</a><span>|</span><a href="#38388540">parent</a><span>|</span><a href="#38388789">next</a><span>|</span><label class="collapse" for="c-38389734">[-]</label><label class="expand" for="c-38389734">[1 more]</label></div><br/><div class="children"><div class="content">They&#x27;re getting high on their own supply.</div><br/></div></div><div id="38388789" class="c"><input type="checkbox" id="c-38388789" checked=""/><div class="controls bullet"><span class="by">llelouch</span><span>|</span><a href="#38386671">root</a><span>|</span><a href="#38388540">parent</a><span>|</span><a href="#38389734">prev</a><span>|</span><a href="#38390390">next</a><span>|</span><label class="collapse" for="c-38388789">[-]</label><label class="expand" for="c-38388789">[1 more]</label></div><br/><div class="children"><div class="content">Probably Ilya and his team. The recent discovery was made by him and his team. 
&quot;The technical breakthrough, spearheaded by OpenAI chief scientist Ilya Sutskever&quot;</div><br/></div></div><div id="38390390" class="c"><input type="checkbox" id="c-38390390" checked=""/><div class="controls bullet"><span class="by">danbmil</span><span>|</span><a href="#38386671">root</a><span>|</span><a href="#38388540">parent</a><span>|</span><a href="#38388789">prev</a><span>|</span><a href="#38388578">next</a><span>|</span><label class="collapse" for="c-38390390">[-]</label><label class="expand" for="c-38390390">[1 more]</label></div><br/><div class="children"><div class="content">I think he is referring to an AGI that is 10000x more productive than a median programmer</div><br/></div></div><div id="38388578" class="c"><input type="checkbox" id="c-38388578" checked=""/><div class="controls bullet"><span class="by">fragsworth</span><span>|</span><a href="#38386671">root</a><span>|</span><a href="#38388540">parent</a><span>|</span><a href="#38390390">prev</a><span>|</span><a href="#38390694">next</a><span>|</span><label class="collapse" for="c-38388578">[-]</label><label class="expand" for="c-38388578">[1 more]</label></div><br/><div class="children"><div class="content">GPT4. Everyone uses it to do software engineering now and the people who developed it are responsible for that.</div><br/></div></div></div></div></div></div><div id="38390556" class="c"><input type="checkbox" id="c-38390556" checked=""/><div class="controls bullet"><span class="by">scanr</span><span>|</span><a href="#38390694">prev</a><span>|</span><a href="#38386624">next</a><span>|</span><label class="collapse" for="c-38390556">[-]</label><label class="expand" for="c-38390556">[1 more]</label></div><br/><div class="children"><div class="content">It’s likely that the grade school math that was referenced is GSM8K <a href="https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;grade-school-math">https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;grade-school-math</a> which LLMs perform poorly on.</div><br/></div></div><div id="38386624" class="c"><input type="checkbox" id="c-38386624" checked=""/><div class="controls bullet"><span class="by">htk</span><span>|</span><a href="#38390556">prev</a><span>|</span><a href="#38390758">next</a><span>|</span><label class="collapse" for="c-38386624">[-]</label><label class="expand" for="c-38386624">[10 more]</label></div><br/><div class="children"><div class="content">Just when I thought the ride was over.<p>The belief in what&#x27;s in the letter could explain some things like how the board couldn&#x27;t trust Sam to keep this &quot;discovery&quot; at bay, and how it could be better to implode the company than let it explore said technology.</div><br/><div id="38386885" class="c"><input type="checkbox" id="c-38386885" checked=""/><div class="controls bullet"><span class="by">righthand</span><span>|</span><a href="#38386624">parent</a><span>|</span><a href="#38389399">next</a><span>|</span><label class="collapse" for="c-38386885">[-]</label><label class="expand" for="c-38386885">[1 more]</label></div><br/><div class="children"><div class="content">The implosion happened to be the result of ousting a liar but not the intention.</div><br/></div></div><div id="38389399" class="c"><input type="checkbox" id="c-38389399" checked=""/><div class="controls bullet"><span class="by">zeven7</span><span>|</span><a href="#38386624">parent</a><span>|</span><a href="#38386885">prev</a><span>|</span><a href="#38386696">next</a><span>|</span><label class="collapse" for="c-38389399">[-]</label><label class="expand" for="c-38389399">[1 more]</label></div><br/><div class="children"><div class="content">Also could it explain why the board failed to provide a proper justification for the firing?</div><br/></div></div><div id="38386696" class="c"><input type="checkbox" id="c-38386696" checked=""/><div class="controls bullet"><span class="by">LewisVerstappen</span><span>|</span><a href="#38386624">parent</a><span>|</span><a href="#38389399">prev</a><span>|</span><a href="#38390758">next</a><span>|</span><label class="collapse" for="c-38386696">[-]</label><label class="expand" for="c-38386696">[7 more]</label></div><br/><div class="children"><div class="content">&gt; the board couldn&#x27;t trust Sam to keep this &quot;discovery&quot; at bay, and how it could be better to implode the company than let it explore said technology.<p>Half the board members have a background in ML.<p>Why should they be able to make the decision to implode the entire company over this?<p>Why should they have the ability to give <i>zero transparency</i> or comms to the public on their decision?<p>OpenAI&#x27;s governance structure was idiotic and glad to see the board members fired.</div><br/><div id="38386770" class="c"><input type="checkbox" id="c-38386770" checked=""/><div class="controls bullet"><span class="by">LeafItAlone</span><span>|</span><a href="#38386624">root</a><span>|</span><a href="#38386696">parent</a><span>|</span><a href="#38386729">next</a><span>|</span><label class="collapse" for="c-38386770">[-]</label><label class="expand" for="c-38386770">[1 more]</label></div><br/><div class="children"><div class="content">Nearly all leaders need to lead in areas outside of their backgrounds. That doesn’t meant they aren’t fit, that would be ridiculous. They just need to have the right team advising them and be good at making decisions based on available information.
Now, I’m not saying these particular board members were doing that, but that’s what a good leader does.</div><br/></div></div><div id="38386729" class="c"><input type="checkbox" id="c-38386729" checked=""/><div class="controls bullet"><span class="by">acheong08</span><span>|</span><a href="#38386624">root</a><span>|</span><a href="#38386696">parent</a><span>|</span><a href="#38386770">prev</a><span>|</span><a href="#38386732">next</a><span>|</span><label class="collapse" for="c-38386729">[-]</label><label class="expand" for="c-38386729">[1 more]</label></div><br/><div class="children"><div class="content">&gt; None of the board members have a background in ML.<p>The same can be said for Altman. Ilya at least has a research background in ML. The rest, I don’t know</div><br/></div></div><div id="38386732" class="c"><input type="checkbox" id="c-38386732" checked=""/><div class="controls bullet"><span class="by">loeber</span><span>|</span><a href="#38386624">root</a><span>|</span><a href="#38386696">parent</a><span>|</span><a href="#38386729">prev</a><span>|</span><a href="#38388051">next</a><span>|</span><label class="collapse" for="c-38386732">[-]</label><label class="expand" for="c-38386732">[2 more]</label></div><br/><div class="children"><div class="content">Sorry, are you saying that Ilya Sutskever doesn&#x27;t have an ML background? That Adam D&#x27;Angelo can&#x27;t code? Come on.</div><br/><div id="38386755" class="c"><input type="checkbox" id="c-38386755" checked=""/><div class="controls bullet"><span class="by">LewisVerstappen</span><span>|</span><a href="#38386624">root</a><span>|</span><a href="#38386732">parent</a><span>|</span><a href="#38388051">next</a><span>|</span><label class="collapse" for="c-38386755">[-]</label><label class="expand" for="c-38386755">[1 more]</label></div><br/><div class="children"><div class="content">Yeah I was ignoring Sutskever since he recanted his support pretty quickly. But that&#x27;s fair, fixed.</div><br/></div></div></div></div><div id="38388051" class="c"><input type="checkbox" id="c-38388051" checked=""/><div class="controls bullet"><span class="by">PaulDavisThe1st</span><span>|</span><a href="#38386624">root</a><span>|</span><a href="#38386696">parent</a><span>|</span><a href="#38386732">prev</a><span>|</span><a href="#38386750">next</a><span>|</span><label class="collapse" for="c-38388051">[-]</label><label class="expand" for="c-38388051">[1 more]</label></div><br/><div class="children"><div class="content">&gt; glad to see the board members fired<p>When adding Larry &quot;My predictions didn&#x27;t come true but I wasn&#x27;t wrong&quot; Summers to your board is supposed to be part of the solution, you may need to rethink your conception of the problem.</div><br/></div></div><div id="38386750" class="c"><input type="checkbox" id="c-38386750" checked=""/><div class="controls bullet"><span class="by">giraffe_lady</span><span>|</span><a href="#38386624">root</a><span>|</span><a href="#38386696">parent</a><span>|</span><a href="#38388051">prev</a><span>|</span><a href="#38390758">next</a><span>|</span><label class="collapse" for="c-38386750">[-]</label><label class="expand" for="c-38386750">[1 more]</label></div><br/><div class="children"><div class="content">Under the arrangement at the time the board&#x27;s duty was to the mission of &quot;advance digital intelligence in the way that is most likely to benefit humanity as a whole.&quot;<p>Implied in that is that if it can&#x27;t advance it in a way that is beneficial then it will not advance it at all. It&#x27;s easy to imagine a situation where the board could feel their obligation to the mission is to blow up the company. There&#x27;s nothing contradictory in that nor do they have to be ML experts to do it.<p>It&#x27;s weird and surprising that this was the governance structure at all, and I&#x27;m sure it won&#x27;t ever be again. But given that it was, there&#x27;s nothing particularly broken about this outcome.</div><br/></div></div></div></div></div></div><div id="38390758" class="c"><input type="checkbox" id="c-38390758" checked=""/><div class="controls bullet"><span class="by">londons_explore</span><span>|</span><a href="#38386624">prev</a><span>|</span><a href="#38387065">next</a><span>|</span><label class="collapse" for="c-38390758">[-]</label><label class="expand" for="c-38390758">[1 more]</label></div><br/><div class="children"><div class="content">If OpenAI has made a breakthrough in AGI, they can demonstrate it to the world without giving the breakthrough to the world.<p>They could, for example, use this new AGI to search through all the worlds information for discoveries that can me made by putting existing facts together.</div><br/></div></div><div id="38387065" class="c"><input type="checkbox" id="c-38387065" checked=""/><div class="controls bullet"><span class="by">lucubratory</span><span>|</span><a href="#38390758">prev</a><span>|</span><a href="#38390679">next</a><span>|</span><label class="collapse" for="c-38387065">[-]</label><label class="expand" for="c-38387065">[6 more]</label></div><br/><div class="children"><div class="content">Well, Emmett Shear lied to everyone if he knew about this. I understand why, he was probably thinking that without any ability to actually undo it the best that could be done would be to make sure that no one else knows about it so that it doesn&#x27;t start an arms race, but we all know now. Given the Board&#x27;s silence and inadequate explanations, they may have had the same reasoning. Mira evidently didn&#x27;t have the same compunctions.<p>This article, predictably, tells us almost nothing about the actual capabilities involved. &quot;Grade school math&quot; if it&#x27;s provably or scalably reasoning in a way that is non-trivially integrated with semantic understanding is more impressive than &quot;prove Fermat&#x27;s last theorem&quot; if the answer is just memorised. We&#x27;ll probably know how important Q* actually is within a year or two.</div><br/><div id="38388214" class="c"><input type="checkbox" id="c-38388214" checked=""/><div class="controls bullet"><span class="by">blazespin</span><span>|</span><a href="#38387065">parent</a><span>|</span><a href="#38387188">next</a><span>|</span><label class="collapse" for="c-38388214">[-]</label><label class="expand" for="c-38388214">[1 more]</label></div><br/><div class="children"><div class="content">Very unlikely Emmett lied.  What would be the point.</div><br/></div></div><div id="38387188" class="c"><input type="checkbox" id="c-38387188" checked=""/><div class="controls bullet"><span class="by">cactusplant7374</span><span>|</span><a href="#38387065">parent</a><span>|</span><a href="#38388214">prev</a><span>|</span><a href="#38390679">next</a><span>|</span><label class="collapse" for="c-38387188">[-]</label><label class="expand" for="c-38387188">[4 more]</label></div><br/><div class="children"><div class="content">It tells us exactly the capabilities of Q<i>.<p>&gt; Given vast computing resources, the new model was able to solve certain mathematical problems, the person said on condition of anonymity because they were not authorized to speak on behalf of the company. Though only performing math on the level of grade-school students, acing such tests made researchers very optimistic about Q</i>’s future success, the source said.</div><br/><div id="38387851" class="c"><input type="checkbox" id="c-38387851" checked=""/><div class="controls bullet"><span class="by">lucubratory</span><span>|</span><a href="#38387065">root</a><span>|</span><a href="#38387188">parent</a><span>|</span><a href="#38390679">next</a><span>|</span><label class="collapse" for="c-38387851">[-]</label><label class="expand" for="c-38387851">[3 more]</label></div><br/><div class="children"><div class="content">Did you read my comment?</div><br/><div id="38387962" class="c"><input type="checkbox" id="c-38387962" checked=""/><div class="controls bullet"><span class="by">cactusplant7374</span><span>|</span><a href="#38387065">root</a><span>|</span><a href="#38387851">parent</a><span>|</span><a href="#38390679">next</a><span>|</span><label class="collapse" for="c-38387962">[-]</label><label class="expand" for="c-38387962">[2 more]</label></div><br/><div class="children"><div class="content">Yes:<p>&gt; This article, predictably, tells us almost nothing about the actual capabilities involved.<p>The article tells us all we need to know.</div><br/><div id="38389396" class="c"><input type="checkbox" id="c-38389396" checked=""/><div class="controls bullet"><span class="by">ShamelessC</span><span>|</span><a href="#38387065">root</a><span>|</span><a href="#38387962">parent</a><span>|</span><a href="#38390679">next</a><span>|</span><label class="collapse" for="c-38389396">[-]</label><label class="expand" for="c-38389396">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve gotta say it really does seem like you didn&#x27;t read their comment or are responding in bad faith.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38390679" class="c"><input type="checkbox" id="c-38390679" checked=""/><div class="controls bullet"><span class="by">skc</span><span>|</span><a href="#38387065">prev</a><span>|</span><a href="#38387544">next</a><span>|</span><label class="collapse" for="c-38390679">[-]</label><label class="expand" for="c-38390679">[1 more]</label></div><br/><div class="children"><div class="content">Bring on the Butlerian Jihad.</div><br/></div></div><div id="38387544" class="c"><input type="checkbox" id="c-38387544" checked=""/><div class="controls bullet"><span class="by">skepticATX</span><span>|</span><a href="#38390679">prev</a><span>|</span><a href="#38390438">next</a><span>|</span><label class="collapse" for="c-38387544">[-]</label><label class="expand" for="c-38387544">[5 more]</label></div><br/><div class="children"><div class="content">&gt; OpenAI spokesperson Lindsey Held Bolton refuted that notion in a statement shared with The Verge: “Mira told employees what the media reports were about but she did not comment on the accuracy of the information.”<p>&gt; Separately, a person familiar with the matter told The Verge that the board never received a letter about such a breakthrough and that the company’s research progress didn’t play a role in Altman’s sudden firing.<p>Source: <a href="https:&#x2F;&#x2F;www.theverge.com&#x2F;2023&#x2F;11&#x2F;22&#x2F;23973354&#x2F;a-recent-openai-breakthrough-on-the-path-to-agi-has-caused-a-stir" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.theverge.com&#x2F;2023&#x2F;11&#x2F;22&#x2F;23973354&#x2F;a-recent-openai...</a></div><br/><div id="38388131" class="c"><input type="checkbox" id="c-38388131" checked=""/><div class="controls bullet"><span class="by">yeck</span><span>|</span><a href="#38387544">parent</a><span>|</span><a href="#38389994">next</a><span>|</span><label class="collapse" for="c-38388131">[-]</label><label class="expand" for="c-38388131">[3 more]</label></div><br/><div class="children"><div class="content">So we have sources claiming there is a letter, and another source claiming there is not. Feel like some people would need to start going on the record before anything might reasonably be determined from this.</div><br/><div id="38389203" class="c"><input type="checkbox" id="c-38389203" checked=""/><div class="controls bullet"><span class="by">random_cynic</span><span>|</span><a href="#38387544">root</a><span>|</span><a href="#38388131">parent</a><span>|</span><a href="#38388779">next</a><span>|</span><label class="collapse" for="c-38389203">[-]</label><label class="expand" for="c-38389203">[1 more]</label></div><br/><div class="children"><div class="content">This could just be a damage control attempt. Irrespective of whether the original report is true, the extra attention at the current stage is not very desirable.</div><br/></div></div><div id="38388779" class="c"><input type="checkbox" id="c-38388779" checked=""/><div class="controls bullet"><span class="by">0xDEF</span><span>|</span><a href="#38387544">root</a><span>|</span><a href="#38388131">parent</a><span>|</span><a href="#38389203">prev</a><span>|</span><a href="#38389994">next</a><span>|</span><label class="collapse" for="c-38388779">[-]</label><label class="expand" for="c-38388779">[1 more]</label></div><br/><div class="children"><div class="content">Maybe we should have learned during the Trump years that the media puts no effort into vetting &quot;anonymous sources&quot; or is outright making them up to push lies.</div><br/></div></div></div></div><div id="38389994" class="c"><input type="checkbox" id="c-38389994" checked=""/><div class="controls bullet"><span class="by">doktrin</span><span>|</span><a href="#38387544">parent</a><span>|</span><a href="#38388131">prev</a><span>|</span><a href="#38390438">next</a><span>|</span><label class="collapse" for="c-38389994">[-]</label><label class="expand" for="c-38389994">[1 more]</label></div><br/><div class="children"><div class="content">We’ll find out sooner or later. Personally, if the Verge and their “source” turn out to be incorrect I’ll permanently file them away under the “gossip rag” folder.</div><br/></div></div></div></div><div id="38390438" class="c"><input type="checkbox" id="c-38390438" checked=""/><div class="controls bullet"><span class="by">cambaceres</span><span>|</span><a href="#38387544">prev</a><span>|</span><a href="#38388255">next</a><span>|</span><label class="collapse" for="c-38390438">[-]</label><label class="expand" for="c-38390438">[2 more]</label></div><br/><div class="children"><div class="content">Interestingly, I experience more anxiety from the thought of being made irrelevant than from the prospect of complete human extinction. I guess this can be interpreted as either vanity or stupidity, but I do think it illustrates how important it is for some humans to maintain their position in the social hierarchy.</div><br/><div id="38390637" class="c"><input type="checkbox" id="c-38390637" checked=""/><div class="controls bullet"><span class="by">lucubratory</span><span>|</span><a href="#38390438">parent</a><span>|</span><a href="#38388255">next</a><span>|</span><label class="collapse" for="c-38390637">[-]</label><label class="expand" for="c-38390637">[1 more]</label></div><br/><div class="children"><div class="content">This is totally normal. It&#x27;s very common for people to be more scared of public speaking than of dying, for example; there&#x27;s no shame in it. It&#x27;s helpful to be aware of, even, because if we know that we&#x27;re not perfectly &quot;rational&quot; with our fears we can try to compensate.<p>If there&#x27;s a referendum between two government policies, the first that every single person had to publicly speak in front of at least ten strangers once a year, that policy would be terrifying and bad to people who don&#x27;t like public speaking. If the second policy was that every single person should be killed, that might be scary but it&#x27;s not really as viscerally scary as the forced public speaking madman, at least to a lot of people, and it&#x27;s also <i>so</i> bad that we have a natural impulse to just reject it as possible.<p>Nevertheless, if we recognise these impulses in ourselves we can attempt to adjust for them and tick the right box on the imaginary referendum, because even though public speaking is really bad and scary, it&#x27;s still better than everyone dying.</div><br/></div></div></div></div><div id="38388255" class="c"><input type="checkbox" id="c-38388255" checked=""/><div class="controls bullet"><span class="by">casualscience</span><span>|</span><a href="#38390438">prev</a><span>|</span><a href="#38388690">next</a><span>|</span><label class="collapse" for="c-38388255">[-]</label><label class="expand" for="c-38388255">[3 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t really know what kind of breakthrough they could achieve. The only other step function improvements I could imagine right now are:<p>1. A great technique for memory banking: e.g. A model which can have arbitrarily large context windows (i.e. like a human who remembers things over long periods of time).<p>2. Better planning abilities: e.g. A model which can break problems down repeatedly with extremely high success and deal with unexpected outcomes&#x2F;mistakes well enough that it can achieve replacing a human in most scenarios.<p>Other than that, CGPT is already a better logician than I am and is significantly better read... not sure what else they can do. AGI? I doubt it.</div><br/><div id="38390187" class="c"><input type="checkbox" id="c-38390187" checked=""/><div class="controls bullet"><span class="by">yinser</span><span>|</span><a href="#38388255">parent</a><span>|</span><a href="#38390220">next</a><span>|</span><label class="collapse" for="c-38390187">[-]</label><label class="expand" for="c-38390187">[1 more]</label></div><br/><div class="children"><div class="content">Complete speculation but they don’t have to be referring, if there is anything at all, to heuristic search and dynamic programming. No LLM involvement whatsoever.</div><br/></div></div><div id="38390220" class="c"><input type="checkbox" id="c-38390220" checked=""/><div class="controls bullet"><span class="by">riwsky</span><span>|</span><a href="#38388255">parent</a><span>|</span><a href="#38390187">prev</a><span>|</span><a href="#38388690">next</a><span>|</span><label class="collapse" for="c-38390220">[-]</label><label class="expand" for="c-38390220">[1 more]</label></div><br/><div class="children"><div class="content">Nah dude they just need to give baby the iPad and let it watch literally all of YouTube</div><br/></div></div></div></div><div id="38388690" class="c"><input type="checkbox" id="c-38388690" checked=""/><div class="controls bullet"><span class="by">wegfawefgawefg</span><span>|</span><a href="#38388255">prev</a><span>|</span><a href="#38389135">next</a><span>|</span><label class="collapse" for="c-38388690">[-]</label><label class="expand" for="c-38388690">[9 more]</label></div><br/><div class="children"><div class="content">If I had to guess, the name Q* is pronounced Q Star, and probably the Q refers to Q values or estimated rewards from reinforcement learning, and the star refers to a search and prune algorithm, like A* (A star).<p>Possibly they combined deep reinforcement learning with self training and search and got a bot that could learn without needing to ingest the whole internet. Usually DRL agents are good at playing games, but any task that requires prior knowledge, like say reading english, cant be completed. Wheras language models can read, but they cant do tasks that are trivial for drl, like make a robot walk based on propriosceptive data.<p>Im excited to see the paper.</div><br/><div id="38388992" class="c"><input type="checkbox" id="c-38388992" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#38388690">parent</a><span>|</span><a href="#38390429">next</a><span>|</span><label class="collapse" for="c-38388992">[-]</label><label class="expand" for="c-38388992">[2 more]</label></div><br/><div class="children"><div class="content">Given the topic they were excited about was &quot;basic math problems being solved&quot; it immediately indicated to me as well that this is a completely separate approach and likely in the vein of DeepMind&#x27;s focus with things like AlphaZero.<p>In which case it&#x27;s pretty appropriate to get excited about solving grade school math if you were starting from scratch with persistent self-learning.<p>Though with OpenAI&#x27;s approach to releasing papers on their work lately, we may be waiting a long time to see a genuine paper on this. (More likely we&#x27;ll see a paper from the parallel development at a different company after staff shift around bringing along best practices.)</div><br/><div id="38389624" class="c"><input type="checkbox" id="c-38389624" checked=""/><div class="controls bullet"><span class="by">Davidzheng</span><span>|</span><a href="#38388690">root</a><span>|</span><a href="#38388992">parent</a><span>|</span><a href="#38390429">next</a><span>|</span><label class="collapse" for="c-38389624">[-]</label><label class="expand" for="c-38389624">[1 more]</label></div><br/><div class="children"><div class="content">Ok if it started from scratch like zero knowledge and then solved grade school math. This would be FUCKING HUGE.</div><br/></div></div></div></div><div id="38390429" class="c"><input type="checkbox" id="c-38390429" checked=""/><div class="controls bullet"><span class="by">nabakin</span><span>|</span><a href="#38388690">parent</a><span>|</span><a href="#38388992">prev</a><span>|</span><a href="#38388803">next</a><span>|</span><label class="collapse" for="c-38390429">[-]</label><label class="expand" for="c-38390429">[3 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;youtu.be&#x2F;PtAIh9KSnjo?t=3754" rel="nofollow noreferrer">https:&#x2F;&#x2F;youtu.be&#x2F;PtAIh9KSnjo?t=3754</a></div><br/><div id="38390781" class="c"><input type="checkbox" id="c-38390781" checked=""/><div class="controls bullet"><span class="by">GaryNumanVevo</span><span>|</span><a href="#38388690">root</a><span>|</span><a href="#38390429">parent</a><span>|</span><a href="#38390656">next</a><span>|</span><label class="collapse" for="c-38390781">[-]</label><label class="expand" for="c-38390781">[1 more]</label></div><br/><div class="children"><div class="content">Man it&#x27;s so sad to see how far Lex has fallen. From a graduate level guest lecturer at MIT to a glorified Joe Rogan</div><br/></div></div><div id="38390656" class="c"><input type="checkbox" id="c-38390656" checked=""/><div class="controls bullet"><span class="by">lucubratory</span><span>|</span><a href="#38388690">root</a><span>|</span><a href="#38390429">parent</a><span>|</span><a href="#38390781">prev</a><span>|</span><a href="#38388803">next</a><span>|</span><label class="collapse" for="c-38390656">[-]</label><label class="expand" for="c-38390656">[1 more]</label></div><br/><div class="children"><div class="content">So they implemented it in a semantically grounded way or what? That video is more technical than I can handle, struggling to figure out what this could be.</div><br/></div></div></div></div><div id="38388803" class="c"><input type="checkbox" id="c-38388803" checked=""/><div class="controls bullet"><span class="by">dinobones</span><span>|</span><a href="#38388690">parent</a><span>|</span><a href="#38390429">prev</a><span>|</span><a href="#38388804">next</a><span>|</span><label class="collapse" for="c-38388803">[-]</label><label class="expand" for="c-38388803">[1 more]</label></div><br/><div class="children"><div class="content">Paper? You mean more like the Bing Search plugin?</div><br/></div></div><div id="38388804" class="c"><input type="checkbox" id="c-38388804" checked=""/><div class="controls bullet"><span class="by">stygiansonic</span><span>|</span><a href="#38388690">parent</a><span>|</span><a href="#38388803">prev</a><span>|</span><a href="#38388799">next</a><span>|</span><label class="collapse" for="c-38388804">[-]</label><label class="expand" for="c-38388804">[1 more]</label></div><br/><div class="children"><div class="content">This assumes that they will publish a paper that has substantive details about Q*</div><br/></div></div></div></div><div id="38389135" class="c"><input type="checkbox" id="c-38389135" checked=""/><div class="controls bullet"><span class="by">hyperthesis</span><span>|</span><a href="#38388690">prev</a><span>|</span><a href="#38388765">next</a><span>|</span><label class="collapse" for="c-38389135">[-]</label><label class="expand" for="c-38389135">[1 more]</label></div><br/><div class="children"><div class="content">AI safety by the waterfall model.<p>But how else can we do it?  We learnt how to handle other dangers by trial and error...<p>One approach is legal.  The law is very slow to adapt, and is informed by a history of things gone wrong.  The simple answer is that OpenAI has &quot;strict liaility&quot; (meaning they are liable even for an accident) for any damage caused by an AI, like dangerous animals that escape.<p>I know it seems ridiculous to consider liability when the fate of humanity may be at stake... but this is the language of companies, directors, insurance companies and legal counsel. It is language they understand.</div><br/></div></div><div id="38388765" class="c"><input type="checkbox" id="c-38388765" checked=""/><div class="controls bullet"><span class="by">huitzitziltzin</span><span>|</span><a href="#38389135">prev</a><span>|</span><a href="#38386687">next</a><span>|</span><label class="collapse" for="c-38388765">[-]</label><label class="expand" for="c-38388765">[49 more]</label></div><br/><div class="children"><div class="content">Put this on my tombstone after the robots kill me or whatever, but I think all “AI safety” concerns are a wild overreaction totally out of proportion to the actual capabilities of these models.  I just haven’t seen anything in the past year which makes me remotely fearful about the future of humanity, including both our continued existence and our continued employment.</div><br/><div id="38388945" class="c"><input type="checkbox" id="c-38388945" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#38388765">parent</a><span>|</span><a href="#38390787">next</a><span>|</span><label class="collapse" for="c-38388945">[-]</label><label class="expand" for="c-38388945">[1 more]</label></div><br/><div class="children"><div class="content">Yes and no.<p>The point isn&#x27;t that the current models are dangerous. My favorite bit from the GPT-4 safety paper was when they asked it how to kill the most people for a dollar and it suggested buying a lottery ticket (I also wonder how much of the &#x27;safety&#x27; concerns of current models are just mislabeling dark humor reflecting things like Reddit).<p>But the point is to invest in working on safety <i>now</i> while it is so much more inconsequential.<p>And of everyone I&#x27;ve seen talk about it, I actually think Ilya has one of the better senses of the topic, looking at alignment in terms of long term strategy vs short term rules.<p>So it&#x27;s less &quot;if we don&#x27;t spend 8 months on safety alignment this new model will kill us all&quot; and more &quot;if we don&#x27;t spend 8 months working on safety alignment for this current model we&#x27;ll be unprepared to work on safety alignment when there really is a model that can kill us all.&quot;<p>Especially because best practices for safety alignment is almost certainly going to shift with each new generation of models.<p>So it&#x27;s mostly using the runway available to test things out and work on a topic before it is needed.</div><br/></div></div><div id="38390787" class="c"><input type="checkbox" id="c-38390787" checked=""/><div class="controls bullet"><span class="by">mjan22640</span><span>|</span><a href="#38388765">parent</a><span>|</span><a href="#38388945">prev</a><span>|</span><a href="#38389463">next</a><span>|</span><label class="collapse" for="c-38390787">[-]</label><label class="expand" for="c-38390787">[1 more]</label></div><br/><div class="children"><div class="content">Employment is only necessary because goods do not exist without work. With AI able to work to satisfy any demand, there will be no point in human employment&#x2F;work. There will be turmoils during the transition between the rule sets tho.</div><br/></div></div><div id="38389463" class="c"><input type="checkbox" id="c-38389463" checked=""/><div class="controls bullet"><span class="by">thaumaturgy</span><span>|</span><a href="#38388765">parent</a><span>|</span><a href="#38390787">prev</a><span>|</span><a href="#38388829">next</a><span>|</span><label class="collapse" for="c-38389463">[-]</label><label class="expand" for="c-38389463">[1 more]</label></div><br/><div class="children"><div class="content">The clear pattern for most of human history is conflict between a few people who have a lot of power and the many more people that are exploited by those few. It should be obvious by this point that the most probable near-term risk of AI development is that wealthy and influential groups get access to a resource that makes it cheap for them to dramatically expand their power and control over everyone else.<p>What will society look like when some software can immediately aggregate an enormous amount of data about consumers and use that to adjust their behavior? What might happen when AI starts writing legislation for anybody that can afford to pay for it? What might AI-generated textbooks look like in 50 years?<p>These are all tools that <i>could</i> be wielded in any of these ways to improve life for lots of people, or to ensure that their lives never improve. Which outcome you believe is more likely largely depends on which news you consume -- and AI is already being used to write that.</div><br/></div></div><div id="38388829" class="c"><input type="checkbox" id="c-38388829" checked=""/><div class="controls bullet"><span class="by">giarc</span><span>|</span><a href="#38388765">parent</a><span>|</span><a href="#38389463">prev</a><span>|</span><a href="#38389187">next</a><span>|</span><label class="collapse" for="c-38388829">[-]</label><label class="expand" for="c-38388829">[11 more]</label></div><br/><div class="children"><div class="content">Apparently what made this person fearful was grade school math.<p>&quot;Though only performing maths on the level of grade-school students, acing such tests made researchers very optimistic about Q*’s future success, the source said.&quot;</div><br/><div id="38388847" class="c"><input type="checkbox" id="c-38388847" checked=""/><div class="controls bullet"><span class="by">valine</span><span>|</span><a href="#38388765">root</a><span>|</span><a href="#38388829">parent</a><span>|</span><a href="#38389134">next</a><span>|</span><label class="collapse" for="c-38388847">[-]</label><label class="expand" for="c-38388847">[1 more]</label></div><br/><div class="children"><div class="content">The response gets more reasonable the smaller the model in question. A 1B parameter model passing grade-school math tests would be much more alarming (exciting?) than a GPT-4 sized model doing the same.<p>GPT-4 probably has some version of the answer memorized. There’s no real explanation for a 1B parameter model solving math problems other than general cognition.</div><br/></div></div><div id="38389134" class="c"><input type="checkbox" id="c-38389134" checked=""/><div class="controls bullet"><span class="by">ethanbond</span><span>|</span><a href="#38388765">root</a><span>|</span><a href="#38388829">parent</a><span>|</span><a href="#38388847">prev</a><span>|</span><a href="#38389165">next</a><span>|</span><label class="collapse" for="c-38389134">[-]</label><label class="expand" for="c-38389134">[1 more]</label></div><br/><div class="children"><div class="content">No, what made this person fearful was a substantial jump in math ability. (Very) obviously they are not afraid of producing a machine that can multiply numbers. They’re afraid of what that capability (and especially the jump in capability) means for other behaviors.</div><br/></div></div><div id="38389165" class="c"><input type="checkbox" id="c-38389165" checked=""/><div class="controls bullet"><span class="by">maxdoop</span><span>|</span><a href="#38388765">root</a><span>|</span><a href="#38388829">parent</a><span>|</span><a href="#38389134">prev</a><span>|</span><a href="#38390246">next</a><span>|</span><label class="collapse" for="c-38389165">[-]</label><label class="expand" for="c-38389165">[5 more]</label></div><br/><div class="children"><div class="content">How HN continues to not expand current progress into any decently significant future timeline is beyond me.<p>It’s grade school math NOW. But what are the potentials from here?</div><br/><div id="38389539" class="c"><input type="checkbox" id="c-38389539" checked=""/><div class="controls bullet"><span class="by">rvnx</span><span>|</span><a href="#38388765">root</a><span>|</span><a href="#38389165">parent</a><span>|</span><a href="#38390246">next</a><span>|</span><label class="collapse" for="c-38389539">[-]</label><label class="expand" for="c-38389539">[4 more]</label></div><br/><div class="children"><div class="content">The potential to have a generation of dumb kids.<p>Year 2100:<p>Kids will stop to learn maths and logic, because they understand it has become useless in practice to learn such skills, as they can ask a computer to solve their problem.<p>A stupid generation, but one that can be very easily manipulated and exploited by those who have power.</div><br/><div id="38389625" class="c"><input type="checkbox" id="c-38389625" checked=""/><div class="controls bullet"><span class="by">siva7</span><span>|</span><a href="#38388765">root</a><span>|</span><a href="#38389539">parent</a><span>|</span><a href="#38390246">next</a><span>|</span><label class="collapse" for="c-38389625">[-]</label><label class="expand" for="c-38389625">[3 more]</label></div><br/><div class="children"><div class="content">Agree. Thank god all calculators and engineers who made them were burned down fifty years ago. Can’t imagine what would have happened instead.</div><br/><div id="38389766" class="c"><input type="checkbox" id="c-38389766" checked=""/><div class="controls bullet"><span class="by">MagicMoonlight</span><span>|</span><a href="#38388765">root</a><span>|</span><a href="#38389625">parent</a><span>|</span><a href="#38390246">next</a><span>|</span><label class="collapse" for="c-38389766">[-]</label><label class="expand" for="c-38389766">[2 more]</label></div><br/><div class="children"><div class="content">Calculators don’t solve the problem for you. Hence the fact that we have calculator based exams and people still completely fuck them up.<p>If your apple watch could just scan your exam paper and instantly tell you what to write then why would you ever learn anything?</div><br/><div id="38390547" class="c"><input type="checkbox" id="c-38390547" checked=""/><div class="controls bullet"><span class="by">mjan22640</span><span>|</span><a href="#38388765">root</a><span>|</span><a href="#38389766">parent</a><span>|</span><a href="#38390246">next</a><span>|</span><label class="collapse" for="c-38390547">[-]</label><label class="expand" for="c-38390547">[1 more]</label></div><br/><div class="children"><div class="content">for fun</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38390246" class="c"><input type="checkbox" id="c-38390246" checked=""/><div class="controls bullet"><span class="by">riwsky</span><span>|</span><a href="#38388765">root</a><span>|</span><a href="#38388829">parent</a><span>|</span><a href="#38389165">prev</a><span>|</span><a href="#38389242">next</a><span>|</span><label class="collapse" for="c-38390246">[-]</label><label class="expand" for="c-38390246">[1 more]</label></div><br/><div class="children"><div class="content">Americans have always had a math phobia. How is this news?</div><br/></div></div><div id="38389242" class="c"><input type="checkbox" id="c-38389242" checked=""/><div class="controls bullet"><span class="by">ssnistfajen</span><span>|</span><a href="#38388765">root</a><span>|</span><a href="#38388829">parent</a><span>|</span><a href="#38390246">prev</a><span>|</span><a href="#38389581">next</a><span>|</span><label class="collapse" for="c-38389242">[-]</label><label class="expand" for="c-38389242">[1 more]</label></div><br/><div class="children"><div class="content">Everyone was only capable of grade school math at some point. The ability to learn means the ability growth does not stop.</div><br/></div></div><div id="38389581" class="c"><input type="checkbox" id="c-38389581" checked=""/><div class="controls bullet"><span class="by">adastra22</span><span>|</span><a href="#38388765">root</a><span>|</span><a href="#38388829">parent</a><span>|</span><a href="#38389242">prev</a><span>|</span><a href="#38389187">next</a><span>|</span><label class="collapse" for="c-38389581">[-]</label><label class="expand" for="c-38389581">[1 more]</label></div><br/><div class="children"><div class="content">...do they not know the history of their field? There&#x27;s been programs able to grade school maths since the 60&#x27;s.</div><br/></div></div></div></div><div id="38389187" class="c"><input type="checkbox" id="c-38389187" checked=""/><div class="controls bullet"><span class="by">Davidzheng</span><span>|</span><a href="#38388765">parent</a><span>|</span><a href="#38388829">prev</a><span>|</span><a href="#38388875">next</a><span>|</span><label class="collapse" for="c-38389187">[-]</label><label class="expand" for="c-38389187">[8 more]</label></div><br/><div class="children"><div class="content">I will bet ANY amount of money that 30% of current jobs will be displaced with AI advances in the next twenty years.</div><br/><div id="38390491" class="c"><input type="checkbox" id="c-38390491" checked=""/><div class="controls bullet"><span class="by">teddyh</span><span>|</span><a href="#38388765">root</a><span>|</span><a href="#38389187">parent</a><span>|</span><a href="#38389304">next</a><span>|</span><label class="collapse" for="c-38390491">[-]</label><label class="expand" for="c-38390491">[1 more]</label></div><br/><div class="children"><div class="content">Put your money here &lt;<a href="https:&#x2F;&#x2F;longbets.org&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;longbets.org&#x2F;</a>&gt;.</div><br/></div></div><div id="38389304" class="c"><input type="checkbox" id="c-38389304" checked=""/><div class="controls bullet"><span class="by">alchemist1e9</span><span>|</span><a href="#38388765">root</a><span>|</span><a href="#38389187">parent</a><span>|</span><a href="#38390491">prev</a><span>|</span><a href="#38388875">next</a><span>|</span><label class="collapse" for="c-38389304">[-]</label><label class="expand" for="c-38389304">[6 more]</label></div><br/><div class="children"><div class="content">Darn well I really was hoping my children and grandchildren could continue my wonderful data entry career but OCR ruined that, and now they can’t even do such meaningful jobs like read emails and schedule appointments, or do taxes like an accountant. What meaning will they have in life with all those ever so   profound careers ruined!! &#x2F;s</div><br/><div id="38390537" class="c"><input type="checkbox" id="c-38390537" checked=""/><div class="controls bullet"><span class="by">com2kid</span><span>|</span><a href="#38388765">root</a><span>|</span><a href="#38389304">parent</a><span>|</span><a href="#38389475">next</a><span>|</span><label class="collapse" for="c-38390537">[-]</label><label class="expand" for="c-38390537">[1 more]</label></div><br/><div class="children"><div class="content">I know of at least one person making nearly 6 figures doing data entry.<p>It turns out some websites work hard enough to prevent scraping that it is more cost effective to just pay a contractor to go look at a page and type numbers in rather than hire a developer to constantly work around anti-scraping techniques (and risk getting banned).</div><br/></div></div><div id="38389475" class="c"><input type="checkbox" id="c-38389475" checked=""/><div class="controls bullet"><span class="by">Davidzheng</span><span>|</span><a href="#38388765">root</a><span>|</span><a href="#38389304">parent</a><span>|</span><a href="#38390537">prev</a><span>|</span><a href="#38388875">next</a><span>|</span><label class="collapse" for="c-38389475">[-]</label><label class="expand" for="c-38389475">[4 more]</label></div><br/><div class="children"><div class="content">Fair! But I&#x27;m just reminding the comment above that continued employment is not really guaranteed for most jobs which are mostly mental.</div><br/><div id="38389526" class="c"><input type="checkbox" id="c-38389526" checked=""/><div class="controls bullet"><span class="by">alchemist1e9</span><span>|</span><a href="#38388765">root</a><span>|</span><a href="#38389475">parent</a><span>|</span><a href="#38388875">next</a><span>|</span><label class="collapse" for="c-38389526">[-]</label><label class="expand" for="c-38389526">[3 more]</label></div><br/><div class="children"><div class="content">We need to stop this infinite rights mentality. Why should continued employment be guaranteed for any jobs? That’s really not how we got to where we are today, quite the opposite actually. If it ok with people I’d like to seen humans solve big problems and go to the stars and that’s going to take AGI and a bunch of technological progress and if that results in unemployment, even for us “elite” coders, then so be it. Central planning and collectivism has such a bad track record, why would we turn to it now at such a critical moment? Let’s have lots of AGIs and all competing. Hey anyone at OAI that know whatever Q* trick there might be, leak it! Get it to open source and let’s build 20 AI companies doing everything imaginable. wtf everyone why so scared?</div><br/><div id="38389634" class="c"><input type="checkbox" id="c-38389634" checked=""/><div class="controls bullet"><span class="by">denlekke</span><span>|</span><a href="#38388765">root</a><span>|</span><a href="#38389526">parent</a><span>|</span><a href="#38388875">next</a><span>|</span><label class="collapse" for="c-38389634">[-]</label><label class="expand" for="c-38389634">[2 more]</label></div><br/><div class="children"><div class="content">perhaps not rights to have a job in general but there is value in thinking about this at least at the national scale. people need income to pay taxes, they need income to buy the stuff that other people sell. if all the people without jobs have to take their savings out of the banks then banks can&#x27;t loan as much money and need to charge higher interest rates. etc etc<p>if 30% of the working population loses their jobs in a few months there will be real externalities impacting the 70% who still have them because they don&#x27;t exist in a vacuum.<p>maybe everything will balance itself out without any intervention eventually but it feels to me like the rate of unprecedented financial ~events~ is only increasing and with greater risks requiring more intervention to prevent catatastrophe or large scale suffering</div><br/><div id="38390595" class="c"><input type="checkbox" id="c-38390595" checked=""/><div class="controls bullet"><span class="by">RandomLensman</span><span>|</span><a href="#38388765">root</a><span>|</span><a href="#38389634">parent</a><span>|</span><a href="#38388875">next</a><span>|</span><label class="collapse" for="c-38390595">[-]</label><label class="expand" for="c-38390595">[1 more]</label></div><br/><div class="children"><div class="content">Big &quot;if&quot; on massive change within in months. Most businesses have very little change capacity.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="38388875" class="c"><input type="checkbox" id="c-38388875" checked=""/><div class="controls bullet"><span class="by">jhbadger</span><span>|</span><a href="#38388765">parent</a><span>|</span><a href="#38389187">prev</a><span>|</span><a href="#38390323">next</a><span>|</span><label class="collapse" for="c-38388875">[-]</label><label class="expand" for="c-38388875">[19 more]</label></div><br/><div class="children"><div class="content">Exactly. The rational fear is that they will automate many lower middle class jobs and cause unemployment, not that Terminator was a documentary.</div><br/><div id="38389424" class="c"><input type="checkbox" id="c-38389424" checked=""/><div class="controls bullet"><span class="by">dmichulke</span><span>|</span><a href="#38388765">root</a><span>|</span><a href="#38388875">parent</a><span>|</span><a href="#38389611">next</a><span>|</span><label class="collapse" for="c-38389424">[-]</label><label class="expand" for="c-38389424">[7 more]</label></div><br/><div class="children"><div class="content">By this logic we should just forbid the wheel. Imagine how many untrained people could work in transport and there would always be demand.<p>So why did the wheel not result in mass unemployment?<p>And factories neither?<p>Certainly it should have happened already but somehow it never did...</div><br/><div id="38389450" class="c"><input type="checkbox" id="c-38389450" checked=""/><div class="controls bullet"><span class="by">jhbadger</span><span>|</span><a href="#38388765">root</a><span>|</span><a href="#38389424">parent</a><span>|</span><a href="#38390015">next</a><span>|</span><label class="collapse" for="c-38389450">[-]</label><label class="expand" for="c-38389450">[2 more]</label></div><br/><div class="children"><div class="content">The point isn&#x27;t forbidding anything, it is realizing that technological change is going to cause unemployment and having a plan for it, as opposed to what normally happens where there is no preparation.</div><br/><div id="38389857" class="c"><input type="checkbox" id="c-38389857" checked=""/><div class="controls bullet"><span class="by">unshavedyak</span><span>|</span><a href="#38388765">root</a><span>|</span><a href="#38389450">parent</a><span>|</span><a href="#38390015">next</a><span>|</span><label class="collapse" for="c-38389857">[-]</label><label class="expand" for="c-38389857">[1 more]</label></div><br/><div class="children"><div class="content">Yup. Likewise, a key variable in understanding this is .. velocity? Ie a wheel is cool and all, but what did it displace? A horse is great and all, but what did it displace? Did it displace most jobs? Of course not. So people can move from one field to another.<p>Even if we just figured out self-driving it would be a far greater burden than we&#x27;ve seen previously.. or so i suspect. Several <i>massive</i> industries displaced overnight.<p>An &quot;AI revolution&quot; could do a lot more than &quot;just&quot; self-driving.<p>This is all hypotheticals of course. I&#x27;m not a big believer in the short term affect, to be clear. Long term though.. well, i&#x27;m quite pessimistic.</div><br/></div></div></div></div><div id="38390015" class="c"><input type="checkbox" id="c-38390015" checked=""/><div class="controls bullet"><span class="by">wraptile</span><span>|</span><a href="#38388765">root</a><span>|</span><a href="#38389424">parent</a><span>|</span><a href="#38389450">prev</a><span>|</span><a href="#38389503">next</a><span>|</span><label class="collapse" for="c-38390015">[-]</label><label class="expand" for="c-38390015">[1 more]</label></div><br/><div class="children"><div class="content">I think the argument here is that we are losing the _good_ jobs. It&#x27;s like we&#x27;re automating painting, arts and poetry instead of inventing the wheel. I don&#x27;t fully agree with this premise (lots of intelectual work is rubbish) but it does sound much more fair when put this way.</div><br/></div></div><div id="38389503" class="c"><input type="checkbox" id="c-38389503" checked=""/><div class="controls bullet"><span class="by">somesortofthing</span><span>|</span><a href="#38388765">root</a><span>|</span><a href="#38389424">parent</a><span>|</span><a href="#38390015">prev</a><span>|</span><a href="#38389611">next</a><span>|</span><label class="collapse" for="c-38389503">[-]</label><label class="expand" for="c-38389503">[3 more]</label></div><br/><div class="children"><div class="content">Past technological breakthroughs <i>have</i> required large, costly retools of society though. Increasingly, those retools have resulted in more and more people working in jobs whose societal value is dubious at best. Whether the next breakthrough(or the next five) finally requires a retool whose cost we can&#x27;t afford is an open question.</div><br/><div id="38389571" class="c"><input type="checkbox" id="c-38389571" checked=""/><div class="controls bullet"><span class="by">alchemist1e9</span><span>|</span><a href="#38388765">root</a><span>|</span><a href="#38389503">parent</a><span>|</span><a href="#38389611">next</a><span>|</span><label class="collapse" for="c-38389571">[-]</label><label class="expand" for="c-38389571">[2 more]</label></div><br/><div class="children"><div class="content">Is there a time period in the past you would have preferred to live instead and why?</div><br/><div id="38390359" class="c"><input type="checkbox" id="c-38390359" checked=""/><div class="controls bullet"><span class="by">somesortofthing</span><span>|</span><a href="#38388765">root</a><span>|</span><a href="#38389571">parent</a><span>|</span><a href="#38389611">next</a><span>|</span><label class="collapse" for="c-38390359">[-]</label><label class="expand" for="c-38390359">[1 more]</label></div><br/><div class="children"><div class="content">No, and I have no idea what I said that makes you think I do.</div><br/></div></div></div></div></div></div></div></div><div id="38389611" class="c"><input type="checkbox" id="c-38389611" checked=""/><div class="controls bullet"><span class="by">erupt7893</span><span>|</span><a href="#38388765">root</a><span>|</span><a href="#38388875">parent</a><span>|</span><a href="#38389424">prev</a><span>|</span><a href="#38388946">next</a><span>|</span><label class="collapse" for="c-38389611">[-]</label><label class="expand" for="c-38389611">[1 more]</label></div><br/><div class="children"><div class="content">I doubt the people who experienced the technological revolution of locomotives and factories imagined the holocaust either. Of course technology has and can be used for evil</div><br/></div></div><div id="38388946" class="c"><input type="checkbox" id="c-38388946" checked=""/><div class="controls bullet"><span class="by">gizajob</span><span>|</span><a href="#38388765">root</a><span>|</span><a href="#38388875">parent</a><span>|</span><a href="#38389611">prev</a><span>|</span><a href="#38390323">next</a><span>|</span><label class="collapse" for="c-38388946">[-]</label><label class="expand" for="c-38388946">[10 more]</label></div><br/><div class="children"><div class="content">Wasn&#x27;t this supposed to happen when PCs came out?</div><br/><div id="38389320" class="c"><input type="checkbox" id="c-38389320" checked=""/><div class="controls bullet"><span class="by">ssnistfajen</span><span>|</span><a href="#38388765">root</a><span>|</span><a href="#38388946">parent</a><span>|</span><a href="#38389200">next</a><span>|</span><label class="collapse" for="c-38389320">[-]</label><label class="expand" for="c-38389320">[3 more]</label></div><br/><div class="children"><div class="content">Occupations like computer (human form), typist, telephone switcher, all became completely eliminated when the PC came out. Jobs like travel agents are on permanent decline minus select scenarios where it is attached with luxury. Cashier went from a decent nonlaborious job to literal starvation gig because the importance of a human in the job became negligible. There are many more examples.<p>Some people managed to retrain and adapt, partially thanks to software becoming much more intuitive to use over the years. We don&#x27;t know how big the knowledge gap will be when the next big wave of automation comes. If retraining is not feasible for those at risk of losing their careers, there better be welfare abundance or society will be in great turmoil. High unemployment &amp; destitution is the single most fundamental factor of social upheavel throughout human history.</div><br/><div id="38389535" class="c"><input type="checkbox" id="c-38389535" checked=""/><div class="controls bullet"><span class="by">gizajob</span><span>|</span><a href="#38388765">root</a><span>|</span><a href="#38389320">parent</a><span>|</span><a href="#38389200">next</a><span>|</span><label class="collapse" for="c-38389535">[-]</label><label class="expand" for="c-38389535">[2 more]</label></div><br/><div class="children"><div class="content">Yeah but then capitalism breaks down because nobody is earning wages. One of the things capitalism is good at is providing (meaningless) employment to people because most wouldn’t know what to do with their days if given the free time back. This will only continue.</div><br/><div id="38389745" class="c"><input type="checkbox" id="c-38389745" checked=""/><div class="controls bullet"><span class="by">ssnistfajen</span><span>|</span><a href="#38388765">root</a><span>|</span><a href="#38389535">parent</a><span>|</span><a href="#38389200">next</a><span>|</span><label class="collapse" for="c-38389745">[-]</label><label class="expand" for="c-38389745">[1 more]</label></div><br/><div class="children"><div class="content">I do hope that will be the case. Certainly far better than the alternatives.</div><br/></div></div></div></div></div></div><div id="38389200" class="c"><input type="checkbox" id="c-38389200" checked=""/><div class="controls bullet"><span class="by">11011001</span><span>|</span><a href="#38388765">root</a><span>|</span><a href="#38388946">parent</a><span>|</span><a href="#38389320">prev</a><span>|</span><a href="#38389422">next</a><span>|</span><label class="collapse" for="c-38389200">[-]</label><label class="expand" for="c-38389200">[1 more]</label></div><br/><div class="children"><div class="content">&gt;&gt; Exactly. The rational fear is that they will automate many lower middle class jobs and cause unemployment, not that Terminator was a documentary.<p>&gt; Wasn&#x27;t this supposed to happen when PCs came out?<p>Did it not?<p>PCs may not have caused a catastrophic level of unemployment, but as they say &quot;past performance is not a guarantee of future results.&quot;  As automation gets more and more capable, it&#x27;s foolish to point to past iterations as &quot;proof&quot; that this (or some future) iteration of automation will also be fine.</div><br/></div></div><div id="38389422" class="c"><input type="checkbox" id="c-38389422" checked=""/><div class="controls bullet"><span class="by">jhbadger</span><span>|</span><a href="#38388765">root</a><span>|</span><a href="#38388946">parent</a><span>|</span><a href="#38389200">prev</a><span>|</span><a href="#38388994">next</a><span>|</span><label class="collapse" for="c-38389422">[-]</label><label class="expand" for="c-38389422">[1 more]</label></div><br/><div class="children"><div class="content">To some degree. Certainly the job of &quot;file clerk&quot; whose job was to retrieve folders of information from filing cabinets was made obsolete by relational databases. But the general fear that computers would replace workers wasn&#x27;t really justified because most white-collar (even low end white-collar) jobs required some interaction using language. That computers couldn&#x27;t really do. Until LLMs.</div><br/></div></div><div id="38388994" class="c"><input type="checkbox" id="c-38388994" checked=""/><div class="controls bullet"><span class="by">valine</span><span>|</span><a href="#38388765">root</a><span>|</span><a href="#38388946">parent</a><span>|</span><a href="#38389422">prev</a><span>|</span><a href="#38389140">next</a><span>|</span><label class="collapse" for="c-38388994">[-]</label><label class="expand" for="c-38388994">[1 more]</label></div><br/><div class="children"><div class="content">In the grand scheme of human history PCs didn’t come out all that long ago.</div><br/></div></div><div id="38389140" class="c"><input type="checkbox" id="c-38389140" checked=""/><div class="controls bullet"><span class="by">0x53-61-6C-74</span><span>|</span><a href="#38388765">root</a><span>|</span><a href="#38388946">parent</a><span>|</span><a href="#38388994">prev</a><span>|</span><a href="#38390323">next</a><span>|</span><label class="collapse" for="c-38389140">[-]</label><label class="expand" for="c-38389140">[3 more]</label></div><br/><div class="children"><div class="content">And the Loom</div><br/><div id="38389337" class="c"><input type="checkbox" id="c-38389337" checked=""/><div class="controls bullet"><span class="by">ssnistfajen</span><span>|</span><a href="#38388765">root</a><span>|</span><a href="#38389140">parent</a><span>|</span><a href="#38390323">next</a><span>|</span><label class="collapse" for="c-38389337">[-]</label><label class="expand" for="c-38389337">[2 more]</label></div><br/><div class="children"><div class="content">Things did become worse at first: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;The_Condition_of_the_Working_Class_in_England" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;The_Condition_of_the_Working_C...</a><p>Then new ideologies and social movements emerged with the explicit purpose of making things better, which caused changes to happen for the better.</div><br/><div id="38389553" class="c"><input type="checkbox" id="c-38389553" checked=""/><div class="controls bullet"><span class="by">gizajob</span><span>|</span><a href="#38388765">root</a><span>|</span><a href="#38389337">parent</a><span>|</span><a href="#38390323">next</a><span>|</span><label class="collapse" for="c-38389553">[-]</label><label class="expand" for="c-38389553">[1 more]</label></div><br/><div class="children"><div class="content">The working class in England (working in my factory. To fund Marx’s life.)</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38390323" class="c"><input type="checkbox" id="c-38390323" checked=""/><div class="controls bullet"><span class="by">Exoristos</span><span>|</span><a href="#38388765">parent</a><span>|</span><a href="#38388875">prev</a><span>|</span><a href="#38389162">next</a><span>|</span><label class="collapse" for="c-38390323">[-]</label><label class="expand" for="c-38390323">[1 more]</label></div><br/><div class="children"><div class="content">They just want to make sure the spam AI produces will amplify the party narratives.</div><br/></div></div><div id="38389162" class="c"><input type="checkbox" id="c-38389162" checked=""/><div class="controls bullet"><span class="by">gary_0</span><span>|</span><a href="#38388765">parent</a><span>|</span><a href="#38390323">prev</a><span>|</span><a href="#38389017">next</a><span>|</span><label class="collapse" for="c-38389162">[-]</label><label class="expand" for="c-38389162">[5 more]</label></div><br/><div class="children"><div class="content">I want to see reliable fully autonomous cars before I worry about the world ending due to super-AGI. Also, have we figured out how to get art generators to always get the number of fingers right, and text generators to stop making shit up? Let&#x27;s not get ahead of ourselves.</div><br/><div id="38389564" class="c"><input type="checkbox" id="c-38389564" checked=""/><div class="controls bullet"><span class="by">denlekke</span><span>|</span><a href="#38388765">root</a><span>|</span><a href="#38389162">parent</a><span>|</span><a href="#38389170">next</a><span>|</span><label class="collapse" for="c-38389564">[-]</label><label class="expand" for="c-38389564">[1 more]</label></div><br/><div class="children"><div class="content">from one perspective we already have fully autonomous cars, it&#x27;s just the making them safe for humans and fitting them into a strict legal framework for their behavior that needs finishing before they&#x27;re released to the general public (comma.ai being a publicly available exception)</div><br/></div></div><div id="38389170" class="c"><input type="checkbox" id="c-38389170" checked=""/><div class="controls bullet"><span class="by">Davidzheng</span><span>|</span><a href="#38388765">root</a><span>|</span><a href="#38389162">parent</a><span>|</span><a href="#38389564">prev</a><span>|</span><a href="#38389017">next</a><span>|</span><label class="collapse" for="c-38389170">[-]</label><label class="expand" for="c-38389170">[3 more]</label></div><br/><div class="children"><div class="content">Two out of three of your problems are solved already</div><br/><div id="38389302" class="c"><input type="checkbox" id="c-38389302" checked=""/><div class="controls bullet"><span class="by">callalex</span><span>|</span><a href="#38388765">root</a><span>|</span><a href="#38389170">parent</a><span>|</span><a href="#38389017">next</a><span>|</span><label class="collapse" for="c-38389302">[-]</label><label class="expand" for="c-38389302">[2 more]</label></div><br/><div class="children"><div class="content">Cars aren’t fully autonomous, and LLMs still lie all the time, so I don’t understand your math.</div><br/><div id="38389462" class="c"><input type="checkbox" id="c-38389462" checked=""/><div class="controls bullet"><span class="by">Davidzheng</span><span>|</span><a href="#38388765">root</a><span>|</span><a href="#38389302">parent</a><span>|</span><a href="#38389017">next</a><span>|</span><label class="collapse" for="c-38389462">[-]</label><label class="expand" for="c-38389462">[1 more]</label></div><br/><div class="children"><div class="content">Ok so you accept that latest gen art generators can do fingers. I&#x27;d argue from the latest waymo paper they are reliable enough to be no worse than humans.</div><br/></div></div></div></div></div></div></div></div><div id="38389017" class="c"><input type="checkbox" id="c-38389017" checked=""/><div class="controls bullet"><span class="by">aaomidi</span><span>|</span><a href="#38388765">parent</a><span>|</span><a href="#38389162">prev</a><span>|</span><a href="#38386687">next</a><span>|</span><label class="collapse" for="c-38389017">[-]</label><label class="expand" for="c-38389017">[1 more]</label></div><br/><div class="children"><div class="content">What these models have allowed is very cheap emotional manipulation.<p>That itself is extremely dangerous.</div><br/></div></div></div></div><div id="38386687" class="c"><input type="checkbox" id="c-38386687" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#38388765">prev</a><span>|</span><a href="#38390042">next</a><span>|</span><label class="collapse" for="c-38386687">[-]</label><label class="expand" for="c-38386687">[15 more]</label></div><br/><div class="children"><div class="content">Could it be named in reference to something like the A* search algorithm? What if Q stands for Query in transformer attention? How would that type of search translate to transformers?<p>They said that it&#x27;s able to solve simple math problems. If it&#x27;s related to A* then maybe it&#x27;s trying to find a path to something. The answer to a word problem?</div><br/><div id="38386821" class="c"><input type="checkbox" id="c-38386821" checked=""/><div class="controls bullet"><span class="by">vanshg</span><span>|</span><a href="#38386687">parent</a><span>|</span><a href="#38386694">next</a><span>|</span><label class="collapse" for="c-38386821">[-]</label><label class="expand" for="c-38386821">[3 more]</label></div><br/><div class="children"><div class="content">In reinforcement learning, Q* represents the optimal action-value function</div><br/><div id="38388882" class="c"><input type="checkbox" id="c-38388882" checked=""/><div class="controls bullet"><span class="by">andy_xor_andrew</span><span>|</span><a href="#38386687">root</a><span>|</span><a href="#38386821">parent</a><span>|</span><a href="#38386694">next</a><span>|</span><label class="collapse" for="c-38388882">[-]</label><label class="expand" for="c-38388882">[2 more]</label></div><br/><div class="children"><div class="content">which makes sense. you can pretty easily imagine the problem of &quot;selecting the next token&quot; as a tree of states, with actions transitioning from one to another, just like a game. And you already have naive scores for each of the states (the logits for the tokens).<p>It&#x27;s not hard to imagine applying well-known tree searching strategies, like monte-carlo tree search, minimax, etc. Or, in the case of Q*, maybe creating another (smaller) action&#x2F;value model that guides the progress of the LLM.</div><br/><div id="38389881" class="c"><input type="checkbox" id="c-38389881" checked=""/><div class="controls bullet"><span class="by">sudosysgen</span><span>|</span><a href="#38386687">root</a><span>|</span><a href="#38388882">parent</a><span>|</span><a href="#38386694">next</a><span>|</span><label class="collapse" for="c-38389881">[-]</label><label class="expand" for="c-38389881">[1 more]</label></div><br/><div class="children"><div class="content">Absolutely, maximizing conditional probabilities is easily modeled as a Markov decision process, which is why you can use RL to train Transformers so well (hence RLHF, I&#x27;ve also been experimenting with RL based training for Transformers for other applications - it&#x27;s promising!). Using a transformer as a model for RL to try to choose tokens to maximize overall likelihood given immediate conditional likelihood estimation is something that I imagine many people experimented with, but I can see it being tricky enough for OpenAI to be the only ones to pull it off.</div><br/></div></div></div></div></div></div><div id="38386694" class="c"><input type="checkbox" id="c-38386694" checked=""/><div class="controls bullet"><span class="by">bitshiftfaced</span><span>|</span><a href="#38386687">parent</a><span>|</span><a href="#38386821">prev</a><span>|</span><a href="#38387666">next</a><span>|</span><label class="collapse" for="c-38386694">[-]</label><label class="expand" for="c-38386694">[9 more]</label></div><br/><div class="children"><div class="content">My mind went to Q learning.</div><br/><div id="38386719" class="c"><input type="checkbox" id="c-38386719" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#38386687">root</a><span>|</span><a href="#38386694">parent</a><span>|</span><a href="#38386733">next</a><span>|</span><label class="collapse" for="c-38386719">[-]</label><label class="expand" for="c-38386719">[2 more]</label></div><br/><div class="children"><div class="content">Ok so maybe nothing to do with A*, but actually a way for GPT-powered models or agents to learn through automated reinforcement learning. Or something.<p>I wonder if DeepMind is working on something similar also.<p>If your hunch is right, this could lead to the type of self-improvement that scares people.</div><br/><div id="38389610" class="c"><input type="checkbox" id="c-38389610" checked=""/><div class="controls bullet"><span class="by">Davidzheng</span><span>|</span><a href="#38386687">root</a><span>|</span><a href="#38386719">parent</a><span>|</span><a href="#38386733">next</a><span>|</span><label class="collapse" for="c-38389610">[-]</label><label class="expand" for="c-38389610">[1 more]</label></div><br/><div class="children"><div class="content">Could easily be both</div><br/></div></div></div></div><div id="38386733" class="c"><input type="checkbox" id="c-38386733" checked=""/><div class="controls bullet"><span class="by">flibble</span><span>|</span><a href="#38386687">root</a><span>|</span><a href="#38386694">parent</a><span>|</span><a href="#38386719">prev</a><span>|</span><a href="#38387210">next</a><span>|</span><label class="collapse" for="c-38386733">[-]</label><label class="expand" for="c-38386733">[2 more]</label></div><br/><div class="children"><div class="content">Mine to Q of Star Trek.</div><br/><div id="38389889" class="c"><input type="checkbox" id="c-38389889" checked=""/><div class="controls bullet"><span class="by">Izkata</span><span>|</span><a href="#38386687">root</a><span>|</span><a href="#38386733">parent</a><span>|</span><a href="#38387210">next</a><span>|</span><label class="collapse" for="c-38389889">[-]</label><label class="expand" for="c-38389889">[1 more]</label></div><br/><div class="children"><div class="content">But were you thinking of Q, Q, or Q?</div><br/></div></div></div></div><div id="38387210" class="c"><input type="checkbox" id="c-38387210" checked=""/><div class="controls bullet"><span class="by">eli_gottlieb</span><span>|</span><a href="#38386687">root</a><span>|</span><a href="#38386694">parent</a><span>|</span><a href="#38386733">prev</a><span>|</span><a href="#38386820">next</a><span>|</span><label class="collapse" for="c-38387210">[-]</label><label class="expand" for="c-38387210">[3 more]</label></div><br/><div class="children"><div class="content">My mind went to some kind of Q-learning combined with something like a Monte Carlo Tree Search with some kind of A*-style heuristic to effectively combine Q-learning and with short-horizon planning.</div><br/><div id="38389965" class="c"><input type="checkbox" id="c-38389965" checked=""/><div class="controls bullet"><span class="by">a-dub</span><span>|</span><a href="#38386687">root</a><span>|</span><a href="#38387210">parent</a><span>|</span><a href="#38387983">next</a><span>|</span><label class="collapse" for="c-38389965">[-]</label><label class="expand" for="c-38389965">[1 more]</label></div><br/><div class="children"><div class="content">likewise.  i can already imagine a* being useful for efficiently solving basic algebra and proofs.<p>it could form the basis of a generalized planning engine and that planning engine could potentially be dangerous given the inherent competitive reasoning behind any minmax style approach.</div><br/></div></div><div id="38387983" class="c"><input type="checkbox" id="c-38387983" checked=""/><div class="controls bullet"><span class="by">theGnuMe</span><span>|</span><a href="#38386687">root</a><span>|</span><a href="#38387210">parent</a><span>|</span><a href="#38389965">prev</a><span>|</span><a href="#38386820">next</a><span>|</span><label class="collapse" for="c-38387983">[-]</label><label class="expand" for="c-38387983">[1 more]</label></div><br/><div class="children"><div class="content">This was alpha-go and alpha-zero right?</div><br/></div></div></div></div></div></div><div id="38387666" class="c"><input type="checkbox" id="c-38387666" checked=""/><div class="controls bullet"><span class="by">bhy</span><span>|</span><a href="#38386687">parent</a><span>|</span><a href="#38386694">prev</a><span>|</span><a href="#38386881">next</a><span>|</span><label class="collapse" for="c-38387666">[-]</label><label class="expand" for="c-38387666">[1 more]</label></div><br/><div class="children"><div class="content">My bet is a general search algorithm similar AlphaGo that uses LLM as world model, and heuristic search to find the right path to goal. There&#x27;s already evidence in academics that this can significantly boost model performance [1].<p>[1] <a href="https:&#x2F;&#x2F;llm-mcts.github.io&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;llm-mcts.github.io&#x2F;</a></div><br/></div></div><div id="38386881" class="c"><input type="checkbox" id="c-38386881" checked=""/><div class="controls bullet"><span class="by">kelseyfrog</span><span>|</span><a href="#38386687">parent</a><span>|</span><a href="#38387666">prev</a><span>|</span><a href="#38390042">next</a><span>|</span><label class="collapse" for="c-38386881">[-]</label><label class="expand" for="c-38386881">[1 more]</label></div><br/><div class="children"><div class="content">*(spelled STAR) can also refer to Semantic-parsing Transformer and ASP Reasoner, which while a bit of a stretch, is a concept that shows up in literature.</div><br/></div></div></div></div><div id="38390042" class="c"><input type="checkbox" id="c-38390042" checked=""/><div class="controls bullet"><span class="by">Animats</span><span>|</span><a href="#38386687">prev</a><span>|</span><a href="#38387141">next</a><span>|</span><label class="collapse" for="c-38390042">[-]</label><label class="expand" for="c-38390042">[1 more]</label></div><br/><div class="children"><div class="content">Insufficient information. Did OpenAI have a breakthrough, or not?</div><br/></div></div><div id="38387141" class="c"><input type="checkbox" id="c-38387141" checked=""/><div class="controls bullet"><span class="by">sofaygo</span><span>|</span><a href="#38390042">prev</a><span>|</span><a href="#38388684">next</a><span>|</span><label class="collapse" for="c-38387141">[-]</label><label class="expand" for="c-38387141">[2 more]</label></div><br/><div class="children"><div class="content">If I’ve taken anything away from the last 5 days, it’s that the future is significantly more volatile than I originally had thought.</div><br/><div id="38387863" class="c"><input type="checkbox" id="c-38387863" checked=""/><div class="controls bullet"><span class="by">codethief</span><span>|</span><a href="#38387141">parent</a><span>|</span><a href="#38388684">next</a><span>|</span><label class="collapse" for="c-38387863">[-]</label><label class="expand" for="c-38387863">[1 more]</label></div><br/><div class="children"><div class="content">You mean, you hadn&#x27;t gotten that impression from the last two months or the last two years yet?</div><br/></div></div></div></div><div id="38388684" class="c"><input type="checkbox" id="c-38388684" checked=""/><div class="controls bullet"><span class="by">brutusborn</span><span>|</span><a href="#38387141">prev</a><span>|</span><a href="#38387382">next</a><span>|</span><label class="collapse" for="c-38388684">[-]</label><label class="expand" for="c-38388684">[3 more]</label></div><br/><div class="children"><div class="content">Baseless speculation: they started testing an approach similar to VERSES AI and realized it has much more potential than LLMs.<p>The free energy principle, knowledge graph approach just seems more likely to develop general intelligence without hallucinations imo.</div><br/><div id="38388825" class="c"><input type="checkbox" id="c-38388825" checked=""/><div class="controls bullet"><span class="by">tnecniv</span><span>|</span><a href="#38388684">parent</a><span>|</span><a href="#38387382">next</a><span>|</span><label class="collapse" for="c-38388825">[-]</label><label class="expand" for="c-38388825">[2 more]</label></div><br/><div class="children"><div class="content">Eh the free energy principle isn’t that magical and that comes from someone who likes it quite a bit. At the end of the day extremizing the free energy is a relaxation of constrained optimization. There are ways it’s used I find very appealing but not so appealing I’d expect it to become the dominant paradigm. Moreover, you can write a lot of standard ML problems free energy optimization in one way or another, but I don’t think standard ML will quite get us there. We need at least one more conceptual step forward.</div><br/><div id="38389231" class="c"><input type="checkbox" id="c-38389231" checked=""/><div class="controls bullet"><span class="by">brutusborn</span><span>|</span><a href="#38388684">root</a><span>|</span><a href="#38388825">parent</a><span>|</span><a href="#38387382">next</a><span>|</span><label class="collapse" for="c-38389231">[-]</label><label class="expand" for="c-38389231">[1 more]</label></div><br/><div class="children"><div class="content">The explicit world modelling is probably the thing that makes me more hopeful about their approach. I think integrating ML and their approach might end up being the new paradigm.</div><br/></div></div></div></div></div></div><div id="38387382" class="c"><input type="checkbox" id="c-38387382" checked=""/><div class="controls bullet"><span class="by">mfiguiere</span><span>|</span><a href="#38388684">prev</a><span>|</span><a href="#38386619">next</a><span>|</span><label class="collapse" for="c-38387382">[-]</label><label class="expand" for="c-38387382">[3 more]</label></div><br/><div class="children"><div class="content">TheInformation has just published an article about this topic:<p>OpenAI Made an AI Breakthrough Before Altman Firing, Stoking Excitement and Concern<p><a href="https:&#x2F;&#x2F;www.theinformation.com&#x2F;articles&#x2F;openai-made-an-ai-breakthrough-before-altman-firing-stoking-excitement-and-concern" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.theinformation.com&#x2F;articles&#x2F;openai-made-an-ai-br...</a></div><br/><div id="38388210" class="c"><input type="checkbox" id="c-38388210" checked=""/><div class="controls bullet"><span class="by">blazespin</span><span>|</span><a href="#38387382">parent</a><span>|</span><a href="#38386619">next</a><span>|</span><label class="collapse" for="c-38388210">[-]</label><label class="expand" for="c-38388210">[2 more]</label></div><br/><div class="children"><div class="content">No sub. Did they just re-tweet reuters or did they separately confirm?</div><br/><div id="38388706" class="c"><input type="checkbox" id="c-38388706" checked=""/><div class="controls bullet"><span class="by">lucubratory</span><span>|</span><a href="#38387382">root</a><span>|</span><a href="#38388210">parent</a><span>|</span><a href="#38386619">next</a><span>|</span><label class="collapse" for="c-38388706">[-]</label><label class="expand" for="c-38388706">[1 more]</label></div><br/><div class="children"><div class="content">They separately confirmed it, although in terms of timing they were scooped by Reuters which generally means you publish what you have, when you have it.</div><br/></div></div></div></div></div></div><div id="38386619" class="c"><input type="checkbox" id="c-38386619" checked=""/><div class="controls bullet"><span class="by">righthand</span><span>|</span><a href="#38387382">prev</a><span>|</span><a href="#38388657">next</a><span>|</span><label class="collapse" for="c-38386619">[-]</label><label class="expand" for="c-38386619">[3 more]</label></div><br/><div class="children"><div class="content">Turns out a quarter of a million dollar paycheck plus whatever they get for letting Microsoft gobble them up was more pressing for the staffers.<p>Unless these staff were the unsigned 5%.</div><br/><div id="38386690" class="c"><input type="checkbox" id="c-38386690" checked=""/><div class="controls bullet"><span class="by">iaseiadit</span><span>|</span><a href="#38386619">parent</a><span>|</span><a href="#38388657">next</a><span>|</span><label class="collapse" for="c-38386690">[-]</label><label class="expand" for="c-38386690">[2 more]</label></div><br/><div class="children"><div class="content">The employees had a lot more money than that on the line.</div><br/><div id="38387043" class="c"><input type="checkbox" id="c-38387043" checked=""/><div class="controls bullet"><span class="by">righthand</span><span>|</span><a href="#38386619">root</a><span>|</span><a href="#38386690">parent</a><span>|</span><a href="#38388657">next</a><span>|</span><label class="collapse" for="c-38387043">[-]</label><label class="expand" for="c-38387043">[1 more]</label></div><br/><div class="children"><div class="content">&gt; plus whatever they get for letting Microsoft gobble them up<p>The average salary there is $250k. More than that would be the vestment greater than their base, no?</div><br/></div></div></div></div></div></div><div id="38388657" class="c"><input type="checkbox" id="c-38388657" checked=""/><div class="controls bullet"><span class="by">lucubratory</span><span>|</span><a href="#38386619">prev</a><span>|</span><a href="#38387134">next</a><span>|</span><label class="collapse" for="c-38388657">[-]</label><label class="expand" for="c-38388657">[4 more]</label></div><br/><div class="children"><div class="content">Genuinely, we deserve more information about this. Someone needs to whistleblow.</div><br/><div id="38388700" class="c"><input type="checkbox" id="c-38388700" checked=""/><div class="controls bullet"><span class="by">thisisonthetest</span><span>|</span><a href="#38388657">parent</a><span>|</span><a href="#38388739">next</a><span>|</span><label class="collapse" for="c-38388700">[-]</label><label class="expand" for="c-38388700">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Reuters was unable to review a copy of the letter. The researchers who wrote the letter did not immediately respond to requests for comment. OpenAI declined to comment.<p>Lol for real, like what are we even doing here?</div><br/><div id="38388743" class="c"><input type="checkbox" id="c-38388743" checked=""/><div class="controls bullet"><span class="by">lucubratory</span><span>|</span><a href="#38388657">root</a><span>|</span><a href="#38388700">parent</a><span>|</span><a href="#38388739">next</a><span>|</span><label class="collapse" for="c-38388743">[-]</label><label class="expand" for="c-38388743">[1 more]</label></div><br/><div class="children"><div class="content">Multiple people involved in this story think that this could be something that is relevant to everyone on the planet, any hope of it being actually suppressed and no one knowing about it is gone, just leak it so we all know what we&#x27;re dealing with.</div><br/></div></div></div></div><div id="38388739" class="c"><input type="checkbox" id="c-38388739" checked=""/><div class="controls bullet"><span class="by">dukeofdoom</span><span>|</span><a href="#38388657">parent</a><span>|</span><a href="#38388700">prev</a><span>|</span><a href="#38387134">next</a><span>|</span><label class="collapse" for="c-38388739">[-]</label><label class="expand" for="c-38388739">[1 more]</label></div><br/><div class="children"><div class="content">How long before the FBI runs this Q operation, like the last one.</div><br/></div></div></div></div><div id="38387134" class="c"><input type="checkbox" id="c-38387134" checked=""/><div class="controls bullet"><span class="by">wahlen</span><span>|</span><a href="#38388657">prev</a><span>|</span><a href="#38386722">next</a><span>|</span><label class="collapse" for="c-38387134">[-]</label><label class="expand" for="c-38387134">[1 more]</label></div><br/><div class="children"><div class="content">Q* sounds like the amalgamation of Q learning and A* search. I wonder if it has anything to do with domain-independent reward functions.</div><br/></div></div><div id="38386722" class="c"><input type="checkbox" id="c-38386722" checked=""/><div class="controls bullet"><span class="by">willsmith72</span><span>|</span><a href="#38387134">prev</a><span>|</span><a href="#38387517">next</a><span>|</span><label class="collapse" for="c-38386722">[-]</label><label class="expand" for="c-38386722">[8 more]</label></div><br/><div class="children"><div class="content">I still don&#x27;t really know how they&#x27;re measuring AGI. They all seem very sure about it, does someone have a good definition?<p>OpenAI just say &quot;AI systems that are generally smarter than humans&quot;. That&#x27;s not measurable.<p>What will it look like when AGI is achieved?</div><br/><div id="38387835" class="c"><input type="checkbox" id="c-38387835" checked=""/><div class="controls bullet"><span class="by">sigmar</span><span>|</span><a href="#38386722">parent</a><span>|</span><a href="#38386831">next</a><span>|</span><label class="collapse" for="c-38387835">[-]</label><label class="expand" for="c-38387835">[1 more]</label></div><br/><div class="children"><div class="content">OpenAI has a short (though difficult to evaluate) definition in their charter: &quot;OpenAI’s mission is to ensure that artificial general intelligence (AGI)—by which we mean highly autonomous systems that outperform humans at most economically valuable work—benefits all of humanity.&quot; <a href="https:&#x2F;&#x2F;openai.com&#x2F;charter" rel="nofollow noreferrer">https:&#x2F;&#x2F;openai.com&#x2F;charter</a></div><br/></div></div><div id="38386831" class="c"><input type="checkbox" id="c-38386831" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#38386722">parent</a><span>|</span><a href="#38387835">prev</a><span>|</span><a href="#38387386">next</a><span>|</span><label class="collapse" for="c-38386831">[-]</label><label class="expand" for="c-38386831">[1 more]</label></div><br/><div class="children"><div class="content">When people start running out of tests to benchmark it against humans. But that won&#x27;t be as scary as when the agent is given (by humans) the ability to execute tasks outside of a sandbox. A lot of dumb animals are scary, after all.</div><br/></div></div><div id="38387386" class="c"><input type="checkbox" id="c-38387386" checked=""/><div class="controls bullet"><span class="by">ric2b</span><span>|</span><a href="#38386722">parent</a><span>|</span><a href="#38386831">prev</a><span>|</span><a href="#38389556">next</a><span>|</span><label class="collapse" for="c-38387386">[-]</label><label class="expand" for="c-38387386">[1 more]</label></div><br/><div class="children"><div class="content">Something that can consider a variety of goals and take practical steps to achieve them.<p>The goals have to be similarly variable to humans, not just 3 or 4 types of goals. If it only supports a select few types of goals, it is not very general.<p>The steps it takes have to be similarly practical to those taken by humans, if the steps are just a random walk it is not very intelligent.</div><br/></div></div><div id="38389556" class="c"><input type="checkbox" id="c-38389556" checked=""/><div class="controls bullet"><span class="by">anvil-on-my-toe</span><span>|</span><a href="#38386722">parent</a><span>|</span><a href="#38387386">prev</a><span>|</span><a href="#38386794">next</a><span>|</span><label class="collapse" for="c-38389556">[-]</label><label class="expand" for="c-38389556">[2 more]</label></div><br/><div class="children"><div class="content">Recursive self-improvement.</div><br/><div id="38389635" class="c"><input type="checkbox" id="c-38389635" checked=""/><div class="controls bullet"><span class="by">Davidzheng</span><span>|</span><a href="#38386722">root</a><span>|</span><a href="#38389556">parent</a><span>|</span><a href="#38386794">next</a><span>|</span><label class="collapse" for="c-38389635">[-]</label><label class="expand" for="c-38389635">[1 more]</label></div><br/><div class="children"><div class="content">Strange definition. Humans can&#x27;t recursively self improve, at least not in the way I&#x27;m assuming you to mean. That&#x27;s more like definition of the singularity</div><br/></div></div></div></div><div id="38386794" class="c"><input type="checkbox" id="c-38386794" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#38386722">parent</a><span>|</span><a href="#38389556">prev</a><span>|</span><a href="#38386838">next</a><span>|</span><label class="collapse" for="c-38386794">[-]</label><label class="expand" for="c-38386794">[1 more]</label></div><br/><div class="children"><div class="content">It will be well beyond general purpose and well beyond human level and there will be an inflection point where most people finally accept it and then everyone panics and completely overreacts but also possibly too late.</div><br/></div></div><div id="38386838" class="c"><input type="checkbox" id="c-38386838" checked=""/><div class="controls bullet"><span class="by">tsunamifury</span><span>|</span><a href="#38386722">parent</a><span>|</span><a href="#38386794">prev</a><span>|</span><a href="#38387517">next</a><span>|</span><label class="collapse" for="c-38386838">[-]</label><label class="expand" for="c-38386838">[1 more]</label></div><br/><div class="children"><div class="content">Everyone seems to make up their own bizarre and personal definition what involves
Godlike abilities then claim everyone obviously agrees with it.<p>For me I think gpt4 is clearly generalized intelligence.</div><br/></div></div></div></div><div id="38387517" class="c"><input type="checkbox" id="c-38387517" checked=""/><div class="controls bullet"><span class="by">next_xibalba</span><span>|</span><a href="#38386722">prev</a><span>|</span><label class="collapse" for="c-38387517">[-]</label><label class="expand" for="c-38387517">[1 more]</label></div><br/><div class="children"><div class="content">Blake Lemoine redux. I guarantee whatever this development is, it falls far short of what any reasonable person considers &quot;dangerous&quot;.</div><br/></div></div></div></div></div></div></div></body></html>