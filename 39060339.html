<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1705741252715" as="style"/><link rel="stylesheet" href="styles.css?v=1705741252715"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://ceph.io/en/news/blog/2024/ceph-a-journey-to-1tibps/">Ceph: A Journey to 1 TiB/s</a> <span class="domain">(<a href="https://ceph.io">ceph.io</a>)</span></div><div class="subtext"><span>davidmr</span> | <span>132 comments</span></div><br/><div><div id="39064883" class="c"><input type="checkbox" id="c-39064883" checked=""/><div class="controls bullet"><span class="by">amadio</span><span>|</span><a href="#39062490">next</a><span>|</span><label class="collapse" for="c-39064883">[-]</label><label class="expand" for="c-39064883">[2 more]</label></div><br/><div class="children"><div class="content">Nice article! We&#x27;ve also recently reached the mark of 1TB&#x2F;s at CERN, but with EOS (<a href="https:&#x2F;&#x2F;cern.ch&#x2F;eos" rel="nofollow">https:&#x2F;&#x2F;cern.ch&#x2F;eos</a>), not ceph:
<a href="https:&#x2F;&#x2F;www.home.cern&#x2F;news&#x2F;news&#x2F;computing&#x2F;exabyte-disk-storage-cern" rel="nofollow">https:&#x2F;&#x2F;www.home.cern&#x2F;news&#x2F;news&#x2F;computing&#x2F;exabyte-disk-stora...</a><p>Our EOS clusters have a lot more nodes, however, and use mostly HDDs. CERN also uses ceph extensively.</div><br/><div id="39065481" class="c"><input type="checkbox" id="c-39065481" checked=""/><div class="controls bullet"><span class="by">theyinwhy</span><span>|</span><a href="#39064883">parent</a><span>|</span><a href="#39062490">next</a><span>|</span><label class="collapse" for="c-39065481">[-]</label><label class="expand" for="c-39065481">[1 more]</label></div><br/><div class="children"><div class="content">Great! What&#x27;s your take on ceph? Is the idea to migrate to EOS long term?</div><br/></div></div></div></div><div id="39062490" class="c"><input type="checkbox" id="c-39062490" checked=""/><div class="controls bullet"><span class="by">alberth</span><span>|</span><a href="#39064883">prev</a><span>|</span><a href="#39061325">next</a><span>|</span><label class="collapse" for="c-39062490">[-]</label><label class="expand" for="c-39062490">[7 more]</label></div><br/><div class="children"><div class="content">Ceph has an interesting history.<p>It was created at Dreamhost (DH), for their internal needs by the founders.<p>DH was doing effectively IaaS &amp; PaaS before those were industry coined words (VPS, managed OS&#x2F;database&#x2F;app-servers).<p>They spun Ceph off and Redhat bought it.<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;DreamHost" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;DreamHost</a></div><br/><div id="39063398" class="c"><input type="checkbox" id="c-39063398" checked=""/><div class="controls bullet"><span class="by">artyom</span><span>|</span><a href="#39062490">parent</a><span>|</span><a href="#39062622">next</a><span>|</span><label class="collapse" for="c-39063398">[-]</label><label class="expand" for="c-39063398">[2 more]</label></div><br/><div class="children"><div class="content">Yeah, as a customer (still one) I remember their &quot;Hey, we&#x27;re going to build this Ceph thing, maybe it ends up being cool&quot; blog entry (or newsletter?) kinda just sharing what they were toying with. It was a time of no marketing copy and not crafting every sentence to sell you things.<p>I think it was the university project of one of the founders, and the others jumped in supporting it. Docker has a similar origins story as far as I know.</div><br/><div id="39064341" class="c"><input type="checkbox" id="c-39064341" checked=""/><div class="controls bullet"><span class="by">pas</span><span>|</span><a href="#39062490">root</a><span>|</span><a href="#39063398">parent</a><span>|</span><a href="#39062622">next</a><span>|</span><label class="collapse" for="c-39064341">[-]</label><label class="expand" for="c-39064341">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Sage_Weil" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Sage_Weil</a> right?<p><a href="https:&#x2F;&#x2F;ceph.com&#x2F;assets&#x2F;pdfs&#x2F;weil-crush-sc06.pdf" rel="nofollow">https:&#x2F;&#x2F;ceph.com&#x2F;assets&#x2F;pdfs&#x2F;weil-crush-sc06.pdf</a></div><br/></div></div></div></div><div id="39062622" class="c"><input type="checkbox" id="c-39062622" checked=""/><div class="controls bullet"><span class="by">epistasis</span><span>|</span><a href="#39062490">parent</a><span>|</span><a href="#39063398">prev</a><span>|</span><a href="#39061325">next</a><span>|</span><label class="collapse" for="c-39062622">[-]</label><label class="expand" for="c-39062622">[4 more]</label></div><br/><div class="children"><div class="content">A bit more to the story is that it was created also at UC Santa Cruz, by Sage Weil, a Dreamhost founder, while he was doing graduate work there. UCSC has had a lot of good storage research.</div><br/><div id="39063493" class="c"><input type="checkbox" id="c-39063493" checked=""/><div class="controls bullet"><span class="by">AdamJacobMuller</span><span>|</span><a href="#39062490">root</a><span>|</span><a href="#39062622">parent</a><span>|</span><a href="#39065289">next</a><span>|</span><label class="collapse" for="c-39063493">[-]</label><label class="expand" for="c-39063493">[1 more]</label></div><br/><div class="children"><div class="content">I remember the first time I deployed ceph, would have been around 2010 or 2011, had some really major issues which would nearly resulted in data loss and due to someone else not realizing what &quot;this cluster is experimental, do not store any important data here&quot; meant, the data on ceph was the only copy of the irreplaceable data in the world, loosing the data would have been fairly catastrophic for us.<p>I ended up on the ceph IRC channel and eventually had Sage helping me fix the issues directly, helping me find bugs and writing patches to fix them in realtime.<p>Super amazingly nice guy that he was willing to help, never once chastised me for being so stupid (even though I was), also wicked smart.</div><br/></div></div><div id="39065289" class="c"><input type="checkbox" id="c-39065289" checked=""/><div class="controls bullet"><span class="by">antongribok</span><span>|</span><a href="#39062490">root</a><span>|</span><a href="#39062622">parent</a><span>|</span><a href="#39063493">prev</a><span>|</span><a href="#39063030">next</a><span>|</span><label class="collapse" for="c-39065289">[-]</label><label class="expand" for="c-39065289">[1 more]</label></div><br/><div class="children"><div class="content">Sage is one of the nicest, down to earth, super smart individuals I&#x27;ve met.<p>I&#x27;ve talked to him at a few OpenStack and Ceph conferences, and he&#x27;s always very patient answering questions.</div><br/></div></div><div id="39063030" class="c"><input type="checkbox" id="c-39063030" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#39062490">root</a><span>|</span><a href="#39062622">parent</a><span>|</span><a href="#39065289">prev</a><span>|</span><a href="#39061325">next</a><span>|</span><label class="collapse" for="c-39063030">[-]</label><label class="expand" for="c-39063030">[1 more]</label></div><br/><div class="children"><div class="content">the fighting banana slugs</div><br/></div></div></div></div></div></div><div id="39061325" class="c"><input type="checkbox" id="c-39061325" checked=""/><div class="controls bullet"><span class="by">stuff4ben</span><span>|</span><a href="#39062490">prev</a><span>|</span><a href="#39061551">next</a><span>|</span><label class="collapse" for="c-39061325">[-]</label><label class="expand" for="c-39061325">[5 more]</label></div><br/><div class="children"><div class="content">I used to love doing experiments like this. I was afforded that luxury as a tech lead back when I was at Cisco setting up Kubernetes on bare metal and getting to play with setting up GlusterFS and Ceph just to learn and see which was better. This was back in 2017&#x2F;2018 if I recall. Good ole days. Loved this writeup!</div><br/><div id="39062123" class="c"><input type="checkbox" id="c-39062123" checked=""/><div class="controls bullet"><span class="by">knicholes</span><span>|</span><a href="#39061325">parent</a><span>|</span><a href="#39065393">next</a><span>|</span><label class="collapse" for="c-39062123">[-]</label><label class="expand" for="c-39062123">[3 more]</label></div><br/><div class="children"><div class="content">I had to run a bunch of benchmarks to compare speeds of not just AWS instance types, but actual individual instances in each type, as some NVME SSDs have been more used than others in order to lube up some Aerospike response times.  Crazy.</div><br/><div id="39063483" class="c"><input type="checkbox" id="c-39063483" checked=""/><div class="controls bullet"><span class="by">j33zusjuice</span><span>|</span><a href="#39061325">root</a><span>|</span><a href="#39062123">parent</a><span>|</span><a href="#39065393">next</a><span>|</span><label class="collapse" for="c-39063483">[-]</label><label class="expand" for="c-39063483">[2 more]</label></div><br/><div class="children"><div class="content">Ad-tech, or?</div><br/><div id="39064904" class="c"><input type="checkbox" id="c-39064904" checked=""/><div class="controls bullet"><span class="by">knicholes</span><span>|</span><a href="#39061325">root</a><span>|</span><a href="#39063483">parent</a><span>|</span><a href="#39065393">next</a><span>|</span><label class="collapse" for="c-39064904">[-]</label><label class="expand" for="c-39064904">[1 more]</label></div><br/><div class="children"><div class="content">Yeah. Serving profiles for customized ad selection.</div><br/></div></div></div></div></div></div><div id="39065393" class="c"><input type="checkbox" id="c-39065393" checked=""/><div class="controls bullet"><span class="by">redrove</span><span>|</span><a href="#39061325">parent</a><span>|</span><a href="#39062123">prev</a><span>|</span><a href="#39061551">next</a><span>|</span><label class="collapse" for="c-39065393">[-]</label><label class="expand" for="c-39065393">[1 more]</label></div><br/><div class="children"><div class="content">A Heketi man! I had the same experience around the same years, what a blast. Everything was so new..and broken!</div><br/></div></div></div></div><div id="39061551" class="c"><input type="checkbox" id="c-39061551" checked=""/><div class="controls bullet"><span class="by">amluto</span><span>|</span><a href="#39061325">prev</a><span>|</span><a href="#39063690">next</a><span>|</span><label class="collapse" for="c-39061551">[-]</label><label class="expand" for="c-39061551">[14 more]</label></div><br/><div class="children"><div class="content">I wish someone would try to scale the nodes down. The system described here is ~300W&#x2F;node for 10 disks&#x2F;node, so 30W or so per disk. That’s a fair amount of overhead, and it also requires quite a lot of storage to get any redundancy at all.<p>I bet some engineering effort could divide the whole thing by 10.  Build a tiny SBC with 4 PCIe lanes for NVMe, 2x10GbE (as two SFP+ sockets), and a just-fast-enough ARM or RISC-V CPU.  Perhaps an eMMC chip or SD slot for boot.<p>This could scale down to just a few nodes, and it reduces the exposure to a single failure taking out 10 disks at a time.<p>I bet a lot of copies of this system could fit in a 4U enclosure.  Optionally the same enclosure could contain two entirely independent switches to aggregate the internal nodes.</div><br/><div id="39063675" class="c"><input type="checkbox" id="c-39063675" checked=""/><div class="controls bullet"><span class="by">evanreichard</span><span>|</span><a href="#39061551">parent</a><span>|</span><a href="#39062578">next</a><span>|</span><label class="collapse" for="c-39063675">[-]</label><label class="expand" for="c-39063675">[2 more]</label></div><br/><div class="children"><div class="content">I used to run a 5 node Ceph cluster on a bunch of ODROID-HC2&#x27;s [0]. Was a royal pain to get installed (armhf processor). But once it was running it worked great. Just slow with the single 1Gb NIC.<p>Was just a learning experience at the time.<p>[0] <a href="https:&#x2F;&#x2F;www.hardkernel.com&#x2F;shop&#x2F;odroid-hc2-home-cloud-two&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.hardkernel.com&#x2F;shop&#x2F;odroid-hc2-home-cloud-two&#x2F;</a></div><br/><div id="39065427" class="c"><input type="checkbox" id="c-39065427" checked=""/><div class="controls bullet"><span class="by">eurekin</span><span>|</span><a href="#39061551">root</a><span>|</span><a href="#39063675">parent</a><span>|</span><a href="#39062578">next</a><span>|</span><label class="collapse" for="c-39065427">[-]</label><label class="expand" for="c-39065427">[1 more]</label></div><br/><div class="children"><div class="content">Same here, but on PI 4b&#x27;s. 6 node cluster with a 2tb hdd and 512 Tb ssd per node. CEPH made a huge impression on me, as in I didn&#x27;t recognize how extensive the package was. I went up to 122mb&#x2F;s and thought it&#x27;s too little for my hack-NAS replacement :)<p>The functionality: mixing various pool types on the same set of SSD&#x27;s, different redundancy types (erasure coded, replicated) was very impressive. Now I can&#x27;t help but look down at a RAID NAS in comparision. Still, some extra packages like the NFS exporter were not ready for the arm architecture</div><br/></div></div></div></div><div id="39062578" class="c"><input type="checkbox" id="c-39062578" checked=""/><div class="controls bullet"><span class="by">Palomides</span><span>|</span><a href="#39061551">parent</a><span>|</span><a href="#39063675">prev</a><span>|</span><a href="#39064729">next</a><span>|</span><label class="collapse" for="c-39062578">[-]</label><label class="expand" for="c-39062578">[4 more]</label></div><br/><div class="children"><div class="content">here&#x27;s a weird calculation:<p>this cluster does something vaguely like 0.8 gigabits per second per watt (1 terabyte&#x2F;s * 8 bits per byte * 1024 gb per tb &#x2F; 34 nodes &#x2F; 300 watts<p>a new mac mini (super efficient arm system) runs around 10 watts in interactive usage and can do 10 gigabits per second network, so maybe 1 gigabit per second per watt of data<p>so OP&#x27;s cluster, back of the envelope, is basically the same bits per second per watt that a very efficient arm system can do<p>I don&#x27;t think running tiny nodes would actually get you any more efficiency, and would probably cost more! performance per watt is quite good on powerful servers now<p>anyway, this is all open source software running on off-the-shelf hardware, you can do it yourself for a few hundred bucks</div><br/><div id="39062755" class="c"><input type="checkbox" id="c-39062755" checked=""/><div class="controls bullet"><span class="by">amluto</span><span>|</span><a href="#39061551">root</a><span>|</span><a href="#39062578">parent</a><span>|</span><a href="#39062744">next</a><span>|</span><label class="collapse" for="c-39062755">[-]</label><label class="expand" for="c-39062755">[2 more]</label></div><br/><div class="children"><div class="content">I think the Mac Mini has massively more compute than needed for this kind of work. It also has a power supply, and computer power supplies are generally not amazing at low output.<p>I’m imagining something quite specialized. Use a low frequency CPU with either vector units or even DMA engines optimized for the specific workloads needed, or go all out and arrange for data to be DMAed directly between the disk and the NIC.</div><br/><div id="39062768" class="c"><input type="checkbox" id="c-39062768" checked=""/><div class="controls bullet"><span class="by">Palomides</span><span>|</span><a href="#39061551">root</a><span>|</span><a href="#39062755">parent</a><span>|</span><a href="#39062744">next</a><span>|</span><label class="collapse" for="c-39062768">[-]</label><label class="expand" for="c-39062768">[1 more]</label></div><br/><div class="children"><div class="content">sounds like a DPU (mellanox bluefield for example), they&#x27;re entire ARM systems with a high speed NIC all on a PCIe card, I think the bluefield ones can even directly interface over the bus to nvme drives without the host system involved</div><br/></div></div></div></div><div id="39062744" class="c"><input type="checkbox" id="c-39062744" checked=""/><div class="controls bullet"><span class="by">3abiton</span><span>|</span><a href="#39061551">root</a><span>|</span><a href="#39062578">parent</a><span>|</span><a href="#39062755">prev</a><span>|</span><a href="#39064729">next</a><span>|</span><label class="collapse" for="c-39062744">[-]</label><label class="expand" for="c-39062744">[1 more]</label></div><br/><div class="children"><div class="content">Trusting your maths, damn Apple did a great job on their M design.</div><br/></div></div></div></div><div id="39064729" class="c"><input type="checkbox" id="c-39064729" checked=""/><div class="controls bullet"><span class="by">somat</span><span>|</span><a href="#39061551">parent</a><span>|</span><a href="#39062578">prev</a><span>|</span><a href="#39062344">next</a><span>|</span><label class="collapse" for="c-39064729">[-]</label><label class="expand" for="c-39064729">[2 more]</label></div><br/><div class="children"><div class="content">I have always wanted to set up a ceph system with one drive per node. The ideal form factor would be a drive with a couple network interfaces built in. western digital had a press release about an experiment they did that was exactly this, but it never ended up with drive you could buy.<p>The hardkernel HC2 SOC was a nearly ideal form factor for this, and I still have a stack of them laying around that I bought to make a ceph cluster, but I ran out of steam when I figured out they were 32bit. not to say it would be impossible I just never did it.</div><br/><div id="39065633" class="c"><input type="checkbox" id="c-39065633" checked=""/><div class="controls bullet"><span class="by">progval</span><span>|</span><a href="#39061551">root</a><span>|</span><a href="#39064729">parent</a><span>|</span><a href="#39062344">next</a><span>|</span><label class="collapse" for="c-39065633">[-]</label><label class="expand" for="c-39065633">[1 more]</label></div><br/><div class="children"><div class="content">I used to use Ceph Luminous (v12) on these, they worked fine. Unfortunately, a bug in Nautilus (v14) prevented 32-bits and 64-bits archs from talking to each other. Pacific (v16) allegedly solves this, but I didn&#x27;t try it: <a href="https:&#x2F;&#x2F;ceph.com&#x2F;en&#x2F;news&#x2F;blog&#x2F;2021&#x2F;v16-2-5-pacific-released&#x2F;" rel="nofollow">https:&#x2F;&#x2F;ceph.com&#x2F;en&#x2F;news&#x2F;blog&#x2F;2021&#x2F;v16-2-5-pacific-released&#x2F;</a><p>If you want to try it with a more modern (and 64-bits) device, the hardkernel HC4 might do it for you. It&#x27;s conceptually similar to the HC2 but has two drives. Unfortunately it only has double the RAM (4GB), which is probably not enough anymore.</div><br/></div></div></div></div><div id="39062344" class="c"><input type="checkbox" id="c-39062344" checked=""/><div class="controls bullet"><span class="by">kbenson</span><span>|</span><a href="#39061551">parent</a><span>|</span><a href="#39064729">prev</a><span>|</span><a href="#39062069">next</a><span>|</span><label class="collapse" for="c-39062344">[-]</label><label class="expand" for="c-39062344">[1 more]</label></div><br/><div class="children"><div class="content">There probably is a sweet spot for power to speed, but I think it&#x27;s possibly a bit larger than you suggest.  There&#x27;s overhead from the other components as well.  For example, the Mellanox NIC seems to utilize about 20W itself, and while the reduced numbers of drives might allow for a single port NIC which seems to use about half the power, if we&#x27;re going to increase the number of cables (3 per 12 disks instead of 2 per 5), we&#x27;re not just increasing the power usage of the nodes themselves put also possible increasing the power usage or changing the type of switch required to combine the nodes.<p>If looked at as a whole, it appears to be more about whether you&#x27;re combining resources at a low level (on the PCI bus on nodes) or a high level (in the switching infrastructure), and we should be careful not to push power (or complexity, as is often a similar goal) to a separate part of the system that is out of our immediate thoughts but still very much part of the system.  Then again, sometimes parts of the system are much better at handling the complexity for certain cases, so in those cases that can be a definite win.</div><br/></div></div><div id="39062244" class="c"><input type="checkbox" id="c-39062244" checked=""/><div class="controls bullet"><span class="by">booi</span><span>|</span><a href="#39061551">parent</a><span>|</span><a href="#39062069">prev</a><span>|</span><a href="#39065254">next</a><span>|</span><label class="collapse" for="c-39062244">[-]</label><label class="expand" for="c-39062244">[1 more]</label></div><br/><div class="children"><div class="content">Is that a lot of overhead? The disk itself uses about 10W and high speed controllers use about 75W leaves pretty much 100W for the rest of the system including overhead of about 10%. Scale up the system to 16 disks and there’s not a lot of room for improvement</div><br/></div></div><div id="39065254" class="c"><input type="checkbox" id="c-39065254" checked=""/><div class="controls bullet"><span class="by">walrus01</span><span>|</span><a href="#39061551">parent</a><span>|</span><a href="#39062244">prev</a><span>|</span><a href="#39062208">next</a><span>|</span><label class="collapse" for="c-39065254">[-]</label><label class="expand" for="c-39065254">[1 more]</label></div><br/><div class="children"><div class="content">10 Gbps is increasingly obsolete with very low cost 100 Gbps switches and 100Gbps interfaces. Something would have to be really tiny and low cost to justify doing a ceph setup with 10Gbps interfaces now...  If you&#x27;re at that scale of very small stuff you are probably better off doing local NVME storage on each server instead.</div><br/></div></div><div id="39062208" class="c"><input type="checkbox" id="c-39062208" checked=""/><div class="controls bullet"><span class="by">jeffbee</span><span>|</span><a href="#39061551">parent</a><span>|</span><a href="#39065254">prev</a><span>|</span><a href="#39063690">next</a><span>|</span><label class="collapse" for="c-39062208">[-]</label><label class="expand" for="c-39062208">[1 more]</label></div><br/><div class="children"><div class="content">I think the chief source of inefficiency in this architecture would be the NVMe controller. When the operating system and the NVMe device are at arm&#x27;s length, there is natural inefficiency, as the controller needs to infer the intent of the request and do its best in terms of placement and wear leveling. The new FDP (flexible data placement) features try to address this by giving the operating system more control. The best thing would be to just hoist it all up into the host operating system and present the flash, as nearly as possible, as a giant field of dumb transistors that happens to be a PCIe device. With layers of abstraction removed, the hardware unit could be something like an Atom with integrated 100gbps NICs and a proportional amount of flash to achieve the desired system parallelism.</div><br/></div></div></div></div><div id="39063690" class="c"><input type="checkbox" id="c-39063690" checked=""/><div class="controls bullet"><span class="by">chx</span><span>|</span><a href="#39061551">prev</a><span>|</span><a href="#39065448">next</a><span>|</span><label class="collapse" for="c-39063690">[-]</label><label class="expand" for="c-39063690">[7 more]</label></div><br/><div class="children"><div class="content">There was a point in history when the total amount of digital data stored worldwide reached 1TiB for the first time. It is extremely likely this day was within the last sixty years.<p>And here we are moving that amount of data every second on the servers of a fairly random entity. We not talking of a nation state or a supranatural research effort.</div><br/><div id="39064273" class="c"><input type="checkbox" id="c-39064273" checked=""/><div class="controls bullet"><span class="by">qingcharles</span><span>|</span><a href="#39063690">parent</a><span>|</span><a href="#39064357">next</a><span>|</span><label class="collapse" for="c-39064273">[-]</label><label class="expand" for="c-39064273">[1 more]</label></div><br/><div class="children"><div class="content">That reminds me of a calculation I did which showed that my desktop PC would be more powerful than all of the computers on the planet combined in like 1978 :D</div><br/></div></div><div id="39064357" class="c"><input type="checkbox" id="c-39064357" checked=""/><div class="controls bullet"><span class="by">fiddlerwoaroof</span><span>|</span><a href="#39063690">parent</a><span>|</span><a href="#39064273">prev</a><span>|</span><a href="#39065448">next</a><span>|</span><label class="collapse" for="c-39064357">[-]</label><label class="expand" for="c-39064357">[5 more]</label></div><br/><div class="children"><div class="content">It’s at least 20ish years ago: I remember an old sysadmin talking about managing petabytes before 2003</div><br/><div id="39064466" class="c"><input type="checkbox" id="c-39064466" checked=""/><div class="controls bullet"><span class="by">aspenmayer</span><span>|</span><a href="#39063690">root</a><span>|</span><a href="#39064357">parent</a><span>|</span><a href="#39065448">next</a><span>|</span><label class="collapse" for="c-39064466">[-]</label><label class="expand" for="c-39064466">[4 more]</label></div><br/><div class="children"><div class="content">Those numbers seem reasonable in that context. I first started using BitTorrent around that time as well, and it wasn&#x27;t uncommon to see many users long-term seeding multiple hundreds of gigabytes of Linux ISOs alone.<p>Here’s another usage scenario with data usage numbers I found a while back.<p>&gt; A 2004 paper published in ACM Transactions on Programming Languages and Systems shows how Hancock code can sift calling card records, long distance calls, IP addresses and internet traffic dumps, and even track the physical movements of mobile phone customers as their signal moves from cell site to cell site.<p>&gt; With Hancock, &quot;analysts could store sufficiently precise information to enable new applications previously thought to be infeasible,&quot; the program authors wrote. AT&amp;T uses Hancock code to sift 9 GB of telephone traffic data a night, according to the paper.<p><a href="https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20200309221602&#x2F;https:&#x2F;&#x2F;www.wired.com&#x2F;2007&#x2F;10&#x2F;att-invents-pro&#x2F;" rel="nofollow">https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20200309221602&#x2F;https:&#x2F;&#x2F;www.wired...</a></div><br/><div id="39065104" class="c"><input type="checkbox" id="c-39065104" checked=""/><div class="controls bullet"><span class="by">fiddlerwoaroof</span><span>|</span><a href="#39063690">root</a><span>|</span><a href="#39064466">parent</a><span>|</span><a href="#39064792">next</a><span>|</span><label class="collapse" for="c-39065104">[-]</label><label class="expand" for="c-39065104">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, at the other end of the scale, it sounds like Apple is now managing exabytes: <a href="https:&#x2F;&#x2F;read.engineerscodex.com&#x2F;p&#x2F;how-apple-built-icloud-to-store-billions" rel="nofollow">https:&#x2F;&#x2F;read.engineerscodex.com&#x2F;p&#x2F;how-apple-built-icloud-to-...</a><p>This is pretty mind-boggling to me.</div><br/></div></div><div id="39064792" class="c"><input type="checkbox" id="c-39064792" checked=""/><div class="controls bullet"><span class="by">ComputerGuru</span><span>|</span><a href="#39063690">root</a><span>|</span><a href="#39064466">parent</a><span>|</span><a href="#39065104">prev</a><span>|</span><a href="#39065448">next</a><span>|</span><label class="collapse" for="c-39064792">[-]</label><label class="expand" for="c-39064792">[2 more]</label></div><br/><div class="children"><div class="content">I archived Hancock here over a decade ago, stumbled upon it via HN at the time if I’m not mistaken: <a href="https:&#x2F;&#x2F;github.com&#x2F;mqudsi&#x2F;hancock">https:&#x2F;&#x2F;github.com&#x2F;mqudsi&#x2F;hancock</a></div><br/><div id="39065022" class="c"><input type="checkbox" id="c-39065022" checked=""/><div class="controls bullet"><span class="by">aspenmayer</span><span>|</span><a href="#39063690">root</a><span>|</span><a href="#39064792">parent</a><span>|</span><a href="#39065448">next</a><span>|</span><label class="collapse" for="c-39065022">[-]</label><label class="expand" for="c-39065022">[1 more]</label></div><br/><div class="children"><div class="content">That’s pretty cool. I remember someone on that repo from while back and was surprised to see their name pop up again. Thanks for archiving this!<p>Corinna Cortes et al wrote the paper(s) on Hancock and also the Communities of Interest paper referenced in the Wired article I linked to. She’s apparently a pretty big deal and went on to work at Google after her prestigious work at AT&amp;T.<p>Hancock: A Language for Extracting Signatures from Data<p><a href="https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?view_op=view_citation&amp;hl=en&amp;user=U_IVY50AAAAJ&amp;citation_for_view=U_IVY50AAAAJ:NaGl4SEjCO4C" rel="nofollow">https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?view_op=view_citation&amp;h...</a><p>Hancock: A Language for Analyzing Transactional Data Streams<p><a href="https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?view_op=view_citation&amp;hl=en&amp;user=U_IVY50AAAAJ&amp;citation_for_view=U_IVY50AAAAJ:V3AGJWp-ZtQC" rel="nofollow">https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?view_op=view_citation&amp;h...</a><p>Communities of Interest<p><a href="https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?view_op=view_citation&amp;hl=en&amp;user=U_IVY50AAAAJ&amp;citation_for_view=U_IVY50AAAAJ:eQOLeE2rZwMC" rel="nofollow">https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?view_op=view_citation&amp;h...</a></div><br/></div></div></div></div></div></div></div></div></div></div><div id="39065448" class="c"><input type="checkbox" id="c-39065448" checked=""/><div class="controls bullet"><span class="by">kylegalbraith</span><span>|</span><a href="#39063690">prev</a><span>|</span><a href="#39062141">next</a><span>|</span><label class="collapse" for="c-39065448">[-]</label><label class="expand" for="c-39065448">[1 more]</label></div><br/><div class="children"><div class="content">This is a fascinating read. We run a Ceph storage cluster for persisting Docker layer cache [0]. We went from using EBS to Ceph and saw a massive difference in throughput. Went from a write throughput of 146 MB&#x2F;s and 3,000 IOPS to 900 MB&#x2F;s and 30,000 IOPS.<p>The best part is that it pretty much just works. Very little babysitting with the exception of the occasional fs trim or something.<p>It’s been a massive improvement for our caching system.<p>[0] <a href="https:&#x2F;&#x2F;depot.dev&#x2F;blog&#x2F;cache-v2-faster-builds">https:&#x2F;&#x2F;depot.dev&#x2F;blog&#x2F;cache-v2-faster-builds</a></div><br/></div></div><div id="39062141" class="c"><input type="checkbox" id="c-39062141" checked=""/><div class="controls bullet"><span class="by">MPSimmons</span><span>|</span><a href="#39065448">prev</a><span>|</span><a href="#39061969">next</a><span>|</span><label class="collapse" for="c-39062141">[-]</label><label class="expand" for="c-39062141">[1 more]</label></div><br/><div class="children"><div class="content">The worst problems I&#x27;ve had with in-cluster dynamic storage were never strictly IO related, and were more the storage controller software in kubernetes having problems with real-world problems like pods dying and the PVCs not attaching until after very long timeouts expired, with the pod sitting in ContainerCreating until the PVC lock was freed.<p>This has happened in multiple clusters, using rook&#x2F;ceph as well as Longhorn.</div><br/></div></div><div id="39061969" class="c"><input type="checkbox" id="c-39061969" checked=""/><div class="controls bullet"><span class="by">mrb</span><span>|</span><a href="#39062141">prev</a><span>|</span><a href="#39063723">next</a><span>|</span><label class="collapse" for="c-39061969">[-]</label><label class="expand" for="c-39061969">[5 more]</label></div><br/><div class="children"><div class="content">I wanted to see how 1 TiB&#x2F;s compares to the actual theoretical limits of the hardware. So here is what I found:<p>The cluster has 68 nodes, each a Dell PowerEdge R6615 (<a href="https:&#x2F;&#x2F;www.delltechnologies.com&#x2F;asset&#x2F;en-us&#x2F;products&#x2F;servers&#x2F;technical-support&#x2F;poweredge-r6615-technical-guide.pdf" rel="nofollow">https:&#x2F;&#x2F;www.delltechnologies.com&#x2F;asset&#x2F;en-us&#x2F;products&#x2F;server...</a>). The R6615 configuration they run is the one with 10 U.2 drive bays. The U.2 link carries data over 4 PCIe gen4 lanes. Each PCIe lane is capable of 16 Gbit&#x2F;s. The lanes have negligible ~3% overhead thanks to 128b-132b encoding.<p>This means each U.2 link has a maximum link bandwith of 16 * 4 = 64 Gbit&#x2F;s or 8 Gbyte&#x2F;s. However the U.2 NVMe drives they use are Dell 15.36TB Enterprise NVMe Read Intensive AG, which appear to be capable of 7 Gbyte&#x2F;s read throughput (<a href="https:&#x2F;&#x2F;www.serversupply.com&#x2F;SSD%20W-TRAY&#x2F;NVMe&#x2F;15.36TB&#x2F;DELL&#x2F;182NW_356114.htm" rel="nofollow">https:&#x2F;&#x2F;www.serversupply.com&#x2F;SSD%20W-TRAY&#x2F;NVMe&#x2F;15.36TB&#x2F;DELL&#x2F;...</a>). So they are not bottlenecked by the U.2 link (8 Gbyte&#x2F;s).<p>Each node has 10 U.2 drive, so each node can do local read I&#x2F;O at a maximum of 10 * 7 = 70 Gbyte&#x2F;s.<p>However each node has a network bandwith of only 200 Gbit&#x2F;s (2 x 100GbE Mellanox ConnectX-6) which is only 25 Gbyte&#x2F;s. This implies that remote reads are under-utilizing the drives (capable of 70 Gbyte&#x2F;s). The network is the bottleneck.<p>Assuming no additional network bottlenecks (they don&#x27;t describe the network architecture), this implies the 68 nodes can provide 68 * 25 = 1700 Gbyte&#x2F;s of network reads. The author benchmarked 1 TiB&#x2F;s actually exactly 1025 GiB&#x2F;s = 1101 Gbyte&#x2F;s which is 65% of the maximum theoretical 1700 Gbyte&#x2F;s. That&#x27;s pretty decent, but in theory it&#x27;s still possible to be doing a bit better assuming all nodes can concurrently truly saturate their 200 Gbit&#x2F;s network link.<p>Reading this whole blog post, I got the impression ceph&#x27;s complexity hits the CPU pretty hard. Not compiling a module with -O2 (&quot;Fix Three&quot;: linked by the author: <a href="https:&#x2F;&#x2F;bugs.launchpad.net&#x2F;ubuntu&#x2F;+source&#x2F;ceph&#x2F;+bug&#x2F;1894453" rel="nofollow">https:&#x2F;&#x2F;bugs.launchpad.net&#x2F;ubuntu&#x2F;+source&#x2F;ceph&#x2F;+bug&#x2F;1894453</a>) can reduce performance &quot;up to 5x slower with some workloads&quot; (<a href="https:&#x2F;&#x2F;bugs.gentoo.org&#x2F;733316" rel="nofollow">https:&#x2F;&#x2F;bugs.gentoo.org&#x2F;733316</a>) is pretty unexpected, for a pure I&#x2F;O workload. Also what&#x27;s up with OSD&#x27;s threads causing excessive CPU waste grabbing the IOMMU spinlock? I agree with the conclusion that the OSD threading model is suboptimal. A relatively simple synthetic 100% read benchmark should not expose a threading contention if that part of ceph&#x27;s software architecture was well designed (which is fixable, so I hope the ceph devs prioritize this.)</div><br/><div id="39065134" class="c"><input type="checkbox" id="c-39065134" checked=""/><div class="controls bullet"><span class="by">magicalhippo</span><span>|</span><a href="#39061969">parent</a><span>|</span><a href="#39063585">next</a><span>|</span><label class="collapse" for="c-39065134">[-]</label><label class="expand" for="c-39065134">[2 more]</label></div><br/><div class="children"><div class="content">They&#x27;re benchmarking random IO though, and the disks can &quot;only&quot; do a bit over 1000k random 4k read IOPS, which translates to about 5 GiB&#x2F;s. With 320 OSDs thats around 1.6 TiB&#x2F;s.<p>At least thats the number I could find. Not exactly tons of reviews on these enterprise NVMe disks...<p>Still, that seems like a good match to the NICs. At this scale most workloads will likely appear as random IO at the storage layer anyway.</div><br/><div id="39065683" class="c"><input type="checkbox" id="c-39065683" checked=""/><div class="controls bullet"><span class="by">mrb</span><span>|</span><a href="#39061969">root</a><span>|</span><a href="#39065134">parent</a><span>|</span><a href="#39063585">next</a><span>|</span><label class="collapse" for="c-39065683">[-]</label><label class="expand" for="c-39065683">[1 more]</label></div><br/><div class="children"><div class="content">The benchmark were they accomplish 1025 GiB&#x2F;s is for sequential reads. For random reads they do 25.5M iops or ~100 GiB&#x2F;s. See last table, column &quot;630 OSDs (3x)&quot;.</div><br/></div></div></div></div><div id="39063585" class="c"><input type="checkbox" id="c-39063585" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#39061969">parent</a><span>|</span><a href="#39065134">prev</a><span>|</span><a href="#39063723">next</a><span>|</span><label class="collapse" for="c-39063585">[-]</label><label class="expand" for="c-39063585">[2 more]</label></div><br/><div class="children"><div class="content">I think PCIe TLP overhead and NVMe commands account for the difference between 7 and 8 GB&#x2F;s.</div><br/><div id="39063678" class="c"><input type="checkbox" id="c-39063678" checked=""/><div class="controls bullet"><span class="by">mrb</span><span>|</span><a href="#39061969">root</a><span>|</span><a href="#39063585">parent</a><span>|</span><a href="#39063723">next</a><span>|</span><label class="collapse" for="c-39063678">[-]</label><label class="expand" for="c-39063678">[1 more]</label></div><br/><div class="children"><div class="content">You are probably right. Reading some old notes of mine when I was fine-tuning PCIe bandwith on my ZFS server, I had discovered back then that a PCIe Max_Payload_Size of 256 bytes limited usable bandwidth to about 74% of the link&#x27;s theoretical max. I had calculated that 512 and 1024 bytes (the maximum) would raise it to respectively about 86% and 93% (but my SATA controllers didn&#x27;t support a value greater than 256.)</div><br/></div></div></div></div></div></div><div id="39063723" class="c"><input type="checkbox" id="c-39063723" checked=""/><div class="controls bullet"><span class="by">one_buggy_boi</span><span>|</span><a href="#39061969">prev</a><span>|</span><a href="#39064147">next</a><span>|</span><label class="collapse" for="c-39063723">[-]</label><label class="expand" for="c-39063723">[4 more]</label></div><br/><div class="children"><div class="content">Is modern Ceph appropriate for transactional database storage, how is the IO latency? I&#x27;d like to move to a cheaper cfs that can compete with systems like Oracle&#x27;s clustered file system or DBs backed by something like Veritas. Veritas supports multi-petabyte DBs and I haven&#x27;t seen much outside of it or ocfs that similarly scales with acceptable latency</div><br/><div id="39065220" class="c"><input type="checkbox" id="c-39065220" checked=""/><div class="controls bullet"><span class="by">antongribok</span><span>|</span><a href="#39063723">parent</a><span>|</span><a href="#39063831">next</a><span>|</span><label class="collapse" for="c-39065220">[-]</label><label class="expand" for="c-39065220">[2 more]</label></div><br/><div class="children"><div class="content">Not sure about putting DBs on CephFS directly, but Ceph RBD can definitely run RDBMS workloads.<p>You need to pay attention to the kind of hardware you use, but you can definitely get Ceph down to 0.5-0.6 ms latency on block workloads doing single thread, single queue, sync 4K writes.<p>Source, I run Ceph at work doing pretty much this.</div><br/><div id="39065564" class="c"><input type="checkbox" id="c-39065564" checked=""/><div class="controls bullet"><span class="by">patrakov</span><span>|</span><a href="#39063723">root</a><span>|</span><a href="#39065220">parent</a><span>|</span><a href="#39063831">next</a><span>|</span><label class="collapse" for="c-39065564">[-]</label><label class="expand" for="c-39065564">[1 more]</label></div><br/><div class="children"><div class="content">It is important to specify which kind of latency percentile this is. Checking on a customer&#x27;s cluster (made from 336 SATA SSDs in 15 servers, so not the best one in the world):<p><pre><code>  50th percentile = 1.75 ms
  90th percentile = 3.15 ms
  99th percentile = 9.54 ms
</code></pre>
That&#x27;s with 700 MB&#x2F;s of reads and 200 MB&#x2F;s of writes, or approximately 7000 reads IOPS and 9000 writes IOPS.</div><br/></div></div></div></div><div id="39063831" class="c"><input type="checkbox" id="c-39063831" checked=""/><div class="controls bullet"><span class="by">samcat116</span><span>|</span><a href="#39063723">parent</a><span>|</span><a href="#39065220">prev</a><span>|</span><a href="#39064147">next</a><span>|</span><label class="collapse" for="c-39063831">[-]</label><label class="expand" for="c-39063831">[1 more]</label></div><br/><div class="children"><div class="content">Latency is quite poor, I wouldn&#x27;t recommend running high performance database loads there.</div><br/></div></div></div></div><div id="39064147" class="c"><input type="checkbox" id="c-39064147" checked=""/><div class="controls bullet"><span class="by">rafaelturk</span><span>|</span><a href="#39063723">prev</a><span>|</span><a href="#39064902">next</a><span>|</span><label class="collapse" for="c-39064147">[-]</label><label class="expand" for="c-39064147">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m playing a lot with MicroCeph. Its  aopinionated low TOS, friendly setup of Ceph. Looking forward additional comments. Planning to use it in production and replace lots of NAS servers.</div><br/></div></div><div id="39064902" class="c"><input type="checkbox" id="c-39064902" checked=""/><div class="controls bullet"><span class="by">brobinson</span><span>|</span><a href="#39064147">prev</a><span>|</span><a href="#39064459">next</a><span>|</span><label class="collapse" for="c-39064902">[-]</label><label class="expand" for="c-39064902">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m curious what the performance difference would be on a modern kernel.</div><br/></div></div><div id="39064459" class="c"><input type="checkbox" id="c-39064459" checked=""/><div class="controls bullet"><span class="by">nghnam</span><span>|</span><a href="#39064902">prev</a><span>|</span><a href="#39061077">next</a><span>|</span><label class="collapse" for="c-39064459">[-]</label><label class="expand" for="c-39064459">[1 more]</label></div><br/><div class="children"><div class="content">My old company ran public and private cloud with Openstack and Ceph. We had 20 Supermicro (24 disks per server) storage nodes  and total capacity was 3PB. We learnt some experiences, especially  a flapping disk made whole system performance degraded. Solution was removing bad sector disk as soon as possible.</div><br/></div></div><div id="39061077" class="c"><input type="checkbox" id="c-39061077" checked=""/><div class="controls bullet"><span class="by">matheusmoreira</span><span>|</span><a href="#39064459">prev</a><span>|</span><a href="#39064274">next</a><span>|</span><label class="collapse" for="c-39061077">[-]</label><label class="expand" for="c-39061077">[50 more]</label></div><br/><div class="children"><div class="content">Does anyone have experience running ceph in a home lab? Last time I looked into it, there were quite significant hardware requirements.</div><br/><div id="39061103" class="c"><input type="checkbox" id="c-39061103" checked=""/><div class="controls bullet"><span class="by">nullwarp</span><span>|</span><a href="#39061077">parent</a><span>|</span><a href="#39061168">next</a><span>|</span><label class="collapse" for="c-39061103">[-]</label><label class="expand" for="c-39061103">[18 more]</label></div><br/><div class="children"><div class="content">There still are. As someone who has done both production and homelab deployments: unless you are specifically just looking for experience with it and just setting up a demo - don&#x27;t bother.<p>When it works, it works great - when it goes wrong it&#x27;s a huge headache.<p>Edit: As just an edit, if distributed storage is just something you are interested in there are much better options for a homelab setup:<p>- seaweedfs has been rock solid for me for years in both small and huge scales. we actually moved our production ceph setup to this.<p>- longhorn was solid for me when i was in the k8s world<p>- glusterfs is still fine as long as you know what you are going into.</div><br/><div id="39061257" class="c"><input type="checkbox" id="c-39061257" checked=""/><div class="controls bullet"><span class="by">dataangel</span><span>|</span><a href="#39061077">root</a><span>|</span><a href="#39061103">parent</a><span>|</span><a href="#39061596">next</a><span>|</span><label class="collapse" for="c-39061257">[-]</label><label class="expand" for="c-39061257">[4 more]</label></div><br/><div class="children"><div class="content">I really wish there was a benchmark comparing all of these + MinIO and S3. I&#x27;m in the market for a key value store, using S3 for now but eyeing moving to my own hardware in the future and having to do all the work to compare these is one of the major things making me procrastinate.</div><br/><div id="39061731" class="c"><input type="checkbox" id="c-39061731" checked=""/><div class="controls bullet"><span class="by">rglullis</span><span>|</span><a href="#39061077">root</a><span>|</span><a href="#39061257">parent</a><span>|</span><a href="#39061704">next</a><span>|</span><label class="collapse" for="c-39061731">[-]</label><label class="expand" for="c-39061731">[1 more]</label></div><br/><div class="children"><div class="content">Minio gives you &quot;only&quot; S3 object storage. I&#x27;ve setup a 3-node Minio cluster for object storage on Hetzner, each server having 4x10TB, for ~50€&#x2F;month each. This means 80TB usable data for ~150€&#x2F;month. It can be worth it if you are trying to avoid egress fees, but if I were building a data lake or anything where the data was used mostly for internal services, I&#x27;d just stick with S3.</div><br/></div></div><div id="39061704" class="c"><input type="checkbox" id="c-39061704" checked=""/><div class="controls bullet"><span class="by">woopwoop24</span><span>|</span><a href="#39061077">root</a><span>|</span><a href="#39061257">parent</a><span>|</span><a href="#39061731">prev</a><span>|</span><a href="#39061596">next</a><span>|</span><label class="collapse" for="c-39061704">[-]</label><label class="expand" for="c-39061704">[2 more]</label></div><br/><div class="children"><div class="content">minio is good but you really need fast disks.
They also really don&#x27;t like, when you want to change the size of your cluster setup. 
No plan to add cache disks, they just say use faster disks.
I have it running, goes smoothly but not really user friendly to optimize</div><br/></div></div></div></div><div id="39061596" class="c"><input type="checkbox" id="c-39061596" checked=""/><div class="controls bullet"><span class="by">rglullis</span><span>|</span><a href="#39061077">root</a><span>|</span><a href="#39061103">parent</a><span>|</span><a href="#39061257">prev</a><span>|</span><a href="#39061601">next</a><span>|</span><label class="collapse" for="c-39061596">[-]</label><label class="expand" for="c-39061596">[1 more]</label></div><br/><div class="children"><div class="content">&gt; glusterfs is still fine as long as you know what you are going into.<p>Does that include storage volumes for databases? I was using glusterFS as a way to scale my swarm cluster horizontally and I am reasonably sure that it corrupted one database to the point I lost more than a few hours of data. I was quite satisfied with the setup until I hit that.<p>I know that I am considered crazy for sticking with Docker Swarm until now, but aside from this lingering issue with how to manage stateful services, I&#x27;ve honestly don&#x27;t feel the need to move yet to k8s. My clusters is ~10 nodes running &lt; 30 stacks and it&#x27;s not like I have tens of people working with me on it.</div><br/></div></div><div id="39061601" class="c"><input type="checkbox" id="c-39061601" checked=""/><div class="controls bullet"><span class="by">bityard</span><span>|</span><a href="#39061077">root</a><span>|</span><a href="#39061103">parent</a><span>|</span><a href="#39061596">prev</a><span>|</span><a href="#39061237">next</a><span>|</span><label class="collapse" for="c-39061601">[-]</label><label class="expand" for="c-39061601">[1 more]</label></div><br/><div class="children"><div class="content">Ceph is sort of a storage all-in-one: it provides object storage, block storage, and network file storage. May I ask, which of these are you using seaweedfs for? Is it as performant as Ceph claims to be?</div><br/></div></div><div id="39061237" class="c"><input type="checkbox" id="c-39061237" checked=""/><div class="controls bullet"><span class="by">reactordev</span><span>|</span><a href="#39061077">root</a><span>|</span><a href="#39061103">parent</a><span>|</span><a href="#39061601">prev</a><span>|</span><a href="#39062201">next</a><span>|</span><label class="collapse" for="c-39061237">[-]</label><label class="expand" for="c-39061237">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;d throw minio [1] in the list there as well for homelab k8s object storage.<p>[1] <a href="https:&#x2F;&#x2F;min.io&#x2F;" rel="nofollow">https:&#x2F;&#x2F;min.io&#x2F;</a></div><br/><div id="39062032" class="c"><input type="checkbox" id="c-39062032" checked=""/><div class="controls bullet"><span class="by">speedgoose</span><span>|</span><a href="#39061077">root</a><span>|</span><a href="#39061237">parent</a><span>|</span><a href="#39062201">next</a><span>|</span><label class="collapse" for="c-39062032">[-]</label><label class="expand" for="c-39062032">[1 more]</label></div><br/><div class="children"><div class="content">Also garage. <a href="https:&#x2F;&#x2F;garagehq.deuxfleurs.fr&#x2F;" rel="nofollow">https:&#x2F;&#x2F;garagehq.deuxfleurs.fr&#x2F;</a></div><br/></div></div></div></div><div id="39062201" class="c"><input type="checkbox" id="c-39062201" checked=""/><div class="controls bullet"><span class="by">asadhaider</span><span>|</span><a href="#39061077">root</a><span>|</span><a href="#39061103">parent</a><span>|</span><a href="#39061237">prev</a><span>|</span><a href="#39062988">next</a><span>|</span><label class="collapse" for="c-39062201">[-]</label><label class="expand" for="c-39062201">[2 more]</label></div><br/><div class="children"><div class="content">I thought it was popular for people running Proxmox clusters</div><br/><div id="39064083" class="c"><input type="checkbox" id="c-39064083" checked=""/><div class="controls bullet"><span class="by">geerlingguy</span><span>|</span><a href="#39061077">root</a><span>|</span><a href="#39062201">parent</a><span>|</span><a href="#39062988">next</a><span>|</span><label class="collapse" for="c-39064083">[-]</label><label class="expand" for="c-39064083">[1 more]</label></div><br/><div class="children"><div class="content">It is, and if you have a few nodes with at least 10 GbE networking, it&#x27;s certainly the best clustered storage option I can think of.</div><br/></div></div></div></div><div id="39062988" class="c"><input type="checkbox" id="c-39062988" checked=""/><div class="controls bullet"><span class="by">matheusmoreira</span><span>|</span><a href="#39061077">root</a><span>|</span><a href="#39061103">parent</a><span>|</span><a href="#39062201">prev</a><span>|</span><a href="#39061168">next</a><span>|</span><label class="collapse" for="c-39062988">[-]</label><label class="expand" for="c-39062988">[7 more]</label></div><br/><div class="children"><div class="content">I just want to hoard data. I hate having to delete stuff to make space. Things disappear from the web every day. I should hold onto them.<p>My requirements for a storage solution are:<p>&gt; Single root file system<p>&gt; Storage device failure tolerance<p>&gt; Gradual expansion capability<p>The problem with every storage solution I&#x27;ve ever seen is the lack of gradual expandability. I&#x27;m not a corporation, I&#x27;m just a guy. I don&#x27;t have the money to buy 200 hard disks all at once. I need to gradually expand capacity as needed.<p>I was attracted to this ceph because it apparently allows you to throw a bunch of drives of any make and model at it and it just pools them all up without complaining. The complexity is nightmarish though.<p>ZFS is nearly perfect but when it comes to expanding capacity it&#x27;s just as bad as RAID. Expansion features seem to be <i>just</i> about to land for quite a few years now. I remember getting excited about it after seeing news here only for people to deflate my expectations. Btrfs has a flexible block allocator which is just what I need but... It&#x27;s btrfs.</div><br/><div id="39063356" class="c"><input type="checkbox" id="c-39063356" checked=""/><div class="controls bullet"><span class="by">chromatin</span><span>|</span><a href="#39061077">root</a><span>|</span><a href="#39062988">parent</a><span>|</span><a href="#39064183">next</a><span>|</span><label class="collapse" for="c-39063356">[-]</label><label class="expand" for="c-39063356">[2 more]</label></div><br/><div class="children"><div class="content">&gt; ZFS is nearly perfect but when it comes to expanding capacity it&#x27;s just as bad as RAID.<p>if you don&#x27;t mind the overhead of a &quot;pool of mirrors&quot; approach [1], then it is easy to expand storage by adding pairs of disks! This is how my home NAS is configured.<p>[1] <a href="https:&#x2F;&#x2F;jrs-s.net&#x2F;2015&#x2F;02&#x2F;06&#x2F;zfs-you-should-use-mirror-vdevs-not-raidz&#x2F;" rel="nofollow">https:&#x2F;&#x2F;jrs-s.net&#x2F;2015&#x2F;02&#x2F;06&#x2F;zfs-you-should-use-mirror-vdevs...</a></div><br/><div id="39063879" class="c"><input type="checkbox" id="c-39063879" checked=""/><div class="controls bullet"><span class="by">roygbiv2</span><span>|</span><a href="#39061077">root</a><span>|</span><a href="#39063356">parent</a><span>|</span><a href="#39064183">next</a><span>|</span><label class="collapse" for="c-39063879">[-]</label><label class="expand" for="c-39063879">[1 more]</label></div><br/><div class="children"><div class="content">This is also exactly how mine is done. Started off with a bunch of 2TB disks. I&#x27;ve now got a mixture of 16TB down to 4TB, all in the original pool.</div><br/></div></div></div></div><div id="39064183" class="c"><input type="checkbox" id="c-39064183" checked=""/><div class="controls bullet"><span class="by">deadbunny</span><span>|</span><a href="#39061077">root</a><span>|</span><a href="#39062988">parent</a><span>|</span><a href="#39063356">prev</a><span>|</span><a href="#39064966">next</a><span>|</span><label class="collapse" for="c-39064183">[-]</label><label class="expand" for="c-39064183">[1 more]</label></div><br/><div class="children"><div class="content">ZFS using mirrors is extremely easy to expand. Need more space and you have small drives? Replace the drives in a mirror one by one with bigger ones. Need more space and already have huge drives? Just add another vdev mirror. And the added benefit of not living in fear of drive failure while resilvering as it is much faster with mirrors than raidX.<p>Sure the density isn&#x27;t great as you&#x27;re essentially running at 50% or raw storage but - touches wood - my home zpool has been running strong for about a decade doing the above from 6x 6tb drives (3x 6tb mirrors) to 16x 10-20tb drives (8x mirrors, differing sized drives but matched per mirror like a 10tb x2 mirror, a 16tb x2 mirror etc).<p>Edit: Just realised someone else as already mentioned a pool or mirrors. Consider this another +1.</div><br/></div></div><div id="39064966" class="c"><input type="checkbox" id="c-39064966" checked=""/><div class="controls bullet"><span class="by">amadio</span><span>|</span><a href="#39061077">root</a><span>|</span><a href="#39062988">parent</a><span>|</span><a href="#39064183">prev</a><span>|</span><a href="#39063502">next</a><span>|</span><label class="collapse" for="c-39064966">[-]</label><label class="expand" for="c-39064966">[1 more]</label></div><br/><div class="children"><div class="content">EOS (<a href="https:&#x2F;&#x2F;cern.ch&#x2F;eos" rel="nofollow">https:&#x2F;&#x2F;cern.ch&#x2F;eos</a>, <a href="https:&#x2F;&#x2F;github.com&#x2F;cern-eos&#x2F;eos">https:&#x2F;&#x2F;github.com&#x2F;cern-eos&#x2F;eos</a>) is probably a bit more complicated than other solutions to setup and manage, but does allow to add&#x2F;remove new disks and nodes serving data on the fly. This is essential to let us upgrade harware of the clusters serving experimental data with minimal to no downtime.</div><br/></div></div><div id="39063502" class="c"><input type="checkbox" id="c-39063502" checked=""/><div class="controls bullet"><span class="by">bityard</span><span>|</span><a href="#39061077">root</a><span>|</span><a href="#39062988">parent</a><span>|</span><a href="#39064966">prev</a><span>|</span><a href="#39061168">next</a><span>|</span><label class="collapse" for="c-39063502">[-]</label><label class="expand" for="c-39063502">[2 more]</label></div><br/><div class="children"><div class="content">On a single host, you could do this with LVM. Add a pair of disks, make them a RAID 1, create a physical volume on them, then a volume group, then a logical volume with XFS on top. To expand, you add a pair of disks, RAID 1 them, and add them to the LVM. It&#x27;s a little stupid, but it would work.<p>If multiple nodes are not off the table, also look into seaweedfs.<p>Also consider how (or if) you are going to back up your hoard of data.</div><br/><div id="39063608" class="c"><input type="checkbox" id="c-39063608" checked=""/><div class="controls bullet"><span class="by">matheusmoreira</span><span>|</span><a href="#39061077">root</a><span>|</span><a href="#39063502">parent</a><span>|</span><a href="#39061168">next</a><span>|</span><label class="collapse" for="c-39063608">[-]</label><label class="expand" for="c-39063608">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Also consider how (or if) you are going to back up your hoard of data.<p>I actually emailed backblaze years ago about their supposedly unlimited consumer backup plan. Asked them if they would <i>really</i> allow me to dump into their systems dozens of terabytes of encrypted undeduplicable data. They responded that yes, they would. Still didn&#x27;t believe them, these corporations never really mean it when they say unlimited. Plus they had no Linux software.</div><br/></div></div></div></div></div></div></div></div><div id="39061168" class="c"><input type="checkbox" id="c-39061168" checked=""/><div class="controls bullet"><span class="by">ianlevesque</span><span>|</span><a href="#39061077">parent</a><span>|</span><a href="#39061103">prev</a><span>|</span><a href="#39061498">next</a><span>|</span><label class="collapse" for="c-39061168">[-]</label><label class="expand" for="c-39061168">[1 more]</label></div><br/><div class="children"><div class="content">I played around with it and it has a very cool web UI, object storage &amp; file storage, but it was very hard to get decent performance and it was possible to get the metadata daemons stuck pretty easily with a small cluster. Ultimately when the fun wore off I just put zfs on a single box instead.</div><br/></div></div><div id="39061498" class="c"><input type="checkbox" id="c-39061498" checked=""/><div class="controls bullet"><span class="by">victorhooi</span><span>|</span><a href="#39061077">parent</a><span>|</span><a href="#39061168">prev</a><span>|</span><a href="#39061331">next</a><span>|</span><label class="collapse" for="c-39061498">[-]</label><label class="expand" for="c-39061498">[1 more]</label></div><br/><div class="children"><div class="content">I have some experience with Ceph, both for work, and with homelab-y stuff.<p>First, bear in mind that Ceph is a <i>distributed</i> storage system - so the idea is that you will have multiple nodes.<p>For learning, you can definitely virtualise it all on a single box - but you&#x27;ll have a better time with discrete physical machines.<p>Also, Ceph does prefer physical access to disks (similar to ZFS).<p>And you do need decent networking connectivity - I think that&#x27;s the main thing people think of, when they think of high hardware requirements for Ceph. Ideally 10Gbe at the minimum - although more if you want higher performance - there can be a lot of network traffic, particularly with things like backfill. (25Gbps if you can find that gear cheap for homelab - 50Gbps is a technological dead-end. 100Gbps works well).<p>But honestly, for a homelab, a cheap mini PC or NUC with 10Gbe will work fine, and you should get acceptable performance, and it&#x27;ll be good for learning.<p>You can install Ceph directly on bare-metal, or if you want to do the homelab k8s route, you can use Rook (<a href="https:&#x2F;&#x2F;rook.io&#x2F;" rel="nofollow">https:&#x2F;&#x2F;rook.io&#x2F;</a>).<p>Hope this helps, and good luck! Let me know if you have any other questions.</div><br/></div></div><div id="39061331" class="c"><input type="checkbox" id="c-39061331" checked=""/><div class="controls bullet"><span class="by">reactordev</span><span>|</span><a href="#39061077">parent</a><span>|</span><a href="#39061498">prev</a><span>|</span><a href="#39064893">next</a><span>|</span><label class="collapse" for="c-39061331">[-]</label><label class="expand" for="c-39061331">[2 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a blog post they did where they setup Ceph on some rPI 4&#x27;s. I&#x27;d say that&#x27;s not significant hardware at all. [1]<p>[1] <a href="https:&#x2F;&#x2F;ceph.io&#x2F;en&#x2F;news&#x2F;blog&#x2F;2022&#x2F;install-ceph-in-a-raspberrypi-4-cluster&#x2F;" rel="nofollow">https:&#x2F;&#x2F;ceph.io&#x2F;en&#x2F;news&#x2F;blog&#x2F;2022&#x2F;install-ceph-in-a-raspberr...</a></div><br/><div id="39061516" class="c"><input type="checkbox" id="c-39061516" checked=""/><div class="controls bullet"><span class="by">m463</span><span>|</span><a href="#39061077">root</a><span>|</span><a href="#39061331">parent</a><span>|</span><a href="#39064893">next</a><span>|</span><label class="collapse" for="c-39061516">[-]</label><label class="expand" for="c-39061516">[1 more]</label></div><br/><div class="children"><div class="content">I think &quot;significant&quot; turns out to mean the number of nodes required.</div><br/></div></div></div></div><div id="39064893" class="c"><input type="checkbox" id="c-39064893" checked=""/><div class="controls bullet"><span class="by">antongribok</span><span>|</span><a href="#39061077">parent</a><span>|</span><a href="#39061331">prev</a><span>|</span><a href="#39062106">next</a><span>|</span><label class="collapse" for="c-39064893">[-]</label><label class="expand" for="c-39064893">[3 more]</label></div><br/><div class="children"><div class="content">I run Ceph on some Raspberry Pi 4s.  It&#x27;s super reliable, and with cephadm it&#x27;s very easy[1] to install and maintain.<p>My household is already 100% on Linux, so having a native network filesystem that I can just mount from any laptop is very handy.<p>Works great over Tailscale too, so I don&#x27;t even have to be at home.<p>[1]  I run a large install of Ceph at work, so &quot;easy&quot; might be a bit relative.</div><br/><div id="39064979" class="c"><input type="checkbox" id="c-39064979" checked=""/><div class="controls bullet"><span class="by">dcplaya</span><span>|</span><a href="#39061077">root</a><span>|</span><a href="#39064893">parent</a><span>|</span><a href="#39062106">next</a><span>|</span><label class="collapse" for="c-39064979">[-]</label><label class="expand" for="c-39064979">[2 more]</label></div><br/><div class="children"><div class="content">What are your speeds? Do you rub ceph FS too?<p>I&#x27;m trying to do similar.</div><br/><div id="39065097" class="c"><input type="checkbox" id="c-39065097" checked=""/><div class="controls bullet"><span class="by">antongribok</span><span>|</span><a href="#39061077">root</a><span>|</span><a href="#39064979">parent</a><span>|</span><a href="#39062106">next</a><span>|</span><label class="collapse" for="c-39065097">[-]</label><label class="expand" for="c-39065097">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s been a while since I&#x27;ve done some benchmarks, but it can definitely do 40MB&#x2F;s sustained writes, which is very good given the single 1GbE links on each node, and 5TB SMR drives.<p>Latency is hilariously terrible though. It&#x27;s funny to open a text file over the network in vi, paste a long blob of text and watch it sync that line by line over the network.<p>If by &quot;rub&quot; you mean scrub, then yes, although I increased the scrub intervals.  There&#x27;s no need to scrub everything every week.</div><br/></div></div></div></div></div></div><div id="39062106" class="c"><input type="checkbox" id="c-39062106" checked=""/><div class="controls bullet"><span class="by">aaronax</span><span>|</span><a href="#39061077">parent</a><span>|</span><a href="#39064893">prev</a><span>|</span><a href="#39062020">next</a><span>|</span><label class="collapse" for="c-39062106">[-]</label><label class="expand" for="c-39062106">[1 more]</label></div><br/><div class="children"><div class="content">I just set up a three-node Proxmox+Ceph cluster a few weeks ago.  Three Optiplex desktops 7040, 3060, and 7060 and 4x SSDs of 1TB and 2TB mix (was 5 until I noticed one of my scavenged SSDs was failed).  Single 1gbps network on each so I am seeing 30-120MB&#x2F;s disk performance depending on things.  I think in a few months I will upgrade to 10gbps for about $400.<p>I&#x27;m about 1&#x2F;2 through the process of moving my 15 virtual machines over.  It is a little slow but tolerable.  Not having to decide on RAIDs or a NAS ahead of time is amazing.  I can throw disks and nodes at it whenever.</div><br/></div></div><div id="39062020" class="c"><input type="checkbox" id="c-39062020" checked=""/><div class="controls bullet"><span class="by">mcronce</span><span>|</span><a href="#39061077">parent</a><span>|</span><a href="#39062106">prev</a><span>|</span><a href="#39061558">next</a><span>|</span><label class="collapse" for="c-39062020">[-]</label><label class="expand" for="c-39062020">[1 more]</label></div><br/><div class="children"><div class="content">I run Ceph in my lab.  It&#x27;s pretty heavy on CPU, but it works well as long as you&#x27;re willing to spring for fast networking (at least 10Gb, ideally 40+) and at least a few nodes with 6+ disks each if you&#x27;re using spinners.  You can probably get away with far fewer disks per node if you&#x27;re going all-SSD.</div><br/></div></div><div id="39061558" class="c"><input type="checkbox" id="c-39061558" checked=""/><div class="controls bullet"><span class="by">chomp</span><span>|</span><a href="#39061077">parent</a><span>|</span><a href="#39062020">prev</a><span>|</span><a href="#39062596">next</a><span>|</span><label class="collapse" for="c-39061558">[-]</label><label class="expand" for="c-39061558">[1 more]</label></div><br/><div class="children"><div class="content">I’ve ran Ceph in my home lab since Jewel (~8 years ago). Currently up to 70TB storage on a single node. Have been pretty successful vertically scaling, but will have to add a 2nd node here in a bit.<p>Ceph isn’t the fastest, but it’s incredibly resilient and scalable. Haven’t needed any crazy hardware requirements, just ram and an i7.</div><br/></div></div><div id="39062596" class="c"><input type="checkbox" id="c-39062596" checked=""/><div class="controls bullet"><span class="by">sgarland</span><span>|</span><a href="#39061077">parent</a><span>|</span><a href="#39061558">prev</a><span>|</span><a href="#39064162">next</a><span>|</span><label class="collapse" for="c-39062596">[-]</label><label class="expand" for="c-39062596">[2 more]</label></div><br/><div class="children"><div class="content">Yes. I first tried it with Rook, and that was a disaster, so I shifted to Longhorn. That has had its own share of problems, and is quite slow. Finally, I let Proxmox manage Ceph for me, and it’s been a dream. So far I haven’t migrated my K8s workloads to it, but I’ve used it for RDBMS storage (DBs in VMs), and it works flawlessly.<p>I don’t have an incredibly great setup, either: 3x Dell R620s (Ivy Bridge-era Xeons), and 1GBe. Proxmox’s corosync has a dedicated switch, but that’s about it. The disks are nice to be fair - Samsung PM863 3.84 TB NVMe. They are absolutely bottlenecked by the LAN at the moment.<p>I plan on upgrading to 10GBe as soon as I can convince myself to pay for an L3 10G switch.</div><br/><div id="39064832" class="c"><input type="checkbox" id="c-39064832" checked=""/><div class="controls bullet"><span class="by">sixdonuts</span><span>|</span><a href="#39061077">root</a><span>|</span><a href="#39062596">parent</a><span>|</span><a href="#39064162">next</a><span>|</span><label class="collapse" for="c-39064832">[-]</label><label class="expand" for="c-39064832">[1 more]</label></div><br/><div class="children"><div class="content">Just get a 25G switch and MM fiber. 25G switches are cheaper, use less power and can work with 10 and 25G SFPs.</div><br/></div></div></div></div><div id="39064162" class="c"><input type="checkbox" id="c-39064162" checked=""/><div class="controls bullet"><span class="by">mikecoles</span><span>|</span><a href="#39061077">parent</a><span>|</span><a href="#39062596">prev</a><span>|</span><a href="#39061833">next</a><span>|</span><label class="collapse" for="c-39064162">[-]</label><label class="expand" for="c-39064162">[1 more]</label></div><br/><div class="children"><div class="content">Works great, depending on what you want to do. Running on SBCs or computers with cheap sata cards will greatly reduce the performance. It&#x27;s been running well for years after I found out the issues regarding SMR drives and the SATA card bottlenecks.<p>45Drives has a homelab setup if you&#x27;re looking for a canned solution.</div><br/></div></div><div id="39061833" class="c"><input type="checkbox" id="c-39061833" checked=""/><div class="controls bullet"><span class="by">willglynn</span><span>|</span><a href="#39061077">parent</a><span>|</span><a href="#39064162">prev</a><span>|</span><a href="#39062971">next</a><span>|</span><label class="collapse" for="c-39061833">[-]</label><label class="expand" for="c-39061833">[2 more]</label></div><br/><div class="children"><div class="content">The hardware minimums are real, and the complexity floor is significant. Do <i>not</i> deploy Ceph unless you mean it.<p>I started considering alternatives when my NAS crossed 100 TB of HDDs, and when a scary scrub prompted me to replace all the HDDs, I finally pulled the trigger. (ZFS resilvered everything fine, but replacing every disk sequentially gave me a lot of time to think.) Today I have far more HDD capacity and a few hundred terabytes of NVMe, and despite its challenges, I wouldn&#x27;t dare run anything like it without Ceph.</div><br/><div id="39063874" class="c"><input type="checkbox" id="c-39063874" checked=""/><div class="controls bullet"><span class="by">samcat116</span><span>|</span><a href="#39061077">root</a><span>|</span><a href="#39061833">parent</a><span>|</span><a href="#39062971">next</a><span>|</span><label class="collapse" for="c-39063874">[-]</label><label class="expand" for="c-39063874">[1 more]</label></div><br/><div class="children"><div class="content">Can I ask what you use all that storage for on your NAS?</div><br/></div></div></div></div><div id="39062971" class="c"><input type="checkbox" id="c-39062971" checked=""/><div class="controls bullet"><span class="by">mmerlin</span><span>|</span><a href="#39061077">parent</a><span>|</span><a href="#39061833">prev</a><span>|</span><a href="#39061151">next</a><span>|</span><label class="collapse" for="c-39062971">[-]</label><label class="expand" for="c-39062971">[1 more]</label></div><br/><div class="children"><div class="content">Proxmox makes Ceph easy, even with just one single server if you are homelabbing...<p>I had 4 NUCs running Proxmox+Ceph for a few years, and apart from slightly annoying slowness syncing after spinning the machines up from cold start, it all ran very smoothly.</div><br/></div></div><div id="39061151" class="c"><input type="checkbox" id="c-39061151" checked=""/><div class="controls bullet"><span class="by">bluedino</span><span>|</span><a href="#39061077">parent</a><span>|</span><a href="#39062971">prev</a><span>|</span><a href="#39061139">next</a><span>|</span><label class="collapse" for="c-39061151">[-]</label><label class="expand" for="c-39061151">[4 more]</label></div><br/><div class="children"><div class="content">Related question, how does someone get into working with Ceph? Other than working somewhere that already uses it.</div><br/><div id="39063142" class="c"><input type="checkbox" id="c-39063142" checked=""/><div class="controls bullet"><span class="by">hathawsh</span><span>|</span><a href="#39061077">root</a><span>|</span><a href="#39061151">parent</a><span>|</span><a href="#39061338">next</a><span>|</span><label class="collapse" for="c-39063142">[-]</label><label class="expand" for="c-39063142">[1 more]</label></div><br/><div class="children"><div class="content">The recommended way to set up Ceph is cephadm, a single-file Python script that is a multi-tool for both creating and administering clusters.<p><a href="https:&#x2F;&#x2F;docs.ceph.com&#x2F;en&#x2F;latest&#x2F;cephadm&#x2F;" rel="nofollow">https:&#x2F;&#x2F;docs.ceph.com&#x2F;en&#x2F;latest&#x2F;cephadm&#x2F;</a><p>To learn about Ceph, I recommend you create at least 3 KVM virtual machines (using virt-manager) on a development box, network them together, and use cephadm to set up a cluster between the VMs. The RAM and storage requirements aren&#x27;t huge (Ceph can run on Raspberry Pis, after all) and I find it a lot easier to figure things out when I have a desktop window for every node.<p>I recently set up Ceph twice. Now that Ceph (specifically RBD) is providing the storage for virtual machines, I can live-migrate VMs between hosts and reboot hosts (with zero guest downtime) anytime I need. I&#x27;m impressed with how well it works.</div><br/></div></div><div id="39061338" class="c"><input type="checkbox" id="c-39061338" checked=""/><div class="controls bullet"><span class="by">SteveNuts</span><span>|</span><a href="#39061077">root</a><span>|</span><a href="#39061151">parent</a><span>|</span><a href="#39063142">prev</a><span>|</span><a href="#39061467">next</a><span>|</span><label class="collapse" for="c-39061338">[-]</label><label class="expand" for="c-39061338">[1 more]</label></div><br/><div class="children"><div class="content">You could start by installing Proxmox on old machines you have, it uses Ceph for its distributed storage, if you choose to use it.</div><br/></div></div><div id="39061467" class="c"><input type="checkbox" id="c-39061467" checked=""/><div class="controls bullet"><span class="by">candiddevmike</span><span>|</span><a href="#39061077">root</a><span>|</span><a href="#39061151">parent</a><span>|</span><a href="#39061338">prev</a><span>|</span><a href="#39061139">next</a><span>|</span><label class="collapse" for="c-39061467">[-]</label><label class="expand" for="c-39061467">[1 more]</label></div><br/><div class="children"><div class="content">Look into the Rook project</div><br/></div></div></div></div><div id="39061139" class="c"><input type="checkbox" id="c-39061139" checked=""/><div class="controls bullet"><span class="by">loeg</span><span>|</span><a href="#39061077">parent</a><span>|</span><a href="#39061151">prev</a><span>|</span><a href="#39061477">next</a><span>|</span><label class="collapse" for="c-39061139">[-]</label><label class="expand" for="c-39061139">[9 more]</label></div><br/><div class="children"><div class="content">Why would you bother with a distributed filesystem when you don&#x27;t have to?</div><br/><div id="39061561" class="c"><input type="checkbox" id="c-39061561" checked=""/><div class="controls bullet"><span class="by">imiric</span><span>|</span><a href="#39061077">root</a><span>|</span><a href="#39061139">parent</a><span>|</span><a href="#39061959">next</a><span>|</span><label class="collapse" for="c-39061561">[-]</label><label class="expand" for="c-39061561">[2 more]</label></div><br/><div class="children"><div class="content">For the same reason you would use one in enterprise deployments: if setup properly, it&#x27;s easier to scale. You don&#x27;t need to invest in a huge storage server upfront, but could build it out as needed with cheap nodes. Assuming it works painlessly as a single node filesystem, of which I&#x27;m not yet convinced if the existing solutions do.</div><br/><div id="39062677" class="c"><input type="checkbox" id="c-39062677" checked=""/><div class="controls bullet"><span class="by">loeg</span><span>|</span><a href="#39061077">root</a><span>|</span><a href="#39061561">parent</a><span>|</span><a href="#39061959">next</a><span>|</span><label class="collapse" for="c-39062677">[-]</label><label class="expand" for="c-39062677">[1 more]</label></div><br/><div class="children"><div class="content">&gt; if setup properly, it&#x27;s easier to scale<p>For home use&#x2F;needs, I think vertical scaling is much easier.</div><br/></div></div></div></div><div id="39061959" class="c"><input type="checkbox" id="c-39061959" checked=""/><div class="controls bullet"><span class="by">m463</span><span>|</span><a href="#39061077">root</a><span>|</span><a href="#39061139">parent</a><span>|</span><a href="#39061561">prev</a><span>|</span><a href="#39061246">next</a><span>|</span><label class="collapse" for="c-39061959">[-]</label><label class="expand" for="c-39061959">[1 more]</label></div><br/><div class="children"><div class="content">lol, wrong place to ask questions of such practicality.<p>that said, I played with virtualization and I didn&#x27;t need to.<p>but then I retired a machine or two and it has been very helpful.<p>And I used to just use physical disks and partitions.  But with the VMs I started using volume manager.  It became easier to grow and shrink storage.<p>and...<p>well, now a lot of this is second nature.  I can spin up a new &quot;machine&quot; for a project and it doesn&#x27;t affect anything else.  I have better backups.  I can move a virtual machine.<p>yeah, there are extra layers of abstraction but hey.</div><br/></div></div><div id="39061246" class="c"><input type="checkbox" id="c-39061246" checked=""/><div class="controls bullet"><span class="by">iwontberude</span><span>|</span><a href="#39061077">root</a><span>|</span><a href="#39061139">parent</a><span>|</span><a href="#39061959">prev</a><span>|</span><a href="#39062049">next</a><span>|</span><label class="collapse" for="c-39061246">[-]</label><label class="expand" for="c-39061246">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s cool to cluster everything for some people (myself included). I see it more like a design constraint than a pure benefit.</div><br/></div></div><div id="39062049" class="c"><input type="checkbox" id="c-39062049" checked=""/><div class="controls bullet"><span class="by">erulabs</span><span>|</span><a href="#39061077">root</a><span>|</span><a href="#39061139">parent</a><span>|</span><a href="#39061246">prev</a><span>|</span><a href="#39063415">next</a><span>|</span><label class="collapse" for="c-39062049">[-]</label><label class="expand" for="c-39062049">[3 more]</label></div><br/><div class="children"><div class="content">So that when you <i>do</i> have to, you know how to do it.</div><br/><div id="39063125" class="c"><input type="checkbox" id="c-39063125" checked=""/><div class="controls bullet"><span class="by">loeg</span><span>|</span><a href="#39061077">root</a><span>|</span><a href="#39062049">parent</a><span>|</span><a href="#39063415">next</a><span>|</span><label class="collapse" for="c-39063125">[-]</label><label class="expand" for="c-39063125">[2 more]</label></div><br/><div class="children"><div class="content">I think most of us will go our whole lives never having to deploy Ceph, especially at home.</div><br/><div id="39064354" class="c"><input type="checkbox" id="c-39064354" checked=""/><div class="controls bullet"><span class="by">erulabs</span><span>|</span><a href="#39061077">root</a><span>|</span><a href="#39063125">parent</a><span>|</span><a href="#39063415">next</a><span>|</span><label class="collapse" for="c-39064354">[-]</label><label class="expand" for="c-39064354">[1 more]</label></div><br/><div class="children"><div class="content">You’re absolutely not wrong - but asking a devops engineer why they over engineered their home cluster is sort of like asking a mechanic “why is your car so fast? Couldn’t you just take the bus?”</div><br/></div></div></div></div></div></div><div id="39063415" class="c"><input type="checkbox" id="c-39063415" checked=""/><div class="controls bullet"><span class="by">matheusmoreira</span><span>|</span><a href="#39061077">root</a><span>|</span><a href="#39061139">parent</a><span>|</span><a href="#39062049">prev</a><span>|</span><a href="#39061477">next</a><span>|</span><label class="collapse" for="c-39063415">[-]</label><label class="expand" for="c-39063415">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m indifferent towards the distributed nature thing. What I want is ceph&#x27;s ability to pool any combination of drives of any make, model and capacity into organized redundant fault tolerant storage, and its ability to add arbitrary drives to that pool at any point in the system&#x27;s lifetime. RAID-like solutions require identical drives and can&#x27;t be easily expanded.</div><br/></div></div></div></div><div id="39061477" class="c"><input type="checkbox" id="c-39061477" checked=""/><div class="controls bullet"><span class="by">m463</span><span>|</span><a href="#39061077">parent</a><span>|</span><a href="#39061139">prev</a><span>|</span><a href="#39063028">next</a><span>|</span><label class="collapse" for="c-39061477">[-]</label><label class="expand" for="c-39061477">[1 more]</label></div><br/><div class="children"><div class="content">I think you need 3 or was it 5 machines?<p>proxmox will use it - just click to install</div><br/></div></div><div id="39063028" class="c"><input type="checkbox" id="c-39063028" checked=""/><div class="controls bullet"><span class="by">louwrentius</span><span>|</span><a href="#39061077">parent</a><span>|</span><a href="#39061477">prev</a><span>|</span><a href="#39064274">next</a><span>|</span><label class="collapse" for="c-39063028">[-]</label><label class="expand" for="c-39063028">[1 more]</label></div><br/><div class="children"><div class="content">If you want decent performance, you need a lot of OSDs especially if you use HDD. But a lot of consumer SDDs will suffer terrible performance degradation with writes depending on the circumstances and workloads.</div><br/></div></div></div></div><div id="39064274" class="c"><input type="checkbox" id="c-39064274" checked=""/><div class="controls bullet"><span class="by">hinkley</span><span>|</span><a href="#39061077">prev</a><span>|</span><a href="#39063005">next</a><span>|</span><label class="collapse" for="c-39064274">[-]</label><label class="expand" for="c-39064274">[1 more]</label></div><br/><div class="children"><div class="content">Sure would be nice if you defined some acronyms.</div><br/></div></div><div id="39063005" class="c"><input type="checkbox" id="c-39063005" checked=""/><div class="controls bullet"><span class="by">louwrentius</span><span>|</span><a href="#39064274">prev</a><span>|</span><a href="#39062340">next</a><span>|</span><label class="collapse" for="c-39063005">[-]</label><label class="expand" for="c-39063005">[1 more]</label></div><br/><div class="children"><div class="content">Remember, random IOPs without latency is a meaningless figure.</div><br/></div></div><div id="39062340" class="c"><input type="checkbox" id="c-39062340" checked=""/><div class="controls bullet"><span class="by">einpoklum</span><span>|</span><a href="#39063005">prev</a><span>|</span><a href="#39063476">next</a><span>|</span><label class="collapse" for="c-39062340">[-]</label><label class="expand" for="c-39062340">[3 more]</label></div><br/><div class="children"><div class="content">Where can I read about the rationale for ceph as a project? I&#x27;m not familiar with it.</div><br/><div id="39063069" class="c"><input type="checkbox" id="c-39063069" checked=""/><div class="controls bullet"><span class="by">jacobwg</span><span>|</span><a href="#39062340">parent</a><span>|</span><a href="#39062906">next</a><span>|</span><label class="collapse" for="c-39063069">[-]</label><label class="expand" for="c-39063069">[1 more]</label></div><br/><div class="children"><div class="content">Not sure how common the use-case is, but we&#x27;re using Ceph to effectively roll our own EBS inside AWS on top of i3en EC2 instances. For us it&#x27;s about 30% cheaper than the base EBS cost, but provides access to 10x the IOPS of base gp3 volumes.<p>The downside is durability and operations - we have to keep Ceph alive and are responsible for making sure the data is persistent. That said, we&#x27;re storing cache from container builds, so in the worst-case where we lose the storage cluster, we can run builds without cache while we restore.</div><br/></div></div><div id="39062906" class="c"><input type="checkbox" id="c-39062906" checked=""/><div class="controls bullet"><span class="by">jseutter</span><span>|</span><a href="#39062340">parent</a><span>|</span><a href="#39063069">prev</a><span>|</span><a href="#39063476">next</a><span>|</span><label class="collapse" for="c-39062906">[-]</label><label class="expand" for="c-39062906">[1 more]</label></div><br/><div class="children"><div class="content"><a href="http:&#x2F;&#x2F;www.45drives.com&#x2F;blog&#x2F;ceph&#x2F;what-is-ceph-why-our-customers-love-it&#x2F;" rel="nofollow">http:&#x2F;&#x2F;www.45drives.com&#x2F;blog&#x2F;ceph&#x2F;what-is-ceph-why-our-custo...</a> is a pretty good introduction.  Basically you can take off-the-shelf hardware and keep expanding your storage cluster and ceph will scale fairly linearly up through hundreds of nodes.  It is seeing quite a bit of use in things like Kubernetes and OpenShift as a cheap and cheerful alternative to SANs.  It is not without complexity, so if you don&#x27;t know you need it, it&#x27;s probably not worth the hassle.</div><br/></div></div></div></div><div id="39063476" class="c"><input type="checkbox" id="c-39063476" checked=""/><div class="controls bullet"><span class="by">peter_d_sherman</span><span>|</span><a href="#39062340">prev</a><span>|</span><a href="#39060935">next</a><span>|</span><label class="collapse" for="c-39063476">[-]</label><label class="expand" for="c-39063476">[4 more]</label></div><br/><div class="children"><div class="content">Ceph is interesting... open source software whose only purpose is to implement a distributed file system...<p>Functionally, Linux implements a file system (well, several!) as well (in addition to many other OS features) -- but (usually!) only on top of local hardware.<p>There seems to be some missing software here -- if we examine these two paradigms side-by-side.<p>For example, what if I want a Linux (or more broadly, a general OS) -- but one that doesn&#x27;t manage a local file system or local storage at all?<p>One that operates solely using the network, solely using a distributed file system that Ceph, or software like Ceph, would provide?<p>Conversely, what if I don&#x27;t want to run a full OS on a network machine, a network node that manages its own local storage?<p>The only thing I can think of to solve those types of problems -- is:<p><i>What if the Linux filesystem was written such that it was a completely separate piece of software, and a distributed file system like Ceph, and not dependent on the other kernel source code</i> (although, still complilable into the kernel as most linux components normally are)...<p>A lot of work?  Probably!<p>But there seems to be some software need for something between a solely distributed file system as Ceph is, and a completely monolithic &quot;everything baked in&quot; (but not distributed!) OS&#x2F;kernel as Linux is...<p>Note that I am just thinking aloud here -- I probably am wrong and&#x2F;or misinformed on one or more fronts!<p>So, kindly take this random &quot;thinking aloud&quot; post -- with the proverbial &quot;grain of salt!&quot; :-)</div><br/><div id="39063617" class="c"><input type="checkbox" id="c-39063617" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#39063476">parent</a><span>|</span><a href="#39060935">next</a><span>|</span><label class="collapse" for="c-39063617">[-]</label><label class="expand" for="c-39063617">[3 more]</label></div><br/><div class="children"><div class="content"><i>what if I want a Linux ... that doesn&#x27;t manage a local file system or local storage at all [but] operates solely using the network, solely using a distributed file system</i><p>Linux can boot from NFS although that&#x27;s kind of lost knowledge. Booting from CephFS might even be possible if you put the right parts in the initrd.</div><br/><div id="39064107" class="c"><input type="checkbox" id="c-39064107" checked=""/><div class="controls bullet"><span class="by">lmz</span><span>|</span><a href="#39063476">root</a><span>|</span><a href="#39063617">parent</a><span>|</span><a href="#39060935">next</a><span>|</span><label class="collapse" for="c-39064107">[-]</label><label class="expand" for="c-39064107">[2 more]</label></div><br/><div class="children"><div class="content">NFS root docs here  <a href="https:&#x2F;&#x2F;www.kernel.org&#x2F;doc&#x2F;Documentation&#x2F;filesystems&#x2F;nfs&#x2F;nfsroot.txt" rel="nofollow">https:&#x2F;&#x2F;www.kernel.org&#x2F;doc&#x2F;Documentation&#x2F;filesystems&#x2F;nfs&#x2F;nfs...</a></div><br/><div id="39065229" class="c"><input type="checkbox" id="c-39065229" checked=""/><div class="controls bullet"><span class="by">peter_d_sherman</span><span>|</span><a href="#39063476">root</a><span>|</span><a href="#39064107">parent</a><span>|</span><a href="#39060935">next</a><span>|</span><label class="collapse" for="c-39065229">[-]</label><label class="expand" for="c-39065229">[1 more]</label></div><br/><div class="children"><div class="content">NFS is an excellent point!<p>NFS (now that I think about it!) -- brings up two additional software engineering considerations:<p>1) Distributed <i>file system protocol</i>.<p>2) Software that implements that distributed (or at least remote&#x2F;network) file system -- via that <i>file system protocol</i>.<p>NFS is both.<p>That&#x27;s not a bad thing(!) -- but ideally from a software engineering &quot;separation of concerns&quot; perspective, this future software layer&#x2F;level would ideally be decoupled from the underlying protocol -- that is, it might have a &quot;plug-in&quot; protocol architecture, where various 3rd party file system protocols (somewhat analogous to drivers) could be &quot;plugged-in&quot;...<p>But NFS could definitely be used to boot&#x2F;run Linux over the network, and is definitely a step in the right direction, and something worth evaluating for these purposes... its source code is definitely worth looking at...<p>So, an excellent point!</div><br/></div></div></div></div></div></div></div></div><div id="39060935" class="c"><input type="checkbox" id="c-39060935" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#39063476">prev</a><span>|</span><label class="collapse" for="c-39060935">[-]</label><label class="expand" for="c-39060935">[23 more]</label></div><br/><div class="children"><div class="content">What router&#x2F;switch one would use for such speed?</div><br/><div id="39061232" class="c"><input type="checkbox" id="c-39061232" checked=""/><div class="controls bullet"><span class="by">NavinF</span><span>|</span><a href="#39060935">parent</a><span>|</span><a href="#39061201">next</a><span>|</span><label class="collapse" for="c-39061232">[-]</label><label class="expand" for="c-39061232">[13 more]</label></div><br/><div class="children"><div class="content">Linked article says they used 68 machines with 2 x 100GbE Mellanox ConnectX-6 cards. So any 100G pizza box switches should work.<p>Note that 36 port 56G switches are dirt cheap on eBay and 4tbps is good enough for most homelab use cases</div><br/><div id="39061272" class="c"><input type="checkbox" id="c-39061272" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#39060935">root</a><span>|</span><a href="#39061232">parent</a><span>|</span><a href="#39061201">next</a><span>|</span><label class="collapse" for="c-39061272">[-]</label><label class="expand" for="c-39061272">[12 more]</label></div><br/><div class="children"><div class="content">&gt; So any 100G pizza box switches should work.<p>but will it be able to handle combined TB&#x2F;s traffic?</div><br/><div id="39061402" class="c"><input type="checkbox" id="c-39061402" checked=""/><div class="controls bullet"><span class="by">aaronax</span><span>|</span><a href="#39060935">root</a><span>|</span><a href="#39061272">parent</a><span>|</span><a href="#39061397">next</a><span>|</span><label class="collapse" for="c-39061402">[-]</label><label class="expand" for="c-39061402">[1 more]</label></div><br/><div class="children"><div class="content">Yes.  Most network switches can handle all ports at 100% utilization in both directions simultaneously.<p>Take for example the Mellanox SX6790 available for less than $100 on eBay.  It has 36 56gbps ports.  36 * 2 * 56 = 4032gbps and it is stated to have a switching capacity of 4.032Tbps.<p>Edit: I guess you are asking how one would possibly sip 1TiB&#x2F;s of data into a given client.  You would need multiple clients spread across several switches to generate such load.  Or maybe some freaky link aggregation.  10x 800gbps links for your client, plus at least 10x 800gbps links out to the servers.</div><br/></div></div><div id="39061397" class="c"><input type="checkbox" id="c-39061397" checked=""/><div class="controls bullet"><span class="by">baq</span><span>|</span><a href="#39060935">root</a><span>|</span><a href="#39061272">parent</a><span>|</span><a href="#39061402">prev</a><span>|</span><a href="#39061495">next</a><span>|</span><label class="collapse" for="c-39061397">[-]</label><label class="expand" for="c-39061397">[3 more]</label></div><br/><div class="children"><div class="content">any switch which can&#x27;t handle full load on all ports isn&#x27;t worthy of the name &#x27;switch&#x27;, it&#x27;s more like &#x27;toy network appliance&#x27;</div><br/><div id="39061482" class="c"><input type="checkbox" id="c-39061482" checked=""/><div class="controls bullet"><span class="by">birdman3131</span><span>|</span><a href="#39060935">root</a><span>|</span><a href="#39061397">parent</a><span>|</span><a href="#39061495">next</a><span>|</span><label class="collapse" for="c-39061482">[-]</label><label class="expand" for="c-39061482">[2 more]</label></div><br/><div class="children"><div class="content">I will forever be scarred by the &quot;Gigabit&quot; switches of old that were 2 gigabit ports and 22 100mb ports. Coworker bought it missing the nuance.</div><br/><div id="39061518" class="c"><input type="checkbox" id="c-39061518" checked=""/><div class="controls bullet"><span class="by">bombcar</span><span>|</span><a href="#39060935">root</a><span>|</span><a href="#39061482">parent</a><span>|</span><a href="#39061495">next</a><span>|</span><label class="collapse" for="c-39061518">[-]</label><label class="expand" for="c-39061518">[1 more]</label></div><br/><div class="children"><div class="content">Still happens, gotta see if the top speed mentioned is an uplink or normal ports.</div><br/></div></div></div></div></div></div><div id="39061495" class="c"><input type="checkbox" id="c-39061495" checked=""/><div class="controls bullet"><span class="by">bombcar</span><span>|</span><a href="#39060935">root</a><span>|</span><a href="#39061272">parent</a><span>|</span><a href="#39061397">prev</a><span>|</span><a href="#39061201">next</a><span>|</span><label class="collapse" for="c-39061495">[-]</label><label class="expand" for="c-39061495">[7 more]</label></div><br/><div class="children"><div class="content">Even the bargain Mikrotik can do 1.2Tbps <a href="https:&#x2F;&#x2F;mikrotik.com&#x2F;product&#x2F;crs518_16xs_2xq" rel="nofollow">https:&#x2F;&#x2F;mikrotik.com&#x2F;product&#x2F;crs518_16xs_2xq</a></div><br/><div id="39061545" class="c"><input type="checkbox" id="c-39061545" checked=""/><div class="controls bullet"><span class="by">margalabargala</span><span>|</span><a href="#39060935">root</a><span>|</span><a href="#39061495">parent</a><span>|</span><a href="#39061549">next</a><span>|</span><label class="collapse" for="c-39061545">[-]</label><label class="expand" for="c-39061545">[5 more]</label></div><br/><div class="children"><div class="content">For those curious, a &quot;bargain&quot; on a 100gbps switch means about $1350</div><br/><div id="39061966" class="c"><input type="checkbox" id="c-39061966" checked=""/><div class="controls bullet"><span class="by">epistasis</span><span>|</span><a href="#39060935">root</a><span>|</span><a href="#39061545">parent</a><span>|</span><a href="#39062640">next</a><span>|</span><label class="collapse" for="c-39061966">[-]</label><label class="expand" for="c-39061966">[3 more]</label></div><br/><div class="children"><div class="content">On a cluster with more than $1M of NVMe disks, that does actually seem like a bargain.<p>(Note that the linked MikroTik switch only has 100gbe on a few ports, and wouldn&#x27;t really classify as a full 100gbe switch to most people)</div><br/><div id="39062176" class="c"><input type="checkbox" id="c-39062176" checked=""/><div class="controls bullet"><span class="by">margalabargala</span><span>|</span><a href="#39060935">root</a><span>|</span><a href="#39061966">parent</a><span>|</span><a href="#39062640">next</a><span>|</span><label class="collapse" for="c-39062176">[-]</label><label class="expand" for="c-39062176">[2 more]</label></div><br/><div class="children"><div class="content">Sure- I don&#x27;t mean to imply that it isn&#x27;t. I can absolutely see how that&#x27;s inexpensive for 100gbe equipment.<p>That was more for the benefit of others like myself, who were wondering if &quot;bargain&quot; was comparative, or inexpensive enough that it might be worth buying one next time they upgraded switches. For me personally it&#x27;s still an order of magnitude away from that.</div><br/><div id="39062725" class="c"><input type="checkbox" id="c-39062725" checked=""/><div class="controls bullet"><span class="by">bombcar</span><span>|</span><a href="#39060935">root</a><span>|</span><a href="#39062176">parent</a><span>|</span><a href="#39062640">next</a><span>|</span><label class="collapse" for="c-39062725">[-]</label><label class="expand" for="c-39062725">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;mikrotik.com&#x2F;product&#x2F;crs305_1g_4s_in" rel="nofollow">https:&#x2F;&#x2F;mikrotik.com&#x2F;product&#x2F;crs305_1g_4s_in</a> is the sweet spot right now for home users. Four 10g ports and a 1g, you can use the 1g for “uplink” to the internet and one of the 10g for your “big old Nortel gigabit switch with 10g uplink” and one for your Mac and two for your NAS and VM server. ;)<p>Direct cables are moderately cheap, and modules for 10g Ethernet aren’t insanely expensive.</div><br/></div></div></div></div></div></div><div id="39062640" class="c"><input type="checkbox" id="c-39062640" checked=""/><div class="controls bullet"><span class="by">Palomides</span><span>|</span><a href="#39060935">root</a><span>|</span><a href="#39061545">parent</a><span>|</span><a href="#39061966">prev</a><span>|</span><a href="#39061549">next</a><span>|</span><label class="collapse" for="c-39062640">[-]</label><label class="expand" for="c-39062640">[1 more]</label></div><br/><div class="children"><div class="content">there&#x27;s usually some used dx010 (32x100gbe) on ebay for less than $500<p>the cheapest new 100gbe switch I know of is the mikrotik CRS504-4XQ-IN (4x100gbe, around $650)</div><br/></div></div></div></div><div id="39061549" class="c"><input type="checkbox" id="c-39061549" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#39060935">root</a><span>|</span><a href="#39061495">parent</a><span>|</span><a href="#39061545">prev</a><span>|</span><a href="#39061201">next</a><span>|</span><label class="collapse" for="c-39061549">[-]</label><label class="expand" for="c-39061549">[1 more]</label></div><br/><div class="children"><div class="content">TB != Tb..</div><br/></div></div></div></div></div></div></div></div><div id="39061201" class="c"><input type="checkbox" id="c-39061201" checked=""/><div class="controls bullet"><span class="by">epistasis</span><span>|</span><a href="#39060935">parent</a><span>|</span><a href="#39061232">prev</a><span>|</span><a href="#39061046">next</a><span>|</span><label class="collapse" for="c-39061201">[-]</label><label class="expand" for="c-39061201">[1 more]</label></div><br/><div class="children"><div class="content">Given their configuration of just 4U spread across 17 racks, there&#x27;s likely a bunch of compute in the rest of the rack, and 1-2 top of rack switches like this:<p><a href="https:&#x2F;&#x2F;www.qct.io&#x2F;product&#x2F;index&#x2F;Switch&#x2F;Ethernet-Switch&#x2F;T7000-Series&#x2F;QuantaMesh-T7032-IX7D" rel="nofollow">https:&#x2F;&#x2F;www.qct.io&#x2F;product&#x2F;index&#x2F;Switch&#x2F;Ethernet-Switch&#x2F;T700...</a><p>And then you connect the TOR switches to higher level switches in something like a Clos distribution to get the desired bandwidth between any two nodes:<p><a href="https:&#x2F;&#x2F;www.techtarget.com&#x2F;searchnetworking&#x2F;definition&#x2F;Clos-network" rel="nofollow">https:&#x2F;&#x2F;www.techtarget.com&#x2F;searchnetworking&#x2F;definition&#x2F;Clos-...</a></div><br/></div></div><div id="39061046" class="c"><input type="checkbox" id="c-39061046" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#39060935">parent</a><span>|</span><a href="#39061201">prev</a><span>|</span><label class="collapse" for="c-39061046">[-]</label><label class="expand" for="c-39061046">[8 more]</label></div><br/><div class="children"><div class="content">800Gbps via OSFP and QSFP-DD are already a thing. Multiple vendors have NICs and switches for that.</div><br/><div id="39061511" class="c"><input type="checkbox" id="c-39061511" checked=""/><div class="controls bullet"><span class="by">_zoltan_</span><span>|</span><a href="#39060935">root</a><span>|</span><a href="#39061046">parent</a><span>|</span><a href="#39062428">next</a><span>|</span><label class="collapse" for="c-39061511">[-]</label><label class="expand" for="c-39061511">[2 more]</label></div><br/><div class="children"><div class="content">can you show me a 800G NIC?<p>the switch is fine, I&#x27;m buying 64x800G switches, but NIC wise I&#x27;m limited to 400Gbit.</div><br/><div id="39062516" class="c"><input type="checkbox" id="c-39062516" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#39060935">root</a><span>|</span><a href="#39061511">parent</a><span>|</span><a href="#39062428">next</a><span>|</span><label class="collapse" for="c-39062516">[-]</label><label class="expand" for="c-39062516">[1 more]</label></div><br/><div class="children"><div class="content">fair enough, it seems I was mistaken about the NIC. I guess that has to wait for PCIe 6 and should arrive soon-ish.</div><br/></div></div></div></div><div id="39062428" class="c"><input type="checkbox" id="c-39062428" checked=""/><div class="controls bullet"><span class="by">CyberDildonics</span><span>|</span><a href="#39060935">root</a><span>|</span><a href="#39061046">parent</a><span>|</span><a href="#39061511">prev</a><span>|</span><label class="collapse" for="c-39062428">[-]</label><label class="expand" for="c-39062428">[5 more]</label></div><br/><div class="children"><div class="content">16x PCIe 4.0 is 32GB&#x2F;s  16x PCIe 5.0 should be 64 GB&#x2F;s, how is any computer using 100 GB&#x2F;s ?</div><br/><div id="39062531" class="c"><input type="checkbox" id="c-39062531" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#39060935">root</a><span>|</span><a href="#39062428">parent</a><span>|</span><label class="collapse" for="c-39062531">[-]</label><label class="expand" for="c-39062531">[4 more]</label></div><br/><div class="children"><div class="content">I was talking about Gigabit&#x2F;s, not Gigabyte&#x2F;s.<p>The article however actually talks about Terabyte&#x2F;s scale, albeit not over a single node.</div><br/><div id="39062556" class="c"><input type="checkbox" id="c-39062556" checked=""/><div class="controls bullet"><span class="by">CyberDildonics</span><span>|</span><a href="#39060935">root</a><span>|</span><a href="#39062531">parent</a><span>|</span><label class="collapse" for="c-39062556">[-]</label><label class="expand" for="c-39062556">[3 more]</label></div><br/><div class="children"><div class="content">800 gigabits is 100 gigabytes which is still more than PCIe 5.0 16x 64 gigabyte per second bandwidth.<p>You said there were 800 gigabit network cards, I&#x27;m wondering how that much bandwidth makes it to the card in the first place.<p><i>The article however actually talks about Terabyte&#x2F;s scale, albeit not over a single node.</i><p>This does not have anything to do with what you originally said, you were talking about 800gb single ports.</div><br/><div id="39063410" class="c"><input type="checkbox" id="c-39063410" checked=""/><div class="controls bullet"><span class="by">NavinF</span><span>|</span><a href="#39060935">root</a><span>|</span><a href="#39062556">parent</a><span>|</span><a href="#39062773">next</a><span>|</span><label class="collapse" for="c-39063410">[-]</label><label class="expand" for="c-39063410">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not aware of any 800G cards, but FYI a single Mellanox card can use two PCIe x16 slots to avoid NUMA issues on dual-socket servers: <a href="https:&#x2F;&#x2F;www.nvidia.com&#x2F;en-us&#x2F;networking&#x2F;ethernet&#x2F;socket-direct&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.nvidia.com&#x2F;en-us&#x2F;networking&#x2F;ethernet&#x2F;socket-dire...</a><p>So the software infra for using multiple slots already exists and doesn&#x27;t require any special config. Oh and some cards can use PCIe slots across multiple hosts. No idea why you&#x27;d want to do that, but you can.</div><br/></div></div><div id="39062773" class="c"><input type="checkbox" id="c-39062773" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#39060935">root</a><span>|</span><a href="#39062556">parent</a><span>|</span><a href="#39063410">prev</a><span>|</span><label class="collapse" for="c-39062773">[-]</label><label class="expand" for="c-39062773">[1 more]</label></div><br/><div class="children"><div class="content">Yes, apparently I was mistaken about the NICs. They don&#x27;t seem to be available yet.<p>But it&#x27;s not a PCIe limitation. There are PCIe devices out there which use 32 lanes, so you could achieve the bandwidth even on PCIe5.<p><a href="https:&#x2F;&#x2F;www.servethehome.com&#x2F;ocp-nic-3-0-form-factors-quick-guide-intel-broadcom-nvidia-meta-inspur-dell-emc-hpe-lenovo-gigabyte-supermicro&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.servethehome.com&#x2F;ocp-nic-3-0-form-factors-quick-...</a></div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>