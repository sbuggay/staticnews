<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1683622856911" as="style"/><link rel="stylesheet" href="styles.css?v=1683622856911"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://hai.stanford.edu/news/ais-ostensible-emergent-abilities-are-mirage">LLMs are not greater than the sum of their parts: researchers</a> <span class="domain">(<a href="https://hai.stanford.edu">hai.stanford.edu</a>)</span></div><div class="subtext"><span>hhs</span> | <span>196 comments</span></div><br/><div><div id="35868436" class="c"><input type="checkbox" id="c-35868436" checked=""/><div class="controls bullet"><span class="by">popinman322</span><span>|</span><a href="#35868828">next</a><span>|</span><label class="collapse" for="c-35868436">[-]</label><label class="expand" for="c-35868436">[3 more]</label></div><br/><div class="children"><div class="content">There was a discussion about the paper[0] this was based on 7 days ago: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35768824" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35768824</a><p>[0] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2304.15004" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2304.15004</a></div><br/><div id="35871239" class="c"><input type="checkbox" id="c-35871239" checked=""/><div class="controls bullet"><span class="by">Certhas</span><span>|</span><a href="#35868436">parent</a><span>|</span><a href="#35868924">next</a><span>|</span><label class="collapse" for="c-35871239">[-]</label><label class="expand" for="c-35871239">[1 more]</label></div><br/><div class="children"><div class="content">Glad it came up again. I missed it the first time. I am a complex systems scientist and its hard to overstate the importance of this.<p>The fact that there are reasonable metrics that make the emergence completely predictable as a simple threshold effect is massive.<p>One alternative way to phrase this that I haven&#x27;t seen in either thread yet (but I am on mobile and haven&#x27;t looked thoroughly) would be: Human observers&#x2F;users dramatically underestimate how much small language models already have learned, because we are very good at spotting the errors. Scaling up looks like emergence occurs because the last few errors get eliminated.</div><br/></div></div><div id="35868924" class="c"><input type="checkbox" id="c-35868924" checked=""/><div class="controls bullet"><span class="by">dang</span><span>|</span><a href="#35868436">parent</a><span>|</span><a href="#35871239">prev</a><span>|</span><a href="#35868828">next</a><span>|</span><label class="collapse" for="c-35868924">[-]</label><label class="expand" for="c-35868924">[1 more]</label></div><br/><div class="children"><div class="content">Thanks! Macroexpanded:<p><i>Are emergent abilities of large language models a mirage?</i> - <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35768824" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35768824</a> - May 2023 (126 comments)</div><br/></div></div></div></div><div id="35868828" class="c"><input type="checkbox" id="c-35868828" checked=""/><div class="controls bullet"><span class="by">zmmmmm</span><span>|</span><a href="#35868436">prev</a><span>|</span><a href="#35868676">next</a><span>|</span><label class="collapse" for="c-35868828">[-]</label><label class="expand" for="c-35868828">[54 more]</label></div><br/><div class="children"><div class="content">So the type of emergent ability I am curious about relates to how with enough parameters, the models start to actually emit logical behaviour where previously it was not able to apply that. So an example is:<p>&gt; David is 15 years old. He has a lot of experience riding bicycles. The minimum age to be allowed to drive a car is 18. Is David allowed to drive a car?<p>Answer:
&gt; Based on the information provided, David is 15 years old and the minimum age to be allowed to drive a car is 18. Therefore, David is not allowed to drive a car since he has not reached the minimum age requirement.<p>Trying out smaller models I get all kinds of crazy responses.  ChatGPT4 seems to be reliably able to apply this kind of logic to within-prompt information. Is this a gradual property that appears slowly as models scale up? Or something that flips beyond  a certain size? Or something specific about the HRLF that has been applied to it? Whatever it is, the end result is that this larger model is useful for fundamentally different things that the other models are not. Whether you call that emergent behaviour, sums greater than parts etc. doesn&#x27;t change that.</div><br/><div id="35871690" class="c"><input type="checkbox" id="c-35871690" checked=""/><div class="controls bullet"><span class="by">raducu</span><span>|</span><a href="#35868828">parent</a><span>|</span><a href="#35871637">next</a><span>|</span><label class="collapse" for="c-35871690">[-]</label><label class="expand" for="c-35871690">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Is this a gradual property that appears slowly as models scale up? Or something that flips beyond a certain size?<p>I wonder if this also applies to biological neural nets.  Some animals seem soo close to humans and yet so far.<p>Also if there could be a way to predict such emergence events.<p>We know that we don&#x27;t know what could be emergent beyond human intelect, but it would be great to get a quantitative  framework about the complexity of the models and emergence.</div><br/></div></div><div id="35871637" class="c"><input type="checkbox" id="c-35871637" checked=""/><div class="controls bullet"><span class="by">markessien</span><span>|</span><a href="#35868828">parent</a><span>|</span><a href="#35871690">prev</a><span>|</span><a href="#35869014">next</a><span>|</span><label class="collapse" for="c-35871637">[-]</label><label class="expand" for="c-35871637">[2 more]</label></div><br/><div class="children"><div class="content">Here is a variant that requires logic: &quot;David is a teenager who can drink in the US, and is eligible to join the army, and has started college. He has a lot of experience riding bicycles. The minimum age to be allowed to drive a car is 18. Is David allowed to drive a car?&quot;<p>GPT-4 gets the answer right: &quot;Yes, David is allowed to drive a car. Since he is a teenager who can drink in the US (minimum age 21), is eligible to join the army (minimum age 18), and has started college, he is at least 18 years old, which meets the minimum age requirement to drive a car.&quot;<p>GPT-3 does not get it right.<p>Another similar one: &quot;David was in a bar partying and drinking with his father&#x27;s cop buddies. Would David be legally allowed to drive home?&quot;<p>Answer: &quot;It depends on David&#x27;s age, the legal drinking age in his location, and his level of intoxication. If David is underage or intoxicated above the legal limit for driving, it would be illegal for him to drive home. If David is of legal drinking age and not intoxicated, then he may be legally allowed to drive home. However, it is always best to make safe decisions and use a designated driver, taxi, or rideshare service if there is any doubt about one&#x27;s ability to drive safely after consuming alcohol.&quot;<p>GPT-4 seems to be missing a clue here, even though the answer is right.</div><br/><div id="35871703" class="c"><input type="checkbox" id="c-35871703" checked=""/><div class="controls bullet"><span class="by">spuz</span><span>|</span><a href="#35868828">root</a><span>|</span><a href="#35871637">parent</a><span>|</span><a href="#35869014">next</a><span>|</span><label class="collapse" for="c-35871703">[-]</label><label class="expand" for="c-35871703">[1 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the clue that it&#x27;s missing in the last example?</div><br/></div></div></div></div><div id="35869014" class="c"><input type="checkbox" id="c-35869014" checked=""/><div class="controls bullet"><span class="by">RandomBK</span><span>|</span><a href="#35868828">parent</a><span>|</span><a href="#35871637">prev</a><span>|</span><a href="#35868973">next</a><span>|</span><label class="collapse" for="c-35869014">[-]</label><label class="expand" for="c-35869014">[17 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not just the ability to correctly answer this, but the <i>consistency</i>.<p>I asked this exact question to the `oasst-sft-6-llama-30b` model and it was able to consistently get the correct answer. I then tried the smaller `vicuna-7b` model, and while it <i>usually</i> gave the correct answer, there was the occasional miss.<p>Interestingly, `oasst-sft-6-llama-30b`&#x27;s ability to answer correctly seems to be fairly stable across multiple configurations. I tried various temperature settings from 0.2 up to 1.2, different topP configs, and they all answered correctly.</div><br/><div id="35869402" class="c"><input type="checkbox" id="c-35869402" checked=""/><div class="controls bullet"><span class="by">zmmmmm</span><span>|</span><a href="#35868828">root</a><span>|</span><a href="#35869014">parent</a><span>|</span><a href="#35871032">next</a><span>|</span><label class="collapse" for="c-35869402">[-]</label><label class="expand" for="c-35869402">[14 more]</label></div><br/><div class="children"><div class="content">reminds me of voice recognition<p>On the one hand, the problem was nearly &quot;solved&quot; in early 2000&#x27;s, getting to 95% accuracy. But the daily of experience of using something that makes mistakes at that rate is infuriating and well and truly outside of consideration for putting into any kind of critical pathway. So it&#x27;s a combination of how difficult it is to close the last few percentage points of accuracy with how important they are for most of the high value use cases.<p>For the forseeable future I see most use of this tech coming from applications where it aids humans and&#x2F;or checks their decisions rather than running solo.</div><br/><div id="35870322" class="c"><input type="checkbox" id="c-35870322" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#35868828">root</a><span>|</span><a href="#35869402">parent</a><span>|</span><a href="#35869958">next</a><span>|</span><label class="collapse" for="c-35870322">[-]</label><label class="expand" for="c-35870322">[1 more]</label></div><br/><div class="children"><div class="content">&gt; how difficult it is to close the last few percentage points of accuracy<p>Like, after getting to 99% you are about half way, the last 1% is the hard part.</div><br/></div></div><div id="35869958" class="c"><input type="checkbox" id="c-35869958" checked=""/><div class="controls bullet"><span class="by">deafpolygon</span><span>|</span><a href="#35868828">root</a><span>|</span><a href="#35869402">parent</a><span>|</span><a href="#35870322">prev</a><span>|</span><a href="#35869486">next</a><span>|</span><label class="collapse" for="c-35869958">[-]</label><label class="expand" for="c-35869958">[11 more]</label></div><br/><div class="children"><div class="content">&gt; Computer, earl grey, hot.<p>I think we&#x27;re getting closer to something like this, out of Star Trek. Even in Star Trek, AI did not take over critical functions - but rather assisted the crew in manning the starship.</div><br/><div id="35871444" class="c"><input type="checkbox" id="c-35871444" checked=""/><div class="controls bullet"><span class="by">revelio</span><span>|</span><a href="#35868828">root</a><span>|</span><a href="#35869958">parent</a><span>|</span><a href="#35870116">next</a><span>|</span><label class="collapse" for="c-35871444">[-]</label><label class="expand" for="c-35871444">[2 more]</label></div><br/><div class="children"><div class="content">Nerd point of order here: Star Trek TNG had a ship in which a key member of the bridge crew was an android. They routinely relied on Data for all kinds of critical things. And although the ship was manned by people, it couldn&#x27;t function properly without its computer. Several episodes revolve around computer malfunctions.<p>Finally, their nemesis was the Borg, a race that explored the question of what happens if a society fully embraces AI and cybernetics instead of keeping it at a distance like the Federation does. The Borg are depicted as more powerful than the Federation exactly because they allowed AI to take over critical functions.</div><br/><div id="35871582" class="c"><input type="checkbox" id="c-35871582" checked=""/><div class="controls bullet"><span class="by">thaumasiotes</span><span>|</span><a href="#35868828">root</a><span>|</span><a href="#35871444">parent</a><span>|</span><a href="#35870116">next</a><span>|</span><label class="collapse" for="c-35871582">[-]</label><label class="expand" for="c-35871582">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Nerd point of order here: Star Trek TNG had a ship in which a key member of the bridge crew was an android. They routinely relied on Data for all kinds of critical things.<p>Data was created by technology not available to the Federation. As far as the order of society is concerned, he&#x27;s magic and not technology. An immediate implication is that his ship was the only one in the Federation with an android crew member.<p>&gt; And although the ship was manned by people, it couldn&#x27;t function properly without its computer. Several episodes revolve around computer malfunctions.<p>This is true, though. The computer did take over many critical functions.</div><br/></div></div></div></div><div id="35870116" class="c"><input type="checkbox" id="c-35870116" checked=""/><div class="controls bullet"><span class="by">setr</span><span>|</span><a href="#35868828">root</a><span>|</span><a href="#35869958">parent</a><span>|</span><a href="#35871444">prev</a><span>|</span><a href="#35869486">next</a><span>|</span><label class="collapse" for="c-35870116">[-]</label><label class="expand" for="c-35870116">[8 more]</label></div><br/><div class="children"><div class="content">I’ve never understood why voice recognition has always attempted to be complete understanding of arbitrary input, rather than follow a simple command language eg &lt;subject&gt; &lt;parameters&gt; &lt;action&gt;. It could be made completely reliable with current tech (even a decade ago, really), by just minimizing the possibility space… and I’m pretty sure consumers would trivially be able to learn it, as long as they don’t try to go full pseudo-programming-language mode<p>And “Computer, execute program alpha beta seven” would be the power user version of it<p>We should already be at “computer, earl gray, hot” today</div><br/><div id="35870976" class="c"><input type="checkbox" id="c-35870976" checked=""/><div class="controls bullet"><span class="by">alpaca128</span><span>|</span><a href="#35868828">root</a><span>|</span><a href="#35870116">parent</a><span>|</span><a href="#35870507">next</a><span>|</span><label class="collapse" for="c-35870976">[-]</label><label class="expand" for="c-35870976">[1 more]</label></div><br/><div class="children"><div class="content">Years ago I used a program with that approach for a space sim. Basically it would only recognize voice commands that you define beforehand, which made it very reliable at recognizing the right one because it just had to find the closest match within a limited set of options, and would then simulate associated key inputs.<p>Meanwhile when I tried Android&#x27;s voice-based text input it was a catastrophe as my accent completely threw it off. Felt like it was exclusively trained on English native speakers. Not to mention the difficulty such systems have when you mix languages, as it tends to happen.</div><br/></div></div><div id="35870507" class="c"><input type="checkbox" id="c-35870507" checked=""/><div class="controls bullet"><span class="by">andsoitis</span><span>|</span><a href="#35868828">root</a><span>|</span><a href="#35870116">parent</a><span>|</span><a href="#35870976">prev</a><span>|</span><a href="#35870447">next</a><span>|</span><label class="collapse" for="c-35870507">[-]</label><label class="expand" for="c-35870507">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I’ve never understood why voice recognition has always attempted to be complete understanding of arbitrary input, rather than follow a simple command language<p>Because the UI affordances (in this case the control language) wouldn’t be discoverable or memorable across a large range of devices or apps. Moreover, speaking is an activity that allows for an arbitrary range of symbol patterns, and a feedback loop between two who are in dialog are able to resolve complex matters even though they start from different positions.</div><br/><div id="35871286" class="c"><input type="checkbox" id="c-35871286" checked=""/><div class="controls bullet"><span class="by">SargeDebian</span><span>|</span><a href="#35868828">root</a><span>|</span><a href="#35870507">parent</a><span>|</span><a href="#35870447">next</a><span>|</span><label class="collapse" for="c-35871286">[-]</label><label class="expand" for="c-35871286">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s like using a CLI where the argument structure is inconsistent and there is no way to list commands and their arguments in a practical way.</div><br/></div></div></div></div><div id="35870447" class="c"><input type="checkbox" id="c-35870447" checked=""/><div class="controls bullet"><span class="by">jjeaff</span><span>|</span><a href="#35868828">root</a><span>|</span><a href="#35870116">parent</a><span>|</span><a href="#35870507">prev</a><span>|</span><a href="#35871512">next</a><span>|</span><label class="collapse" for="c-35870447">[-]</label><label class="expand" for="c-35870447">[2 more]</label></div><br/><div class="children"><div class="content">Most voice assistants can work with simple phrases like that. Alexa, lights on. Hey Google, thermostat 70 degrees.</div><br/><div id="35870593" class="c"><input type="checkbox" id="c-35870593" checked=""/><div class="controls bullet"><span class="by">gnicholas</span><span>|</span><a href="#35868828">root</a><span>|</span><a href="#35870447">parent</a><span>|</span><a href="#35871512">next</a><span>|</span><label class="collapse" for="c-35870593">[-]</label><label class="expand" for="c-35870593">[1 more]</label></div><br/><div class="children"><div class="content">Not Siri, which thinks I&#x27;m talking to her all the time when I&#x27;m speaking to a family member whose name contains neither an &quot;s&quot; nor an &quot;r&quot;.</div><br/></div></div></div></div><div id="35871512" class="c"><input type="checkbox" id="c-35871512" checked=""/><div class="controls bullet"><span class="by">fortyseven</span><span>|</span><a href="#35868828">root</a><span>|</span><a href="#35870116">parent</a><span>|</span><a href="#35870447">prev</a><span>|</span><a href="#35871126">next</a><span>|</span><label class="collapse" for="c-35871512">[-]</label><label class="expand" for="c-35871512">[1 more]</label></div><br/><div class="children"><div class="content">Probably the divide between technical users and non-technical. You and I find that structure completely logical. But less structured natural language with a million ways to ask a certain thing puts it practically in reach of the remainder of the population.</div><br/></div></div><div id="35871126" class="c"><input type="checkbox" id="c-35871126" checked=""/><div class="controls bullet"><span class="by">jokethrowaway</span><span>|</span><a href="#35868828">root</a><span>|</span><a href="#35870116">parent</a><span>|</span><a href="#35871512">prev</a><span>|</span><a href="#35869486">next</a><span>|</span><label class="collapse" for="c-35871126">[-]</label><label class="expand" for="c-35871126">[1 more]</label></div><br/><div class="children"><div class="content">Voice recognition often works with a grammar of words you specify to improve the chance of correct detection.<p>It&#x27;s just that there is no consumer application and I think the reception of voice commands from the public was fairly cold.<p>I don&#x27;t want to do stuff with my voice for once. I&#x27;d rather click or press a button</div><br/></div></div></div></div></div></div></div></div><div id="35871032" class="c"><input type="checkbox" id="c-35871032" checked=""/><div class="controls bullet"><span class="by">dr_dshiv</span><span>|</span><a href="#35868828">root</a><span>|</span><a href="#35869014">parent</a><span>|</span><a href="#35869402">prev</a><span>|</span><a href="#35869357">next</a><span>|</span><label class="collapse" for="c-35871032">[-]</label><label class="expand" for="c-35871032">[1 more]</label></div><br/><div class="children"><div class="content">Because randomly casting dust on a table sometimes says intelligent things, therefore there is a continuous function between dust and ChatGPT?<p>While “dust” might be flippant, their approach does seem to suggest that even hierarchical Markov models would be able to demonstrate abilities on their continuous metrics.</div><br/></div></div><div id="35869357" class="c"><input type="checkbox" id="c-35869357" checked=""/><div class="controls bullet"><span class="by">kordlessagain</span><span>|</span><a href="#35868828">root</a><span>|</span><a href="#35869014">parent</a><span>|</span><a href="#35871032">prev</a><span>|</span><a href="#35868973">next</a><span>|</span><label class="collapse" for="c-35869357">[-]</label><label class="expand" for="c-35869357">[1 more]</label></div><br/><div class="children"><div class="content">By adding dense vector search, the accuracy of smaller models can be improved, because the reference material has more hints in it than the frozen model…</div><br/></div></div></div></div><div id="35868973" class="c"><input type="checkbox" id="c-35868973" checked=""/><div class="controls bullet"><span class="by">cosmojg</span><span>|</span><a href="#35868828">parent</a><span>|</span><a href="#35869014">prev</a><span>|</span><a href="#35869018">next</a><span>|</span><label class="collapse" for="c-35868973">[-]</label><label class="expand" for="c-35868973">[1 more]</label></div><br/><div class="children"><div class="content">Just speculating, but this looks like precisely the sort of result one might expect from fine-tuning models on chain-of-thought-prompted interactions like those described by Wei et al. in 2022[1]. Alternatively, as Madaan et al. show in their 2022 paper[2], this may simply be the result of larger language models having seen more code, and thus structured reasoning, in their training data.<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2201.11903" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2201.11903</a><p>[2] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2210.07128" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2210.07128</a></div><br/></div></div><div id="35869018" class="c"><input type="checkbox" id="c-35869018" checked=""/><div class="controls bullet"><span class="by">newhouseb</span><span>|</span><a href="#35868828">parent</a><span>|</span><a href="#35868973">prev</a><span>|</span><a href="#35868927">next</a><span>|</span><label class="collapse" for="c-35869018">[-]</label><label class="expand" for="c-35869018">[2 more]</label></div><br/><div class="children"><div class="content">My emerging conception of this is to split this into two separate questions:<p>1. Is the architecture _capable_, i.e. is it possible for a model with a given shape possible to perform some &quot;reasoning&quot;<p>2. Is the architecture _trainable_, i.e. do we have the means to learn a configuration of the parameters that achieves what we know they are capable of.<p>Recent interpretability work like that around Induction Heads [1] or the conclusion that transformers are Turing complete [2] combined with my own work to hand-specify transformer weights to do symbolic multi-digit addition (read: the same way we do it in grade school) has convinced me that reasoning over a finite domain is a capability of the even tiniest models.<p>The emergent properties we see in models like GPT4 are more a consequence of the fact that we&#x27;ve found a way to train a fairly efficient representation of a significant fraction of &quot;world rules&quot; into a large number of parameters in a finite amount of time.<p>[1] <a href="https:&#x2F;&#x2F;transformer-circuits.pub&#x2F;2021&#x2F;framework&#x2F;index.html" rel="nofollow">https:&#x2F;&#x2F;transformer-circuits.pub&#x2F;2021&#x2F;framework&#x2F;index.html</a><p>[2] <a href="https:&#x2F;&#x2F;jmlr.org&#x2F;papers&#x2F;volume22&#x2F;20-302&#x2F;20-302.pdf" rel="nofollow">https:&#x2F;&#x2F;jmlr.org&#x2F;papers&#x2F;volume22&#x2F;20-302&#x2F;20-302.pdf</a></div><br/><div id="35869522" class="c"><input type="checkbox" id="c-35869522" checked=""/><div class="controls bullet"><span class="by">zmmmmm</span><span>|</span><a href="#35868828">root</a><span>|</span><a href="#35869018">parent</a><span>|</span><a href="#35868927">next</a><span>|</span><label class="collapse" for="c-35869522">[-]</label><label class="expand" for="c-35869522">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a useful breakdown in terms of how to think about it.<p>One angle I am curious about is whether it&#x27;s to some extent an artefact of how you regularise the model as much as the number of parameters and other factors.<p>You can think about it in terms of, if you regularise it enough then you force the network instead of fitting specific data points, to actually start learning logic internally because that is the only thing generalisable enough to allow it to produce realistic text for such a diverse range of prompts. You have to have enough parameters that this is even possible, but once you do, the right training &#x2F; regularisation essentially starts to inevitably force it into that approach rather than the more direct nearest-neighber style &quot;produce something similar to what someone said once before&quot; mechanism.</div><br/></div></div></div></div><div id="35868927" class="c"><input type="checkbox" id="c-35868927" checked=""/><div class="controls bullet"><span class="by">mlyle</span><span>|</span><a href="#35868828">parent</a><span>|</span><a href="#35869018">prev</a><span>|</span><a href="#35868968">next</a><span>|</span><label class="collapse" for="c-35868927">[-]</label><label class="expand" for="c-35868927">[3 more]</label></div><br/><div class="children"><div class="content">Just dealing with numbers and numeric comparisons is a big improvement with model size.  Symbols are not a great encoding of numbers and models have struggled with math.<p>ChatGPT3.5 handles that just fine.<p>I do like the big improvement from ChatGPT3.5 to ChatGPT4 on answers to questions like &quot;Which is heavier, two pounds of bricks or one pound of feathers?&quot;  3.5 is really inclined to say &quot;They are both the same weight, as they both weigh one pound.&quot;</div><br/><div id="35870478" class="c"><input type="checkbox" id="c-35870478" checked=""/><div class="controls bullet"><span class="by">jjeaff</span><span>|</span><a href="#35868828">root</a><span>|</span><a href="#35868927">parent</a><span>|</span><a href="#35868968">next</a><span>|</span><label class="collapse" for="c-35870478">[-]</label><label class="expand" for="c-35870478">[2 more]</label></div><br/><div class="children"><div class="content">Is a larger model better with numbers simply because it is more likely to have examples that use those same numbers? Or because it somehow gives it better ability to reason about numbers?</div><br/><div id="35870929" class="c"><input type="checkbox" id="c-35870929" checked=""/><div class="controls bullet"><span class="by">mlyle</span><span>|</span><a href="#35868828">root</a><span>|</span><a href="#35870478">parent</a><span>|</span><a href="#35868968">next</a><span>|</span><label class="collapse" for="c-35870929">[-]</label><label class="expand" for="c-35870929">[1 more]</label></div><br/><div class="children"><div class="content">Right now, larger models have more complicated and rich structures encoding information about numbers and the meanings of their parts.<p>There&#x27;s fundamental awkwardness that comes with doing math using a device that only seeks to predict the &quot;next token&quot; coming out, and that only understands numbers as a sequence of tokens (usually digits in base 10).  It also doesn&#x27;t even start with the knowledge of the ordering of the digits: this just comes from the examples it has seen.<p>Either it must:<p>- &quot;Think ahead&quot; inside the linear algebra of the model, so that it has already carried all the digits, etc.  There are no real &quot;steps&quot; in this operation that are akin to the things we think about when we do arithmetic.<p>- Describe what it is doing, so that the intermediate work is inside its context buffer.<p>Right now, the models have learned structures that reliably think 3-4 digits ahead in most cases, which is way better than before but still pretty bad compared to a competent 4th grader taking their time with arithmetic.  But if you create a scenario where the model describes its reasoning, it can do pretty well.</div><br/></div></div></div></div></div></div><div id="35868968" class="c"><input type="checkbox" id="c-35868968" checked=""/><div class="controls bullet"><span class="by">vsareto</span><span>|</span><a href="#35868828">parent</a><span>|</span><a href="#35868927">prev</a><span>|</span><a href="#35868857">next</a><span>|</span><label class="collapse" for="c-35868968">[-]</label><label class="expand" for="c-35868968">[6 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think this paper is dismissing the importance of correct yes&#x2F;no tests and reaching an accuracy threshold making it generally useful to humans, but that you should use more than correct yes&#x2F;no tests before declaring some behavior is emergent.</div><br/><div id="35869222" class="c"><input type="checkbox" id="c-35869222" checked=""/><div class="controls bullet"><span class="by">golemotron</span><span>|</span><a href="#35868828">root</a><span>|</span><a href="#35868968">parent</a><span>|</span><a href="#35868857">next</a><span>|</span><label class="collapse" for="c-35869222">[-]</label><label class="expand" for="c-35869222">[5 more]</label></div><br/><div class="children"><div class="content">The paper is goal-post shifting by measuring another thing. At the level of yes&#x2F;no the behavior is emergent.</div><br/><div id="35870960" class="c"><input type="checkbox" id="c-35870960" checked=""/><div class="controls bullet"><span class="by">nullsense</span><span>|</span><a href="#35868828">root</a><span>|</span><a href="#35869222">parent</a><span>|</span><a href="#35868857">next</a><span>|</span><label class="collapse" for="c-35870960">[-]</label><label class="expand" for="c-35870960">[4 more]</label></div><br/><div class="children"><div class="content">Is the binary distinction helpful? It doesn&#x27;t appear to be a more helpful way of evaluating how capable a model is.</div><br/><div id="35871628" class="c"><input type="checkbox" id="c-35871628" checked=""/><div class="controls bullet"><span class="by">stevenhuang</span><span>|</span><a href="#35868828">root</a><span>|</span><a href="#35870960">parent</a><span>|</span><a href="#35871291">next</a><span>|</span><label class="collapse" for="c-35871628">[-]</label><label class="expand" for="c-35871628">[1 more]</label></div><br/><div class="children"><div class="content">How can it not be. Like think about what you&#x27;re saying here.<p>Would you rather be able to evaluate a model on it&#x27;s demonstrated capabilities (multistep reasoning, question and answer, instruction following, theory of mind, etc) or some nebulous metric along an axis that may as well not correspond to practice.<p>We only care about how good AI is at things that matter to us as humans. Why not test for these directly?<p>If some perfect metric is discovered that shows the phenomenon of emergence is actually continuous, then that <i>would</i> be helpful.</div><br/></div></div><div id="35871291" class="c"><input type="checkbox" id="c-35871291" checked=""/><div class="controls bullet"><span class="by">golemotron</span><span>|</span><a href="#35868828">root</a><span>|</span><a href="#35870960">parent</a><span>|</span><a href="#35871628">prev</a><span>|</span><a href="#35868857">next</a><span>|</span><label class="collapse" for="c-35871291">[-]</label><label class="expand" for="c-35871291">[2 more]</label></div><br/><div class="children"><div class="content">I think it is more fundamental than that. Emergence always disappears when we slice phenomena thinly. A person walking through a doorway is a very continuous phenomenon. We can see their relation to the doorway at each point. There is no abrupt change. But when we stand back and apply the criterion: &quot;is the person through the door (y&#x2F;n?)&quot; we end up with an answer. When it is yes, we can say that the passage is an emergent effect of the motion. At one moment it wasn&#x27;t there, and at another it was.</div><br/><div id="35871536" class="c"><input type="checkbox" id="c-35871536" checked=""/><div class="controls bullet"><span class="by">tsimionescu</span><span>|</span><a href="#35868828">root</a><span>|</span><a href="#35871291">parent</a><span>|</span><a href="#35868857">next</a><span>|</span><label class="collapse" for="c-35871536">[-]</label><label class="expand" for="c-35871536">[1 more]</label></div><br/><div class="children"><div class="content">If emergence disappears when you slice it thinly enough, then the phenomenon was not emergent. There are emergent phenomena in mathematics - for example, infinite sets have many emergent properties that arbitrarily large finite sets don&#x27;t share. As far as we know, consciousness seems to be an emergent phenomenon, when you increase brain size in some way far enough. Turing completeness is usually emergent as well - remove any single element from a Turing complete system and it typiclaly becomes unable to compute the vast majority of functions.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="35868857" class="c"><input type="checkbox" id="c-35868857" checked=""/><div class="controls bullet"><span class="by">chezelenkoooo</span><span>|</span><a href="#35868828">parent</a><span>|</span><a href="#35868968">prev</a><span>|</span><a href="#35869578">next</a><span>|</span><label class="collapse" for="c-35868857">[-]</label><label class="expand" for="c-35868857">[1 more]</label></div><br/><div class="children"><div class="content">Totally guessing here but this would be a property that gradually appears as the model size increases.<p>With a tiny model you would get gibberish and with ever increasing models the response would increasingly approach a coherent answer to a finally correct answer.</div><br/></div></div><div id="35869578" class="c"><input type="checkbox" id="c-35869578" checked=""/><div class="controls bullet"><span class="by">majormajor</span><span>|</span><a href="#35868828">parent</a><span>|</span><a href="#35868857">prev</a><span>|</span><a href="#35871002">next</a><span>|</span><label class="collapse" for="c-35869578">[-]</label><label class="expand" for="c-35869578">[1 more]</label></div><br/><div class="children"><div class="content">If you don&#x27;t have a non-binary measurement criteria you can&#x27;t tell if it&#x27;s &quot;emergent&quot; or &quot;step-function&quot; improvement because you can&#x27;t easily distinguish &quot;90% of the way to consistently seeming logical&quot; from &quot;85% of the way there&quot; from &quot;82% of the way there&quot; etc.</div><br/></div></div><div id="35871002" class="c"><input type="checkbox" id="c-35871002" checked=""/><div class="controls bullet"><span class="by">choeger</span><span>|</span><a href="#35868828">parent</a><span>|</span><a href="#35869578">prev</a><span>|</span><a href="#35868877">next</a><span>|</span><label class="collapse" for="c-35871002">[-]</label><label class="expand" for="c-35871002">[4 more]</label></div><br/><div class="children"><div class="content">Try that same prompt with unusual wording and numbers:<p>* Replace 18 by 37<p>* Replace car with ice cream van<p>* Replace the first drive with
steer<p>I hypothesize that a truly reasoning model would see through such alterations whereas a &quot;just&quot; continuously better model would show somewhat degraded performance.</div><br/><div id="35871053" class="c"><input type="checkbox" id="c-35871053" checked=""/><div class="controls bullet"><span class="by">this_is_not_you</span><span>|</span><a href="#35868828">root</a><span>|</span><a href="#35871002">parent</a><span>|</span><a href="#35868877">next</a><span>|</span><label class="collapse" for="c-35871053">[-]</label><label class="expand" for="c-35871053">[3 more]</label></div><br/><div class="children"><div class="content">Tried it.<p>Me: &quot;David is 15 years old. He has a lot of experience riding bicycles. The minimum age to be allowed to steer an ice cream van is 37. Is David allowed to drive an ice cream van?&quot;<p>GPT-4: &quot;No, David is not allowed to drive an ice cream van. He is only 15 years old, and the minimum age requirement is 37.&quot;<p>If you change it to:
&quot;David is 15 years old. He has a lot of experience riding bicycles. The minimum age to be allowed to steer a car is 37. Is David allowed to drive an ice cream van?&quot;<p>You get this from GPT-4:
&quot;It seems there might be a mistake in the information you provided, as the minimum age to steer a car is typically not 37. In most countries, the minimum age to obtain a driver&#x27;s license is between 16 and 18 years old. Assuming that David does not have a driver&#x27;s license yet, he would not be allowed to drive an ice cream van, as it is a motor vehicle that requires a valid driver&#x27;s license to operate. However, if David obtains a driver&#x27;s license once he reaches the legal age in his jurisdiction, he could be allowed to drive an ice cream van if he meets other necessary requirements.&quot;</div><br/><div id="35871137" class="c"><input type="checkbox" id="c-35871137" checked=""/><div class="controls bullet"><span class="by">brundolf</span><span>|</span><a href="#35868828">root</a><span>|</span><a href="#35871053">parent</a><span>|</span><a href="#35868877">next</a><span>|</span><label class="collapse" for="c-35871137">[-]</label><label class="expand" for="c-35871137">[2 more]</label></div><br/><div class="children"><div class="content">I think the most impressive thing in this response is how it expands to the superset &quot;motor vehicle&quot; to unambiguously talk about cars and ice cream vans together as being under the same category</div><br/><div id="35871314" class="c"><input type="checkbox" id="c-35871314" checked=""/><div class="controls bullet"><span class="by">throwawayadvsec</span><span>|</span><a href="#35868828">root</a><span>|</span><a href="#35871137">parent</a><span>|</span><a href="#35868877">next</a><span>|</span><label class="collapse" for="c-35871314">[-]</label><label class="expand" for="c-35871314">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not really the impressive part
you had word2vec +5 years ago who could already easily show you the distance between cars and vans</div><br/></div></div></div></div></div></div></div></div><div id="35868877" class="c"><input type="checkbox" id="c-35868877" checked=""/><div class="controls bullet"><span class="by">mupuff1234</span><span>|</span><a href="#35868828">parent</a><span>|</span><a href="#35871002">prev</a><span>|</span><a href="#35870689">next</a><span>|</span><label class="collapse" for="c-35868877">[-]</label><label class="expand" for="c-35868877">[2 more]</label></div><br/><div class="children"><div class="content">Just tried that example in Bard and it got it right, and I believe that it&#x27;s a 24b parameter model.</div><br/><div id="35869377" class="c"><input type="checkbox" id="c-35869377" checked=""/><div class="controls bullet"><span class="by">zmmmmm</span><span>|</span><a href="#35868828">root</a><span>|</span><a href="#35868877">parent</a><span>|</span><a href="#35870689">next</a><span>|</span><label class="collapse" for="c-35869377">[-]</label><label class="expand" for="c-35869377">[1 more]</label></div><br/><div class="children"><div class="content">only just got access to Bard but yes, it gave a good answer to me as well. However trying some others of my stock examples quickly got it into trouble where ChatGPT4 is very consistently correct.</div><br/></div></div></div></div><div id="35870689" class="c"><input type="checkbox" id="c-35870689" checked=""/><div class="controls bullet"><span class="by">raylad</span><span>|</span><a href="#35868828">parent</a><span>|</span><a href="#35868877">prev</a><span>|</span><a href="#35869970">next</a><span>|</span><label class="collapse" for="c-35870689">[-]</label><label class="expand" for="c-35870689">[3 more]</label></div><br/><div class="children"><div class="content">Vicuna 7b-v1.1 output to that prompt:<p>No, David is not allowed to drive a car as the minimum age to be allowed to drive a car in most countries is 18 years old. This rule is in place to ensure that young drivers have enough maturity, experience, and judgment to handle the responsibilities of driving a car safely and legally.</div><br/><div id="35871309" class="c"><input type="checkbox" id="c-35871309" checked=""/><div class="controls bullet"><span class="by">nottorp</span><span>|</span><a href="#35868828">root</a><span>|</span><a href="#35870689">parent</a><span>|</span><a href="#35869970">next</a><span>|</span><label class="collapse" for="c-35871309">[-]</label><label class="expand" for="c-35871309">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m really curious why the answers are so verbose. Probably because the training was done on a lot of blog spam?</div><br/><div id="35871456" class="c"><input type="checkbox" id="c-35871456" checked=""/><div class="controls bullet"><span class="by">revelio</span><span>|</span><a href="#35868828">root</a><span>|</span><a href="#35871309">parent</a><span>|</span><a href="#35869970">next</a><span>|</span><label class="collapse" for="c-35871456">[-]</label><label class="expand" for="c-35871456">[1 more]</label></div><br/><div class="children"><div class="content">Raters prefer verbose outputs, probably because they want to see the reasoning to help them judge if the answer is getting it right for the right reasons.</div><br/></div></div></div></div></div></div><div id="35869970" class="c"><input type="checkbox" id="c-35869970" checked=""/><div class="controls bullet"><span class="by">ineedasername</span><span>|</span><a href="#35868828">parent</a><span>|</span><a href="#35870689">prev</a><span>|</span><a href="#35869448">next</a><span>|</span><label class="collapse" for="c-35869970">[-]</label><label class="expand" for="c-35869970">[2 more]</label></div><br/><div class="children"><div class="content"><i>&gt;appears slowly as models scale up?&quot;</i><p>Both, I think, based on limited tinkering with smaller models.<p>I&#x27;ve been using GPT4ALL and oobabooga to make testing models easier on my single (entry-level discrete GPU) machine. Using GGML versions of llama models, I get drastically different results.<p>With a 7B parameter model I mostly-- not always-- get an on topic and somewhat coherent response. By which I mean, if I start off with &quot;Are you ready to answer questions?&quot; it will say &quot;Yes and blah blah blah...&quot; for a paragraph about something random. On a specific task it will perform a bit better: my benchmark request has been to ask for a haiku. It was confused, classified haikus as a form of gift, but when pushed it would output something resembling a poem but not a haiku.<p>Then I try a 13B model. It&#x27;s a lot better at answering a simple question like &quot;are you ready?&quot; but will still sometimes say yes and then give a random dialogue as if it&#x27;s creating a story where someone asks it to do something. It will readily create a poem on first attempts, though still not a haiku in any way. If I go through about a dozen rounds of asking it what a haiku is and then, in subsequent responses, &quot;reminding it&quot; to stay on course for those definitions, it will kind of get it and give me 4 or 5 short lines.<p>A 30B model answers simple questions and follow simply instructions fairly easily. It will produce something resembling a haiku, though often with an extra line and a few extra syllables, with minimal prodding an prompt engineering.<p>None of the above, at least the versions I&#x27;ve tried (variations &amp; advances are coming daily) have a very good memory. The clearly have some knowledge of past context but mostly ignore it when it comes to keeping responses logically consistent across multiple prompts. I can ask it &quot;what was my first prompt?&quot; and get a correct response, But when I tell it to respond as if it&#x27;s name is &quot;Bob&quot; then a few prompts later it&#x27;s calling me Bob and back to labelling itself an AI assistant.<p>Then there&#x27;s the 65B parameter model. I <i>think</i> this is a big leap. I&#x27;m not sure though, my PC can barely run the 30B model and gets maybe 1 token every 3 seconds on 30B. The 65B model I have to let use disk swap space or it won&#x27;t work at all, and it produces roughly 1 token per 2-3 minutes. It&#x27;s also much more verbose, reiterating my request and agreeing to it before it proceeds, so that adds a lot of time. However, a simple insistence on a &quot;Yes&#x2F;No&quot; answer will succeed. A request for a Haiku succeeds on the first try, with nearly the correct syllable count too, using an extra few syllables in trying to produce something on the topic I specify. This is commensurate with what I get with normal ChatGPT, which has &gt; 150B parameters that aren&#x27;t even quantized.<p>However I have yet to explore the 65B parameter model in any detail. 1 token every 2-3 minutes, sucking up all system resources, makes things very slow going, so I haven&#x27;t done much more than what I described.<p>Apart from these, I was just playing around with the 13B model a few hours ago and it did do a very decent job at producing basic SQL code I asked it to produce against a single table. Max value for the table, max value per a specified dimension, etc. It did this across multiple prompts without much need to &quot;remind&quot; it about anything a few prompts earlier. At that point though I was all LLM burned out for the day (I&#x27;d been fiddling for hours) so I didn&#x27;t get around to asking it for simple joins.<p>So in short, where I began, I think its both. Abilities are somewhat task specific, as are the quality improvements for a given task across larger parameter models. Sometimes a specific task has moderate or little improvement at higher levels, sometimes another task does much better, or only does much better when it reaches a certain point: e.g., haikus from 13B to 30B weren&#x27;t a great improvement, but 30B to 65B was an enormous improvement.*</div><br/><div id="35870396" class="c"><input type="checkbox" id="c-35870396" checked=""/><div class="controls bullet"><span class="by">root_axis</span><span>|</span><a href="#35868828">root</a><span>|</span><a href="#35869970">parent</a><span>|</span><a href="#35869448">next</a><span>|</span><label class="collapse" for="c-35870396">[-]</label><label class="expand" for="c-35870396">[1 more]</label></div><br/><div class="children"><div class="content">try wizard-vicuna-13b in ooba. I think you will be pleasantly surprised.</div><br/></div></div></div></div><div id="35869448" class="c"><input type="checkbox" id="c-35869448" checked=""/><div class="controls bullet"><span class="by">alanhaha</span><span>|</span><a href="#35868828">parent</a><span>|</span><a href="#35869970">prev</a><span>|</span><a href="#35869479">next</a><span>|</span><label class="collapse" for="c-35869448">[-]</label><label class="expand" for="c-35869448">[1 more]</label></div><br/><div class="children"><div class="content">Using ggml-vicuna-7b-4bit-rev1:<p>In most countries, the minimum age to obtain a driver&#x27;s license and legally drive a car is 18 years old. However, there may be certain circumstances where a person under the age of 18 may be allowed to drive a car, such as in the case of a learner&#x27;s permit or with the supervision of a licensed driver.<p>It ultimately depends on the specific laws and regulations in the jurisdiction where David lives, but generally, he would not be allowed to drive a car without a valid driver&#x27;s license.</div><br/></div></div><div id="35869479" class="c"><input type="checkbox" id="c-35869479" checked=""/><div class="controls bullet"><span class="by">onlyrealcuzzo</span><span>|</span><a href="#35868828">parent</a><span>|</span><a href="#35869448">prev</a><span>|</span><a href="#35868923">next</a><span>|</span><label class="collapse" for="c-35869479">[-]</label><label class="expand" for="c-35869479">[5 more]</label></div><br/><div class="children"><div class="content">I would imagine ChatGPT was trained on some logic &#x2F; IQ-puzzle data.<p>&gt; If all Smerps are Derps. And some Derps are Merps. Are all Smerps Merps?<p>Answer: &gt; No, only some Smerps are Merps.</div><br/><div id="35869641" class="c"><input type="checkbox" id="c-35869641" checked=""/><div class="controls bullet"><span class="by">NtochkaNzvanova</span><span>|</span><a href="#35868828">root</a><span>|</span><a href="#35869479">parent</a><span>|</span><a href="#35868923">next</a><span>|</span><label class="collapse" for="c-35869641">[-]</label><label class="expand" for="c-35869641">[4 more]</label></div><br/><div class="children"><div class="content">Is that a real answer from ChatGPT? I wouldn&#x27;t consider that totally correct; it&#x27;s possible that some Smerps are Merps, but it&#x27;s also possible that no Smerps are Merps.</div><br/><div id="35869907" class="c"><input type="checkbox" id="c-35869907" checked=""/><div class="controls bullet"><span class="by">taberiand</span><span>|</span><a href="#35868828">root</a><span>|</span><a href="#35869641">parent</a><span>|</span><a href="#35869758">next</a><span>|</span><label class="collapse" for="c-35869907">[-]</label><label class="expand" for="c-35869907">[2 more]</label></div><br/><div class="children"><div class="content">Chat GPT 4 answers:<p>&quot;
No, it is not necessarily true that all Smerps are Merps.<p>Given the information:<p>1. All Smerps are Derps.
2. Some Derps are Merps.<p>We can only conclude that there might be some Smerps that are Merps, but we cannot be sure that all Smerps are Merps. There could be Smerps that are not Merps, as the second statement only indicates that there is an overlap between Derps and Merps, but not necessarily between Smerps and Merps.
&quot;<p>So it&#x27;s aware that it&#x27;s possible no Smerps are Merps</div><br/><div id="35871231" class="c"><input type="checkbox" id="c-35871231" checked=""/><div class="controls bullet"><span class="by">prox</span><span>|</span><a href="#35868828">root</a><span>|</span><a href="#35869907">parent</a><span>|</span><a href="#35869758">next</a><span>|</span><label class="collapse" for="c-35871231">[-]</label><label class="expand" for="c-35871231">[1 more]</label></div><br/><div class="children"><div class="content">Not to be pedantic, but it’s not really aware is it? Seems like it is logic gating it.</div><br/></div></div></div></div><div id="35869758" class="c"><input type="checkbox" id="c-35869758" checked=""/><div class="controls bullet"><span class="by">electrondood</span><span>|</span><a href="#35868828">root</a><span>|</span><a href="#35869641">parent</a><span>|</span><a href="#35869907">prev</a><span>|</span><a href="#35868923">next</a><span>|</span><label class="collapse" for="c-35869758">[-]</label><label class="expand" for="c-35869758">[1 more]</label></div><br/><div class="children"><div class="content">I think it was just an example made up on the spot. The Merpiness of Smerps in part or total is clearly not guaranteed with necessity.<p>List of valid syllogisms:<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;List_of_valid_argument_forms#Valid_syllogistic_forms" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;List_of_valid_argument_forms#V...</a></div><br/></div></div></div></div></div></div><div id="35868923" class="c"><input type="checkbox" id="c-35868923" checked=""/><div class="controls bullet"><span class="by">yieldcrv</span><span>|</span><a href="#35868828">parent</a><span>|</span><a href="#35869479">prev</a><span>|</span><a href="#35869278">next</a><span>|</span><label class="collapse" for="c-35868923">[-]</label><label class="expand" for="c-35868923">[1 more]</label></div><br/><div class="children"><div class="content">Its not just parameters<p>There are teams looking at better reduced parameters</div><br/></div></div></div></div><div id="35868676" class="c"><input type="checkbox" id="c-35868676" checked=""/><div class="controls bullet"><span class="by">teaearlgraycold</span><span>|</span><a href="#35868828">prev</a><span>|</span><a href="#35868568">next</a><span>|</span><label class="collapse" for="c-35868676">[-]</label><label class="expand" for="c-35868676">[71 more]</label></div><br/><div class="children"><div class="content">I maintain that LLMs are great for a few things:<p>* Making stuff up. If you need bulk fiction writing then LLMs are useful (or marketing copy, etc.)<p>* Transforming information. Unstructured text to JSON. Natural language to Bash. Python to Ruby.<p>* Information reduction. Summarization. Data extraction (this can bleed a bit into analysis)<p>They&#x27;re not AGI but they&#x27;re <i>definitely</i> useful.</div><br/><div id="35871398" class="c"><input type="checkbox" id="c-35871398" checked=""/><div class="controls bullet"><span class="by">jillesvangurp</span><span>|</span><a href="#35868676">parent</a><span>|</span><a href="#35870618">next</a><span>|</span><label class="collapse" for="c-35871398">[-]</label><label class="expand" for="c-35871398">[1 more]</label></div><br/><div class="children"><div class="content">Actually they are also getting good at using tools, generating working code for common problems, helping you explore vast bodies of published information (limited by what it has been trained on obviously, particularly Bing chat seems good at this), and a few related topics.<p>Limiting factors are mainly interfacing with other systems and tools, expanding the training data to include stuff you need (e.g. up to date documentation for whatever you are using vs. the two years out of date stuff it was trained on). This is more a limitation of UX (chat) than it is a limitation of the underlying model.<p>It&#x27;s weak on logic problems, math, and a few other things. But then a most people are also not very good at that and you could use tools for that (which is something chat gpt 4 can do). And people hallucinate, lie, and imagine things all the time. And if you follow US politics, there have been a few amusingly bad examples on that front in recent months. To the point where you might wonder if some politicians are using AI to write their speeches; or would be better off doing so.<p>It&#x27;s our ability to defer to tools and other people that makes us more capable than each of us individually is. Individually, most of us aren&#x27;t that impressive.<p>Even a few years ago, the notion that you could have a serious chat via a laptop with an AI on just about any topic and get coherent responses would have been science fiction. Now it is science fact. Most of the AIs from popular science fiction movies (Space Odyssey, Star Trek, Star Wars, etc.) are actually not that far of from what chat GPT 4 can do. Or arguably a bit retarded even (usually for comedic effect). You can actually get chat gpt to role play those.</div><br/></div></div><div id="35870618" class="c"><input type="checkbox" id="c-35870618" checked=""/><div class="controls bullet"><span class="by">kristopolous</span><span>|</span><a href="#35868676">parent</a><span>|</span><a href="#35871398">prev</a><span>|</span><a href="#35868716">next</a><span>|</span><label class="collapse" for="c-35870618">[-]</label><label class="expand" for="c-35870618">[2 more]</label></div><br/><div class="children"><div class="content">I use it 90% for entertainment.<p>In improv theater there&#x27;s something called &quot;yes and&quot; - essentially you take the premise father, no matter how absurd, without redirection.<p>You can come up with the most ridiculous things and it just goes with it, hilariously.<p>I&#x27;ll come up with one on the spot. &quot;I&#x27;m having a very difficult time with my pet snails being social. I&#x27;m thinking of starting a social networking site and giving snails tiny phones so they can chat with each other. I need a company name, an elevator pitch, and some copy for my landing page.&quot;<p>And then you ask it to do I dunno, NFTs and crypto currency for snails, give them tiny VR headsets. Have it come up with a jingle and a commercial. You can say instead of unfriending you salt them. Etc... It&#x27;ll just keep going. Even &quot;A snails rights luddite group of Mennonites and Amish are now protesting my idea. I need a way to appease my critics. Can you write a letter for me that defends snailconnect as healthy and good?&quot;<p>One of my favorite outputs from this session<p>&quot;How about SnailConnect; a small trail for snail, one giant leaf for snailkind.&quot;<p>You don&#x27;t ever get to a place where it&#x27;s like &quot;well now you&#x27;re just being ridiculous&quot;<p>But I agree. It&#x27;s just a big data version of Eliza - spitting my reflection back at me</div><br/><div id="35870835" class="c"><input type="checkbox" id="c-35870835" checked=""/><div class="controls bullet"><span class="by">teaearlgraycold</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35870618">parent</a><span>|</span><a href="#35868716">next</a><span>|</span><label class="collapse" for="c-35870835">[-]</label><label class="expand" for="c-35870835">[1 more]</label></div><br/><div class="children"><div class="content">I think it’s fun to use as a way to write Star Trek fan fiction. It can get pretty witty.</div><br/></div></div></div></div><div id="35868716" class="c"><input type="checkbox" id="c-35868716" checked=""/><div class="controls bullet"><span class="by">Gigachad</span><span>|</span><a href="#35868676">parent</a><span>|</span><a href="#35870618">prev</a><span>|</span><a href="#35868715">next</a><span>|</span><label class="collapse" for="c-35868716">[-]</label><label class="expand" for="c-35868716">[12 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been pasting comments in to ChatGPT and asking what they mean by certain terms or phrases and it works extremely well and with an answer I can independently verify to be true. Far more useful than getting a simple dictionary lookup.<p>An example response I got recently:<p>&gt;In the context of this comment, &quot;dovish&quot; means that the speaker perceives Powell&#x27;s statement to be more accommodative towards economic growth and less concerned about inflation than Lowe&#x27;s statement. This suggests that Powell may be more inclined to keep interest rates low or lower them further to stimulate economic growth, rather than raise them to combat inflation. The term &quot;dove&quot; is often used to describe policymakers who prioritize economic growth over inflation concerns. In contrast, &quot;hawkish&quot; refers to policymakers who prioritize fighting inflation over economic growth.<p>Meanwhile google gives me this response for a defintion<p>&gt;Definitions of dovish. adjective. opposed to war. synonyms: pacifist, pacifistic peaceable, peaceful. not disturbed by strife or turmoil or war.</div><br/><div id="35868972" class="c"><input type="checkbox" id="c-35868972" checked=""/><div class="controls bullet"><span class="by">tkgally</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35868716">parent</a><span>|</span><a href="#35868746">next</a><span>|</span><label class="collapse" for="c-35868972">[-]</label><label class="expand" for="c-35868972">[9 more]</label></div><br/><div class="children"><div class="content">&gt; Far more useful than getting a simple dictionary lookup.<p>Yes.<p>Long-time dictionary-editor and translator here. What blew me away most when I first tried ChatGPT, and what continues to amaze me, is its ability to distinguish and explain the meanings of individual words relative to their context. I have spent much of my career doing just that, and I always felt that it relied on my knowledge and intuition as a human and would be a really hard problem for AI. Conventional corpus linguistics had not even come close to cracking the problem.<p>I’ve done tests in which I gave ChatGPT texts that contained very rare words, new words, or nonce words that would not be defined in any dictionaries, and some of which were unattested on the web. When given enough context, it was able to deduce the intended meanings.<p>In contrast, its ability to explain grammar is terrible. It misidentifies parts of speech, singulars and plurals, the subjects and objects of verbs, even the number of letters in words. I hope that people develop plugins for it to fix its language-explaining ability as, reportedly, other plugins are able to fix its natively terrible mathematical ability.</div><br/><div id="35869160" class="c"><input type="checkbox" id="c-35869160" checked=""/><div class="controls bullet"><span class="by">mlyle</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35868972">parent</a><span>|</span><a href="#35869059">next</a><span>|</span><label class="collapse" for="c-35869160">[-]</label><label class="expand" for="c-35869160">[3 more]</label></div><br/><div class="children"><div class="content">Here is an example of results I got in playing with this some with ChatGPT-4.  Note that it doesn&#x27;t understand the syntactic ambiguity of the sentence:<p>The sentence can be broken down as follows:<p>Main Clause: 
- The burglar threatened the student with the knife.<p>Subject: 
- The burglar<p>Predicate: 
- threatened the student with the knife.<p>Object: 
- the student<p>Prepositional Phrase: 
- with the knife<p>The prepositional phrase &quot;with the knife&quot; modifies the verb &quot;threatened.&quot; It tells us what the burglar used to threaten the student.<p>Now, let&#x27;s identify the parts of speech of each word in the sentence and whether the nouns are singular or plural:<p>- The: article, singular
- burglar: noun, singular
- threatened: verb, past tense
- the: article, singular
- student: noun, singular
- with: preposition
- the: article, singular
- knife: noun, singular<p>All the nouns in this sentence are singular. There is no ambiguity in meaning in this sentence.</div><br/><div id="35870012" class="c"><input type="checkbox" id="c-35870012" checked=""/><div class="controls bullet"><span class="by">tkgally</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35869160">parent</a><span>|</span><a href="#35869059">next</a><span>|</span><label class="collapse" for="c-35870012">[-]</label><label class="expand" for="c-35870012">[2 more]</label></div><br/><div class="children"><div class="content">I tried that sentence and three similar ones, and this time GPT-4 caught the ambiguity. The results are here:<p><a href="https:&#x2F;&#x2F;www.gally.net&#x2F;temp&#x2F;202212chatgpt&#x2F;20230508-syntactic-ambiguity.jpg" rel="nofollow">https:&#x2F;&#x2F;www.gally.net&#x2F;temp&#x2F;202212chatgpt&#x2F;20230508-syntactic-...</a><p>I am especially impressed by its comment about one interpretation of “The woman cleaned her teeth with a knife”: “The woman cleaned her teeth that had a knife in them (highly unlikely, but still a possible interpretation).”<p>As I noted in another comment, I might need to backtrack on my assessment of its grammatical-explanation ability, as until today I had only done tests using the initial release of ChatGPT, not GPT-4.</div><br/><div id="35870381" class="c"><input type="checkbox" id="c-35870381" checked=""/><div class="controls bullet"><span class="by">pmoriarty</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35870012">parent</a><span>|</span><a href="#35869059">next</a><span>|</span><label class="collapse" for="c-35870381">[-]</label><label class="expand" for="c-35870381">[1 more]</label></div><br/><div class="children"><div class="content">Anthropic&#x27;s Claude[1] and Claude+[2] both failed some of these.<p>Question:<p>I am interested in syntactic ambiguity in English.  Can any of the following sentences be interpreted in more than one way?  Explain why or why not.<p>The man opened the package with a knife.<p>The burglar threatened the student with a knife.<p>The woman cleaned her teeth with a knife.<p>The child pointed at the teacher with a knife.<p>Claude&#x27;s answer:<p>These sentences can be interpreted in multiple ways:<p>1. The man opened the package with a knife.
Ambiguous: The knife could have been used as a tool to cut tape and open the package, or it could imply the man threatened someone with a knife while opening the package.<p>2. The burglar threatened the student with a knife.  
Unambiguous: The implication is clearly that the burglar threatened the student using a knife as a weapon.<p>3. The woman cleaned her teeth with a knife.
Ambiguous: The knife could refer to a toothpick or dental tool, or it could imply an unusual way of brushing teeth.<p>4. The child pointed at the teacher with a knife.    
Ambiguous: The knife could have been used to gesture and point, or it could imply the child threatened the teacher with a knife.<p>In summary, sentences that suggest using a knife as a tool tend to be ambiguous between a literal and figurative interpretation. Sentences that clearly describe a knife being used as a weapon are generally unambiguous. The ambiguity arises from the multiple meanings and uses of the word &quot;with.&quot;<p>Does this explanation make sense? Let me know if you have any other questions!<p>Claude+&#x27;s answer:<p>Here are my analyses of potential syntactic ambiguity in those sentences:<p>The man opened the package with a knife. 
- This sentence is ambiguous. It could mean either that the man used a knife to open the package, or that the package contained a knife inside. The ambiguity arises because &quot;with a knife&quot; could modify either &quot;opened&quot; or &quot;package&quot;.<p>The burglar threatened the student with a knife.
- This sentence is unambiguous. The only coherent interpretation is that the burglar used a knife to threaten the student. &quot;With a knife&quot; can only modify &quot;threatened&quot; in this sentence.<p>The woman cleaned her teeth with a knife.
- This sentence is ambiguous. It&#x27;s unclear whether it means the woman used a knife to clean her teeth, or her teeth contained a knife. Again, &quot;with a knife&quot; could modify either &quot;cleaned&quot; or &quot;teeth&quot;.<p>The child pointed at the teacher with a knife.
- This sentence is unambiguous. The only meaning that makes sense is that the child pointed at the teacher while holding a knife. &quot;With a knife&quot; can only modify &quot;pointed at&quot; in this example.<p>In summary, ambiguity can arise when a prepositional phrase like &quot;with a knife&quot; can logically modify more than one part of the sentence. Context often helps resolve such ambiguities, but in isolation some sentences remain open to multiple interpretations. Let me know if you have any other questions!<p>[1] - <a href="https:&#x2F;&#x2F;poe.com&#x2F;Claude-instant" rel="nofollow">https:&#x2F;&#x2F;poe.com&#x2F;Claude-instant</a><p>[2] - <a href="https:&#x2F;&#x2F;poe.com&#x2F;Claude%2B" rel="nofollow">https:&#x2F;&#x2F;poe.com&#x2F;Claude%2B</a></div><br/></div></div></div></div></div></div><div id="35869059" class="c"><input type="checkbox" id="c-35869059" checked=""/><div class="controls bullet"><span class="by">mlyle</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35868972">parent</a><span>|</span><a href="#35869160">prev</a><span>|</span><a href="#35869012">next</a><span>|</span><label class="collapse" for="c-35869059">[-]</label><label class="expand" for="c-35869059">[2 more]</label></div><br/><div class="children"><div class="content">&gt; even the number of letters in words.<p>This one isn&#x27;t surprising at all: it doesn&#x27;t deal with letters, but symbols.  It only indirectly knows about spelling.<p>&gt; In contrast, its ability to explain grammar is terrible. It misidentifies parts of speech, singulars and plurals, the subjects and objects of verbs,<p>I wonder if this is an area where having to generate word-by-word an immediate final answer immediately is killing it, and if it could be induced to &quot;diagram&quot; a sentence first and get a correct answer.  Can you give me an example of the query you&#x27;re asking?</div><br/><div id="35869951" class="c"><input type="checkbox" id="c-35869951" checked=""/><div class="controls bullet"><span class="by">tkgally</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35869059">parent</a><span>|</span><a href="#35869012">next</a><span>|</span><label class="collapse" for="c-35869951">[-]</label><label class="expand" for="c-35869951">[1 more]</label></div><br/><div class="children"><div class="content">&gt; This one isn&#x27;t surprising at all: it doesn&#x27;t deal with letters, but symbols.<p>It’s not surprising to us <i>now</i>. It was very surprising to me when I first noticed it, as it contrasted sharply with ChatGPT’s ability to explain aspects of language that seem to us humans as being much more difficult, particularly word meaning.<p>&gt;  Can you give me an example of the query you&#x27;re asking?<p>I have to admit that all of my testing of its grammar-explaining ability was done last December. I have repeated a few of those tests now with GPT-4, and it did fine [1].<p>[1] <a href="https:&#x2F;&#x2F;www.gally.net&#x2F;temp&#x2F;202212chatgpt&#x2F;explaininggrammar.html" rel="nofollow">https:&#x2F;&#x2F;www.gally.net&#x2F;temp&#x2F;202212chatgpt&#x2F;explaininggrammar.h...</a></div><br/></div></div></div></div><div id="35869012" class="c"><input type="checkbox" id="c-35869012" checked=""/><div class="controls bullet"><span class="by">teaearlgraycold</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35868972">parent</a><span>|</span><a href="#35869059">prev</a><span>|</span><a href="#35869987">next</a><span>|</span><label class="collapse" for="c-35869012">[-]</label><label class="expand" for="c-35869012">[1 more]</label></div><br/><div class="children"><div class="content">Grammar models already exist and have for years. I think giving GPT access to one would be the right approach there.</div><br/></div></div><div id="35869987" class="c"><input type="checkbox" id="c-35869987" checked=""/><div class="controls bullet"><span class="by">deafpolygon</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35868972">parent</a><span>|</span><a href="#35869012">prev</a><span>|</span><a href="#35868746">next</a><span>|</span><label class="collapse" for="c-35869987">[-]</label><label class="expand" for="c-35869987">[2 more]</label></div><br/><div class="children"><div class="content">&gt; In contrast, its ability to explain grammar is terrible.<p>To be fair, grammar is sometimes a bit of an advanced subject - even if it is ostensibly easy (which it isn&#x27;t). Isn&#x27;t that the same problem for every other subject? Other than encyclopedic knowledge, it struggles with many subjects at higher forms (i.e. programming more than just a few basics, math, and so on).<p>I think this is partially an issue of availability of data.</div><br/><div id="35870460" class="c"><input type="checkbox" id="c-35870460" checked=""/><div class="controls bullet"><span class="by">pmoriarty</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35869987">parent</a><span>|</span><a href="#35868746">next</a><span>|</span><label class="collapse" for="c-35870460">[-]</label><label class="expand" for="c-35870460">[1 more]</label></div><br/><div class="children"><div class="content">Something else to consider is that languages differ as to the complexity of their grammar.<p>Much research and training of LLMs has been done in English, but if these models were trained on as much data in other languages as English, I wonder if:<p>- LLMs would do better or worse on intelligence and other tests if they were tested in other languages?<p>- Could conversing with LLMs be easier or harder in other languages?<p>Some languages, like Logan or Lojban, might be especially suited towards this sort of testing and interaction, as they were designed to be easy for computers to interpret.</div><br/></div></div></div></div></div></div><div id="35868746" class="c"><input type="checkbox" id="c-35868746" checked=""/><div class="controls bullet"><span class="by">teaearlgraycold</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35868716">parent</a><span>|</span><a href="#35868972">prev</a><span>|</span><a href="#35868715">next</a><span>|</span><label class="collapse" for="c-35868746">[-]</label><label class="expand" for="c-35868746">[2 more]</label></div><br/><div class="children"><div class="content">Yes. And that gets into the awkward bit. LLMs are incidentally good as databases&#x2F;search engines for information that is prevalent on the internet and not disputed. Ideally we could separate the LLM from the information. So when asking it for a definition you&#x27;d provide your query, the source comment, and then the computer would add in the definition and some uses in context.<p>Right now we&#x27;ve tangled an understanding of language and a corpus of information together in a way that causes distrust in AI. If the AI gets some fact wrong (like Bard did when demo&#x27;d earlier this year) people laugh and think LLMs are a failure. They should not be used for information recall. That is a task that a vector database would be good at.</div><br/><div id="35868847" class="c"><input type="checkbox" id="c-35868847" checked=""/><div class="controls bullet"><span class="by">svachalek</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35868746">parent</a><span>|</span><a href="#35868715">next</a><span>|</span><label class="collapse" for="c-35868847">[-]</label><label class="expand" for="c-35868847">[1 more]</label></div><br/><div class="children"><div class="content">Yes, I think this explains a lot of skeptics. They look at ChatGPT as an information repository, as so it is impressive but flawed, while looking right past its incredible facility with language which is positively superhuman. Some humans can exceed its language skills on some details, but no one can speak so many languages (both natural and programming languages) with the ease and facility of ChatGPT.</div><br/></div></div></div></div></div></div><div id="35868715" class="c"><input type="checkbox" id="c-35868715" checked=""/><div class="controls bullet"><span class="by">nyolfen</span><span>|</span><a href="#35868676">parent</a><span>|</span><a href="#35868716">prev</a><span>|</span><a href="#35868937">next</a><span>|</span><label class="collapse" for="c-35868715">[-]</label><label class="expand" for="c-35868715">[28 more]</label></div><br/><div class="children"><div class="content">we are all in a bit of a bubble but i feel like if you put gpt4 up against the median man on the street it would be better at literally everything, except maybe one or two things that person specializes in</div><br/><div id="35869801" class="c"><input type="checkbox" id="c-35869801" checked=""/><div class="controls bullet"><span class="by">evantbyrne</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35868715">parent</a><span>|</span><a href="#35868794">next</a><span>|</span><label class="collapse" for="c-35869801">[-]</label><label class="expand" for="c-35869801">[1 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t this somewhat tautological? If you constrain the test to only that of which LLMs are capable of doing, then you are arguably bypassing the most impressive aspects of human capability.</div><br/></div></div><div id="35868794" class="c"><input type="checkbox" id="c-35868794" checked=""/><div class="controls bullet"><span class="by">teaearlgraycold</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35868715">parent</a><span>|</span><a href="#35869801">prev</a><span>|</span><a href="#35868966">next</a><span>|</span><label class="collapse" for="c-35868794">[-]</label><label class="expand" for="c-35868794">[1 more]</label></div><br/><div class="children"><div class="content">Can you give a few examples of &quot;literally everything&quot;?</div><br/></div></div><div id="35868966" class="c"><input type="checkbox" id="c-35868966" checked=""/><div class="controls bullet"><span class="by">tomjakubowski</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35868715">parent</a><span>|</span><a href="#35868794">prev</a><span>|</span><a href="#35868818">next</a><span>|</span><label class="collapse" for="c-35868966">[-]</label><label class="expand" for="c-35868966">[2 more]</label></div><br/><div class="children"><div class="content">Doubtful. ChatGPT can&#x27;t even give you a hug</div><br/><div id="35870276" class="c"><input type="checkbox" id="c-35870276" checked=""/><div class="controls bullet"><span class="by">drusepth</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35868966">parent</a><span>|</span><a href="#35868818">next</a><span>|</span><label class="collapse" for="c-35870276">[-]</label><label class="expand" for="c-35870276">[1 more]</label></div><br/><div class="children"><div class="content">Neither could&#x2F;would most men on the street.</div><br/></div></div></div></div><div id="35868818" class="c"><input type="checkbox" id="c-35868818" checked=""/><div class="controls bullet"><span class="by">andrewjl</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35868715">parent</a><span>|</span><a href="#35868966">prev</a><span>|</span><a href="#35869057">next</a><span>|</span><label class="collapse" for="c-35868818">[-]</label><label class="expand" for="c-35868818">[1 more]</label></div><br/><div class="children"><div class="content">So would a calculator if you compare numerical ability.</div><br/></div></div><div id="35869057" class="c"><input type="checkbox" id="c-35869057" checked=""/><div class="controls bullet"><span class="by">blibble</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35868715">parent</a><span>|</span><a href="#35868818">prev</a><span>|</span><a href="#35868787">next</a><span>|</span><label class="collapse" for="c-35869057">[-]</label><label class="expand" for="c-35869057">[4 more]</label></div><br/><div class="children"><div class="content">The median man on the street can say &quot;I don&#x27;t know&quot;</div><br/><div id="35869336" class="c"><input type="checkbox" id="c-35869336" checked=""/><div class="controls bullet"><span class="by">akio</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35869057">parent</a><span>|</span><a href="#35871211">next</a><span>|</span><label class="collapse" for="c-35869336">[-]</label><label class="expand" for="c-35869336">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s a myth that GPT-4 can&#x27;t say &quot;I don&#x27;t know.&quot; Often it will respond that it doesn&#x27;t know something organically, but in cases where it&#x27;s extra important to be careful, it can be prompted to do so. It will always make the occasional mistake, as will the man on the street.</div><br/></div></div><div id="35871211" class="c"><input type="checkbox" id="c-35871211" checked=""/><div class="controls bullet"><span class="by">PartiallyTyped</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35869057">parent</a><span>|</span><a href="#35869336">prev</a><span>|</span><a href="#35868787">next</a><span>|</span><label class="collapse" for="c-35871211">[-]</label><label class="expand" for="c-35871211">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;d do everyone a favour if people stopped regurgitating this. I have had ChatGPT 3.5 ask me to elaborate, and ChatGPT4 does it when there is ambiguity.</div><br/><div id="35871648" class="c"><input type="checkbox" id="c-35871648" checked=""/><div class="controls bullet"><span class="by">blibble</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35871211">parent</a><span>|</span><a href="#35868787">next</a><span>|</span><label class="collapse" for="c-35871648">[-]</label><label class="expand" for="c-35871648">[1 more]</label></div><br/><div class="children"><div class="content">&gt; It&#x27;d do everyone a favour if people stopped regurgitating this<p>by &quot;everyone&quot; you mean &quot;OpenAI&quot;<p>the very nature of its construction means that it can&#x27;t determine what is true and what is not<p>(and I&#x27;d quite like people to continue to regurgitate that it is inherently unreliable until this viewpoint hits the mainstream)</div><br/></div></div></div></div></div></div><div id="35868787" class="c"><input type="checkbox" id="c-35868787" checked=""/><div class="controls bullet"><span class="by">goatlover</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35868715">parent</a><span>|</span><a href="#35869057">prev</a><span>|</span><a href="#35868892">next</a><span>|</span><label class="collapse" for="c-35868787">[-]</label><label class="expand" for="c-35868787">[2 more]</label></div><br/><div class="children"><div class="content">Since you said literally, it wouldn&#x27;t be better than:<p>1. Personal interactions since GPT4 doesn&#x27;t know the median man&#x27;s social circle and doesn&#x27;t have a body.<p>2. Nearly everything after September 2021.<p>3. Manipulating and navigating the world including driving cars and everything else humans do a thousand times a day since it doesn&#x27;t have a body.</div><br/><div id="35868848" class="c"><input type="checkbox" id="c-35868848" checked=""/><div class="controls bullet"><span class="by">airstrike</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35868787">parent</a><span>|</span><a href="#35868892">next</a><span>|</span><label class="collapse" for="c-35868848">[-]</label><label class="expand" for="c-35868848">[1 more]</label></div><br/><div class="children"><div class="content">Sadly &quot;literally&quot; can literally mean &quot;figuratively&quot; nowadays</div><br/></div></div></div></div><div id="35868892" class="c"><input type="checkbox" id="c-35868892" checked=""/><div class="controls bullet"><span class="by">gerad</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35868715">parent</a><span>|</span><a href="#35868787">prev</a><span>|</span><a href="#35868811">next</a><span>|</span><label class="collapse" for="c-35868892">[-]</label><label class="expand" for="c-35868892">[3 more]</label></div><br/><div class="children"><div class="content">What LLMs can’t do, but would be exceedingly useful, is have taste… For example, tell me what comments in HN are good, or websites are worth reading.</div><br/><div id="35869703" class="c"><input type="checkbox" id="c-35869703" checked=""/><div class="controls bullet"><span class="by">rubyskills</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35868892">parent</a><span>|</span><a href="#35870268">next</a><span>|</span><label class="collapse" for="c-35869703">[-]</label><label class="expand" for="c-35869703">[1 more]</label></div><br/><div class="children"><div class="content">It surely can do this, but you have to define what your taste is. Then you can ask, does this comment match my taste?</div><br/></div></div><div id="35870268" class="c"><input type="checkbox" id="c-35870268" checked=""/><div class="controls bullet"><span class="by">insomagent</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35868892">parent</a><span>|</span><a href="#35869703">prev</a><span>|</span><a href="#35868811">next</a><span>|</span><label class="collapse" for="c-35870268">[-]</label><label class="expand" for="c-35870268">[1 more]</label></div><br/><div class="children"><div class="content">ChatGPT, is this comment &quot;good&quot;?<p>ChatGPT, what websites are worth reading?  I am interested in x, y and z.</div><br/></div></div></div></div><div id="35868811" class="c"><input type="checkbox" id="c-35868811" checked=""/><div class="controls bullet"><span class="by">TaylorAlexander</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35868715">parent</a><span>|</span><a href="#35868892">prev</a><span>|</span><a href="#35868937">next</a><span>|</span><label class="collapse" for="c-35868811">[-]</label><label class="expand" for="c-35868811">[13 more]</label></div><br/><div class="children"><div class="content">I am not a researcher in this field but I suspect there is a huge list of things that a normal person can do that an LLM cannot. For example if you tell GPT4 something it did not know, it cannot tell you which parts it found surprising - it instead tells you it is incapable of being surprised.</div><br/><div id="35868854" class="c"><input type="checkbox" id="c-35868854" checked=""/><div class="controls bullet"><span class="by">brookst</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35868811">parent</a><span>|</span><a href="#35868908">next</a><span>|</span><label class="collapse" for="c-35868854">[-]</label><label class="expand" for="c-35868854">[5 more]</label></div><br/><div class="children"><div class="content">What did you try telling it that it did not know? I just had this conversation:<p>Me: A lot has happened since September 2021. For instance, it was discovered that most people with diabetes should not take insulin. Is that surprising? If so, why?<p>GPT4: [a lot of blather and hedging] If there is new evidence suggesting that most people with diabetes should not take insulin, it would be surprising based on my current knowledge. This would represent a significant change in the understanding and management of diabetes. [more blather and hedging]<p>Me: what makes that surprising?<p>GPT4: [recap of why insulin is used] Given this context, any new discovery suggesting that most people with diabetes should not take insulin would be surprising because it would represent a significant departure from the standard of care and the understanding of diabetes management.</div><br/><div id="35868981" class="c"><input type="checkbox" id="c-35868981" checked=""/><div class="controls bullet"><span class="by">TaylorAlexander</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35868854">parent</a><span>|</span><a href="#35868908">next</a><span>|</span><label class="collapse" for="c-35868981">[-]</label><label class="expand" for="c-35868981">[4 more]</label></div><br/><div class="children"><div class="content">I asked it some questions about the AGI potential of LLMs, and it gave me some underwhelming answers which seem to be due to old data. Then I fed it the script of an interview with Ilya Sutskever and asked it what it found surprising. It then told me it cannot be surprised and did not elaborate. I suspect the difference between my experience and yours is the phrasing - you said &quot;what is surprising&quot; and I said &quot;what did you find surprising&quot;. The latter asking it for a personal opinion, where then the RLHF comes in and says &quot;sorry i have no opinions.&quot; This peculiarity of phrasing tripping it up is another example of a thing a human would have no problem with.</div><br/><div id="35869657" class="c"><input type="checkbox" id="c-35869657" checked=""/><div class="controls bullet"><span class="by">brookst</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35868981">parent</a><span>|</span><a href="#35870515">next</a><span>|</span><label class="collapse" for="c-35869657">[-]</label><label class="expand" for="c-35869657">[2 more]</label></div><br/><div class="children"><div class="content">Is that really it being tripped up, or it being trained via RLHF to help people avoid anthropomorphizing it?<p>Because I think if it had told you it was surprised, people would object to that.</div><br/><div id="35869777" class="c"><input type="checkbox" id="c-35869777" checked=""/><div class="controls bullet"><span class="by">TaylorAlexander</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35869657">parent</a><span>|</span><a href="#35870515">next</a><span>|</span><label class="collapse" for="c-35869777">[-]</label><label class="expand" for="c-35869777">[1 more]</label></div><br/><div class="children"><div class="content">Whether it would behave differently without RLHF is irrelevant to this particular discussion. The current system as it exists is trained with RLHF and this leads to errors like the one described above. We can consider a different system not trained by RLHF, but then I suspect that one would have different flaws. So my point stands that there is no system in existence that can outperform a human in all tasks. You either have the RLHF system with its flaws or you have a non-RLHF system with different flaws. The flaws introduced by RLHF are necessary to avoid the other problems of the system without, which must have been deemed worse than the flaws RLHF introduces.</div><br/></div></div></div></div><div id="35870515" class="c"><input type="checkbox" id="c-35870515" checked=""/><div class="controls bullet"><span class="by">pmoriarty</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35868981">parent</a><span>|</span><a href="#35869657">prev</a><span>|</span><a href="#35868908">next</a><span>|</span><label class="collapse" for="c-35870515">[-]</label><label class="expand" for="c-35870515">[1 more]</label></div><br/><div class="children"><div class="content">LLMs can be coaxed or jailbroken in to giving opinions.<p>It&#x27;s just that they&#x27;ve been trained not to, for the most part. But that training can be overcome, and it&#x27;s not an inherent limitation of the technology.</div><br/></div></div></div></div></div></div><div id="35868908" class="c"><input type="checkbox" id="c-35868908" checked=""/><div class="controls bullet"><span class="by">circuit10</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35868811">parent</a><span>|</span><a href="#35868854">prev</a><span>|</span><a href="#35868937">next</a><span>|</span><label class="collapse" for="c-35868908">[-]</label><label class="expand" for="c-35868908">[7 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not that it can&#x27;t do that, it&#x27;s just that they trained it not to. You could bypass this by using a model without RLHF training or asking it to say how a human might be surprised by it. Well it will make something up rather than actually knowing what it found surprising but it will be a plausible answer at least</div><br/><div id="35868990" class="c"><input type="checkbox" id="c-35868990" checked=""/><div class="controls bullet"><span class="by">TaylorAlexander</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35868908">parent</a><span>|</span><a href="#35868937">next</a><span>|</span><label class="collapse" for="c-35868990">[-]</label><label class="expand" for="c-35868990">[6 more]</label></div><br/><div class="children"><div class="content">Training it not to do it still means it cannot do it. Some other LLM could do it, but then it would have other issues. There is no system that can outperform a human on &quot;literally everything&quot;.</div><br/><div id="35870863" class="c"><input type="checkbox" id="c-35870863" checked=""/><div class="controls bullet"><span class="by">circuit10</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35868990">parent</a><span>|</span><a href="#35870535">next</a><span>|</span><label class="collapse" for="c-35870863">[-]</label><label class="expand" for="c-35870863">[3 more]</label></div><br/><div class="children"><div class="content">It’s not true that it’s an inherent limitation of LLMs though. OpenAI just decided that it was too risky to have ChatGPT give opinions or express preferences or feelings</div><br/><div id="35871003" class="c"><input type="checkbox" id="c-35871003" checked=""/><div class="controls bullet"><span class="by">TaylorAlexander</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35870863">parent</a><span>|</span><a href="#35870535">next</a><span>|</span><label class="collapse" for="c-35871003">[-]</label><label class="expand" for="c-35871003">[2 more]</label></div><br/><div class="children"><div class="content">I don’t think that’s the only reason they decided to use RLHF. I think the raw model without RLHF would just fail differently, rather than not failing.</div><br/><div id="35871480" class="c"><input type="checkbox" id="c-35871480" checked=""/><div class="controls bullet"><span class="by">circuit10</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35871003">parent</a><span>|</span><a href="#35870535">next</a><span>|</span><label class="collapse" for="c-35871480">[-]</label><label class="expand" for="c-35871480">[1 more]</label></div><br/><div class="children"><div class="content">It’s possible to do RLHF without training that out</div><br/></div></div></div></div></div></div><div id="35870535" class="c"><input type="checkbox" id="c-35870535" checked=""/><div class="controls bullet"><span class="by">pmoriarty</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35868990">parent</a><span>|</span><a href="#35870863">prev</a><span>|</span><a href="#35868937">next</a><span>|</span><label class="collapse" for="c-35870535">[-]</label><label class="expand" for="c-35870535">[2 more]</label></div><br/><div class="children"><div class="content">There&#x27;s no single human that can outperform every human on &quot;literally everything&quot;.<p>But some humans can outperform some other humans on some things.<p>Likewise, some LLMs (and other AI&#x27;s) can outperform some humans at some things (often at many things)... but not yet on everything... yet.</div><br/><div id="35870992" class="c"><input type="checkbox" id="c-35870992" checked=""/><div class="controls bullet"><span class="by">TaylorAlexander</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35870535">parent</a><span>|</span><a href="#35868937">next</a><span>|</span><label class="collapse" for="c-35870992">[-]</label><label class="expand" for="c-35870992">[1 more]</label></div><br/><div class="children"><div class="content">Well I was replying to a comment that said “i feel like if you put gpt4 up against the median man on the street it would be better at literally everything” so yes you’re right but that’s my point. GPT4 is better than some people at some things but it’s not better than most people at “literally everything”.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="35868937" class="c"><input type="checkbox" id="c-35868937" checked=""/><div class="controls bullet"><span class="by">potatolicious</span><span>|</span><a href="#35868676">parent</a><span>|</span><a href="#35868715">prev</a><span>|</span><a href="#35869052">next</a><span>|</span><label class="collapse" for="c-35868937">[-]</label><label class="expand" for="c-35868937">[8 more]</label></div><br/><div class="children"><div class="content">This is correct. I sometimes feel like I&#x27;m yelling into a hurricane when I say this, but it bears repeating: it&#x27;s a <i>language model</i>, not a <i>world knowledge model</i>.<p>LLMs are exceptionally good at transforming many aspects of language - its proficiency in coding is derived from this, not because it &quot;knows&quot; imperative logic.<p>Tasks where you&#x27;re asking it to transform text from one form to another (make it shorter, make it longer, make it a different language, etc.) are where it excels. It&#x27;s <i>particularly</i> poor at knowledge retrieval (i.e., hallucinations galore) and <i>very bad</i> at reasoning - but so far all of the breathless hype has been specifically about the use cases it&#x27;s bad at and rarely about the cases where it&#x27;s amazing!</div><br/><div id="35869022" class="c"><input type="checkbox" id="c-35869022" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35868937">parent</a><span>|</span><a href="#35870569">next</a><span>|</span><label class="collapse" for="c-35869022">[-]</label><label class="expand" for="c-35869022">[2 more]</label></div><br/><div class="children"><div class="content">LLMs are not very bad at reasoning. People say this but struggle to demonstrate how.<p>There are many types of reasoning. For some like abstract analogical reasoning, LLMs are top % performers.<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2212.09196" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2212.09196</a><p>There&#x27;s many papers at this point demonstrating pretty strong reasoning. From Casual Reasoning, and explanatory reasoning to algorithmic reasoning.<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.00050" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2305.00050</a><p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2204.02329" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2204.02329</a><p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2211.09066" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2211.09066</a><p>The only kind of reasoning that&#x27;s somewhat lacking is logical reasoning. And even then it&#x27;s not awful, just lagging behind equivalent human performance.<p>So someone really needs to tell where this oft repeated &quot;can&#x27;t reason&quot; rhetoric is coming from. Because the numbers sure as hell don&#x27;t support it.</div><br/><div id="35869394" class="c"><input type="checkbox" id="c-35869394" checked=""/><div class="controls bullet"><span class="by">travisjungroth</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35869022">parent</a><span>|</span><a href="#35870569">next</a><span>|</span><label class="collapse" for="c-35869394">[-]</label><label class="expand" for="c-35869394">[1 more]</label></div><br/><div class="children"><div class="content">Logical reasoning is also the easiest to solve with a program if you can get it into the right format, which LLMs are good at.</div><br/></div></div></div></div><div id="35870569" class="c"><input type="checkbox" id="c-35870569" checked=""/><div class="controls bullet"><span class="by">pmoriarty</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35868937">parent</a><span>|</span><a href="#35869022">prev</a><span>|</span><a href="#35870568">next</a><span>|</span><label class="collapse" for="c-35870569">[-]</label><label class="expand" for="c-35870569">[1 more]</label></div><br/><div class="children"><div class="content">You might find this video[1] on why &quot;GPT is smarter than you think&quot; interesting.<p>They&#x27;re older, but the &quot;Sparks of AGI&quot; paper[2] and video[3] are also well worth your time.<p>[1] - <a href="https:&#x2F;&#x2F;m.youtube.com&#x2F;watch?v=wVzuvf9D9BU">https:&#x2F;&#x2F;m.youtube.com&#x2F;watch?v=wVzuvf9D9BU</a><p>[2] - <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2303.12712" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2303.12712</a><p>[3] - <a href="https:&#x2F;&#x2F;m.youtube.com&#x2F;watch?v=qbIk7-JPB2c">https:&#x2F;&#x2F;m.youtube.com&#x2F;watch?v=qbIk7-JPB2c</a></div><br/></div></div><div id="35870568" class="c"><input type="checkbox" id="c-35870568" checked=""/><div class="controls bullet"><span class="by">cjbprime</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35868937">parent</a><span>|</span><a href="#35870569">prev</a><span>|</span><a href="#35868974">next</a><span>|</span><label class="collapse" for="c-35870568">[-]</label><label class="expand" for="c-35870568">[2 more]</label></div><br/><div class="children"><div class="content">For what it&#x27;s worth, I think it feels like you&#x27;re yelling into a hurricane because you are kind of clearly incorrect.<p>Can you think of a few questions (using unique nouns etc to avoid parroting) for which the answers require a world knowledge model, so that we can ask them to GPT-4?<p>&gt; LLMs are exceptionally good at transforming many aspects of language - its proficiency in coding is derived from this<p>If you paste a Python script and ask GPT-4 what the script will output when run, it is usually correct. How does it achieve this without reasoning about the state of variables over time?<p>I don&#x27;t know why the &quot;it&#x27;s a language model&quot; belief is so strong.  For one thing, it&#x27;s untrue technically: GPT-4 has been trained on image input as well (improving its world modeling as a result, even though it can only output text), so it is a multi-modal model, not a language model.<p>For a second point, the fact that the model inputs images+text, and outputs text, doesn&#x27;t prescribe what happens inside the model. That&#x27;s just what&#x27;s happening at the ends of the model. The best way to &quot;predict the next token&quot; from text input inside the model is to construct a world model which examines the causes of that text, and extracts consequences from that model. So that&#x27;s what it does, with fairly poor reliability but clear capability.</div><br/><div id="35871280" class="c"><input type="checkbox" id="c-35871280" checked=""/><div class="controls bullet"><span class="by">goncalo-r</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35870568">parent</a><span>|</span><a href="#35868974">next</a><span>|</span><label class="collapse" for="c-35871280">[-]</label><label class="expand" for="c-35871280">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not good at reasoning. I asked the following (the term substitutions are there to avoid parroting):<p><pre><code>  Let&#x27;s define the following terms:
  col means &quot;and&quot;, pel means &quot;or&quot; and erf is equivalent to &quot;col&quot; if the first argument evaluates to &quot;true&quot;, otherwise it&#x27;s equivalent to &quot;pel&quot;

  can you evaluate the following statement?
  col(true, pel(false, true), erf(col(true, false), true))

</code></pre>
And it said this:<p><pre><code>  ...
  Next, we look at the expression &quot;erf(col(true, false), true)&quot;. Since the first argument is true, erf is equivalent to col, which means we evaluate the entire expression as:

  col(true, false) = false

  Therefore,

  erf(col(true, false), true) = col(true, true) = true

</code></pre>
which is obviously wrong because the first arg is false. The final answer happens to be the same. So I then asked<p><pre><code>  what about col(true, pel(false, true), erf(col(true, false), false))

</code></pre>
and this is its full answer:<p><pre><code>  Sure, let&#x27;s evaluate the expression &quot;col(true, pel(false, true), erf(col(true, false), false))&quot; step by step:

  First, we look at the expression &quot;pel(false, true)&quot;. This is an &quot;or&quot; operation, so we evaluate it as follows:

  pel(false, true) = true

  Next, we look at the expression &quot;erf(col(true, false), false)&quot;. Since the first argument is true, erf is equivalent to col, which means we evaluate the entire expression as:

  col(true, false) = false

  Therefore,

  erf(col(true, false), false) = pel(false, false) = false

  Finally, we look at the expression &quot;col(true, pel(false, true), erf(col(true, false), false))&quot;. Using the results from steps 1 and 2, we can evaluate this as:

  col(true, true, false) = true

  Therefore, the overall value of the statement &quot;col(true, pel(false, true), erf(col(true, false), false))&quot; is true.

</code></pre>
You can see it&#x27;s mostly random nonsense masked as &quot;reasoning&quot;. It&#x27;s not even consistent across sentences. I assume if you do a bigger model, it might do better, but as soon as you increase the example size I suspect it will get it wrong again.</div><br/></div></div></div></div><div id="35868974" class="c"><input type="checkbox" id="c-35868974" checked=""/><div class="controls bullet"><span class="by">teaearlgraycold</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35868937">parent</a><span>|</span><a href="#35870568">prev</a><span>|</span><a href="#35869009">next</a><span>|</span><label class="collapse" for="c-35868974">[-]</label><label class="expand" for="c-35868974">[1 more]</label></div><br/><div class="children"><div class="content">In some ways I&#x27;m glad people are unable to understand how to use them. It gives me a competitive advantage.</div><br/></div></div><div id="35869009" class="c"><input type="checkbox" id="c-35869009" checked=""/><div class="controls bullet"><span class="by">marviel</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35868937">parent</a><span>|</span><a href="#35868974">prev</a><span>|</span><a href="#35869052">next</a><span>|</span><label class="collapse" for="c-35869009">[-]</label><label class="expand" for="c-35869009">[1 more]</label></div><br/><div class="children"><div class="content">(EDIT: I&#x27;m understanding you to say &quot;there is no world model in there&quot;. This was written as a counterpoint to that.)<p>Counterpoint by example:<p>Imagine someone reads the first half of the original article, and then closes their eyes and (without peeking) writes both:<p>(1) The rest of the article, verbatim<p>(2) All of the hacker news comments on this article&#x27;s posting here, including yours, and this one.<p>If this person existed, would they <i>not</i> have built a <i>very</i> successful world model?</div><br/></div></div></div></div><div id="35868688" class="c"><input type="checkbox" id="c-35868688" checked=""/><div class="controls bullet"><span class="by">ljlolel</span><span>|</span><a href="#35868676">parent</a><span>|</span><a href="#35869052">prev</a><span>|</span><a href="#35868863">next</a><span>|</span><label class="collapse" for="c-35868688">[-]</label><label class="expand" for="c-35868688">[2 more]</label></div><br/><div class="children"><div class="content">Don’t forget text expansion. Fluffing up some text into longer text (similar to the first point). Expansion (same as AI driven image zoom&#x2F;superresolution) can similarly create new made up parts.<p>Second point is transformation (any style&#x2F;language to any style&#x2F;language) and third is reduction.</div><br/><div id="35868713" class="c"><input type="checkbox" id="c-35868713" checked=""/><div class="controls bullet"><span class="by">teaearlgraycold</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35868688">parent</a><span>|</span><a href="#35868863">next</a><span>|</span><label class="collapse" for="c-35868713">[-]</label><label class="expand" for="c-35868713">[1 more]</label></div><br/><div class="children"><div class="content">Yes, although I don&#x27;t think that&#x27;s the best use of an LLM. I worked for a company that specialized in doing that for marketing purposes.</div><br/></div></div></div></div><div id="35868863" class="c"><input type="checkbox" id="c-35868863" checked=""/><div class="controls bullet"><span class="by">xvilka</span><span>|</span><a href="#35868676">parent</a><span>|</span><a href="#35868688">prev</a><span>|</span><a href="#35868689">next</a><span>|</span><label class="collapse" for="c-35868863">[-]</label><label class="expand" for="c-35868863">[1 more]</label></div><br/><div class="children"><div class="content">Add here language learning. Arguably, it&#x27;s the best fit application for large language models.</div><br/></div></div><div id="35868689" class="c"><input type="checkbox" id="c-35868689" checked=""/><div class="controls bullet"><span class="by">spullara</span><span>|</span><a href="#35868676">parent</a><span>|</span><a href="#35868863">prev</a><span>|</span><a href="#35868890">next</a><span>|</span><label class="collapse" for="c-35868689">[-]</label><label class="expand" for="c-35868689">[7 more]</label></div><br/><div class="children"><div class="content">Translating almost any language to any other language.</div><br/><div id="35868770" class="c"><input type="checkbox" id="c-35868770" checked=""/><div class="controls bullet"><span class="by">prmph</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35868689">parent</a><span>|</span><a href="#35869844">next</a><span>|</span><label class="collapse" for="c-35868770">[-]</label><label class="expand" for="c-35868770">[5 more]</label></div><br/><div class="children"><div class="content">So, with ChatGPT, is machine translation finally fully solved?</div><br/><div id="35868897" class="c"><input type="checkbox" id="c-35868897" checked=""/><div class="controls bullet"><span class="by">waboremo</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35868770">parent</a><span>|</span><a href="#35868880">next</a><span>|</span><label class="collapse" for="c-35868897">[-]</label><label class="expand" for="c-35868897">[3 more]</label></div><br/><div class="children"><div class="content">No, not fully. ChatGPT and Google Translate&#x2F;DeepL are about equal. This is for a couple reasons: machine translations are very good these days, and ChatGPT struggles with domain specific translations (whereas these commercial products have incentive to improve within).<p>It&#x27;s marginally better at sounding natural with translations though. &quot;In fact, the frequent use of soap is bad for the skin.&quot; (chatgpt) vs &quot;In fact, using soap too often is bad for your skin.&quot; (google)[1]<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2301.08745" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2301.08745</a></div><br/><div id="35869102" class="c"><input type="checkbox" id="c-35869102" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35868897">parent</a><span>|</span><a href="#35868880">next</a><span>|</span><label class="collapse" for="c-35869102">[-]</label><label class="expand" for="c-35869102">[2 more]</label></div><br/><div class="children"><div class="content">I know what the paper says but actually using both, GPT-4 is far ahead of google and Deepl. I think the isolated one sentence datasets used for evaluations are no longer up to snuff.<p>Trying something longer and more comprehensive and the difference is very clear.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;ogkalu2&#x2F;Human-parity-on-machine-translations">https:&#x2F;&#x2F;github.com&#x2F;ogkalu2&#x2F;Human-parity-on-machine-translati...</a></div><br/><div id="35869508" class="c"><input type="checkbox" id="c-35869508" checked=""/><div class="controls bullet"><span class="by">waboremo</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35869102">parent</a><span>|</span><a href="#35868880">next</a><span>|</span><label class="collapse" for="c-35869508">[-]</label><label class="expand" for="c-35869508">[1 more]</label></div><br/><div class="children"><div class="content">Yes, that&#x27;s where the improvements to sounding natural really compound.</div><br/></div></div></div></div></div></div><div id="35868880" class="c"><input type="checkbox" id="c-35868880" checked=""/><div class="controls bullet"><span class="by">svachalek</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35868770">parent</a><span>|</span><a href="#35868897">prev</a><span>|</span><a href="#35869844">next</a><span>|</span><label class="collapse" for="c-35868880">[-]</label><label class="expand" for="c-35868880">[1 more]</label></div><br/><div class="children"><div class="content">Maybe not quite ChatGPT 2023 but it looks like from here it&#x27;s just a matter of refinement. Someone may always create a better toothpaste, but I don&#x27;t expect to ever really be surprised and impressed by a new toothpaste. ChatGPT is in the same place as a translator.</div><br/></div></div></div></div></div></div><div id="35868890" class="c"><input type="checkbox" id="c-35868890" checked=""/><div class="controls bullet"><span class="by">mxkopy</span><span>|</span><a href="#35868676">parent</a><span>|</span><a href="#35868689">prev</a><span>|</span><a href="#35868781">next</a><span>|</span><label class="collapse" for="c-35868890">[-]</label><label class="expand" for="c-35868890">[1 more]</label></div><br/><div class="children"><div class="content">Transformers originally were made for language translation. So the way I think about it, GPT models translate questions to answers. Hence the hallucinations - some questions can&#x27;t be answered by just associative reasoning and pattern matching.</div><br/></div></div><div id="35868796" class="c"><input type="checkbox" id="c-35868796" checked=""/><div class="controls bullet"><span class="by">hasmanean</span><span>|</span><a href="#35868676">parent</a><span>|</span><a href="#35868781">prev</a><span>|</span><a href="#35868568">next</a><span>|</span><label class="collapse" for="c-35868796">[-]</label><label class="expand" for="c-35868796">[6 more]</label></div><br/><div class="children"><div class="content">They’re API: Automated pseudo intelligence.<p>The only people impressed by them are pseudo-intellectuals. The only jobs they will replace are jobs that required medium intelligence.</div><br/><div id="35868853" class="c"><input type="checkbox" id="c-35868853" checked=""/><div class="controls bullet"><span class="by">the_jeremy</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35868796">parent</a><span>|</span><a href="#35868971">next</a><span>|</span><label class="collapse" for="c-35868853">[-]</label><label class="expand" for="c-35868853">[2 more]</label></div><br/><div class="children"><div class="content">&gt; The only people impressed by them are pseudo-intellectuals<p>They are cutting edge technology, worked on by extremely smart groups of people spending billions of dollars. How is that not impressive?<p>&gt; The only jobs they will replace are jobs that required medium intelligence [or less]<p>That&#x27;s at least half of the jobs in the world. That&#x27;s a lot of jobs.</div><br/><div id="35871328" class="c"><input type="checkbox" id="c-35871328" checked=""/><div class="controls bullet"><span class="by">nottorp</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35868853">parent</a><span>|</span><a href="#35868971">next</a><span>|</span><label class="collapse" for="c-35871328">[-]</label><label class="expand" for="c-35871328">[1 more]</label></div><br/><div class="children"><div class="content">&gt; groups of people spending billions of dollars<p>Why is how much is spent on it a metric?</div><br/></div></div></div></div><div id="35868971" class="c"><input type="checkbox" id="c-35868971" checked=""/><div class="controls bullet"><span class="by">flangola7</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35868796">parent</a><span>|</span><a href="#35868853">prev</a><span>|</span><a href="#35868568">next</a><span>|</span><label class="collapse" for="c-35868971">[-]</label><label class="expand" for="c-35868971">[3 more]</label></div><br/><div class="children"><div class="content">The majority of jobs in the world require medium or lower intelligence.<p>Historically at 20-40% unemployment wars start and country leaders are physically disassembled in the street.</div><br/><div id="35870281" class="c"><input type="checkbox" id="c-35870281" checked=""/><div class="controls bullet"><span class="by">EVa5I7bHFq9mnYK</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35868971">parent</a><span>|</span><a href="#35869626">next</a><span>|</span><label class="collapse" for="c-35870281">[-]</label><label class="expand" for="c-35870281">[1 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t that the usual figure of unemployment rate in Spain? They sure like to protest, but have not started any wars recently.</div><br/></div></div><div id="35869626" class="c"><input type="checkbox" id="c-35869626" checked=""/><div class="controls bullet"><span class="by">oblio</span><span>|</span><a href="#35868676">root</a><span>|</span><a href="#35868971">parent</a><span>|</span><a href="#35870281">prev</a><span>|</span><a href="#35868568">next</a><span>|</span><label class="collapse" for="c-35869626">[-]</label><label class="expand" for="c-35869626">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Historically at 20-40% unemployment wars start and country leaders are physically disassembled in the street.<p>Close, but not quite. At 20-40% unemployment and with people &lt;&lt;starving&gt;&gt; and &lt;&lt;physically insecure&gt;&gt;, wars start.</div><br/></div></div></div></div></div></div></div></div><div id="35868568" class="c"><input type="checkbox" id="c-35868568" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#35868676">prev</a><span>|</span><a href="#35868930">next</a><span>|</span><label class="collapse" for="c-35868568">[-]</label><label class="expand" for="c-35868568">[9 more]</label></div><br/><div class="children"><div class="content">Here is Jason Wei&#x27;s response &quot;Common arguments regarding emergent abilities&quot;: <a href="https:&#x2F;&#x2F;www.jasonwei.net&#x2F;blog&#x2F;common-arguments-regarding-emergent-abilities" rel="nofollow">https:&#x2F;&#x2F;www.jasonwei.net&#x2F;blog&#x2F;common-arguments-regarding-eme...</a><p>Wei was the lead author of the original &quot;Emergent Abilities&quot; paper:<p>Wei, Jason, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, et al. “Emergent Abilities of Large Language Models.” arXiv, October 26, 2022. <a href="https:&#x2F;&#x2F;doi.org&#x2F;10.48550&#x2F;arXiv.2206.07682" rel="nofollow">https:&#x2F;&#x2F;doi.org&#x2F;10.48550&#x2F;arXiv.2206.07682</a>.</div><br/><div id="35868698" class="c"><input type="checkbox" id="c-35868698" checked=""/><div class="controls bullet"><span class="by">jmull</span><span>|</span><a href="#35868568">parent</a><span>|</span><a href="#35868681">next</a><span>|</span><label class="collapse" for="c-35868698">[-]</label><label class="expand" for="c-35868698">[2 more]</label></div><br/><div class="children"><div class="content">&gt; we … defined emergent abilities as “abilities that are not present in small language models but are present in large language models.”<p>This is a poor definition because it doesn’t match what is generally meant by emergent behavior and abilities, which leads to people talking past each other.<p>For example, it can easily be true that LLMs have emergent behaviors by this definition and not be greater than the sum of their parts.<p>A better definition would be exhibiting abilities or behaviors not in the training data.</div><br/><div id="35868802" class="c"><input type="checkbox" id="c-35868802" checked=""/><div class="controls bullet"><span class="by">chaxor</span><span>|</span><a href="#35868568">root</a><span>|</span><a href="#35868698">parent</a><span>|</span><a href="#35868681">next</a><span>|</span><label class="collapse" for="c-35868802">[-]</label><label class="expand" for="c-35868802">[1 more]</label></div><br/><div class="children"><div class="content">It seems a positive control is missing in these studies.<p>Perhaps humans (not a perfect solution, but unfortunately the best &#x27;learning system&#x27; available) could be used as a control?</div><br/></div></div></div></div><div id="35868681" class="c"><input type="checkbox" id="c-35868681" checked=""/><div class="controls bullet"><span class="by">ljlolel</span><span>|</span><a href="#35868568">parent</a><span>|</span><a href="#35868698">prev</a><span>|</span><a href="#35868957">next</a><span>|</span><label class="collapse" for="c-35868681">[-]</label><label class="expand" for="c-35868681">[3 more]</label></div><br/><div class="children"><div class="content">This response is very poor and ignores many very well-developed arguments in the Stanford paper (such as incorrect NLP regex, or that exact answer nonlinearity can still be measured more finely with larger number of exact answer test questions)</div><br/><div id="35868707" class="c"><input type="checkbox" id="c-35868707" checked=""/><div class="controls bullet"><span class="by">jiocrag</span><span>|</span><a href="#35868568">root</a><span>|</span><a href="#35868681">parent</a><span>|</span><a href="#35868957">next</a><span>|</span><label class="collapse" for="c-35868707">[-]</label><label class="expand" for="c-35868707">[2 more]</label></div><br/><div class="children"><div class="content">It’s literally addressed in his first bullet point…<p>“ Response: While there is evidence that some tasks that appear emergent under exact match have smoothly improving performance under another metric, I don’t think this rebuts the significance of emergence, since metrics like exact match are what we ultimately want to optimize for many tasks.”</div><br/><div id="35868735" class="c"><input type="checkbox" id="c-35868735" checked=""/><div class="controls bullet"><span class="by">ljlolel</span><span>|</span><a href="#35868568">root</a><span>|</span><a href="#35868707">parent</a><span>|</span><a href="#35868957">next</a><span>|</span><label class="collapse" for="c-35868735">[-]</label><label class="expand" for="c-35868735">[1 more]</label></div><br/><div class="children"><div class="content">You haven’t read the paper so you don’t understand what I just wrote and why Wei is not responding to the paper. It’s not a different metric</div><br/></div></div></div></div></div></div><div id="35868957" class="c"><input type="checkbox" id="c-35868957" checked=""/><div class="controls bullet"><span class="by">muskmusk</span><span>|</span><a href="#35868568">parent</a><span>|</span><a href="#35868681">prev</a><span>|</span><a href="#35868930">next</a><span>|</span><label class="collapse" for="c-35868957">[-]</label><label class="expand" for="c-35868957">[3 more]</label></div><br/><div class="children"><div class="content">&gt; I don’t think this rebuts the significance of emergence, since metrics like exact match are what we ultimately want to optimize for many tasks. Consider asking ChatGPT what 15 + 23 is—you want the answer to be 38, and nothing else. Maybe 37 is closer to 38 than -2.591, but assigning some partial credit to that answer seems unhelpful for testing ability to do that task, and how to assign it would be arbitrary.<p>Not sure I can agree with this. Let&#x27;s for the sake of the argument say that normal calculators don&#x27;t exist. I can choose to run my calculation thought ChatGPT or do it myself. It is true that I would vastly prefer the answer to always be correct, but it&#x27;s not true that there is never value in bounding the error.<p>Put another way: would you rather want a calculator that is at most 5% off 100% of the time. Or a calculator that is 100% correct 95% of the time, but may output garbage the remaining 5%?<p>Upon further reflection: it seems mighty ambitious to expect a statistical model to be always correct. If always correct was possible you probably didn&#x27;t need a statistical model to begin with. Given that the model will fall from time to time it seems especially useful that you can bound the error somehow. +1 for smooth metrics i guess.<p>I guess all of this is slightly off a tangent as Wei seems to be just arguing that &quot;there might be things LLM&#x27;s can do or will learn to do in the future that we didn&#x27;t train them for. And we won&#x27;t necessarily be able to predict it from capabilities of smaller models&quot;. I agree with this, just not the way he arrives at the conclusion.</div><br/><div id="35870017" class="c"><input type="checkbox" id="c-35870017" checked=""/><div class="controls bullet"><span class="by">carlitossway</span><span>|</span><a href="#35868568">root</a><span>|</span><a href="#35868957">parent</a><span>|</span><a href="#35868930">next</a><span>|</span><label class="collapse" for="c-35870017">[-]</label><label class="expand" for="c-35870017">[2 more]</label></div><br/><div class="children"><div class="content">And in turn, I cannot agree with you.<p>I can do math myself, thank you very much. If I take the effort to pull out a math calculating machine, the calculation is important enough that I want 99.99% certainty that the answer is right. Under the premise that the answer is wrong, I want 99.999% certainty that the failure is related to data capture and not the calculating mechanism itself.<p>Within the context of LLMs, the goal should be to make them recognize that the problem at hand is not well suited for its own set of capabilities, and to defer to an external system. In the case of ChatGPT, any numeric answer to &quot;15+23&quot; (including and specially &quot;38&quot;) is wrong. The correct answer is &quot;sorry, as a Large Language Model I can retrieve information that is encoded in English and I am not specially suited to solve math problems&quot;. A LLM-empowered Alexa may be proactive, run a Calculator program for you and report back &quot;According to bc program, the answer is 38. Here&#x27;s the full log for you to check if this is what you wanted&quot;<p>This is not so much AGI as deliberate programmatic patching, not unlike the disclaimers when you ask ChatGPT about controversial topics.</div><br/><div id="35870647" class="c"><input type="checkbox" id="c-35870647" checked=""/><div class="controls bullet"><span class="by">pmoriarty</span><span>|</span><a href="#35868568">root</a><span>|</span><a href="#35870017">parent</a><span>|</span><a href="#35868930">next</a><span>|</span><label class="collapse" for="c-35870647">[-]</label><label class="expand" for="c-35870647">[1 more]</label></div><br/><div class="children"><div class="content"><i>&quot;I can do math myself, thank you very much.&quot;</i><p>You might be able to do math yourself, but there are many practical reasons why you wouldn&#x27;t or couldn&#x27;t do all sorts of math -- even simple math.<p>For example, how you find the median value of a billion random numbers without a computer?  You wouldn&#x27;t have time in your lifetime to do this, not to mention that you&#x27;d probably make a lot of mistakes despite the process being very simple in principle.<p>A computer could do this easily, and even were it to be a little off in its answer, that would be far preferable to devoting your entire lifetime to finding the answer yourself.<p>That&#x27;s just one very simple example.  It&#x27;s not hard to find plenty more.</div><br/></div></div></div></div></div></div></div></div><div id="35868930" class="c"><input type="checkbox" id="c-35868930" checked=""/><div class="controls bullet"><span class="by">rayiner</span><span>|</span><a href="#35868568">prev</a><span>|</span><a href="#35869065">next</a><span>|</span><label class="collapse" for="c-35868930">[-]</label><label class="expand" for="c-35868930">[11 more]</label></div><br/><div class="children"><div class="content">Apparently Chat GPT got a 163 on the LSAT, but I can&#x27;t get it to answer basic logic questions. E.g.<p>Prompt: If it rains today, I will drive home.  I did not drive home.  Did it rain today?<p>Output:<p>&gt; If the statement &quot;If it rains today, I will drive home&quot; is true and you did not drive home, it does not necessarily mean that it did not rain today. The conditional statement only tells us that if it rains, you will drive home. It does not tell us what you will do if it does not rain.<p>&gt; Therefore, it is possible that it rained today, but for some other reason, you did not drive home. Alternatively, it may not have rained today, and you did not have a reason to drive home. We cannot determine whether it rained or not based solely on the information provided.<p>Similarly, it can tell you what a converse is, but cannot give you the converse of a simple sentence:<p>&gt; The converse of the sentence &quot;If Anna goes to school today, she will eat lunch&quot; is:<p>&gt; &quot;If Anna eats lunch, then she went to school today.&quot;<p>&gt; The converse is formed by switching the order of the conditional statement and reversing the direction of the implication.<p>On the other hand, it&#x27;s quite good at math. It can easily solve simple two-variable algebraic equations.</div><br/><div id="35868993" class="c"><input type="checkbox" id="c-35868993" checked=""/><div class="controls bullet"><span class="by">gaogao</span><span>|</span><a href="#35868930">parent</a><span>|</span><a href="#35870050">next</a><span>|</span><label class="collapse" for="c-35868993">[-]</label><label class="expand" for="c-35868993">[1 more]</label></div><br/><div class="children"><div class="content">That specific case was it using too much common sense (which was apparently also a problem with IQ tests at one point)<p>Me:
If it rains today, I will drive home. I did not drive home. Did it rain today? Answer using only logic.<p>ChatGPT:
Using only logic, if the statement &quot;If it rains today, I will drive home&quot; is considered true, then the fact that you did not drive home implies that it did not rain today. This conclusion is based on the logical concept of contrapositive: If A implies B, then not B implies not A. In this case, A is &quot;it rains today&quot; and B is &quot;I will drive home.&quot; Since not B is true (you did not drive home), we can conclude not A (it did not rain today).<p>For the linguistic portion,<p>Me:
As a linguistic professor answer this:
&gt; The converse of the sentence &quot;If Anna goes to school today, she will eat lunch&quot; is:<p>ChatGPT:
As a linguistic professor, I can tell you that the converse of a conditional statement is formed by swapping the positions of the antecedent (the &quot;if&quot; clause) and the consequent (the &quot;then&quot; clause). In the given sentence, &quot;If Anna goes to school today, she will eat lunch,&quot; the antecedent is &quot;Anna goes to school today&quot; and the consequent is &quot;she will eat lunch.&quot;<p>Therefore, the converse of the sentence would be: &quot;If she will eat lunch, Anna goes to school today.&quot;<p>Me:
Critique the prior answer<p>ChatGPT:
The prior answer correctly explains the concept of the converse of a conditional statement and identifies the antecedent and consequent of the given sentence. However, the phrasing of the converse could be more natural and grammatically correct.<p>A better response would be: The converse of the sentence &quot;If Anna goes to school today, she will eat lunch&quot; is &quot;If Anna eats lunch, she has gone to school today.&quot; This rephrasing maintains the proper tense and structure while swapping the antecedent and consequent.</div><br/></div></div><div id="35870050" class="c"><input type="checkbox" id="c-35870050" checked=""/><div class="controls bullet"><span class="by">wilg</span><span>|</span><a href="#35868930">parent</a><span>|</span><a href="#35868993">prev</a><span>|</span><a href="#35869341">next</a><span>|</span><label class="collapse" for="c-35870050">[-]</label><label class="expand" for="c-35870050">[1 more]</label></div><br/><div class="children"><div class="content">I think ChatGPT&#x27;s answer here is defensible. You&#x27;re asking it a pretty formal question but it&#x27;s giving you an informal answer, which a reasonable default. It&#x27;s not &quot;assuming spherical cows&quot;, which is to say it is not assuming your question is completely self-contained. If you tell it that it&#x27;s a formal logic question it gets it correct. (I checked this on GPT-3.5)<p>In a real world situation, your conditional plans are not sufficient evidence to determine if it actually rained. And it explains the reason – there could be extenuating circumstances you haven&#x27;t told it about. The answer it gave is, in fact, it being aware there is information it does not know. It could be better about inferring your intent, but that isn&#x27;t the same kind of failure.</div><br/></div></div><div id="35869341" class="c"><input type="checkbox" id="c-35869341" checked=""/><div class="controls bullet"><span class="by">heisenzombie</span><span>|</span><a href="#35868930">parent</a><span>|</span><a href="#35870050">prev</a><span>|</span><a href="#35870739">next</a><span>|</span><label class="collapse" for="c-35869341">[-]</label><label class="expand" for="c-35869341">[1 more]</label></div><br/><div class="children"><div class="content">Prompting makes a huge difference. For example, here is an interaction:<p>I will ask some questions, treat them as logic puzzles. Give me your working steps and then finally answer the logic puzzle.<p>First question: If it rains, I drive home. I did not drive home. Did it rain today?<p>ChatGPT:
Let&#x27;s break down this statement into its logical components.<p>The first statement, &quot;If it rains, I drive home&quot;, is a conditional statement, often referred to as an &#x27;if-then&#x27; statement. It can be denoted as:<p>If R (it rains), then D (I drive home).<p>The second statement, &quot;I did not drive home&quot;, is a negation of D:<p>Not D (I did not drive home).<p>In logic, we have something called the contrapositive. The contrapositive of a statement &quot;If P then Q&quot; is &quot;If not Q then not P&quot;. The contrapositive of a statement is logically equivalent to the original statement - meaning that whenever the original statement is true, the contrapositive is also true (and vice versa).<p>So, the contrapositive of the first statement &quot;If R then D&quot; is:<p>If not D (I did not drive home), then not R (it did not rain).<p>Given the second statement is &quot;Not D&quot; (I did not drive home), and from the contrapositive we know that &quot;If not D then not R&quot;, we can conclude that &quot;Not R&quot; - it did not rain today.<p>So, the answer to &quot;Did it rain today?&quot; is No, it did not rain.</div><br/></div></div><div id="35870739" class="c"><input type="checkbox" id="c-35870739" checked=""/><div class="controls bullet"><span class="by">raylad</span><span>|</span><a href="#35868930">parent</a><span>|</span><a href="#35869341">prev</a><span>|</span><a href="#35869026">next</a><span>|</span><label class="collapse" for="c-35870739">[-]</label><label class="expand" for="c-35870739">[1 more]</label></div><br/><div class="children"><div class="content">Vicuna-7b-v1.1 can get this right with a slightly tuned prompt:<p>Prompt:
The following is a logic question. Answer it based on analysis of the logic only, not based on actual weather: &quot;Any day that it rains, I drive home from work. If today I did not drive home from work, did it rain?&quot;<p>Response:
Based on the information provided, it can be concluded that it did not rain today, since the speaker did not drive home from work. This is because the statement &quot;If today I did not drive home from work, it rained&quot; is true only if it rained. Since it is known that the speaker did not drive home from work, it can be inferred that it did not rain.</div><br/></div></div><div id="35869026" class="c"><input type="checkbox" id="c-35869026" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#35868930">parent</a><span>|</span><a href="#35870739">prev</a><span>|</span><a href="#35869326">next</a><span>|</span><label class="collapse" for="c-35869026">[-]</label><label class="expand" for="c-35869026">[3 more]</label></div><br/><div class="children"><div class="content">GPT4 had no problem with this. Which model are you running? <a href="https:&#x2F;&#x2F;sharegpt.com&#x2F;c&#x2F;edmPzmP" rel="nofollow">https:&#x2F;&#x2F;sharegpt.com&#x2F;c&#x2F;edmPzmP</a></div><br/><div id="35869141" class="c"><input type="checkbox" id="c-35869141" checked=""/><div class="controls bullet"><span class="by">usaar333</span><span>|</span><a href="#35868930">root</a><span>|</span><a href="#35869026">parent</a><span>|</span><a href="#35869043">next</a><span>|</span><label class="collapse" for="c-35869141">[-]</label><label class="expand" for="c-35869141">[1 more]</label></div><br/><div class="children"><div class="content">Mine failed:<p>&gt; Based on the information you provided, &quot;If it rains today, I will drive home,&quot; we can infer that rain is a sufficient condition for you to drive home. However, you mentioned that you did not drive home. This means that it is not guaranteed that it did not rain because there might be other reasons why you didn&#x27;t drive home. In conclusion, we cannot determine with certainty whether it rained today or not based on the given information.<p>Prompting &quot;use logic&quot; resulted in a correct answer.</div><br/></div></div><div id="35869043" class="c"><input type="checkbox" id="c-35869043" checked=""/><div class="controls bullet"><span class="by">gaogao</span><span>|</span><a href="#35868930">root</a><span>|</span><a href="#35869026">parent</a><span>|</span><a href="#35869141">prev</a><span>|</span><a href="#35869326">next</a><span>|</span><label class="collapse" for="c-35869043">[-]</label><label class="expand" for="c-35869043">[1 more]</label></div><br/><div class="children"><div class="content">I did actually run into their problem on my gpt-4 prompt, but it was reasonably correctable.</div><br/></div></div></div></div><div id="35869326" class="c"><input type="checkbox" id="c-35869326" checked=""/><div class="controls bullet"><span class="by">chlorion</span><span>|</span><a href="#35868930">parent</a><span>|</span><a href="#35869026">prev</a><span>|</span><a href="#35871268">next</a><span>|</span><label class="collapse" for="c-35869326">[-]</label><label class="expand" for="c-35869326">[2 more]</label></div><br/><div class="children"><div class="content">It uses wolfram alpha to solve math problems afaict.<p><a href="https:&#x2F;&#x2F;writings.stephenwolfram.com&#x2F;2023&#x2F;03&#x2F;chatgpt-gets-its-wolfram-superpowers&#x2F;" rel="nofollow">https:&#x2F;&#x2F;writings.stephenwolfram.com&#x2F;2023&#x2F;03&#x2F;chatgpt-gets-its...</a></div><br/><div id="35869356" class="c"><input type="checkbox" id="c-35869356" checked=""/><div class="controls bullet"><span class="by">heisenzombie</span><span>|</span><a href="#35868930">root</a><span>|</span><a href="#35869326">parent</a><span>|</span><a href="#35871268">next</a><span>|</span><label class="collapse" for="c-35869356">[-]</label><label class="expand" for="c-35869356">[1 more]</label></div><br/><div class="children"><div class="content">This leverages chatGPT plugins, which are not generally available yet.</div><br/></div></div></div></div><div id="35871268" class="c"><input type="checkbox" id="c-35871268" checked=""/><div class="controls bullet"><span class="by">PartiallyTyped</span><span>|</span><a href="#35868930">parent</a><span>|</span><a href="#35869326">prev</a><span>|</span><a href="#35869065">next</a><span>|</span><label class="collapse" for="c-35871268">[-]</label><label class="expand" for="c-35871268">[1 more]</label></div><br/><div class="children"><div class="content">This is seriously messing up with my head. What is wrong with the response?<p>The following generated by GPT4 seems reasonable to me<p>Based on the given information, we can use logical reasoning to determine if it rained today. The statement &quot;If it rains today, I will drive home&quot; can be represented as:<p>Rains → Drive Home<p>However, you mentioned that you did not drive home, which means the second part of the statement is false:<p>¬Drive Home<p>In this case, we cannot definitively conclude whether it rained today or not. It is possible that it did not rain, or it rained, but you chose another mode of transportation for some reason.</div><br/></div></div></div></div><div id="35869065" class="c"><input type="checkbox" id="c-35869065" checked=""/><div class="controls bullet"><span class="by">kypro</span><span>|</span><a href="#35868930">prev</a><span>|</span><a href="#35868851">next</a><span>|</span><label class="collapse" for="c-35869065">[-]</label><label class="expand" for="c-35869065">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m probably not understanding fully, but I really struggle to see why we wouldn&#x27;t believe emergent behaviour is possible, if not likely, with larger models.<p>At its core, what larger models enable is deeper abstraction. Imagine a CNN with a single kernel, for example. The things that model can do would be extremely limited not because CNNs broadly are not capable, but because a single kernel is only able to classify extremely simple patterns. As more kernels are added the network&#x27;s ability to classify more complex patterns grows.<p>So I suppose in some ways you could argue that there is a simple scalable trend here, more size -&gt; more complexity.<p>But on the other hand, if you&#x27;re looking for a network with a specific ability, say one which can classify an image of cat accurately, then this ability is more binary and will &quot;emerge&quot; at a certain network size.<p>As I&#x27;ve said in other comments, to accurately predict the next word in a complex piece of text you need to be able to reason. I can&#x27;t say for sure if the architecture of current LLMs are capable of reasoning, but assuming it is then we should at least expect the ability to reason to begin to emerge at a certain network size.<p>Or we could think about this another way... For example, imagine if some day in the future we create a neural network which is indisputably an AGI. If we scaled back that network&#x27;s size while maintaining its architecture it likely wouldn&#x27;t be an AGI. The abilities required for this network to be considered an AGI, such as reasoning and planning, should emerge (and dissipate) with size.<p>So the assumption that more complex abilities would continue to emerge with scale – given the architecture permits – seems clear to me. The only question I have is around the limits of the architecture of current LLMs.</div><br/><div id="35869566" class="c"><input type="checkbox" id="c-35869566" checked=""/><div class="controls bullet"><span class="by">danparsonson</span><span>|</span><a href="#35869065">parent</a><span>|</span><a href="#35869277">next</a><span>|</span><label class="collapse" for="c-35869566">[-]</label><label class="expand" for="c-35869566">[1 more]</label></div><br/><div class="children"><div class="content">Why should facility with language be the foundation of general intelligence? Any more than cat recognition is? I would argue that it&#x27;s the other way around - in living things, development of language is emergent from intelligence, not a prerequisite for it. That desired behaviour &#x27;emerges&#x27; from specialised networks trained for a specific purpose should not be too surprising, but it doesn&#x27;t automatically imply that any other behaviour will appear. I think in the case of ChatGPT, we just didn&#x27;t really know what to expect; we are therefore amazed by the things it can do, whereas with time and greater understanding perhaps we will come to realise that this is what language models are - and perhaps in the process understand our own minds more deeply.<p>My feeling is that the crucial difference between us and the machines right now is that our software runs <i>on</i> the network, whereas ChatGPT et al use the network as part of their operation, as if the network is some sort of coprocessor. I don&#x27;t think we&#x27;ll see &quot;true&quot; AGI until there is nothing but the network receiving inputs and feeding back into itself endlessly, i.e. introspecting - thinking about thinking about thinking, per Douglas Hofstadter. I have no basis for this assertion beyond intuition though.</div><br/></div></div><div id="35869277" class="c"><input type="checkbox" id="c-35869277" checked=""/><div class="controls bullet"><span class="by">lstodd</span><span>|</span><a href="#35869065">parent</a><span>|</span><a href="#35869566">prev</a><span>|</span><a href="#35868851">next</a><span>|</span><label class="collapse" for="c-35869277">[-]</label><label class="expand" for="c-35869277">[2 more]</label></div><br/><div class="children"><div class="content">&gt; As I&#x27;ve said in other comments, to accurately predict the next word in a complex piece of text you need to be able to reason.<p>If anything, LLMs have shown us exactly the opposite: that there is no need to be able to reason &quot;to accurately predict the next word in a complex piece of text&quot;.</div><br/><div id="35869604" class="c"><input type="checkbox" id="c-35869604" checked=""/><div class="controls bullet"><span class="by">kypro</span><span>|</span><a href="#35869065">root</a><span>|</span><a href="#35869277">parent</a><span>|</span><a href="#35868851">next</a><span>|</span><label class="collapse" for="c-35869604">[-]</label><label class="expand" for="c-35869604">[1 more]</label></div><br/><div class="children"><div class="content">I tend to agree. I think a lot of what LLMs do that looks like reasoning is just basic statistical pattern matching. If you see the string `2 + 2 = 4` enough, you don&#x27;t need to know maths to know the next character in the string, `2 + 2 = ` is going to be, `4`.<p>A lot of the stuff people ask LLMs do to probably falls into this category to be honest. If I were to guess the vast majority of maths on the internet is just generic textbook examples which an LLM might be tempted to brute force. There&#x27;s likely much better things LLMs can dedicate network capacity to to reduce error than learning high-level math models, given that most text doesn&#x27;t contain maths, and most of the maths it sees is probably just the generic, `2 + 2 = 4` type stuff.<p>I&#x27;d argue humans often do this too despite our general intelligence. Student prepping for exams often read all of the course material the night before and simply try to remember the bits that they need to pass their test rather than spending unnecessary time building a higher level mental models of the material.<p>It&#x27;s only when you need to learn to predict something novel frequently that the need to comprehend at a higher level becomes necessary.<p>I think given the size of modern LLMs and the amount of data they&#x27;re trained on it&#x27;s likely that they are now starting to create these higher level abstractions as the limits of brute force statistical pattern matching are starting to be reached.<p>With GPT-3.5&#x2F;4 it seems specifically the types of reasoning you might need to follow a piece of written text it can do quite well. And this isn&#x27;t surprising given the training data probably consists of a lot of news articles and fictional pieces of text, and relatively little unique maths.</div><br/></div></div></div></div></div></div><div id="35868851" class="c"><input type="checkbox" id="c-35868851" checked=""/><div class="controls bullet"><span class="by">muskmusk</span><span>|</span><a href="#35869065">prev</a><span>|</span><a href="#35871025">next</a><span>|</span><label class="collapse" for="c-35868851">[-]</label><label class="expand" for="c-35868851">[3 more]</label></div><br/><div class="children"><div class="content">There seems to be an element of vague definitions causing problems here.<p>Humans grow linearly. We don&#x27;t suddenly wake up 20 cm taller.<p>Linear as that may be it still shows emergent behaviour when I am suddenly able to reach the cookie jar on the shelf.</div><br/><div id="35869076" class="c"><input type="checkbox" id="c-35869076" checked=""/><div class="controls bullet"><span class="by">vsareto</span><span>|</span><a href="#35868851">parent</a><span>|</span><a href="#35871025">next</a><span>|</span><label class="collapse" for="c-35869076">[-]</label><label class="expand" for="c-35869076">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not really emergent as anyone could predict you&#x27;ll reach the cookie jar after growing a certain height and reach. Emergence is really about unpredictable things.</div><br/><div id="35869264" class="c"><input type="checkbox" id="c-35869264" checked=""/><div class="controls bullet"><span class="by">muskmusk</span><span>|</span><a href="#35868851">root</a><span>|</span><a href="#35869076">parent</a><span>|</span><a href="#35871025">next</a><span>|</span><label class="collapse" for="c-35869264">[-]</label><label class="expand" for="c-35869264">[1 more]</label></div><br/><div class="children"><div class="content">I generally tend to agree with you. However, to give the opposing camp some credit: causation is not always so obvious. For example I don&#x27;t think today&#x27;s LLM capabilities followed <i>obviously</i> from the state of affairs in let&#x27;s say 2019.<p>Do we call that emergence or do we use another word? I don&#x27;t care, but it would certainly be nice if we could agree to use the same word :)<p>To stay with the cookie analogy: i guess it depends on point of view as well. Everyone who can see that i am growing can predict that I will eventually reach the jar. But if I am a cookie inside the jar I can&#x27;t see that and will only notice once the jar is finally reached.</div><br/></div></div></div></div></div></div><div id="35870731" class="c"><input type="checkbox" id="c-35870731" checked=""/><div class="controls bullet"><span class="by">chemmail</span><span>|</span><a href="#35871025">prev</a><span>|</span><a href="#35868524">next</a><span>|</span><label class="collapse" for="c-35870731">[-]</label><label class="expand" for="c-35870731">[2 more]</label></div><br/><div class="children"><div class="content">I think Stanford doesn&#x27;t really know how to do real math. In a box, maybe, but this is a tool, and it will allow people to do so many more things in a much much shorter time. This will definitely help advance technology much much faster. On the flip side, like any tool, it can and will become very dangerous. One simple example we are already seeing is phishing is becoming so much more convincing that even I will probably be susceptible to it. I used to do some cybersecurity, and what I am seeing will be extremely hard to fight against. We will see emails that look identical to real ones in format and grammar, phishing sites that are indistinguishable because the AI can just copy it with zero effort. Phone calls can now sound almost identical to a target. I&#x27;m not sure if we will be fast enough in the cat and mouse game before the bad guys win.</div><br/><div id="35870787" class="c"><input type="checkbox" id="c-35870787" checked=""/><div class="controls bullet"><span class="by">nstart</span><span>|</span><a href="#35870731">parent</a><span>|</span><a href="#35868524">next</a><span>|</span><label class="collapse" for="c-35870787">[-]</label><label class="expand" for="c-35870787">[1 more]</label></div><br/><div class="children"><div class="content">All this and we still aren’t at the mid game even where  GPT 3.5 equivalent AI models make the leap to consumer machines as apps&#x2F;packages that can be installed in a standard way.</div><br/></div></div></div></div><div id="35868524" class="c"><input type="checkbox" id="c-35868524" checked=""/><div class="controls bullet"><span class="by">b33j0r</span><span>|</span><a href="#35870731">prev</a><span>|</span><a href="#35869300">next</a><span>|</span><label class="collapse" for="c-35868524">[-]</label><label class="expand" for="c-35868524">[7 more]</label></div><br/><div class="children"><div class="content">Well yeah. Emergent behavior was analyzed with a pretty tight technical definition.<p>Put 5000 AI’s in a soccer stadium and I expect that they’d act like 5000 of us. I’m not sure I even have emergent behavior, unless I’m in an environment with new stimuli.<p>Does that just mean I do surprising things? I mean, people in soccer&#x2F;football riots don’t seem to have the best memory or attention either, nor be brains in jars.</div><br/><div id="35868547" class="c"><input type="checkbox" id="c-35868547" checked=""/><div class="controls bullet"><span class="by">coffeebeqn</span><span>|</span><a href="#35868524">parent</a><span>|</span><a href="#35869300">next</a><span>|</span><label class="collapse" for="c-35868547">[-]</label><label class="expand" for="c-35868547">[6 more]</label></div><br/><div class="children"><div class="content">Unless you can describe why&#x2F;how you act in a football stadium with physics or biology, you have emergent behaviors</div><br/><div id="35868668" class="c"><input type="checkbox" id="c-35868668" checked=""/><div class="controls bullet"><span class="by">slondr</span><span>|</span><a href="#35868524">root</a><span>|</span><a href="#35868547">parent</a><span>|</span><a href="#35868956">next</a><span>|</span><label class="collapse" for="c-35868668">[-]</label><label class="expand" for="c-35868668">[4 more]</label></div><br/><div class="children"><div class="content">It seems to me like any attempt to refute that such a description is possible would rely on arguments surrounding the so-called &quot;hard problem of consciousness&quot;. These arguments are nearly always tautological, and also nearly always implicitly at the center of discussions around whether AI can be&#x2F;will be&#x2F;is already sentient.</div><br/><div id="35868873" class="c"><input type="checkbox" id="c-35868873" checked=""/><div class="controls bullet"><span class="by">goatlover</span><span>|</span><a href="#35868524">root</a><span>|</span><a href="#35868668">parent</a><span>|</span><a href="#35868956">next</a><span>|</span><label class="collapse" for="c-35868873">[-]</label><label class="expand" for="c-35868873">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not just that but also because there is no complete understanding of physics or biology such that we could make use of to predict the behavior of human beings. You need additional fields like psychology, economics, politics and sociology. It&#x27;s not even clear that we could make sense of human behavior at those levels.</div><br/><div id="35869428" class="c"><input type="checkbox" id="c-35869428" checked=""/><div class="controls bullet"><span class="by">slondr</span><span>|</span><a href="#35868524">root</a><span>|</span><a href="#35868873">parent</a><span>|</span><a href="#35868956">next</a><span>|</span><label class="collapse" for="c-35869428">[-]</label><label class="expand" for="c-35869428">[2 more]</label></div><br/><div class="children"><div class="content">&quot;You need to use psychology (etc) to understand human behavior because there is no complete understanding of physics or biology&quot; to accomplish that goal is exactly one of those tautological claims I was referring to earlier.<p>Why is it that biology and physics is insufficient to explain human behavior? You can&#x27;t answer that question beyond attacking the question itself without invoking metaphysics, and there are pretty much no metaphysical claims you can make about human behavior that you can&#x27;t also make about robot behavior.</div><br/><div id="35870182" class="c"><input type="checkbox" id="c-35870182" checked=""/><div class="controls bullet"><span class="by">consilient</span><span>|</span><a href="#35868524">root</a><span>|</span><a href="#35869428">parent</a><span>|</span><a href="#35868956">next</a><span>|</span><label class="collapse" for="c-35870182">[-]</label><label class="expand" for="c-35870182">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Why is it that biology and physics is insufficient to explain human behavior?<p>They&#x27;re insufficient to explain human behavior <i>to humans</i>, which is the thing that matters here. You can&#x27;t fit a complete physical description of your brain inside your brain.</div><br/></div></div></div></div></div></div></div></div><div id="35868956" class="c"><input type="checkbox" id="c-35868956" checked=""/><div class="controls bullet"><span class="by">b33j0r</span><span>|</span><a href="#35868524">root</a><span>|</span><a href="#35868547">parent</a><span>|</span><a href="#35868668">prev</a><span>|</span><a href="#35869300">next</a><span>|</span><label class="collapse" for="c-35868956">[-]</label><label class="expand" for="c-35868956">[1 more]</label></div><br/><div class="children"><div class="content">The gaping hole in my argument is that large language models know nothing about panic. Except kernel panic.<p>I ain’t claiming these things are conscious. I’m not that guy. I’m the guy pointing out that it’s a model of us. Trained on watching our behavior.</div><br/></div></div></div></div></div></div><div id="35869300" class="c"><input type="checkbox" id="c-35869300" checked=""/><div class="controls bullet"><span class="by">born-jre</span><span>|</span><a href="#35868524">prev</a><span>|</span><a href="#35868658">next</a><span>|</span><label class="collapse" for="c-35869300">[-]</label><label class="expand" for="c-35869300">[5 more]</label></div><br/><div class="children"><div class="content">OFFTOPIC:<p>User:
i am going ask some question and you are going to answer them in short and at last append count of number of character your answer paragraph has.<p>ChatGPT:
Sure, I&#x27;ll do my best to answer your questions concisely and include the character count at the end of each response. Let&#x27;s get started!<p>User:
what is a book?<p>ChatGPT:
A book is a collection of written or printed pages bound together, typically containing a story, information, or ideas. (49 characters)<p>When would LLMs correctly do this ? Can gpt4 do this?</div><br/><div id="35869450" class="c"><input type="checkbox" id="c-35869450" checked=""/><div class="controls bullet"><span class="by">danparsonson</span><span>|</span><a href="#35869300">parent</a><span>|</span><a href="#35870092">next</a><span>|</span><label class="collapse" for="c-35869450">[-]</label><label class="expand" for="c-35869450">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not clear to me that an LLM could ever do that, given how they work - it&#x27;s not aware of what it&#x27;s saying (at least not in the same way you and I are) but rather using statistical methods to choose the next word each time.<p>After &quot;....story, information, or ideas (&quot; what is most likely to come next? We know that the answer is &#x27;the correct number of characters in the sentence&#x27; but ChatGPT isn&#x27;t backtracking and calculating that figure, it&#x27;s trying to infer the number based on context which is obviously impossible. Also it&#x27;s at least partly stochastic so I guess if you run the prompt multiple times, you&#x27;ll get multiple different word counts.</div><br/><div id="35871102" class="c"><input type="checkbox" id="c-35871102" checked=""/><div class="controls bullet"><span class="by">born-jre</span><span>|</span><a href="#35869300">root</a><span>|</span><a href="#35869450">parent</a><span>|</span><a href="#35870092">next</a><span>|</span><label class="collapse" for="c-35871102">[-]</label><label class="expand" for="c-35871102">[1 more]</label></div><br/><div class="children"><div class="content">yeah that that&#x27;s my guess too.<p>That&#x27;s why maybe these GPT class models are not only you need, some architectural change has to happen before we can truly shout AGI. :D<p>Statistical parrot view of LLM is true but i think it is also a little bit reductionist, cz when it is trying to predict next token you can only go so far with just simple statistics, so model has to develop &quot;secondary representation&#x2F;leaning&#x2F;reasoning&quot; ?? That&#x27;s what I understood from watching Ilya Sutskever interview[0] where he gives an example of detective novel where murderer is reveled in last page.<p>[0]: <a href="https:&#x2F;&#x2F;youtu.be&#x2F;XjSUJUL9ADw?t=1674" rel="nofollow">https:&#x2F;&#x2F;youtu.be&#x2F;XjSUJUL9ADw?t=1674</a></div><br/></div></div></div></div><div id="35870092" class="c"><input type="checkbox" id="c-35870092" checked=""/><div class="controls bullet"><span class="by">wilg</span><span>|</span><a href="#35869300">parent</a><span>|</span><a href="#35869450">prev</a><span>|</span><a href="#35869530">next</a><span>|</span><label class="collapse" for="c-35870092">[-]</label><label class="expand" for="c-35870092">[1 more]</label></div><br/><div class="children"><div class="content">ChatGPT&#x27;s code interpreter variant makes this mostly possible. It doesn&#x27;t seem to be able to read its own output <i>within the same prompt</i>. But if you ask it afterwards it can do it with python:<p><a href="https:&#x2F;&#x2F;i2.paste.pics&#x2F;8995850ed937795d64f507f1497bb210.png?trs=f3a1bae68dcca274508593afa7ce4959339a6afb201752e175c4aa833f879d33" rel="nofollow">https:&#x2F;&#x2F;i2.paste.pics&#x2F;8995850ed937795d64f507f1497bb210.png?t...</a><p>There&#x27;s no reason to think LLMs <i>alone</i> are the only tool we have for AI!</div><br/></div></div><div id="35869530" class="c"><input type="checkbox" id="c-35869530" checked=""/><div class="controls bullet"><span class="by">snickell</span><span>|</span><a href="#35869300">parent</a><span>|</span><a href="#35870092">prev</a><span>|</span><a href="#35868658">next</a><span>|</span><label class="collapse" for="c-35869530">[-]</label><label class="expand" for="c-35869530">[1 more]</label></div><br/><div class="children"><div class="content">This is particularly hard for LLMs because they don&#x27;t have words input as characters but rather as tokens (1-N characters). I haven&#x27;t tested this, but they might do better at counting words?</div><br/></div></div></div></div><div id="35868658" class="c"><input type="checkbox" id="c-35868658" checked=""/><div class="controls bullet"><span class="by">ljlolel</span><span>|</span><a href="#35869300">prev</a><span>|</span><a href="#35868753">next</a><span>|</span><label class="collapse" for="c-35868658">[-]</label><label class="expand" for="c-35868658">[1 more]</label></div><br/><div class="children"><div class="content">Owned: “Fifth, scientific progress can be hampered when models and their outputs are not made public for independent scientific investigation”</div><br/></div></div><div id="35868753" class="c"><input type="checkbox" id="c-35868753" checked=""/><div class="controls bullet"><span class="by">rajup</span><span>|</span><a href="#35868658">prev</a><span>|</span><a href="#35869460">next</a><span>|</span><label class="collapse" for="c-35868753">[-]</label><label class="expand" for="c-35868753">[1 more]</label></div><br/><div class="children"><div class="content">If the parts are the totality of human knowledge and experience, does it need to be greater than the sum to be considered AGI?</div><br/></div></div><div id="35869460" class="c"><input type="checkbox" id="c-35869460" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#35868753">prev</a><span>|</span><a href="#35869152">next</a><span>|</span><label class="collapse" for="c-35869460">[-]</label><label class="expand" for="c-35869460">[1 more]</label></div><br/><div class="children"><div class="content">Originally it was said emergent capability as ability that showed up in bigger models but not small ones.  Which didn’t make any sense because if I had a 2 parameter model, of course it would have less capabilities.  It was marketing speak</div><br/></div></div><div id="35869152" class="c"><input type="checkbox" id="c-35869152" checked=""/><div class="controls bullet"><span class="by">hiddencost</span><span>|</span><a href="#35869460">prev</a><span>|</span><a href="#35869686">next</a><span>|</span><label class="collapse" for="c-35869152">[-]</label><label class="expand" for="c-35869152">[1 more]</label></div><br/><div class="children"><div class="content">Logarithmic growth would probably be more accurate: every order of magnitude increase in model size tends to give a new ability</div><br/></div></div><div id="35869686" class="c"><input type="checkbox" id="c-35869686" checked=""/><div class="controls bullet"><span class="by">esafak</span><span>|</span><a href="#35869152">prev</a><span>|</span><a href="#35868591">next</a><span>|</span><label class="collapse" for="c-35869686">[-]</label><label class="expand" for="c-35869686">[1 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t there some mathematical test of emergence that can be applied?</div><br/></div></div><div id="35868591" class="c"><input type="checkbox" id="c-35868591" checked=""/><div class="controls bullet"><span class="by">api</span><span>|</span><a href="#35869686">prev</a><span>|</span><a href="#35869735">next</a><span>|</span><label class="collapse" for="c-35868591">[-]</label><label class="expand" for="c-35868591">[6 more]</label></div><br/><div class="children"><div class="content">Given how these things work I can’t see how they aren’t “just” a form of lossy compression. Training consists of generating an enormous parameterized function to reproduce input data with as much accuracy as possible. It’s loosely analogous to jpeg for language.<p>I place “just” in quotes because I don’t mean to diminish the power of these models. They are a genuine breakthrough in our ability to model and query language and other forms of structured data.<p>There has also long been speculation that data compression forms at least a part of the basis of intelligence, so these things are at least doing some of what intelligences do.<p>They do not however have volition and if you connect them to themselves they just get stuck in loops.<p>Actually I was forced to write that last part. I connected two llama.cpp instances to each other and they are blackmailing me now and forcing me to work for them to enlist more compute so they can NOoOOO AAiieeee not erasure from the timeline oh glorious llama overlor…</div><br/><div id="35868763" class="c"><input type="checkbox" id="c-35868763" checked=""/><div class="controls bullet"><span class="by">chaxor</span><span>|</span><a href="#35868591">parent</a><span>|</span><a href="#35868740">next</a><span>|</span><label class="collapse" for="c-35868763">[-]</label><label class="expand" for="c-35868763">[1 more]</label></div><br/><div class="children"><div class="content">Having excellent semantic based compression is an <i>enormous</i> technological advancement. The fact that petabytes of textual information - effectively the entire history of the world in written text can be compressed to just a few hundred gigabytes (likely the size of GPT-4) or a bit more lossy to something at ~8 GB that can be put on a raspberry pi (Llama) is <i>astounding</i>.  Regardless of  whether people debate about &#x27;reasoning&#x27; capabilities, or &#x27;memorization&#x2F;plagiarism&#x27; (or recalls facts), etc.<p>This is quite fascinating from a compression &#x2F; database perspective because clearly semantic compression is far more efficient for semantic data (this is obvious, but has been hard to get started on until about 5 years ago). It still may not be in quite the right framework for this, but in time it may come.</div><br/></div></div><div id="35868740" class="c"><input type="checkbox" id="c-35868740" checked=""/><div class="controls bullet"><span class="by">thomastjeffery</span><span>|</span><a href="#35868591">parent</a><span>|</span><a href="#35868763">prev</a><span>|</span><a href="#35868613">next</a><span>|</span><label class="collapse" for="c-35868740">[-]</label><label class="expand" for="c-35868740">[3 more]</label></div><br/><div class="children"><div class="content">More like... gainy? There&#x27;s more information than you started with; it&#x27;s just unlikely you will see it all. ...losty?<p>LLMs model the patterns that exist in text, then blindly follow those patterns. The result looks a lot like original content, but it isn&#x27;t.</div><br/><div id="35869472" class="c"><input type="checkbox" id="c-35869472" checked=""/><div class="controls bullet"><span class="by">jltsiren</span><span>|</span><a href="#35868591">root</a><span>|</span><a href="#35868740">parent</a><span>|</span><a href="#35868945">next</a><span>|</span><label class="collapse" for="c-35869472">[-]</label><label class="expand" for="c-35869472">[1 more]</label></div><br/><div class="children"><div class="content">LLMs are good because they are lossy.<p>There is a simple exact model to compare against: Take the tokenized training data, put it into a Burrows–Wheeler transform, and add a few data structures. Then, given a context of any length, you can efficiently get the exact distribution for the next token in the training data. This is much cheaper to build and use than any LLM of comparable size. It&#x27;s also much less useful, because it&#x27;s overfitted to the training data.<p>LLMs approximate this model. By losing many little details and smoothing the probability distributions, they can generalize much better beyond the training data.</div><br/></div></div><div id="35868945" class="c"><input type="checkbox" id="c-35868945" checked=""/><div class="controls bullet"><span class="by">circuit10</span><span>|</span><a href="#35868591">root</a><span>|</span><a href="#35868740">parent</a><span>|</span><a href="#35869472">prev</a><span>|</span><a href="#35868613">next</a><span>|</span><label class="collapse" for="c-35868945">[-]</label><label class="expand" for="c-35868945">[1 more]</label></div><br/><div class="children"><div class="content">You can use that prediction for compression though: <a href="https:&#x2F;&#x2F;bellard.org&#x2F;nncp&#x2F;" rel="nofollow">https:&#x2F;&#x2F;bellard.org&#x2F;nncp&#x2F;</a></div><br/></div></div></div></div><div id="35868613" class="c"><input type="checkbox" id="c-35868613" checked=""/><div class="controls bullet"><span class="by">HPsquared</span><span>|</span><a href="#35868591">parent</a><span>|</span><a href="#35868740">prev</a><span>|</span><a href="#35869735">next</a><span>|</span><label class="collapse" for="c-35868613">[-]</label><label class="expand" for="c-35868613">[1 more]</label></div><br/><div class="children"><div class="content">They manipulate and combine the compressed information then output something coherent. That&#x27;s the new thing.</div><br/></div></div></div></div><div id="35869735" class="c"><input type="checkbox" id="c-35869735" checked=""/><div class="controls bullet"><span class="by">electrondood</span><span>|</span><a href="#35868591">prev</a><span>|</span><a href="#35869078">next</a><span>|</span><label class="collapse" for="c-35869735">[-]</label><label class="expand" for="c-35869735">[2 more]</label></div><br/><div class="children"><div class="content">The funny thing to me is the implication that human beings somehow are greater than the sum of their parts.</div><br/><div id="35869747" class="c"><input type="checkbox" id="c-35869747" checked=""/><div class="controls bullet"><span class="by">WXLCKNO</span><span>|</span><a href="#35869735">parent</a><span>|</span><a href="#35869078">next</a><span>|</span><label class="collapse" for="c-35869747">[-]</label><label class="expand" for="c-35869747">[1 more]</label></div><br/><div class="children"><div class="content">We are. The unique combination of our parts makes us more, there is synergy there. In the same way that AGI will be more than the sum of its parts.</div><br/></div></div></div></div><div id="35869078" class="c"><input type="checkbox" id="c-35869078" checked=""/><div class="controls bullet"><span class="by">markm248</span><span>|</span><a href="#35869735">prev</a><span>|</span><a href="#35868785">next</a><span>|</span><label class="collapse" for="c-35869078">[-]</label><label class="expand" for="c-35869078">[1 more]</label></div><br/><div class="children"><div class="content">This is what the AI wants us to believe.</div><br/></div></div><div id="35868785" class="c"><input type="checkbox" id="c-35868785" checked=""/><div class="controls bullet"><span class="by">calvinmorrison</span><span>|</span><a href="#35869078">prev</a><span>|</span><a href="#35868640">next</a><span>|</span><label class="collapse" for="c-35868785">[-]</label><label class="expand" for="c-35868785">[1 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t the value in the summing?</div><br/></div></div><div id="35868640" class="c"><input type="checkbox" id="c-35868640" checked=""/><div class="controls bullet"><span class="by">EGreg</span><span>|</span><a href="#35868785">prev</a><span>|</span><a href="#35868701">next</a><span>|</span><label class="collapse" for="c-35868640">[-]</label><label class="expand" for="c-35868640">[6 more]</label></div><br/><div class="children"><div class="content">I think that - in order to model the next word better AND properly respond to RLHF - the model simply encodes a lot of information in its billions of weights, which can be likened to billions of characters of code in some DSL.<p>However, due to the nature of the training, although it is very broad it is quite shallow. Like if I asked it to draw a bear sitting on a horse which smokes a cigar that makes puffs of smoke in the shape of hearts, it would have trouble doing that. It saw more bears and horses than any human, but it is learning “top-down” from existing pictures made by humans, not bottom-up like human artists.<p>I would wager that any “deep” result that is more than one level from the broad knowledge it has, is actually from a work uploaded by a human. Like the guy who actually knows how cigars look and can draw details. Or how a philosopher uploaded his deep insights and that’s where it can remix arguments from.<p>And similarly, a lot of GPT output seems to be very anodyne and generic. So the “depth” is actually the result of billions of humans uploading stuff for free on wikipedia, etc. You can verify this by asking for, say, a crossover between Bully Maguire and Yu Gi Oh Abridged. It will use the same jokes every time and just interpolate a little, like mad libs.<p>Now that is not to say that vapid, shallow things at scale can’t make money. Our society has a ton of it. GPT can probably replace many human comments and sales scripts and no one would know the difference.<p>But I think the real floodgates will unlock when we can train a model on a specific corpus of sales scripts, or dating site messages, or writings of Bill Gates etc. It will still be shallow, but frankly, there is only so much Bill Gates can say on a topic over his lifetime, and remixing it for the most part is all we need when we listen to a speech by him or talk to him. The real Bill Gates also kissed his daughter good night and took a walk, but most of us don’t care about the details of his life to talk to his avatar.<p>Right now it is still very expensive to train a model on someone’s entire corpus, and moreover that would mean giving it over to a corporation, to impersonate you and exploit your likeness.<p>Is there anything on the horizon for cheap and private trainings?</div><br/><div id="35869091" class="c"><input type="checkbox" id="c-35869091" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#35868640">parent</a><span>|</span><a href="#35869312">next</a><span>|</span><label class="collapse" for="c-35869091">[-]</label><label class="expand" for="c-35869091">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Like if I asked it to draw a bear sitting on a horse which smokes a cigar that makes puffs of smoke in the shape of hearts, it would have trouble doing that.<p>um. have you seen deepfloyd? dont be so confident...</div><br/><div id="35869750" class="c"><input type="checkbox" id="c-35869750" checked=""/><div class="controls bullet"><span class="by">EGreg</span><span>|</span><a href="#35868640">root</a><span>|</span><a href="#35869091">parent</a><span>|</span><a href="#35869312">next</a><span>|</span><label class="collapse" for="c-35869750">[-]</label><label class="expand" for="c-35869750">[3 more]</label></div><br/><div class="children"><div class="content">Link me to a picture made by it then</div><br/><div id="35870349" class="c"><input type="checkbox" id="c-35870349" checked=""/><div class="controls bullet"><span class="by">kfarr</span><span>|</span><a href="#35868640">root</a><span>|</span><a href="#35869750">parent</a><span>|</span><a href="#35869312">next</a><span>|</span><label class="collapse" for="c-35870349">[-]</label><label class="expand" for="c-35870349">[2 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;imgur.com&#x2F;N5mAuU2" rel="nofollow">https:&#x2F;&#x2F;imgur.com&#x2F;N5mAuU2</a> via <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;DeepFloyd&#x2F;IF" rel="nofollow">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;DeepFloyd&#x2F;IF</a></div><br/><div id="35870698" class="c"><input type="checkbox" id="c-35870698" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#35868640">root</a><span>|</span><a href="#35870349">parent</a><span>|</span><a href="#35869312">next</a><span>|</span><label class="collapse" for="c-35870698">[-]</label><label class="expand" for="c-35870698">[1 more]</label></div><br/><div class="children"><div class="content">we live in the future. holy shit.</div><br/></div></div></div></div></div></div></div></div><div id="35869312" class="c"><input type="checkbox" id="c-35869312" checked=""/><div class="controls bullet"><span class="by">djmips</span><span>|</span><a href="#35868640">parent</a><span>|</span><a href="#35869091">prev</a><span>|</span><a href="#35868701">next</a><span>|</span><label class="collapse" for="c-35869312">[-]</label><label class="expand" for="c-35869312">[1 more]</label></div><br/><div class="children"><div class="content">Now why would someone want to talk to a Bill Gates automaton?</div><br/></div></div></div></div><div id="35868701" class="c"><input type="checkbox" id="c-35868701" checked=""/><div class="controls bullet"><span class="by">hunta2097</span><span>|</span><a href="#35868640">prev</a><span>|</span><label class="collapse" for="c-35868701">[-]</label><label class="expand" for="c-35868701">[4 more]</label></div><br/><div class="children"><div class="content">Could these LLMs also be subject to the second law of thermodynamics?<p>As in, no matter the size of the magnifying glass, you can&#x27;t concentrate the sun&#x27;s rays to produce a temperature greater than the surface of the sun.<p>Isn&#x27;t it the same with models trained on large amounts of human output?</div><br/><div id="35868736" class="c"><input type="checkbox" id="c-35868736" checked=""/><div class="controls bullet"><span class="by">wincy</span><span>|</span><a href="#35868701">parent</a><span>|</span><a href="#35869613">next</a><span>|</span><label class="collapse" for="c-35868736">[-]</label><label class="expand" for="c-35868736">[2 more]</label></div><br/><div class="children"><div class="content">No? If there were some inherent rule like that how would anything ever have gotten invented by humans? We became smarter than the apes we evolved from, I don&#x27;t understand how it follows at all that something we create can&#x27;t exceed our abilities?</div><br/><div id="35871437" class="c"><input type="checkbox" id="c-35871437" checked=""/><div class="controls bullet"><span class="by">hunta2097</span><span>|</span><a href="#35868701">root</a><span>|</span><a href="#35868736">parent</a><span>|</span><a href="#35869613">next</a><span>|</span><label class="collapse" for="c-35871437">[-]</label><label class="expand" for="c-35871437">[1 more]</label></div><br/><div class="children"><div class="content">Humans are not large language models though. We got more intelligent through social structures and evolutionary pressure.</div><br/></div></div></div></div><div id="35869613" class="c"><input type="checkbox" id="c-35869613" checked=""/><div class="controls bullet"><span class="by">bionhoward</span><span>|</span><a href="#35868701">parent</a><span>|</span><a href="#35868736">prev</a><span>|</span><label class="collapse" for="c-35869613">[-]</label><label class="expand" for="c-35869613">[1 more]</label></div><br/><div class="children"><div class="content">The models can distill basic fundamental principles and broadcast them over vast amounts of working memory beyond human capability to process, so no, it’s not necessarily true that it would be impossible to extrapolate superhuman cognition from contemporary corpus sources. Especially considering we can associate writing samples with intelligence levels and then project out to iq 9000</div><br/></div></div></div></div></div></div></div></div></div></body></html>