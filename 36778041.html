<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1689757254306" as="style"/><link rel="stylesheet" href="styles.css?v=1689757254306"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://simonwillison.net/2023/Jul/18/accessing-llama-2/">Accessing Llama 2 from the command-line with the LLM-replicate plugin</a> <span class="domain">(<a href="https://simonwillison.net">simonwillison.net</a>)</span></div><div class="subtext"><span>simonw</span> | <span>41 comments</span></div><br/><div><div id="36779271" class="c"><input type="checkbox" id="c-36779271" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#36780186">next</a><span>|</span><label class="collapse" for="c-36779271">[-]</label><label class="expand" for="c-36779271">[6 more]</label></div><br/><div class="children"><div class="content">More about my LLM tool (and Python library) here: <a href="https:&#x2F;&#x2F;llm.datasette.io&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;llm.datasette.io&#x2F;</a><p>Here&#x27;s the full implementation of that llm-replicate plugin: <a href="https:&#x2F;&#x2F;github.com&#x2F;simonw&#x2F;llm-replicate&#x2F;blob&#x2F;0.2&#x2F;llm_replicate&#x2F;__init__.py">https:&#x2F;&#x2F;github.com&#x2F;simonw&#x2F;llm-replicate&#x2F;blob&#x2F;0.2&#x2F;llm_replica...</a><p>If you want to write a plugin for some other LLM I have a detailed tutorial here: <a href="https:&#x2F;&#x2F;llm.datasette.io&#x2F;en&#x2F;stable&#x2F;plugins&#x2F;tutorial-model-plugin.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;llm.datasette.io&#x2F;en&#x2F;stable&#x2F;plugins&#x2F;tutorial-model-pl...</a> - plus a bunch of examples linked from here: <a href="https:&#x2F;&#x2F;github.com&#x2F;simonw&#x2F;llm-plugins">https:&#x2F;&#x2F;github.com&#x2F;simonw&#x2F;llm-plugins</a></div><br/><div id="36782265" class="c"><input type="checkbox" id="c-36782265" checked=""/><div class="controls bullet"><span class="by">zyl1n</span><span>|</span><a href="#36779271">parent</a><span>|</span><a href="#36779652">next</a><span>|</span><label class="collapse" for="c-36782265">[-]</label><label class="expand" for="c-36782265">[2 more]</label></div><br/><div class="children"><div class="content">I am not familiar with Replicate, but based on their website, they charge per GPU type. I didn&#x27;t see the GPU type set in the example. Is it baked in as part of the &quot;a16z-infra&#x2F;llama13b-v2-chat&quot; model?</div><br/><div id="36783685" class="c"><input type="checkbox" id="c-36783685" checked=""/><div class="controls bullet"><span class="by">DougBTX</span><span>|</span><a href="#36779271">root</a><span>|</span><a href="#36782265">parent</a><span>|</span><a href="#36779652">next</a><span>|</span><label class="collapse" for="c-36783685">[-]</label><label class="expand" for="c-36783685">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s info about that here: <a href="https:&#x2F;&#x2F;replicate.com&#x2F;a16z-infra&#x2F;llama13b-v2-chat">https:&#x2F;&#x2F;replicate.com&#x2F;a16z-infra&#x2F;llama13b-v2-chat</a><p>&gt; Run time and cost<p>&gt; Predictions run on Nvidia A100 (40GB) GPU hardware. Predictions typically complete within 9 seconds.</div><br/></div></div></div></div><div id="36779652" class="c"><input type="checkbox" id="c-36779652" checked=""/><div class="controls bullet"><span class="by">Anticlockwise</span><span>|</span><a href="#36779271">parent</a><span>|</span><a href="#36782265">prev</a><span>|</span><a href="#36780186">next</a><span>|</span><label class="collapse" for="c-36779652">[-]</label><label class="expand" for="c-36779652">[3 more]</label></div><br/><div class="children"><div class="content">Can you or anyone else comment on how replicate&#x27;s per-second pricing ends up comparing to OpenAI&#x27;s per token pricing when using Llama2?</div><br/><div id="36779939" class="c"><input type="checkbox" id="c-36779939" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#36779271">root</a><span>|</span><a href="#36779652">parent</a><span>|</span><a href="#36780828">next</a><span>|</span><label class="collapse" for="c-36779939">[-]</label><label class="expand" for="c-36779939">[1 more]</label></div><br/><div class="children"><div class="content">My hunch is that OpenAI is a lot cheaper. I&#x27;ve spent $0.26 on 115 seconds of compute with Llama 2 on Replicate so far, which is only a dozen test prompts.</div><br/></div></div><div id="36780828" class="c"><input type="checkbox" id="c-36780828" checked=""/><div class="controls bullet"><span class="by">ta988</span><span>|</span><a href="#36779271">root</a><span>|</span><a href="#36779652">parent</a><span>|</span><a href="#36779939">prev</a><span>|</span><a href="#36780186">next</a><span>|</span><label class="collapse" for="c-36780828">[-]</label><label class="expand" for="c-36780828">[1 more]</label></div><br/><div class="children"><div class="content">It is insanely more expensive on replica and they don&#x27;t have the 70b model yet which will make it even more prohibitive.</div><br/></div></div></div></div></div></div><div id="36780186" class="c"><input type="checkbox" id="c-36780186" checked=""/><div class="controls bullet"><span class="by">xrd</span><span>|</span><a href="#36779271">prev</a><span>|</span><a href="#36779060">next</a><span>|</span><label class="collapse" for="c-36780186">[-]</label><label class="expand" for="c-36780186">[27 more]</label></div><br/><div class="children"><div class="content">I&#x27;m so confused about running these models locally only. When reading about the llm tool, I thought, ok, this helps organize all the pieces on my machine. But then it uses a replicate API key, so clearly it requires a network connection. Is this just to download the models? I feel like we need a new license or packaging model that clearly states whether the computation is happening locally or remotely. It&#x27;s very important to me and often hard to know until I&#x27;m a long way into the installation process.</div><br/><div id="36783429" class="c"><input type="checkbox" id="c-36783429" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#36780186">parent</a><span>|</span><a href="#36780578">next</a><span>|</span><label class="collapse" for="c-36783429">[-]</label><label class="expand" for="c-36783429">[1 more]</label></div><br/><div class="children"><div class="content">For those getting started, the easiest one click installer I&#x27;ve used is Nomic.ai&#x27;s gpt4all: <a href="https:&#x2F;&#x2F;gpt4all.io&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;gpt4all.io&#x2F;</a><p>This runs with a simple GUI on Windows&#x2F;Mac&#x2F;Linux, leverages a fork of llama.cpp on the backend and supports GPU acceleration, and LLaMA, Falcon, MPT, and GPT-J models. It also has API&#x2F;CLI bindings.<p>I just saw a slick new tool <a href="https:&#x2F;&#x2F;ollama.ai&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;ollama.ai&#x2F;</a> that will let you install a llama2-7b with a single `ollama run llama2` command that has a very simple 1-click installer for Apple Silicon Mac (but need to build from source for anything else atm). It looks like it only supports llamas OOTB but it also seems to use llama.cpp (via Go adapter) on the backend - it seemed to be CPU-only on my MBA, but I didn&#x27;t poke too much and it&#x27;s brand new, so we&#x27;ll see.<p>For anyone on HN, they should probably be looking at <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp</a> and <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;ggml">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;ggml</a> directly. If you have a high-end Nvidia consumer card (3090&#x2F;4090) I&#x27;d highly recommend looking into <a href="https:&#x2F;&#x2F;github.com&#x2F;turboderp&#x2F;exllama">https:&#x2F;&#x2F;github.com&#x2F;turboderp&#x2F;exllama</a><p>For those generally confused, the r&#x2F;LocalLLaMA wiki is a good place to start: <a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;wiki&#x2F;guide&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;wiki&#x2F;guide&#x2F;</a><p>I&#x27;ve also been porting my own notes into a single location that tracks models, evals, and has guides focused on local models: <a href="https:&#x2F;&#x2F;llm-tracker.info&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;llm-tracker.info&#x2F;</a></div><br/></div></div><div id="36780578" class="c"><input type="checkbox" id="c-36780578" checked=""/><div class="controls bullet"><span class="by">gcr</span><span>|</span><a href="#36780186">parent</a><span>|</span><a href="#36783429">prev</a><span>|</span><a href="#36780825">next</a><span>|</span><label class="collapse" for="c-36780578">[-]</label><label class="expand" for="c-36780578">[6 more]</label></div><br/><div class="children"><div class="content">The gold standard of local-only model inference for LLaMA, alpaca, and friends is LLaMA-cpp, <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp</a> No dependencies, no GPU needed, just point it to a model snapshot that you download separately on bittorrent. Simple CLI tools that are usable (somewhat) from shell scripts.<p>Hoping they add support for llama 2 soon!</div><br/><div id="36782168" class="c"><input type="checkbox" id="c-36782168" checked=""/><div class="controls bullet"><span class="by">LoganDark</span><span>|</span><a href="#36780186">root</a><span>|</span><a href="#36780578">parent</a><span>|</span><a href="#36780935">next</a><span>|</span><label class="collapse" for="c-36782168">[-]</label><label class="expand" for="c-36782168">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Hoping they add support for llama 2 soon!<p>The 7B and 13B models use the same architecture, so llama.cpp already supports it. (Source: running 13b-chat myself using llama.cpp GPU offload.) It&#x27;s only 70B that has additional extensions that llama.cpp would need to implement.</div><br/></div></div><div id="36780935" class="c"><input type="checkbox" id="c-36780935" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#36780186">root</a><span>|</span><a href="#36780578">parent</a><span>|</span><a href="#36782168">prev</a><span>|</span><a href="#36781277">next</a><span>|</span><label class="collapse" for="c-36780935">[-]</label><label class="expand" for="c-36780935">[1 more]</label></div><br/><div class="children"><div class="content">Using their default CLI tools from a shell script is sadly a little bit tricky.<p>I opened a feature request a while back suggesting they add a --json mode to make that easier, hasn&#x27;t gained much traction though: <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;issues&#x2F;1739">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;issues&#x2F;1739</a></div><br/></div></div><div id="36781277" class="c"><input type="checkbox" id="c-36781277" checked=""/><div class="controls bullet"><span class="by">mikeravkine</span><span>|</span><a href="#36780186">root</a><span>|</span><a href="#36780578">parent</a><span>|</span><a href="#36780935">prev</a><span>|</span><a href="#36781604">next</a><span>|</span><label class="collapse" for="c-36781277">[-]</label><label class="expand" for="c-36781277">[2 more]</label></div><br/><div class="children"><div class="content">Llama2 7B and 13B GGML are up and work with existing llama.cpp no changes needed!  The 70B does require a llama.cpp change, but I&#x27;m sure it won&#x27;t take long.</div><br/><div id="36781828" class="c"><input type="checkbox" id="c-36781828" checked=""/><div class="controls bullet"><span class="by">bcjordan</span><span>|</span><a href="#36780186">root</a><span>|</span><a href="#36781277">parent</a><span>|</span><a href="#36781604">next</a><span>|</span><label class="collapse" for="c-36781828">[-]</label><label class="expand" for="c-36781828">[1 more]</label></div><br/><div class="children"><div class="content">Are there new system requirements known for these?</div><br/></div></div></div></div><div id="36781604" class="c"><input type="checkbox" id="c-36781604" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#36780186">root</a><span>|</span><a href="#36780578">parent</a><span>|</span><a href="#36781277">prev</a><span>|</span><a href="#36780825">next</a><span>|</span><label class="collapse" for="c-36781604">[-]</label><label class="expand" for="c-36781604">[1 more]</label></div><br/><div class="children"><div class="content">The real gold standard is <a href="https:&#x2F;&#x2F;github.com&#x2F;oobabooga&#x2F;text-generation-webui">https:&#x2F;&#x2F;github.com&#x2F;oobabooga&#x2F;text-generation-webui</a><p>Which includes the llama.cpp backend, and a lot more.<p>Unfortunately, despite claiming to be the &quot;Automatic1111&quot; of text generation, it doesn&#x27;t support any of the prompt engineering capabilities (i.e. negative prompts, prompt weights, prompt blending, etc) available in Automatic1111, despite the fact that it&#x27;s not difficult to implement - <a href="https:&#x2F;&#x2F;gist.github.com&#x2F;Hellisotherpeople&#x2F;45c619ee22aac6865ca4bb328eb58faf" rel="nofollow noreferrer">https:&#x2F;&#x2F;gist.github.com&#x2F;Hellisotherpeople&#x2F;45c619ee22aac6865c...</a><p>Luckily for Ooga Booga, no one else supports it either. Why this is? I have no explanation except that the NLP community doesn&#x27;t know jack about prompt engineering, which is Kafkaesque</div><br/></div></div></div></div><div id="36780825" class="c"><input type="checkbox" id="c-36780825" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#36780186">parent</a><span>|</span><a href="#36780578">prev</a><span>|</span><a href="#36783100">next</a><span>|</span><label class="collapse" for="c-36780825">[-]</label><label class="expand" for="c-36780825">[5 more]</label></div><br/><div class="children"><div class="content">My LLM tool can be used for both. That&#x27;s what the plugins are for.<p>It can talk to OpenAI, PaLM 2 and Llama &#x2F; other models on Replicate via API, using API keys - the OpenAI stuff is in the default LLM tool, PaLM 2 uses <a href="https:&#x2F;&#x2F;github.com&#x2F;simonw&#x2F;llm-palm">https:&#x2F;&#x2F;github.com&#x2F;simonw&#x2F;llm-palm</a> and Replicate uses <a href="https:&#x2F;&#x2F;github.com&#x2F;simonw&#x2F;llm-replicate">https:&#x2F;&#x2F;github.com&#x2F;simonw&#x2F;llm-replicate</a><p>It can run local models on your own machine using these two plugins: <a href="https:&#x2F;&#x2F;github.com&#x2F;simonw&#x2F;llm-gpt4all">https:&#x2F;&#x2F;github.com&#x2F;simonw&#x2F;llm-gpt4all</a> and <a href="https:&#x2F;&#x2F;github.com&#x2F;simonw&#x2F;llm-mpt30b">https:&#x2F;&#x2F;github.com&#x2F;simonw&#x2F;llm-mpt30b</a></div><br/><div id="36780847" class="c"><input type="checkbox" id="c-36780847" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#36780186">root</a><span>|</span><a href="#36780825">parent</a><span>|</span><a href="#36783100">next</a><span>|</span><label class="collapse" for="c-36780847">[-]</label><label class="expand" for="c-36780847">[4 more]</label></div><br/><div class="children"><div class="content">Do you have any thoughts on how I can make this more obvious?<p>It&#x27;s covered by the documentation for the individual plugins, but I want to make it as easy as possible for people to understand what&#x27;s going on when they first start using the tool.</div><br/><div id="36781321" class="c"><input type="checkbox" id="c-36781321" checked=""/><div class="controls bullet"><span class="by">xrd</span><span>|</span><a href="#36780186">root</a><span>|</span><a href="#36780847">parent</a><span>|</span><a href="#36781076">next</a><span>|</span><label class="collapse" for="c-36781321">[-]</label><label class="expand" for="c-36781321">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s laziness on my part. I read just your blog post. Had I clicked through to the tool, it clearly says it at the top. My apologies.<p>I&#x27;m very grateful for all the work and writing you are doing about LLMs.<p>Regarding your note about JSON mode with llama.cpp, I&#x27;m writing a wrapper for it on my katarismo project. It is basically the stdout suggestion from that comment, but it is working really well for me when I use it with pocketbase.<p><a href="https:&#x2F;&#x2F;gitlab.p.katarismo.com&#x2F;katarismo&#x2F;backend" rel="nofollow noreferrer">https:&#x2F;&#x2F;gitlab.p.katarismo.com&#x2F;katarismo&#x2F;backend</a></div><br/></div></div><div id="36781076" class="c"><input type="checkbox" id="c-36781076" checked=""/><div class="controls bullet"><span class="by">ricopags</span><span>|</span><a href="#36780186">root</a><span>|</span><a href="#36780847">parent</a><span>|</span><a href="#36781321">prev</a><span>|</span><a href="#36783100">next</a><span>|</span><label class="collapse" for="c-36781076">[-]</label><label class="expand" for="c-36781076">[2 more]</label></div><br/><div class="children"><div class="content">Perhaps something as simple as stating it was first built around OpenAI models and later expanded to local via plugins?<p>I&#x27;ve been meaning to ask you, have you seen&#x2F;used MS Guidance[0] &#x27;language&#x27; at all? I don&#x27;t know if it&#x27;s the right abstraction to interface as a plugin with what you&#x27;ve got in llm cli but there&#x27;s a lot about Guidance that seems incredibly useful to local inference [token healing and acceleration especially].<p>[0]<a href="https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;guidance">https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;guidance</a></div><br/><div id="36781100" class="c"><input type="checkbox" id="c-36781100" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#36780186">root</a><span>|</span><a href="#36781076">parent</a><span>|</span><a href="#36783100">next</a><span>|</span><label class="collapse" for="c-36781100">[-]</label><label class="expand" for="c-36781100">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, I looked at Guidance and I have to admit I don&#x27;t fully get it - my main problem was that I can&#x27;t look at one of their Handlebars templates and figure out exactly what LLM prompts it&#x27;s going to fire and in what order they will be sent.<p>I&#x27;m much happier with a very thin wrapper where I can explicitly see exactly what prompts are processed when, and where prompts are assembled using very simple string manipulation.<p>I&#x27;m thinking I may pull the OpenAI stuff out of LLM core and make that a plugin as well - that way it will be VERY obvious when you install the tool that you get to pick which LLMs you&#x27;re going to work with.</div><br/></div></div></div></div></div></div></div></div><div id="36783100" class="c"><input type="checkbox" id="c-36783100" checked=""/><div class="controls bullet"><span class="by">Tepix</span><span>|</span><a href="#36780186">parent</a><span>|</span><a href="#36780825">prev</a><span>|</span><a href="#36780574">next</a><span>|</span><label class="collapse" for="c-36783100">[-]</label><label class="expand" for="c-36783100">[1 more]</label></div><br/><div class="children"><div class="content">Simon&#x27;s article didn&#x27;t show local usage.<p>Use one of the one-click installers linked in the README of<p><a href="https:&#x2F;&#x2F;github.com&#x2F;oobabooga&#x2F;text-generation-webui">https:&#x2F;&#x2F;github.com&#x2F;oobabooga&#x2F;text-generation-webui</a><p>and you&#x27;re set.<p>Note that just in case you have the hardware necessary to run the biggest available model <i>llama2-70b</i> (for example two RTX 3090 with a total of 48GB of VRAM), there is currently a small bug (with a fix) documented at <a href="https:&#x2F;&#x2F;github.com&#x2F;oobabooga&#x2F;text-generation-webui&#x2F;issues&#x2F;3201">https:&#x2F;&#x2F;github.com&#x2F;oobabooga&#x2F;text-generation-webui&#x2F;issues&#x2F;32...</a></div><br/></div></div><div id="36780574" class="c"><input type="checkbox" id="c-36780574" checked=""/><div class="controls bullet"><span class="by">transcriptase</span><span>|</span><a href="#36780186">parent</a><span>|</span><a href="#36783100">prev</a><span>|</span><a href="#36780501">next</a><span>|</span><label class="collapse" for="c-36780574">[-]</label><label class="expand" for="c-36780574">[11 more]</label></div><br/><div class="children"><div class="content">I’m in the same boat.<p>With text-to-image models I pick out software of my choice and drop a downloaded model into an input directory, then launch a GUI.<p>What is it about LLMs that makes actually being able to run them locally so complex in comparison?<p>I’m genuinely curious as a layman.</div><br/><div id="36780658" class="c"><input type="checkbox" id="c-36780658" checked=""/><div class="controls bullet"><span class="by">gcr</span><span>|</span><a href="#36780186">root</a><span>|</span><a href="#36780574">parent</a><span>|</span><a href="#36781694">next</a><span>|</span><label class="collapse" for="c-36780658">[-]</label><label class="expand" for="c-36780658">[8 more]</label></div><br/><div class="children"><div class="content">Nothing in particular. Setting up a scientific python+pytorch stack is difficult if you&#x27;re unfamiliar with the python packaging ecosystem.<p>If you&#x27;re not on a &quot;happy path&quot; of &quot;ubuntu, nvidia, and anaconda,&quot; then lots of things could go wrong if you&#x27;re configuring from scratch. If you want to run these models efficiently, hardware acceleration is a must, but managing the intersection between {GPU, operating system, architecture, Python version, Python virtualenv location} is tricky.<p>That&#x27;s even before you deal with hardware-specific implementation details!<p>- Running NVidia? Double-check your CUDA library version, kernel module version, (optional) CuDNN, and pytorch version<p>- Running AMD? Double-check your ROCm library version, AMD drivers, and make sure that you use Pytorch provided by AMD with ROCm support<p>- On Apple machines? Double-check that your M1 hardware actually has proper hardware support, then download and install a custom Pytorch distribution linked with M1 support, and make sure that the numpy library version has been properly linked with Accelerate.framework or else your BLAS calls run on the CPU rather than the undocumented AMX coprocessor. If you want to run on the ANE, you&#x27;ll additionally need a working xcode toolchain and a version of the CoreML model compiler that can read your serialized pytorch model files properly.<p>I think the pain of getting things working makes it easier to just throw up one&#x27;s hands and pay someone else to run your model for you.</div><br/><div id="36780854" class="c"><input type="checkbox" id="c-36780854" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#36780186">root</a><span>|</span><a href="#36780658">parent</a><span>|</span><a href="#36781694">next</a><span>|</span><label class="collapse" for="c-36780854">[-]</label><label class="expand" for="c-36780854">[7 more]</label></div><br/><div class="children"><div class="content">It is wild to me how hard it is to run these things GPU-accelerated on an Apple M1&#x2F;M2.<p>The hardware support should be amazing for this, given that the CPU and GPU share the same RAM.<p>I mostly end up running the CPU versions and grumbling about how slow they are.</div><br/><div id="36782496" class="c"><input type="checkbox" id="c-36782496" checked=""/><div class="controls bullet"><span class="by">smoldesu</span><span>|</span><a href="#36780186">root</a><span>|</span><a href="#36780854">parent</a><span>|</span><a href="#36781747">next</a><span>|</span><label class="collapse" for="c-36782496">[-]</label><label class="expand" for="c-36782496">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The hardware support should be amazing for this<p>I mean, caveat emptor; CoreML has been barebones for years, and Apple isn&#x27;t exactly notorious for huge commitment to third party APIs. The writing was on the wall with how Metal was rolled out and how fast OpenCL got dropped, it honestly doesn&#x27;t surprise me at all at this point. Even the current Apple Silicon support in llama.cpp is fudged with NEON and Metal Shaders instead of Apple&#x27;s &quot;Neural Engine&quot;.</div><br/></div></div><div id="36781747" class="c"><input type="checkbox" id="c-36781747" checked=""/><div class="controls bullet"><span class="by">bugglebeetle</span><span>|</span><a href="#36780186">root</a><span>|</span><a href="#36780854">parent</a><span>|</span><a href="#36782496">prev</a><span>|</span><a href="#36781694">next</a><span>|</span><label class="collapse" for="c-36781747">[-]</label><label class="expand" for="c-36781747">[5 more]</label></div><br/><div class="children"><div class="content">GPU acceleration is pretty easy with llama.cpp. You just run make with an extra  flag and then an argument or two at runtime.</div><br/><div id="36782139" class="c"><input type="checkbox" id="c-36782139" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#36780186">root</a><span>|</span><a href="#36781747">parent</a><span>|</span><a href="#36781915">next</a><span>|</span><label class="collapse" for="c-36782139">[-]</label><label class="expand" for="c-36782139">[1 more]</label></div><br/><div class="children"><div class="content">Adrien Brault on Twitter gave me this recipe, which worked perfectly: <a href="https:&#x2F;&#x2F;gist.github.com&#x2F;adrienbrault&#x2F;b76631c56c736def9bc1bc2167b5d129" rel="nofollow noreferrer">https:&#x2F;&#x2F;gist.github.com&#x2F;adrienbrault&#x2F;b76631c56c736def9bc1bc2...</a></div><br/></div></div><div id="36781915" class="c"><input type="checkbox" id="c-36781915" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#36780186">root</a><span>|</span><a href="#36781747">parent</a><span>|</span><a href="#36782139">prev</a><span>|</span><a href="#36781694">next</a><span>|</span><label class="collapse" for="c-36781915">[-]</label><label class="expand" for="c-36781915">[3 more]</label></div><br/><div class="children"><div class="content">Therein lies the problem.<p>I want to write instructions for people to use my software that don&#x27;t expect them to know how to run make, or even to have a C compiler installed.</div><br/><div id="36782019" class="c"><input type="checkbox" id="c-36782019" checked=""/><div class="controls bullet"><span class="by">bugglebeetle</span><span>|</span><a href="#36780186">root</a><span>|</span><a href="#36781915">parent</a><span>|</span><a href="#36781694">next</a><span>|</span><label class="collapse" for="c-36782019">[-]</label><label class="expand" for="c-36782019">[2 more]</label></div><br/><div class="children"><div class="content">Then you more or less need a GUI like OpenAI built with ChatGPT so you control the whole environment. Even setting up LLM in Homebrew required me to do the whole install twice because of some arcane error.</div><br/><div id="36782125" class="c"><input type="checkbox" id="c-36782125" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#36780186">root</a><span>|</span><a href="#36782019">parent</a><span>|</span><a href="#36781694">next</a><span>|</span><label class="collapse" for="c-36782125">[-]</label><label class="expand" for="c-36782125">[1 more]</label></div><br/><div class="children"><div class="content">I think I can fix that by shipping a bottle release - &quot;brew install simonw&#x2F;llm&#x2F;llm&quot; currently attempts to compile Pydantic&#x27;s Rust extensions, which means it runs incredibly slowly.<p>I built a bottle for it which installs much faster, but you currently have to download the file from <a href="https:&#x2F;&#x2F;static.simonwillison.net&#x2F;static&#x2F;2023&#x2F;llm--0.5.arm64_ventura.bottle.1.tar.gz" rel="nofollow noreferrer">https:&#x2F;&#x2F;static.simonwillison.net&#x2F;static&#x2F;2023&#x2F;llm--0.5.arm64_...</a> - I&#x27;ve not yet figured out how to get that to install when the user runs &quot;brew install simonw&#x2F;llm&#x2F;llm&quot; - issue here: <a href="https:&#x2F;&#x2F;github.com&#x2F;simonw&#x2F;llm&#x2F;issues&#x2F;102">https:&#x2F;&#x2F;github.com&#x2F;simonw&#x2F;llm&#x2F;issues&#x2F;102</a></div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="36781694" class="c"><input type="checkbox" id="c-36781694" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#36780186">root</a><span>|</span><a href="#36780574">parent</a><span>|</span><a href="#36780658">prev</a><span>|</span><a href="#36781455">next</a><span>|</span><label class="collapse" for="c-36781694">[-]</label><label class="expand" for="c-36781694">[1 more]</label></div><br/><div class="children"><div class="content">In general the default LLM models are too big to run on a consumer GPU without quantization.<p>That means additional steps or alternative downloads are needed.<p>Also generative graphics models are about 6 months further advanced than generative text models because Stable Diffusion came out before Llama (Llama being the first &quot;useful&quot; smaller-large-language-model)</div><br/></div></div><div id="36781455" class="c"><input type="checkbox" id="c-36781455" checked=""/><div class="controls bullet"><span class="by">fomine3</span><span>|</span><a href="#36780186">root</a><span>|</span><a href="#36780574">parent</a><span>|</span><a href="#36781694">prev</a><span>|</span><a href="#36780501">next</a><span>|</span><label class="collapse" for="c-36781455">[-]</label><label class="expand" for="c-36781455">[1 more]</label></div><br/><div class="children"><div class="content">We know that GPT-4 is the king at this time, so only researchers, developers, and enthusiasts setup local LLMs. They don&#x27;t need easier way to setup. For image generation, Local Stable Diffusion is the king for certain big genre of image generation. Many people including PC newbie try it, so there are many guides.</div><br/></div></div></div></div><div id="36780501" class="c"><input type="checkbox" id="c-36780501" checked=""/><div class="controls bullet"><span class="by">ladberg</span><span>|</span><a href="#36780186">parent</a><span>|</span><a href="#36780574">prev</a><span>|</span><a href="#36780316">next</a><span>|</span><label class="collapse" for="c-36780501">[-]</label><label class="expand" for="c-36780501">[1 more]</label></div><br/><div class="children"><div class="content">Idk, to me this reads pretty clearly remote (for this model in particularly):<p>&gt; My LLM tool provides command-line access to a wide variety of language models, both via web APIs and self-hosted on your own machine ... The brand new llm-replicate plugin provides CLI access to models hosted on Replicate, and this morning a16z-infra released a16z-infra&#x2F;llama13b-v2-chat which provides Replicate API access to the new Llama 2 13B chat model.<p>I&#x27;m also unsure what this would have to do with the license, when you install any other software do you expect the license to tell you whether processing happens locally or in the cloud?</div><br/></div></div></div></div><div id="36779060" class="c"><input type="checkbox" id="c-36779060" checked=""/><div class="controls bullet"><span class="by">peatmoss</span><span>|</span><a href="#36780186">prev</a><span>|</span><a href="#36782726">next</a><span>|</span><label class="collapse" for="c-36779060">[-]</label><label class="expand" for="c-36779060">[5 more]</label></div><br/><div class="children"><div class="content">I feel like Simon&#x27;s been on a tear with these LLM postings. Simon, I really enjoying you swashbuckling through this, and then documenting your travels.</div><br/><div id="36780413" class="c"><input type="checkbox" id="c-36780413" checked=""/><div class="controls bullet"><span class="by">turnsout</span><span>|</span><a href="#36779060">parent</a><span>|</span><a href="#36781295">next</a><span>|</span><label class="collapse" for="c-36780413">[-]</label><label class="expand" for="c-36780413">[1 more]</label></div><br/><div class="children"><div class="content">Does Simon sleep? He&#x27;s unstoppable!</div><br/></div></div><div id="36781295" class="c"><input type="checkbox" id="c-36781295" checked=""/><div class="controls bullet"><span class="by">MuffinFlavored</span><span>|</span><a href="#36779060">parent</a><span>|</span><a href="#36780413">prev</a><span>|</span><a href="#36782726">next</a><span>|</span><label class="collapse" for="c-36781295">[-]</label><label class="expand" for="c-36781295">[3 more]</label></div><br/><div class="children"><div class="content">I kind of wonder the opposite.<p>&gt; created: October 29, 2007<p>&gt; karma: 40256<p>Is he not potentially under the category of &quot;potentially starved for online HackerNews attention&quot;?<p>Why does he feel the need to maintain a blog and libraries, etc. etc. and then submit it and let us all know about it?</div><br/><div id="36781571" class="c"><input type="checkbox" id="c-36781571" checked=""/><div class="controls bullet"><span class="by">haswell</span><span>|</span><a href="#36779060">root</a><span>|</span><a href="#36781295">parent</a><span>|</span><a href="#36782287">next</a><span>|</span><label class="collapse" for="c-36781571">[-]</label><label class="expand" for="c-36781571">[1 more]</label></div><br/><div class="children"><div class="content">Technology is fun! Sharing with others is fun! Why do any of us do anything?</div><br/></div></div><div id="36782287" class="c"><input type="checkbox" id="c-36782287" checked=""/><div class="controls bullet"><span class="by">mdp2021</span><span>|</span><a href="#36779060">root</a><span>|</span><a href="#36781295">parent</a><span>|</span><a href="#36781571">prev</a><span>|</span><a href="#36782726">next</a><span>|</span><label class="collapse" for="c-36782287">[-]</label><label class="expand" for="c-36782287">[1 more]</label></div><br/><div class="children"><div class="content">Because we know we learn from others so we also offer. Constructive reciprocity.<p>Not dissimilar from sharing code.</div><br/></div></div></div></div></div></div><div id="36782726" class="c"><input type="checkbox" id="c-36782726" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#36779060">prev</a><span>|</span><a href="#36780204">next</a><span>|</span><label class="collapse" for="c-36782726">[-]</label><label class="expand" for="c-36782726">[1 more]</label></div><br/><div class="children"><div class="content">This works too now for the 70b model:<p><pre><code>    llm replicate add \
      replicate&#x2F;llama70b-v2-chat \
      --chat --alias llama70b
</code></pre>
Then:<p><pre><code>    llm -m llama70b &quot;Invent an absurd ice cream sundae&quot;</code></pre></div><br/></div></div></div></div></div></div></div></body></html>