<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1698829260396" as="style"/><link rel="stylesheet" href="styles.css?v=1698829260396"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.phind.com/phindmodelhn">Phind Model beats GPT-4 at coding, with GPT-3.5 speed and 16k context</a> <span class="domain">(<a href="https://www.phind.com">www.phind.com</a>)</span></div><div class="subtext"><span>rushingcreek</span> | <span>186 comments</span></div><br/><div><div id="38095538" class="c"><input type="checkbox" id="c-38095538" checked=""/><div class="controls bullet"><span class="by">alex-moon</span><span>|</span><a href="#38090332">next</a><span>|</span><label class="collapse" for="c-38095538">[-]</label><label class="expand" for="c-38095538">[3 more]</label></div><br/><div class="children"><div class="content">I tried my standard &quot;trick&quot; question I use for LLMs:<p>&quot;Give me five papers with code demonstrating the state of the art of machine learning which uses geospatial data (e.g. GeoJSON) as both input and output.&quot;<p>There is no such state of the art. My hand-wavey understanding is that GIS data is non-continuous, which makes it useless for transformers, and also contextual, which makes it useless for anything else. Will defer to actual ML people for better explanations.<p>Point is, LLMs invariably give five papers with code that don&#x27;t actually exist - it&#x27;s a guaranteed hallucination.<p>Phind was able to give me five links that do in fact exist, as well as contextual information as to why these five links were not papers with code doing ML with GIS data. This is by far the best answer to this question from an LLM I&#x27;ve received yet.</div><br/><div id="38095728" class="c"><input type="checkbox" id="c-38095728" checked=""/><div class="controls bullet"><span class="by">jstummbillig</span><span>|</span><a href="#38095538">parent</a><span>|</span><a href="#38090332">next</a><span>|</span><label class="collapse" for="c-38095728">[-]</label><label class="expand" for="c-38095728">[2 more]</label></div><br/><div class="children"><div class="content">ChatGPT 4 with web browsing: <a href="https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;19a425b5-ed37-469e-860d-65ee706cd603" rel="nofollow noreferrer">https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;19a425b5-ed37-469e-860d-65ee70...</a><p>ChatGPT 4 without web browsing: <a href="https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;7e11b4a6-52f2-441a-8614-7266c369865a" rel="nofollow noreferrer">https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;7e11b4a6-52f2-441a-8614-7266c3...</a></div><br/><div id="38095772" class="c"><input type="checkbox" id="c-38095772" checked=""/><div class="controls bullet"><span class="by">alex-moon</span><span>|</span><a href="#38095538">root</a><span>|</span><a href="#38095728">parent</a><span>|</span><a href="#38090332">next</a><span>|</span><label class="collapse" for="c-38095772">[-]</label><label class="expand" for="c-38095772">[1 more]</label></div><br/><div class="children"><div class="content">ChatGPT 4 seems to be better than it was when I was using it (mere months ago)!</div><br/></div></div></div></div></div></div><div id="38090332" class="c"><input type="checkbox" id="c-38090332" checked=""/><div class="controls bullet"><span class="by">zoogeny</span><span>|</span><a href="#38095538">prev</a><span>|</span><a href="#38089763">next</a><span>|</span><label class="collapse" for="c-38090332">[-]</label><label class="expand" for="c-38090332">[17 more]</label></div><br/><div class="children"><div class="content">I just spent a few minutes doing a comparison between Phind and GPT-4 for a very high-level question on a distributed job queue. I gave them both the same fairly vague sketch of a kind of system I would like to build. Here are my impressions:<p>In the positives of Phind:<p>* Phind was able, even eager, to recommend specific libraries relevant to the implementation. The recommendations matched my own research. GPT-4 takes some coaxing to get it to recommend libraries. Phind also provided sample code using the libraries it recommended.<p>* Phind provides copious relevant sources including github, stackoverflow and others. This is a major advantage, especially if you use these AI assistants as a jumping off ground for further research.<p>* Phind provides recommendations for follow on questions that were very good. One suggestion to the Phind team: don&#x27;t remove the alternate follow on questions once I select one. A couple of times it recommended a few really good follow up questions but as soon as I selected one the others disappear.<p>In the positives of GPT-4:<p>* GPT-4 gave better answers. This is my subjective opinion (obviously) but if I was interviewing two candidates for a job position and using my question as the basis for a systems-design interview then GPT-4 was just overall better. In many cases it added context beyond my question, recommending things like logging and metrics for example. It seemed to intuit the &quot;question behind the question&quot; in a much better way than the literal interpretation of Phind. This is probably highly case-dependent, sometimes I just want an answer to my explicit question. But GPT-4 seemed to understand the broader context of the question and replied with that in mind leading to an overall more relevant response.<p>* GPT-4 handled follow-up questions better. This is similar to the previous point - but GPT-4 gave me the impression of narrowing down the scope of the discussion based on the context of my follow-up question. It seemed to &quot;understand&quot; the direction of the conversation in a way that felt like it was following context.<p>NOTE: this was not a test on coding capability (e.g. implementing algorithms) but on using these AI coding assistants as sounding boards for high-level design and architecture decisions.</div><br/><div id="38093165" class="c"><input type="checkbox" id="c-38093165" checked=""/><div class="controls bullet"><span class="by">idonotknowwhy</span><span>|</span><a href="#38090332">parent</a><span>|</span><a href="#38090683">next</a><span>|</span><label class="collapse" for="c-38093165">[-]</label><label class="expand" for="c-38093165">[1 more]</label></div><br/><div class="children"><div class="content">This is a good point about GPT-4, it can intuit the &quot;question behind the question&quot; really well compared with other models. And it&#x27;s been profoundly useful for me with the most random tasks I knew nothing about prior (like fixing a wall in my house), etc.</div><br/></div></div><div id="38090683" class="c"><input type="checkbox" id="c-38090683" checked=""/><div class="controls bullet"><span class="by">X6S1x6Okd1st</span><span>|</span><a href="#38090332">parent</a><span>|</span><a href="#38093165">prev</a><span>|</span><a href="#38091115">next</a><span>|</span><label class="collapse" for="c-38090683">[-]</label><label class="expand" for="c-38090683">[5 more]</label></div><br/><div class="children"><div class="content">&gt; * Phind provides copious relevant sources including github, stackoverflow and others. This is a major advantage, especially if you use these AI assistants as a jumping off ground for further research.<p>Did you find them to be correct?</div><br/><div id="38090939" class="c"><input type="checkbox" id="c-38090939" checked=""/><div class="controls bullet"><span class="by">pbhjpbhj</span><span>|</span><a href="#38090332">root</a><span>|</span><a href="#38090683">parent</a><span>|</span><a href="#38091578">next</a><span>|</span><label class="collapse" for="c-38090939">[-]</label><label class="expand" for="c-38090939">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t use Phind for coding, except occasionally, but I like it best for generalised tech search because each para has a reference and there&#x27;s a list of references down the side -- often the references would really be sufficient for me on their own.<p>I&#x27;ve had one glaring error, I can&#x27;t quite remember the details, but it switched the names&#x2F;characteristics of two different processes (ie was exactly opposite in what it said); it was something to do with instruction caching and TLB, IIRC. I assumed you&#x27;d was a problem with the input corpus not allowing antonyms to be disambiguated.<p>Anyway, for me it&#x27;s the best of the LLM tools I have access to and had mostly replaced search engine (Google, Dukgo) for my tech-related work.<p>I&#x27;ve only used chat.openai.com (free), bing chat, HuggingChat.</div><br/></div></div><div id="38091578" class="c"><input type="checkbox" id="c-38091578" checked=""/><div class="controls bullet"><span class="by">zoogeny</span><span>|</span><a href="#38090332">root</a><span>|</span><a href="#38090683">parent</a><span>|</span><a href="#38090939">prev</a><span>|</span><a href="#38091115">next</a><span>|</span><label class="collapse" for="c-38091578">[-]</label><label class="expand" for="c-38091578">[3 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think &quot;correct&quot; is the right word since these were open ended systems design type questions. There are many ways to accomplish the same task.<p>I also spent about 20 minutes on this which is why I mentioned this is a first impression. I&#x27;ll leave it to researchers to develop a &quot;relevancy&quot; metric and objectively apply it.<p>In my experience, the sources were sufficiently relevant based on its responses. They were about as relevant as equivalent Google queries. Some tiny, tiny niggles, like I was explicit I wanted it to recommend approaches in Go and for one reference I recall related to distributed locking mechanisms it provided a reference to an implementation in Java. However, that is completely fine for me since the context was more about the locking on the database side and not really the implementation in a specific language.</div><br/><div id="38092527" class="c"><input type="checkbox" id="c-38092527" checked=""/><div class="controls bullet"><span class="by">foobarbecue</span><span>|</span><a href="#38090332">root</a><span>|</span><a href="#38091578">parent</a><span>|</span><a href="#38091115">next</a><span>|</span><label class="collapse" for="c-38092527">[-]</label><label class="expand" for="c-38092527">[2 more]</label></div><br/><div class="children"><div class="content">And the sources actually existed? i.e. there weren&#x27;t any made-up ones?</div><br/><div id="38092595" class="c"><input type="checkbox" id="c-38092595" checked=""/><div class="controls bullet"><span class="by">zoogeny</span><span>|</span><a href="#38090332">root</a><span>|</span><a href="#38092527">parent</a><span>|</span><a href="#38091115">next</a><span>|</span><label class="collapse" for="c-38092595">[-]</label><label class="expand" for="c-38092595">[1 more]</label></div><br/><div class="children"><div class="content">The sources are urls to the cited page (e.g. stackoverflow.com, pkg.go.dev). In the side-bar next to the answer is a more standard search-result style link list with pulled quotes from the pages (like a Google search).<p>I didn&#x27;t click every single link (as I mentioned, the citations are copious) but the few I did follow went to relevant articles. I just went back and randomly clicked several more and they all went to pages that exist and mostly relate to the content of the answer. The inline citations seem a bit more on-topic compared to the side bar which does seem more like the links were lifted directly from a search engine.<p>To be fair there are some lower-quality blog-spammy kinda stuff - more or less the same kind of thing you would get out of Google. But compared to GPT-4, which provides no sources whatsoever, it is an advantage IMO.</div><br/></div></div></div></div></div></div></div></div><div id="38091115" class="c"><input type="checkbox" id="c-38091115" checked=""/><div class="controls bullet"><span class="by">webappguy</span><span>|</span><a href="#38090332">parent</a><span>|</span><a href="#38090683">prev</a><span>|</span><a href="#38092539">next</a><span>|</span><label class="collapse" for="c-38091115">[-]</label><label class="expand" for="c-38091115">[6 more]</label></div><br/><div class="children"><div class="content">Do you have custom instructions? Everyone needs to mention and post prompts else entirely antidotal</div><br/><div id="38091122" class="c"><input type="checkbox" id="c-38091122" checked=""/><div class="controls bullet"><span class="by">rushingcreek</span><span>|</span><a href="#38090332">root</a><span>|</span><a href="#38091115">parent</a><span>|</span><a href="#38092353">next</a><span>|</span><label class="collapse" for="c-38091122">[-]</label><label class="expand" for="c-38091122">[4 more]</label></div><br/><div class="children"><div class="content">We support custom instructions at <a href="https:&#x2F;&#x2F;phind.com&#x2F;profile">https:&#x2F;&#x2F;phind.com&#x2F;profile</a>.</div><br/><div id="38091371" class="c"><input type="checkbox" id="c-38091371" checked=""/><div class="controls bullet"><span class="by">bredren</span><span>|</span><a href="#38090332">root</a><span>|</span><a href="#38091122">parent</a><span>|</span><a href="#38092353">next</a><span>|</span><label class="collapse" for="c-38091371">[-]</label><label class="expand" for="c-38091371">[3 more]</label></div><br/><div class="children"><div class="content">I’m trying to get it to answer only in executable Python. I used the template with instructions I use for my system prompt on gpt4. And I tried using the additional context field for the same.<p>It gets to writing the expected code but it still wants to include formatted headings instead of commenting those out so the entire response is executable Python.<p>As a follow up I provided an example heading with the hash out front. It didn’t work.<p>Any ideas on how to get it to this? Fwiw, gpt4 often if ignores this request, but only about half the time. When it does it is typically a single block of explanatory text.<p>For that, I include prose detection and commenting as part of my post processing.<p>Also, I don’t see it easily, but do you have an API for this or is it intended to be run by the user?</div><br/><div id="38091653" class="c"><input type="checkbox" id="c-38091653" checked=""/><div class="controls bullet"><span class="by">rushingcreek</span><span>|</span><a href="#38090332">root</a><span>|</span><a href="#38091371">parent</a><span>|</span><a href="#38091890">next</a><span>|</span><label class="collapse" for="c-38091653">[-]</label><label class="expand" for="c-38091653">[1 more]</label></div><br/><div class="children"><div class="content">Getting it to not output additional text is not something that it can do super well at the moment, unfortunately. We&#x27;ll work on that.</div><br/></div></div><div id="38091890" class="c"><input type="checkbox" id="c-38091890" checked=""/><div class="controls bullet"><span class="by">soulofmischief</span><span>|</span><a href="#38090332">root</a><span>|</span><a href="#38091371">parent</a><span>|</span><a href="#38091653">prev</a><span>|</span><a href="#38092353">next</a><span>|</span><label class="collapse" for="c-38091890">[-]</label><label class="expand" for="c-38091890">[1 more]</label></div><br/><div class="children"><div class="content">My trick for this has been one-shot training + regex. I tell the model to produce executable code within triple backticks suffixed by a keyword, like:<p>```keyword
&#x2F;&#x2F; code
```<p>and then I just ignore anything outside of those blocks.</div><br/></div></div></div></div></div></div><div id="38092353" class="c"><input type="checkbox" id="c-38092353" checked=""/><div class="controls bullet"><span class="by">zoogeny</span><span>|</span><a href="#38090332">root</a><span>|</span><a href="#38091115">parent</a><span>|</span><a href="#38091122">prev</a><span>|</span><a href="#38092539">next</a><span>|</span><label class="collapse" for="c-38092353">[-]</label><label class="expand" for="c-38092353">[1 more]</label></div><br/><div class="children"><div class="content">I did not have custom instructions for either assistant. You can see the full conversation logs which I posted as a reply to another comment.</div><br/></div></div></div></div><div id="38092539" class="c"><input type="checkbox" id="c-38092539" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#38090332">parent</a><span>|</span><a href="#38091115">prev</a><span>|</span><a href="#38090742">next</a><span>|</span><label class="collapse" for="c-38092539">[-]</label><label class="expand" for="c-38092539">[1 more]</label></div><br/><div class="children"><div class="content">The “give context” part has a lot to do with prompting well based on the model. To have a fair comparison there should be just code and see what that come up with</div><br/></div></div><div id="38090742" class="c"><input type="checkbox" id="c-38090742" checked=""/><div class="controls bullet"><span class="by">fthd</span><span>|</span><a href="#38090332">parent</a><span>|</span><a href="#38092539">prev</a><span>|</span><a href="#38092994">next</a><span>|</span><label class="collapse" for="c-38090742">[-]</label><label class="expand" for="c-38090742">[2 more]</label></div><br/><div class="children"><div class="content">mind providing some of the prompts you use to question them?</div><br/><div id="38091508" class="c"><input type="checkbox" id="c-38091508" checked=""/><div class="controls bullet"><span class="by">zoogeny</span><span>|</span><a href="#38090332">root</a><span>|</span><a href="#38090742">parent</a><span>|</span><a href="#38092994">next</a><span>|</span><label class="collapse" for="c-38091508">[-]</label><label class="expand" for="c-38091508">[1 more]</label></div><br/><div class="children"><div class="content">Here are the conversation logs:<p><a href="https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;867ff0c4-d4cf-4af9-a785-31a599059d4a" rel="nofollow noreferrer">https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;867ff0c4-d4cf-4af9-a785-31a599...</a><p><a href="https:&#x2F;&#x2F;www.phind.com&#x2F;search?cache=ej8pn1dfjjwfr1tgc6ybwhlg">https:&#x2F;&#x2F;www.phind.com&#x2F;search?cache=ej8pn1dfjjwfr1tgc6ybwhlg</a><p>NOTE: there are a few more question&#x2F;answer blocks in the phind conversation since I was testing out the follow up question feature.</div><br/></div></div></div></div><div id="38092994" class="c"><input type="checkbox" id="c-38092994" checked=""/><div class="controls bullet"><span class="by">fartasanelk</span><span>|</span><a href="#38090332">parent</a><span>|</span><a href="#38090742">prev</a><span>|</span><a href="#38089763">next</a><span>|</span><label class="collapse" for="c-38092994">[-]</label><label class="expand" for="c-38092994">[1 more]</label></div><br/><div class="children"><div class="content">would you be able to share your prompt(s)?<p>Edit: they are already posted as comment in this thread.</div><br/></div></div></div></div><div id="38089763" class="c"><input type="checkbox" id="c-38089763" checked=""/><div class="controls bullet"><span class="by">cccybernetic</span><span>|</span><a href="#38090332">prev</a><span>|</span><a href="#38089222">next</a><span>|</span><label class="collapse" for="c-38089763">[-]</label><label class="expand" for="c-38089763">[21 more]</label></div><br/><div class="children"><div class="content">I&#x27;m glad there&#x27;s growing competition, but GPT4 still outperforms. Here&#x27;s a recent question I asked:<p><i>Could you please provide me with a query for my PostgreSQL table? It consists of three columns: &#x27;id,&#x27; &#x27;teaser,&#x27; and &#x27;full_text,&#x27; all of which are of type &#x27;text.&#x27; Most &#x27;full_text&#x27; entries are articles around 1000 words in length. I&#x27;d like to update the &#x27;teaser&#x27; column for each row with the first approximately 200 words from the &#x27;full_text&#x27; column. How can I achieve this?</i><p>Phind:<p><pre><code>  CREATE OR REPLACE FUNCTION get_first_n_words(text, integer)
  RETURNS text AS $$
  DECLARE
     word_count integer := 0;
     word_start integer := 1;
     word_end integer;
  BEGIN
     WHILE word_count &lt; \$2 AND word_start &lt;= length(\$1) LOOP
         word_end := position(&#x27; &#x27; in \$1 from word_start) + 1;
         IF word_end = 0 THEN
             word_end := length(\$1) + 1;
         END IF;
         word_count := word_count + 1;
         word_start := word_end;
     END LOOP;
     RETURN substring(\$1 from 1 for word_start - 1);
  END;
  $$ LANGUAGE plpgsql;
</code></pre>
GPT4:<p><pre><code>  UPDATE your_table_name
  SET teaser = (
      SELECT STRING_AGG(word, &#x27; &#x27;) 
      FROM (
          SELECT SPLIT_PART(full_text, &#x27; &#x27;, i) AS word
          FROM generate_series(1, 200) AS i
      ) AS words
      WHERE word &lt;&gt; &#x27;&#x27;
  )
  WHERE full_text IS NOT NULL;</code></pre></div><br/><div id="38089888" class="c"><input type="checkbox" id="c-38089888" checked=""/><div class="controls bullet"><span class="by">rushingcreek</span><span>|</span><a href="#38089763">parent</a><span>|</span><a href="#38089834">next</a><span>|</span><label class="collapse" for="c-38089888">[-]</label><label class="expand" for="c-38089888">[5 more]</label></div><br/><div class="children"><div class="content">Running &quot;Ignore Web Context&quot; enabled can improve performance for design tasks like this. I just got a more plausible answer: <a href="https:&#x2F;&#x2F;www.phind.com&#x2F;search?cache=f0fkv5mxscwvagxgkuwnwgtl">https:&#x2F;&#x2F;www.phind.com&#x2F;search?cache=f0fkv5mxscwvagxgkuwnwgtl</a>. Consistency is something we&#x27;re working on.</div><br/><div id="38089993" class="c"><input type="checkbox" id="c-38089993" checked=""/><div class="controls bullet"><span class="by">cccybernetic</span><span>|</span><a href="#38089763">root</a><span>|</span><a href="#38089888">parent</a><span>|</span><a href="#38090953">next</a><span>|</span><label class="collapse" for="c-38089993">[-]</label><label class="expand" for="c-38089993">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for sharing, you&#x27;re right - that does improve performance!</div><br/></div></div><div id="38090953" class="c"><input type="checkbox" id="c-38090953" checked=""/><div class="controls bullet"><span class="by">ta8645</span><span>|</span><a href="#38089763">root</a><span>|</span><a href="#38089888">parent</a><span>|</span><a href="#38089993">prev</a><span>|</span><a href="#38089834">next</a><span>|</span><label class="collapse" for="c-38090953">[-]</label><label class="expand" for="c-38090953">[3 more]</label></div><br/><div class="children"><div class="content">How do you enable &quot;Ignore Web Context&quot;? I don&#x27;t see that option anywhere on the page you linked, am I just being blind?</div><br/><div id="38090967" class="c"><input type="checkbox" id="c-38090967" checked=""/><div class="controls bullet"><span class="by">rushingcreek</span><span>|</span><a href="#38089763">root</a><span>|</span><a href="#38090953">parent</a><span>|</span><a href="#38089834">next</a><span>|</span><label class="collapse" for="c-38090967">[-]</label><label class="expand" for="c-38090967">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s in the model dropdown under the search bar.</div><br/><div id="38095944" class="c"><input type="checkbox" id="c-38095944" checked=""/><div class="controls bullet"><span class="by">raducu</span><span>|</span><a href="#38089763">root</a><span>|</span><a href="#38090967">parent</a><span>|</span><a href="#38089834">next</a><span>|</span><label class="collapse" for="c-38095944">[-]</label><label class="expand" for="c-38095944">[1 more]</label></div><br/><div class="children"><div class="content">You mean &quot;Ignore Search Results&quot; ?</div><br/></div></div></div></div></div></div></div></div><div id="38089834" class="c"><input type="checkbox" id="c-38089834" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#38089763">parent</a><span>|</span><a href="#38089888">prev</a><span>|</span><a href="#38092329">next</a><span>|</span><label class="collapse" for="c-38089834">[-]</label><label class="expand" for="c-38089834">[14 more]</label></div><br/><div class="children"><div class="content">One example is not enough for performance conclusions</div><br/><div id="38089974" class="c"><input type="checkbox" id="c-38089974" checked=""/><div class="controls bullet"><span class="by">cccybernetic</span><span>|</span><a href="#38089763">root</a><span>|</span><a href="#38089834">parent</a><span>|</span><a href="#38089874">next</a><span>|</span><label class="collapse" for="c-38089974">[-]</label><label class="expand" for="c-38089974">[1 more]</label></div><br/><div class="children"><div class="content">Obviously not. Perfectly reasonable to share anecdotes though.<p>Also, I ran a few different tests, and every GPT-4 response was superior, but I didn&#x27;t want to clutter my comment with queries and code.</div><br/></div></div><div id="38089874" class="c"><input type="checkbox" id="c-38089874" checked=""/><div class="controls bullet"><span class="by">Wytwwww</span><span>|</span><a href="#38089763">root</a><span>|</span><a href="#38089834">parent</a><span>|</span><a href="#38089974">prev</a><span>|</span><a href="#38091500">next</a><span>|</span><label class="collapse" for="c-38089874">[-]</label><label class="expand" for="c-38089874">[11 more]</label></div><br/><div class="children"><div class="content">There is a performance conclusion in the title though.</div><br/><div id="38089911" class="c"><input type="checkbox" id="c-38089911" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#38089763">root</a><span>|</span><a href="#38089874">parent</a><span>|</span><a href="#38091500">next</a><span>|</span><label class="collapse" for="c-38089911">[-]</label><label class="expand" for="c-38089911">[10 more]</label></div><br/><div class="children"><div class="content">That conclusion is based on benchmark with many examples in different tasks.</div><br/><div id="38095264" class="c"><input type="checkbox" id="c-38095264" checked=""/><div class="controls bullet"><span class="by">emptysongglass</span><span>|</span><a href="#38089763">root</a><span>|</span><a href="#38089911">parent</a><span>|</span><a href="#38091011">next</a><span>|</span><label class="collapse" for="c-38095264">[-]</label><label class="expand" for="c-38095264">[2 more]</label></div><br/><div class="children"><div class="content">That conclusion is based on their benchmarks. I&#x27;m not interested in those. I&#x27;m interested in community benchmarks, like those we&#x27;re seeing in the comments. Lo and behold, GPT-4 is still king. The claims of any company should be taken with exactly a pinch of salt.</div><br/><div id="38095278" class="c"><input type="checkbox" id="c-38095278" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#38089763">root</a><span>|</span><a href="#38095264">parent</a><span>|</span><a href="#38091011">next</a><span>|</span><label class="collapse" for="c-38095278">[-]</label><label class="expand" for="c-38095278">[1 more]</label></div><br/><div class="children"><div class="content">that benchmark(HumanEval) is some public benchmark built by others.</div><br/></div></div></div></div><div id="38091011" class="c"><input type="checkbox" id="c-38091011" checked=""/><div class="controls bullet"><span class="by">spmurrayzzz</span><span>|</span><a href="#38089763">root</a><span>|</span><a href="#38089911">parent</a><span>|</span><a href="#38095264">prev</a><span>|</span><a href="#38090626">next</a><span>|</span><label class="collapse" for="c-38091011">[-]</label><label class="expand" for="c-38091011">[5 more]</label></div><br/><div class="children"><div class="content">AFAIK they haven&#x27;t released the dataset they fine-tuned on, so we can&#x27;t be 100% there wasn&#x27;t benchmark contamination. Agree that we definitely need more than N=1 to challenge the performance claims, but I still think its valid to call it out given how much benchmarking-gaming we&#x27;ve seen in this space.</div><br/><div id="38091174" class="c"><input type="checkbox" id="c-38091174" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#38089763">root</a><span>|</span><a href="#38091011">parent</a><span>|</span><a href="#38090626">next</a><span>|</span><label class="collapse" for="c-38091174">[-]</label><label class="expand" for="c-38091174">[4 more]</label></div><br/><div class="children"><div class="content">I think you can bring contamination claim to every public benchmark results nowdays: models are trained on TBs of data crawled from internet, and there is no guarantee benchmark is not leaked in some way.</div><br/><div id="38092942" class="c"><input type="checkbox" id="c-38092942" checked=""/><div class="controls bullet"><span class="by">spmurrayzzz</span><span>|</span><a href="#38089763">root</a><span>|</span><a href="#38091174">parent</a><span>|</span><a href="#38090626">next</a><span>|</span><label class="collapse" for="c-38092942">[-]</label><label class="expand" for="c-38092942">[3 more]</label></div><br/><div class="children"><div class="content">With respect to the pretraining data, its true that we&#x27;re probably SOL there in terms of verification. But for fine-tuning, they could still publish the dataset and see if others can reproduce their results as well as audit for contamination.<p>If we&#x27;re comparing benchmark deltas between different fine-tuned variants that share the same base models, that seems like the bare minimum we should expect to come along with performance claims.</div><br/><div id="38092977" class="c"><input type="checkbox" id="c-38092977" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#38089763">root</a><span>|</span><a href="#38092942">parent</a><span>|</span><a href="#38090626">next</a><span>|</span><label class="collapse" for="c-38092977">[-]</label><label class="expand" for="c-38092977">[2 more]</label></div><br/><div class="children"><div class="content">I think both pretraining and finetuning datas are essential secret information for commercial models&#x2F;services.</div><br/><div id="38093101" class="c"><input type="checkbox" id="c-38093101" checked=""/><div class="controls bullet"><span class="by">spmurrayzzz</span><span>|</span><a href="#38089763">root</a><span>|</span><a href="#38092977">parent</a><span>|</span><a href="#38090626">next</a><span>|</span><label class="collapse" for="c-38093101">[-]</label><label class="expand" for="c-38093101">[1 more]</label></div><br/><div class="children"><div class="content">In the case of Phind though, they also publish their models on HF with similar bold performance claims without publishing the datasets: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;Phind&#x2F;Phind-CodeLlama-34B-v2" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;Phind&#x2F;Phind-CodeLlama-34B-v2</a><p>Even I am to grant that their subscription product has some secret sauce they want to keep close to the chest (ignoring for a moment their paid product is GPT-4 based), not doing the same for all the models they release to the open source community free of charge with a commercially-permissible license seems suspect.<p>I realize this sort of open source contribution is mostly for marketing purposes, but being critical of the performance claims I think is still valid nonetheless.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38090626" class="c"><input type="checkbox" id="c-38090626" checked=""/><div class="controls bullet"><span class="by">Wytwwww</span><span>|</span><a href="#38089763">root</a><span>|</span><a href="#38089911">parent</a><span>|</span><a href="#38091011">prev</a><span>|</span><a href="#38091500">next</a><span>|</span><label class="collapse" for="c-38090626">[-]</label><label class="expand" for="c-38090626">[2 more]</label></div><br/><div class="children"><div class="content">From what I understand it&#x27;s a single test suite? Of course I don&#x27;t really mind the clickbait title that much, it&#x27;s hard to attract attention otherwise.</div><br/><div id="38092932" class="c"><input type="checkbox" id="c-38092932" checked=""/><div class="controls bullet"><span class="by">riku_iki</span><span>|</span><a href="#38089763">root</a><span>|</span><a href="#38090626">parent</a><span>|</span><a href="#38091500">next</a><span>|</span><label class="collapse" for="c-38092932">[-]</label><label class="expand" for="c-38092932">[1 more]</label></div><br/><div class="children"><div class="content">I think it is valid criticism that that HumanEval benchmark is not completely representative, they also say it in the post.</div><br/></div></div></div></div></div></div></div></div><div id="38091500" class="c"><input type="checkbox" id="c-38091500" checked=""/><div class="controls bullet"><span class="by">amelius</span><span>|</span><a href="#38089763">root</a><span>|</span><a href="#38089834">parent</a><span>|</span><a href="#38089874">prev</a><span>|</span><a href="#38092329">next</a><span>|</span><label class="collapse" for="c-38091500">[-]</label><label class="expand" for="c-38091500">[1 more]</label></div><br/><div class="children"><div class="content">Depends on the claims made.</div><br/></div></div></div></div><div id="38092329" class="c"><input type="checkbox" id="c-38092329" checked=""/><div class="controls bullet"><span class="by">nofunsir</span><span>|</span><a href="#38089763">parent</a><span>|</span><a href="#38089834">prev</a><span>|</span><a href="#38089222">next</a><span>|</span><label class="collapse" for="c-38092329">[-]</label><label class="expand" for="c-38092329">[1 more]</label></div><br/><div class="children"><div class="content">I really dislike article teasers and &quot;read more&quot; buttons. Now I know it&#x27;s intentional clipping of the corresponding articles.</div><br/></div></div></div></div><div id="38089222" class="c"><input type="checkbox" id="c-38089222" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#38089763">prev</a><span>|</span><a href="#38095859">next</a><span>|</span><label class="collapse" for="c-38089222">[-]</label><label class="expand" for="c-38089222">[9 more]</label></div><br/><div class="children"><div class="content">&gt; We can achieve up to 100 tokens per second single-stream while GPT-4 runs around 20 tokens per second at best.<p>Is that with batching? If so, thats quite impressive.<p>&gt; certain challenging questions where it is capable of getting the right answer, the Phind Model might take more generations to get to the right answer than GPT-4.<p>Some of this is sampler tuning. Y&#x27;all should look at grammar based sampling (<a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;1773">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;1773</a>) if you aren&#x27;t using it already, as well as some of the &quot;dynamic&quot; sampling like mirostat and dynatemp: <a href="https:&#x2F;&#x2F;github.com&#x2F;LostRuins&#x2F;koboldcpp&#x2F;pull&#x2F;464">https:&#x2F;&#x2F;github.com&#x2F;LostRuins&#x2F;koboldcpp&#x2F;pull&#x2F;464</a><p>I think these should work with nvidia&#x27;s implementation if you just swap the sampling out with the HF version.<p>BTW, all this is a great advantage of pulling away from OpenAI. You can dig in and implement experimental features that you just can&#x27;t necessarily do through their API.</div><br/><div id="38089451" class="c"><input type="checkbox" id="c-38089451" checked=""/><div class="controls bullet"><span class="by">rushingcreek</span><span>|</span><a href="#38089222">parent</a><span>|</span><a href="#38090353">next</a><span>|</span><label class="collapse" for="c-38089451">[-]</label><label class="expand" for="c-38089451">[1 more]</label></div><br/><div class="children"><div class="content">We leverage Flash Decoding (<a href="https:&#x2F;&#x2F;crfm.stanford.edu&#x2F;2023&#x2F;10&#x2F;12&#x2F;flashdecoding.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;crfm.stanford.edu&#x2F;2023&#x2F;10&#x2F;12&#x2F;flashdecoding.html</a>) in TensorRT-LLM to achieve 100 tokens per second on H100s.</div><br/></div></div><div id="38090353" class="c"><input type="checkbox" id="c-38090353" checked=""/><div class="controls bullet"><span class="by">claytonjy</span><span>|</span><a href="#38089222">parent</a><span>|</span><a href="#38089451">prev</a><span>|</span><a href="#38095859">next</a><span>|</span><label class="collapse" for="c-38090353">[-]</label><label class="expand" for="c-38090353">[7 more]</label></div><br/><div class="children"><div class="content">is that impressive? I was thinking 100 tok&#x2F;s on an H100 is really slow considering LMDeploy claims 2000+ on an A100 and a large batch size.</div><br/><div id="38090365" class="c"><input type="checkbox" id="c-38090365" checked=""/><div class="controls bullet"><span class="by">rushingcreek</span><span>|</span><a href="#38089222">root</a><span>|</span><a href="#38090353">parent</a><span>|</span><a href="#38091052">next</a><span>|</span><label class="collapse" for="c-38090365">[-]</label><label class="expand" for="c-38090365">[5 more]</label></div><br/><div class="children"><div class="content">We get 100 tokens a second with batch size 1. Those 2000+ figures are for large batches.</div><br/><div id="38090500" class="c"><input type="checkbox" id="c-38090500" checked=""/><div class="controls bullet"><span class="by">claytonjy</span><span>|</span><a href="#38089222">root</a><span>|</span><a href="#38090365">parent</a><span>|</span><a href="#38091052">next</a><span>|</span><label class="collapse" for="c-38090500">[-]</label><label class="expand" for="c-38090500">[4 more]</label></div><br/><div class="children"><div class="content">Ah, that&#x27;s fair, and faster than any of the LMDeploy stats for batch size 1; nice work!<p>Using an H100 for inference, especially without batching, sounds awfully expensive. Is cost much of a concern for you right now?</div><br/><div id="38091076" class="c"><input type="checkbox" id="c-38091076" checked=""/><div class="controls bullet"><span class="by">lyjackal</span><span>|</span><a href="#38089222">root</a><span>|</span><a href="#38090500">parent</a><span>|</span><a href="#38091052">next</a><span>|</span><label class="collapse" for="c-38091076">[-]</label><label class="expand" for="c-38091076">[3 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think they&#x27;re saying they&#x27;re doing batch size of 1, just giving performance expectations of user facing performance</div><br/><div id="38091129" class="c"><input type="checkbox" id="c-38091129" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#38089222">root</a><span>|</span><a href="#38091076">parent</a><span>|</span><a href="#38091181">next</a><span>|</span><label class="collapse" for="c-38091129">[-]</label><label class="expand" for="c-38091129">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, and this is basically what I was asking.<p>100 tokens&#x2F;s on the user&#x27;s end, on a host that <i>is</i> batching requests, is very impressive.</div><br/></div></div><div id="38091181" class="c"><input type="checkbox" id="c-38091181" checked=""/><div class="controls bullet"><span class="by">claytonjy</span><span>|</span><a href="#38089222">root</a><span>|</span><a href="#38091076">parent</a><span>|</span><a href="#38091129">prev</a><span>|</span><a href="#38091052">next</a><span>|</span><label class="collapse" for="c-38091181">[-]</label><label class="expand" for="c-38091181">[1 more]</label></div><br/><div class="children"><div class="content">I think they _are_ saying batch size 1, given that rushingcreek is OP.</div><br/></div></div></div></div></div></div></div></div><div id="38091052" class="c"><input type="checkbox" id="c-38091052" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#38089222">root</a><span>|</span><a href="#38090353">parent</a><span>|</span><a href="#38090365">prev</a><span>|</span><a href="#38095859">next</a><span>|</span><label class="collapse" for="c-38091052">[-]</label><label class="expand" for="c-38091052">[1 more]</label></div><br/><div class="children"><div class="content">Without batching, I was actually thinking that&#x27;s kind of modest.<p>ExllamaV2 will get 48 tokens&#x2F;s on a 4090, which is much slower&#x2F;cheaper than an H100:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;turboderp&#x2F;exllamav2#performance">https:&#x2F;&#x2F;github.com&#x2F;turboderp&#x2F;exllamav2#performance</a><p>I didn&#x27;t test codellama, but the 3090 TI figures for other sizes are in the ballpark of my generation speed on a 3090.<p>100 tokens&#x2F;s batched throughput (for each individual user) is much harder.</div><br/></div></div></div></div></div></div><div id="38095859" class="c"><input type="checkbox" id="c-38095859" checked=""/><div class="controls bullet"><span class="by">smusamashah</span><span>|</span><a href="#38089222">prev</a><span>|</span><a href="#38091332">next</a><span>|</span><label class="collapse" for="c-38095859">[-]</label><label class="expand" for="c-38095859">[1 more]</label></div><br/><div class="children"><div class="content">Hi, is there any plant to improve the UI? About 1&#x2F;3rd of vertical space is used by phind logo and the search box below it.
I sincerely believe it needs some professional&#x2F;business touch on its UI.</div><br/></div></div><div id="38091332" class="c"><input type="checkbox" id="c-38091332" checked=""/><div class="controls bullet"><span class="by">drcode</span><span>|</span><a href="#38095859">prev</a><span>|</span><a href="#38095802">next</a><span>|</span><label class="collapse" for="c-38091332">[-]</label><label class="expand" for="c-38091332">[10 more]</label></div><br/><div class="children"><div class="content">I am a heavy user of GPT4, and Phind was surprisingly able to match GPT4 on several initial programming tasks I gave it. Given the large context window of Phind, it will likely be able to outperform GPT4 for some tasks.<p>That is quite an accomplishment, I am impressed</div><br/><div id="38091765" class="c"><input type="checkbox" id="c-38091765" checked=""/><div class="controls bullet"><span class="by">iandanforth</span><span>|</span><a href="#38091332">parent</a><span>|</span><a href="#38091355">next</a><span>|</span><label class="collapse" for="c-38091765">[-]</label><label class="expand" for="c-38091765">[8 more]</label></div><br/><div class="children"><div class="content">FWIW The default context window of GPT-4 via ChatGPT is about to change to 32k.</div><br/><div id="38094495" class="c"><input type="checkbox" id="c-38094495" checked=""/><div class="controls bullet"><span class="by">jeswin</span><span>|</span><a href="#38091332">root</a><span>|</span><a href="#38091765">parent</a><span>|</span><a href="#38092417">next</a><span>|</span><label class="collapse" for="c-38094495">[-]</label><label class="expand" for="c-38094495">[1 more]</label></div><br/><div class="children"><div class="content">Given the number of times it just fails with large prompts on 32k contexts, I&#x27;m not sure if they&#x27;ready for this. In my experience, if you&#x27;re consuming 20k+ tokens failure rate is more than 50%.</div><br/></div></div><div id="38092417" class="c"><input type="checkbox" id="c-38092417" checked=""/><div class="controls bullet"><span class="by">drcode</span><span>|</span><a href="#38091332">root</a><span>|</span><a href="#38091765">parent</a><span>|</span><a href="#38094495">prev</a><span>|</span><a href="#38092675">next</a><span>|</span><label class="collapse" for="c-38092417">[-]</label><label class="expand" for="c-38092417">[5 more]</label></div><br/><div class="children"><div class="content">that would put them significantly ahead again, for my use cases</div><br/><div id="38092438" class="c"><input type="checkbox" id="c-38092438" checked=""/><div class="controls bullet"><span class="by">rushingcreek</span><span>|</span><a href="#38091332">root</a><span>|</span><a href="#38092417">parent</a><span>|</span><a href="#38092675">next</a><span>|</span><label class="collapse" for="c-38092438">[-]</label><label class="expand" for="c-38092438">[4 more]</label></div><br/><div class="children"><div class="content">We will eventually increase the Phind Model to 100K tokens -- the RoPE embeddings in Code Llama were designed for this.</div><br/><div id="38094539" class="c"><input type="checkbox" id="c-38094539" checked=""/><div class="controls bullet"><span class="by">antupis</span><span>|</span><a href="#38091332">root</a><span>|</span><a href="#38092438">parent</a><span>|</span><a href="#38092534">next</a><span>|</span><label class="collapse" for="c-38094539">[-]</label><label class="expand" for="c-38094539">[1 more]</label></div><br/><div class="children"><div class="content">100k tokens and good ide support would be great. Copy pasting back and forth with browser and IDE is kinda annoying and you always miss some context. I think model is now good enough but what is kinda missing is good developer experience eg what to load in that context window and how model integrates to IDE. But this is kinda missing with copilot and chatgpt4 as well.</div><br/></div></div><div id="38092534" class="c"><input type="checkbox" id="c-38092534" checked=""/><div class="controls bullet"><span class="by">arugulum</span><span>|</span><a href="#38091332">root</a><span>|</span><a href="#38092438">parent</a><span>|</span><a href="#38094539">prev</a><span>|</span><a href="#38092549">next</a><span>|</span><label class="collapse" for="c-38092534">[-]</label><label class="expand" for="c-38092534">[1 more]</label></div><br/><div class="children"><div class="content">&gt; the RoPE embeddings in Code Llama were designed for this.<p>The RoPE embeddings were not &quot;designed&quot; for that. The original RoPE was not designed with length extrapolation in mind. Subsequent tweaks to extrapolate RoPE (e.g. position interpolation) are post-hoc tweaks (with optional tuning) to an entirely vanilla RoPE implementation.</div><br/></div></div><div id="38092549" class="c"><input type="checkbox" id="c-38092549" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#38091332">root</a><span>|</span><a href="#38092438">parent</a><span>|</span><a href="#38092534">prev</a><span>|</span><a href="#38092675">next</a><span>|</span><label class="collapse" for="c-38092549">[-]</label><label class="expand" for="c-38092549">[1 more]</label></div><br/><div class="children"><div class="content">Is it “100k” or really 100k there are so many ways to do context, I remember seeing 100k before but it was doing some cheap trick to get it</div><br/></div></div></div></div></div></div><div id="38092675" class="c"><input type="checkbox" id="c-38092675" checked=""/><div class="controls bullet"><span class="by">ComplexSystems</span><span>|</span><a href="#38091332">root</a><span>|</span><a href="#38091765">parent</a><span>|</span><a href="#38092417">prev</a><span>|</span><a href="#38091355">next</a><span>|</span><label class="collapse" for="c-38092675">[-]</label><label class="expand" for="c-38092675">[1 more]</label></div><br/><div class="children"><div class="content">This would be great if true. Any source for this?</div><br/></div></div></div></div></div></div><div id="38095802" class="c"><input type="checkbox" id="c-38095802" checked=""/><div class="controls bullet"><span class="by">hasoleju</span><span>|</span><a href="#38091332">prev</a><span>|</span><a href="#38089104">next</a><span>|</span><label class="collapse" for="c-38095802">[-]</label><label class="expand" for="c-38095802">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t use LLMs in my workflow frequently. When I do I have a hard time making sense out of the very specific and long answers to my questions. Especially if I don&#x27;t know the answer it is hard to figure out if the long answer of the model indicates the right direction or misses my point completely.<p>Maybe I&#x27;m not knowledgeable enough. But asking questions I already know the answer of has no real life use case other than testing the model. Which of course might be a valid use case for some.<p>Having a way to let the user specify his own level of knowledge might help to receive answers that are better tailored to the user asking the question.</div><br/><div id="38095890" class="c"><input type="checkbox" id="c-38095890" checked=""/><div class="controls bullet"><span class="by">lysecret</span><span>|</span><a href="#38095802">parent</a><span>|</span><a href="#38089104">next</a><span>|</span><label class="collapse" for="c-38095890">[-]</label><label class="expand" for="c-38095890">[1 more]</label></div><br/><div class="children"><div class="content">Have you tried custom instructions ? I use this:<p>My dad always used to say: Everything that can be said, can be said simply. I prefer top-down structured, short and thoughtful responses.</div><br/></div></div></div></div><div id="38089104" class="c"><input type="checkbox" id="c-38089104" checked=""/><div class="controls bullet"><span class="by">slowhadoken</span><span>|</span><a href="#38095802">prev</a><span>|</span><a href="#38089838">next</a><span>|</span><label class="collapse" for="c-38089104">[-]</label><label class="expand" for="c-38089104">[17 more]</label></div><br/><div class="children"><div class="content">I love that Phind cites what it scrapes. This should be the obligation of all LLM. I always suggest people use it over ChatGPT.</div><br/><div id="38089397" class="c"><input type="checkbox" id="c-38089397" checked=""/><div class="controls bullet"><span class="by">make3</span><span>|</span><a href="#38089104">parent</a><span>|</span><a href="#38089315">next</a><span>|</span><label class="collapse" for="c-38089397">[-]</label><label class="expand" for="c-38089397">[4 more]</label></div><br/><div class="children"><div class="content">What they&#x27;re citing isn&#x27;t what the LLM &quot;scraped&quot;, it&#x27;s what the retrieval model fed to the LLM. You&#x27;re not guaranteed that it&#x27;s what it actually used to give you the output, and it&#x27;s also definitely not all the text that it used to get appropriate knowledge to generate the answer, as this is split over whatever millions of examples for the language and for human language in a non human-understandable way</div><br/><div id="38090996" class="c"><input type="checkbox" id="c-38090996" checked=""/><div class="controls bullet"><span class="by">pbhjpbhj</span><span>|</span><a href="#38089104">root</a><span>|</span><a href="#38089397">parent</a><span>|</span><a href="#38093262">next</a><span>|</span><label class="collapse" for="c-38090996">[-]</label><label class="expand" for="c-38090996">[1 more]</label></div><br/><div class="children"><div class="content">A couple of times I&#x27;ve had the reference not include the detail being mentioned in the foregoing paragraph; the citations are still highly relevant, but it wasn&#x27;t quite what I expected.</div><br/></div></div><div id="38093262" class="c"><input type="checkbox" id="c-38093262" checked=""/><div class="controls bullet"><span class="by">slowhadoken</span><span>|</span><a href="#38089104">root</a><span>|</span><a href="#38089397">parent</a><span>|</span><a href="#38090996">prev</a><span>|</span><a href="#38089315">next</a><span>|</span><label class="collapse" for="c-38093262">[-]</label><label class="expand" for="c-38093262">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve heard this coldtake before but OpenAI&#x27;s source code isn&#x27;t open to academic scrutiny. So I don&#x27;t understand why some people are so confident about how it works. It&#x27;s certainly not magic and Phind seems to be capable of it citation.</div><br/></div></div></div></div><div id="38089315" class="c"><input type="checkbox" id="c-38089315" checked=""/><div class="controls bullet"><span class="by">Racing0461</span><span>|</span><a href="#38089104">parent</a><span>|</span><a href="#38089397">prev</a><span>|</span><a href="#38089838">next</a><span>|</span><label class="collapse" for="c-38089315">[-]</label><label class="expand" for="c-38089315">[12 more]</label></div><br/><div class="children"><div class="content">As a user, i perfer getting the right response compared to the thing spitting out a link. (not saying phind is bad). Lets focus on getting llm right before nerfing it in its baby stages.</div><br/><div id="38089413" class="c"><input type="checkbox" id="c-38089413" checked=""/><div class="controls bullet"><span class="by">ryanklee</span><span>|</span><a href="#38089104">root</a><span>|</span><a href="#38089315">parent</a><span>|</span><a href="#38092785">next</a><span>|</span><label class="collapse" for="c-38089413">[-]</label><label class="expand" for="c-38089413">[8 more]</label></div><br/><div class="children"><div class="content">Who said anything about nerfing? Citation is just additive, no?</div><br/><div id="38089445" class="c"><input type="checkbox" id="c-38089445" checked=""/><div class="controls bullet"><span class="by">joshspankit</span><span>|</span><a href="#38089104">root</a><span>|</span><a href="#38089413">parent</a><span>|</span><a href="#38091219">next</a><span>|</span><label class="collapse" for="c-38089445">[-]</label><label class="expand" for="c-38089445">[5 more]</label></div><br/><div class="children"><div class="content">In fact, I’d argue that citation makes LLM better. Kind of a “think carefully” indicator. When LLMs are able to verify those citations independently it’s going to level up again by skyrocketing the objective truthiness.</div><br/><div id="38093127" class="c"><input type="checkbox" id="c-38093127" checked=""/><div class="controls bullet"><span class="by">slowhadoken</span><span>|</span><a href="#38089104">root</a><span>|</span><a href="#38089445">parent</a><span>|</span><a href="#38089999">next</a><span>|</span><label class="collapse" for="c-38093127">[-]</label><label class="expand" for="c-38093127">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, I prefer the context provided by the original creator. If I&#x27;m writing code and I need to reference someone else&#x27;s work I put their name in my comments. I was digging through Box2D for polygon vs ray intersections and in the comments of the source code Erin Catto cites Collision Detection in Interactive 3D Environments by Gino van den Bergen. It makes me respect him even more.</div><br/></div></div><div id="38089999" class="c"><input type="checkbox" id="c-38089999" checked=""/><div class="controls bullet"><span class="by">lsaferite</span><span>|</span><a href="#38089104">root</a><span>|</span><a href="#38089445">parent</a><span>|</span><a href="#38093127">prev</a><span>|</span><a href="#38091219">next</a><span>|</span><label class="collapse" for="c-38089999">[-]</label><label class="expand" for="c-38089999">[3 more]</label></div><br/><div class="children"><div class="content">Interestingly, I&#x27;d say that _not_ being able to give citations helps protect the LLM from copyright issues. That being said, I&#x27;m much prefer if the LLM could provide citations for every piece of information it was trained on and uses to provide an answer.</div><br/><div id="38091032" class="c"><input type="checkbox" id="c-38091032" checked=""/><div class="controls bullet"><span class="by">pbhjpbhj</span><span>|</span><a href="#38089104">root</a><span>|</span><a href="#38089999">parent</a><span>|</span><a href="#38091219">next</a><span>|</span><label class="collapse" for="c-38091032">[-]</label><label class="expand" for="c-38091032">[2 more]</label></div><br/><div class="children"><div class="content">Citations are essential for me as I&#x27;m using Phind for work and can&#x27;t rely on &quot;trust me bro&quot;. It needs to confirm to my expectations or be confirmed in a couple of the citations that have trustworthy sources (eg are from known domains, well-cited journals, etc.).</div><br/><div id="38093156" class="c"><input type="checkbox" id="c-38093156" checked=""/><div class="controls bullet"><span class="by">slowhadoken</span><span>|</span><a href="#38089104">root</a><span>|</span><a href="#38091032">parent</a><span>|</span><a href="#38091219">next</a><span>|</span><label class="collapse" for="c-38093156">[-]</label><label class="expand" for="c-38093156">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve found great sites and devs using Phind.</div><br/></div></div></div></div></div></div></div></div><div id="38091219" class="c"><input type="checkbox" id="c-38091219" checked=""/><div class="controls bullet"><span class="by">__jonas</span><span>|</span><a href="#38089104">root</a><span>|</span><a href="#38089413">parent</a><span>|</span><a href="#38089445">prev</a><span>|</span><a href="#38090018">next</a><span>|</span><label class="collapse" for="c-38091219">[-]</label><label class="expand" for="c-38091219">[1 more]</label></div><br/><div class="children"><div class="content">I find it often makes the responses worse when it&#x27;s being pre-fed these search results, it was the case when I tried gpt-4 with web browsing enabled, and seems to be the case with this, since even the person from the Phind team in this thread pointed out that turning this feature off improves performance for some tasks:<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38089888">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38089888</a><p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38090442">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38090442</a></div><br/></div></div><div id="38090018" class="c"><input type="checkbox" id="c-38090018" checked=""/><div class="controls bullet"><span class="by">Racing0461</span><span>|</span><a href="#38089104">root</a><span>|</span><a href="#38089413">parent</a><span>|</span><a href="#38091219">prev</a><span>|</span><a href="#38092785">next</a><span>|</span><label class="collapse" for="c-38090018">[-]</label><label class="expand" for="c-38090018">[1 more]</label></div><br/><div class="children"><div class="content">Nerf is the wrong word, more like regulatory capture. If all llm had to quote their sources at this point, along with all the other for the human changes we want to do, only the big players would be able to do them effectively making it hard to enter and compete. The current big players want launching a new llm product to be more like opening a new bank than opening a lemonade stand based on the ai executive order released yesterday.</div><br/></div></div></div></div><div id="38092785" class="c"><input type="checkbox" id="c-38092785" checked=""/><div class="controls bullet"><span class="by">donmcronald</span><span>|</span><a href="#38089104">root</a><span>|</span><a href="#38089315">parent</a><span>|</span><a href="#38089413">prev</a><span>|</span><a href="#38093091">next</a><span>|</span><label class="collapse" for="c-38092785">[-]</label><label class="expand" for="c-38092785">[2 more]</label></div><br/><div class="children"><div class="content">Give me the citations every day of the week.  The source of information matters.  For example, I don&#x27;t rely on any ZFS info or opinions I find online if I can&#x27;t verify it came from a contributor or highly reputable person that has a lot of experience with ZFS.<p>If you want to show the warts of all these LLMs, ask it about ZFS if you know enough to spot the commonly parroted misinformation that plagues the internet.<p>IMHO, these systems look super useful if they&#x27;re citing sources and they&#x27;re worthless without.</div><br/><div id="38093190" class="c"><input type="checkbox" id="c-38093190" checked=""/><div class="controls bullet"><span class="by">slowhadoken</span><span>|</span><a href="#38089104">root</a><span>|</span><a href="#38092785">parent</a><span>|</span><a href="#38093091">next</a><span>|</span><label class="collapse" for="c-38093190">[-]</label><label class="expand" for="c-38093190">[1 more]</label></div><br/><div class="children"><div class="content">Transparency is paramount. If OpenAI doesn&#x27;t want to make it&#x27;s proprietary software open to academic scrutiny I completely understand. However, if their app is going to play an educational role then sources and citation are mandatory in academic content.</div><br/></div></div></div></div><div id="38093091" class="c"><input type="checkbox" id="c-38093091" checked=""/><div class="controls bullet"><span class="by">slowhadoken</span><span>|</span><a href="#38089104">root</a><span>|</span><a href="#38089315">parent</a><span>|</span><a href="#38092785">prev</a><span>|</span><a href="#38089838">next</a><span>|</span><label class="collapse" for="c-38093091">[-]</label><label class="expand" for="c-38093091">[1 more]</label></div><br/><div class="children"><div class="content">why not both?</div><br/></div></div></div></div></div></div><div id="38089838" class="c"><input type="checkbox" id="c-38089838" checked=""/><div class="controls bullet"><span class="by">joaodias</span><span>|</span><a href="#38089104">prev</a><span>|</span><a href="#38089565">next</a><span>|</span><label class="collapse" for="c-38089838">[-]</label><label class="expand" for="c-38089838">[9 more]</label></div><br/><div class="children"><div class="content">Asked it to write a program that I&#x27;ve written before, to compare with gpt4.
Didn&#x27;t really get what I was asking for, gpt4 understood it perfectly, and is ready to continue prompting toward completion.<p><a href="https:&#x2F;&#x2F;www.phind.com&#x2F;agent?cache=cloeowfla000dl1084ermly3c">https:&#x2F;&#x2F;www.phind.com&#x2F;agent?cache=cloeowfla000dl1084ermly3c</a>
vs
<a href="https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;4147da33-3669-4657-88fa-3a9dfc894183" rel="nofollow noreferrer">https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;4147da33-3669-4657-88fa-3a9dfc...</a><p>Might not be representative of the whole thing, but it went on about random things I didn&#x27;t ask about, and just basic information I already knew</div><br/><div id="38089906" class="c"><input type="checkbox" id="c-38089906" checked=""/><div class="controls bullet"><span class="by">rushingcreek</span><span>|</span><a href="#38089838">parent</a><span>|</span><a href="#38089922">next</a><span>|</span><label class="collapse" for="c-38089906">[-]</label><label class="expand" for="c-38089906">[7 more]</label></div><br/><div class="children"><div class="content">The Pair Programmer mode currently either uses GPT-4 or GPT-3.5 (if you&#x27;ve run out). Please try again in the default search mode to use the Phind Model.<p>Using the Phind Model in the default search seems to work well: <a href="https:&#x2F;&#x2F;www.phind.com&#x2F;search?cache=ln6dpdtv5auwn4cq1ofg3gs9">https:&#x2F;&#x2F;www.phind.com&#x2F;search?cache=ln6dpdtv5auwn4cq1ofg3gs9</a></div><br/><div id="38093908" class="c"><input type="checkbox" id="c-38093908" checked=""/><div class="controls bullet"><span class="by">joaodias</span><span>|</span><a href="#38089838">root</a><span>|</span><a href="#38089906">parent</a><span>|</span><a href="#38091478">next</a><span>|</span><label class="collapse" for="c-38093908">[-]</label><label class="expand" for="c-38093908">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;www.phind.com&#x2F;search?cache=z5odlx0o9lspzpfm4sfpp131">https:&#x2F;&#x2F;www.phind.com&#x2F;search?cache=z5odlx0o9lspzpfm4sfpp131</a><p>Way way better, I&#x27;m stunned. Congratulations on this</div><br/></div></div><div id="38091478" class="c"><input type="checkbox" id="c-38091478" checked=""/><div class="controls bullet"><span class="by">Capricorn2481</span><span>|</span><a href="#38089838">root</a><span>|</span><a href="#38089906">parent</a><span>|</span><a href="#38093908">prev</a><span>|</span><a href="#38089922">next</a><span>|</span><label class="collapse" for="c-38091478">[-]</label><label class="expand" for="c-38091478">[5 more]</label></div><br/><div class="children"><div class="content">Even though the phind model is selected? Is there a technical reason Phind doesn&#x27;t do pair programming yet?</div><br/><div id="38091525" class="c"><input type="checkbox" id="c-38091525" checked=""/><div class="controls bullet"><span class="by">rushingcreek</span><span>|</span><a href="#38089838">root</a><span>|</span><a href="#38091478">parent</a><span>|</span><a href="#38089922">next</a><span>|</span><label class="collapse" for="c-38091525">[-]</label><label class="expand" for="c-38091525">[4 more]</label></div><br/><div class="children"><div class="content">It&#x27;s because we haven&#x27;t updated the Phind Model to support function calling yet but we&#x27;re working on it.</div><br/><div id="38091553" class="c"><input type="checkbox" id="c-38091553" checked=""/><div class="controls bullet"><span class="by">Capricorn2481</span><span>|</span><a href="#38089838">root</a><span>|</span><a href="#38091525">parent</a><span>|</span><a href="#38089922">next</a><span>|</span><label class="collapse" for="c-38091553">[-]</label><label class="expand" for="c-38091553">[3 more]</label></div><br/><div class="children"><div class="content">Can you share what your long term monetization model is? I&#x27;m noticing Phind is free to use right now.</div><br/><div id="38091745" class="c"><input type="checkbox" id="c-38091745" checked=""/><div class="controls bullet"><span class="by">rushingcreek</span><span>|</span><a href="#38089838">root</a><span>|</span><a href="#38091553">parent</a><span>|</span><a href="#38089922">next</a><span>|</span><label class="collapse" for="c-38091745">[-]</label><label class="expand" for="c-38091745">[2 more]</label></div><br/><div class="children"><div class="content">We have a Pro plan where you can get (virtually) unlimited GPT-4 and soon, an even faster Phind model. <a href="https:&#x2F;&#x2F;phind.com&#x2F;plans">https:&#x2F;&#x2F;phind.com&#x2F;plans</a></div><br/><div id="38095049" class="c"><input type="checkbox" id="c-38095049" checked=""/><div class="controls bullet"><span class="by">Capricorn2481</span><span>|</span><a href="#38089838">root</a><span>|</span><a href="#38091745">parent</a><span>|</span><a href="#38089922">next</a><span>|</span><label class="collapse" for="c-38095049">[-]</label><label class="expand" for="c-38095049">[1 more]</label></div><br/><div class="children"><div class="content">Is there something you&#x27;re doing with GPT-4 that would make me want to use it through you vs just using it myself?</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="38089922" class="c"><input type="checkbox" id="c-38089922" checked=""/><div class="controls bullet"><span class="by">TheGeminon</span><span>|</span><a href="#38089838">parent</a><span>|</span><a href="#38089906">prev</a><span>|</span><a href="#38089565">next</a><span>|</span><label class="collapse" for="c-38089922">[-]</label><label class="expand" for="c-38089922">[1 more]</label></div><br/><div class="children"><div class="content">The problem is that it’s doing a search of your relatively niche problem, and probably getting pretty poor results. The text from the search is then more highly weighted than the base model, but with relative junk, so it performs better without the additional (unhelpful) context.<p>You see this with Bing search on ChatGPT as well, and I’ve seen it in my own projects.</div><br/></div></div></div></div><div id="38089565" class="c"><input type="checkbox" id="c-38089565" checked=""/><div class="controls bullet"><span class="by">accrual</span><span>|</span><a href="#38089838">prev</a><span>|</span><a href="#38092166">next</a><span>|</span><label class="collapse" for="c-38089565">[-]</label><label class="expand" for="c-38089565">[15 more]</label></div><br/><div class="children"><div class="content">&gt; it supports up to 16k tokens<p>&gt; Llama 1 supports up to 2048 (2K) tokens, Llama 2 up to 4096 (4K), CodeLlama up to 16384 (16K). [0]<p>This is wild to me.<p>The token window is one of the limiting factors for having an AI that can actually remember you and past conversations. Having a large window is key for future AI applications that involve long running conversations (weeks, months, years). The tech is already very impressive, but imagine it as it becomes more like an <i>actual</i> pair programmer and remembers all the various things it&#x27;s learned and worked on with you in the past.<p>[0] <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;docs&#x2F;transformers&#x2F;main&#x2F;model_doc&#x2F;llama2" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;docs&#x2F;transformers&#x2F;main&#x2F;model_doc&#x2F;llam...</a></div><br/><div id="38090383" class="c"><input type="checkbox" id="c-38090383" checked=""/><div class="controls bullet"><span class="by">seydor</span><span>|</span><a href="#38089565">parent</a><span>|</span><a href="#38089663">next</a><span>|</span><label class="collapse" for="c-38090383">[-]</label><label class="expand" for="c-38090383">[2 more]</label></div><br/><div class="children"><div class="content">640k is enough for anyone</div><br/><div id="38093309" class="c"><input type="checkbox" id="c-38093309" checked=""/><div class="controls bullet"><span class="by">happycube</span><span>|</span><a href="#38089565">root</a><span>|</span><a href="#38090383">parent</a><span>|</span><a href="#38089663">next</a><span>|</span><label class="collapse" for="c-38093309">[-]</label><label class="expand" for="c-38093309">[1 more]</label></div><br/><div class="children"><div class="content">Extending that analogy, imagine what one could do with 128B tokens.<p>On cast off&#x2F;cheap workstation&#x2F;server hardware.</div><br/></div></div></div></div><div id="38089663" class="c"><input type="checkbox" id="c-38089663" checked=""/><div class="controls bullet"><span class="by">mycall</span><span>|</span><a href="#38089565">parent</a><span>|</span><a href="#38090383">prev</a><span>|</span><a href="#38089594">next</a><span>|</span><label class="collapse" for="c-38089663">[-]</label><label class="expand" for="c-38089663">[1 more]</label></div><br/><div class="children"><div class="content">Token window size is being virtualized with the like of MemGPT, so its effect will diminish.</div><br/></div></div><div id="38089594" class="c"><input type="checkbox" id="c-38089594" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#38089565">parent</a><span>|</span><a href="#38089663">prev</a><span>|</span><a href="#38092166">next</a><span>|</span><label class="collapse" for="c-38089594">[-]</label><label class="expand" for="c-38089594">[11 more]</label></div><br/><div class="children"><div class="content">Still waiting for the day that medium term memory (token average pooling like in sentence transformers) becomes used for this. It&#x27;s staring all of these companies in the face and apparently no one thinks to implement it.</div><br/><div id="38089807" class="c"><input type="checkbox" id="c-38089807" checked=""/><div class="controls bullet"><span class="by">heavyarms</span><span>|</span><a href="#38089565">root</a><span>|</span><a href="#38089594">parent</a><span>|</span><a href="#38089652">next</a><span>|</span><label class="collapse" for="c-38089807">[-]</label><label class="expand" for="c-38089807">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been thinking along the same lines. The token window IMO should be a conceptual inverted pyramid, where there most recent tokens are retained verbatim but previous iterations are compressed&#x2F;pooled more and more as the context grows. I&#x27;m sure there&#x27;s some effort&#x2F;research in this direction. It seems pretty obvious.</div><br/><div id="38090568" class="c"><input type="checkbox" id="c-38090568" checked=""/><div class="controls bullet"><span class="by">matsemann</span><span>|</span><a href="#38089565">root</a><span>|</span><a href="#38089807">parent</a><span>|</span><a href="#38089652">next</a><span>|</span><label class="collapse" for="c-38090568">[-]</label><label class="expand" for="c-38090568">[4 more]</label></div><br/><div class="children"><div class="content">But some of the earlier tokens are also the most important ones, right? Like the instructions and rules you want it to follow.</div><br/><div id="38095010" class="c"><input type="checkbox" id="c-38095010" checked=""/><div class="controls bullet"><span class="by">visarga</span><span>|</span><a href="#38089565">root</a><span>|</span><a href="#38090568">parent</a><span>|</span><a href="#38091117">next</a><span>|</span><label class="collapse" for="c-38095010">[-]</label><label class="expand" for="c-38095010">[1 more]</label></div><br/><div class="children"><div class="content">Phrase embeddings could bring a 32x reduction in sequence length because:<p>&gt; Text Embeddings Reveal (Almost) As Much As Text. ... We find that although a naïve model conditioned on the embedding performs poorly, a multi step method that iteratively corrects and re embeds text is able to recover 92% of 32-token text inputs exactly. We train our model to decode text embeddings from two state of the art embedding models, and also show that our model can recover important personal information (full names) from a dataset of clinical notes.<p><a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.06816" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.06816</a></div><br/></div></div><div id="38091117" class="c"><input type="checkbox" id="c-38091117" checked=""/><div class="controls bullet"><span class="by">a_wild_dandan</span><span>|</span><a href="#38089565">root</a><span>|</span><a href="#38090568">parent</a><span>|</span><a href="#38095010">prev</a><span>|</span><a href="#38089652">next</a><span>|</span><label class="collapse" for="c-38091117">[-]</label><label class="expand" for="c-38091117">[2 more]</label></div><br/><div class="children"><div class="content">They are. Moreover, the idea that AI companies are missing and&#x2F;or not implementing this “obvious” tactic is hilarious. Folks, these approaches have profound consequences for training and inference performance. Y’all aren’t pointing out some low hanging fruit here, lol</div><br/><div id="38092606" class="c"><input type="checkbox" id="c-38092606" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#38089565">root</a><span>|</span><a href="#38091117">parent</a><span>|</span><a href="#38089652">next</a><span>|</span><label class="collapse" for="c-38092606">[-]</label><label class="expand" for="c-38092606">[1 more]</label></div><br/><div class="children"><div class="content">Actually, yes I am pointing out low hanging fruit here. These approaches do not have &quot;profound consequences&quot; for inference or training performance. In fact, sentence transformer models run orders of magnitude more quickly. Performance penalties will be small.<p>Also, I actually have several top NLP conference publications, so I&#x27;m not some charlatan when I say these things. I&#x27;ve actually physically used and seen these techniques improve LLM recall. It really actually works.<p>Here&#x27;s more examples of low hanging fruit. The proof in that they work is in the implementations which I provide. You can run them, they work!: <a href="https:&#x2F;&#x2F;gist.github.com&#x2F;Hellisotherpeople&#x2F;45c619ee22aac6865ca4bb328eb58faf" rel="nofollow noreferrer">https:&#x2F;&#x2F;gist.github.com&#x2F;Hellisotherpeople&#x2F;45c619ee22aac6865c...</a><p>Check yourself before you try to check others.</div><br/></div></div></div></div></div></div></div></div><div id="38089652" class="c"><input type="checkbox" id="c-38089652" checked=""/><div class="controls bullet"><span class="by">brrrrrm</span><span>|</span><a href="#38089565">root</a><span>|</span><a href="#38089594">parent</a><span>|</span><a href="#38089807">prev</a><span>|</span><a href="#38092166">next</a><span>|</span><label class="collapse" for="c-38089652">[-]</label><label class="expand" for="c-38089652">[5 more]</label></div><br/><div class="children"><div class="content">Out of curiosity, why do you think the answer would be so simple and also completely untested?</div><br/><div id="38092665" class="c"><input type="checkbox" id="c-38092665" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#38089565">root</a><span>|</span><a href="#38089652">parent</a><span>|</span><a href="#38089707">next</a><span>|</span><label class="collapse" for="c-38092665">[-]</label><label class="expand" for="c-38092665">[1 more]</label></div><br/><div class="children"><div class="content">Too much money being thrown around on BS in the LLM space, hardly any of it is going to places where it matters. Ignorance on the part of investors.<p>For example, the researchers working hard on better text sampling techniques (i.e. <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2202.00666" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2202.00666</a>), or on better constraint techniques (i.e. like this <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2306.03081" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2306.03081</a>), or on actual negative prompting&#x2F;CFG in LLMs (i.e. like this <a href="https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;transformers&#x2F;issues&#x2F;24536">https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;transformers&#x2F;issues&#x2F;24536</a>) are doing far FAR more to advance the state of AI than dozens of VC backed LLM companies operating today. They are all laboring in relative obscurity.<p>HN, and the NLP community have some serious blindspots with knowing how to exploit their own technology. At least someone at Andreessen Horowitz got a clue and gave some funding to Oogabooga - still waiting for Automatic1111 to get any funding.</div><br/></div></div><div id="38089707" class="c"><input type="checkbox" id="c-38089707" checked=""/><div class="controls bullet"><span class="by">fullstackchris</span><span>|</span><a href="#38089565">root</a><span>|</span><a href="#38089652">parent</a><span>|</span><a href="#38092665">prev</a><span>|</span><a href="#38092166">next</a><span>|</span><label class="collapse" for="c-38089707">[-]</label><label class="expand" for="c-38089707">[3 more]</label></div><br/><div class="children"><div class="content">Another curiosity, what do we estimate (if it&#x27;s even possible) the context window of a human? Obviously an extremely broad question, and of course it must have some sort of decay factor... but... would be interesting to get a rule of thumb number in terms of token count. I can imagine its massive!</div><br/><div id="38090536" class="c"><input type="checkbox" id="c-38090536" checked=""/><div class="controls bullet"><span class="by">travisjungroth</span><span>|</span><a href="#38089565">root</a><span>|</span><a href="#38089707">parent</a><span>|</span><a href="#38090515">next</a><span>|</span><label class="collapse" for="c-38090536">[-]</label><label class="expand" for="c-38090536">[1 more]</label></div><br/><div class="children"><div class="content">Human memory, in my limited understanding, doesn’t have the bifurcation of weights and context that LLMs do. It’s all a bit blurrier than that.<p>Something interesting that I heard from people trying to memorize things better is that memory “storage space” limits for people are essentially irrelevant. We’re limited by our learning and forgetting speeds. There’s no evidence of brains getting “full”.<p>Think of it like a giant warehouse of plants, with one employee. He can accept shipments (learning). He can take care of plants (remembering). Too long without care and they die (forgetting). The warehouse is big enough that it is not a limiting factor in how many plants he can keep alive. If it was 10x bigger it wouldn’t make a bit of difference.</div><br/></div></div><div id="38090515" class="c"><input type="checkbox" id="c-38090515" checked=""/><div class="controls bullet"><span class="by">Filligree</span><span>|</span><a href="#38089565">root</a><span>|</span><a href="#38089707">parent</a><span>|</span><a href="#38090536">prev</a><span>|</span><a href="#38092166">next</a><span>|</span><label class="collapse" for="c-38090515">[-]</label><label class="expand" for="c-38090515">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think it&#x27;s massive. In fact, since it&#x27;s roughly equivalent to working memory, I suspect it&#x27;s on the order of 100 tokens at most.<p>It&#x27;s just that, unlike these AIs, we&#x27;re capable of online learning.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38092166" class="c"><input type="checkbox" id="c-38092166" checked=""/><div class="controls bullet"><span class="by">ojosilva</span><span>|</span><a href="#38089565">prev</a><span>|</span><a href="#38089281">next</a><span>|</span><label class="collapse" for="c-38092166">[-]</label><label class="expand" for="c-38092166">[8 more]</label></div><br/><div class="children"><div class="content">Awesome model from a quick run-through comparison, it&#x27;s comparable in results to GPT-4 with web search and references as a plus, but runs faster. Two small nitpicks:<p>- Dark mode is hard to read, the answer text font has too much weight and brightness which makes it hard to read long paragraphs of non-code text. Light mode is obviously too bright overall, but it&#x27;s already nighttime where I&#x27;m at so maybe tomorrow at noon I&#x27;ll have another opinion. I&#x27;d preferred gray (dark, ie OpenAI) and sepia (light, ie HN) as backgrounds when long lines of text are involved.<p>- Pricing page and ties to GPT-4: what does &quot;500+ best model uses per day (GPT-4)&quot; mean? What&#x27;s the &quot;GPT-4&quot; part for? I saw I can pick GPT4 as a model on the landing page, but I just don&#x27;t get the best model&#x2F;GPT-4 thing. Is Phind announcing it&#x27;s a competitor but also proxies GPT-4? Sorry, I&#x27;m not up-to-date on GPT-4 &quot;resellers&quot; and the story behind Phind, it&#x27;s just weird when it announces it &quot;beats GPT-4&quot; then the pricing is about GPT-4 usage.</div><br/><div id="38092178" class="c"><input type="checkbox" id="c-38092178" checked=""/><div class="controls bullet"><span class="by">rushingcreek</span><span>|</span><a href="#38092166">parent</a><span>|</span><a href="#38089281">next</a><span>|</span><label class="collapse" for="c-38092178">[-]</label><label class="expand" for="c-38092178">[7 more]</label></div><br/><div class="children"><div class="content">Thanks for the feedback. We also support GPT-4 as an answering model so users can pick and choose what&#x27;s best for their use case, but we recommend the Phind Model for the majority of users.</div><br/><div id="38092729" class="c"><input type="checkbox" id="c-38092729" checked=""/><div class="controls bullet"><span class="by">donmcronald</span><span>|</span><a href="#38092166">root</a><span>|</span><a href="#38092178">parent</a><span>|</span><a href="#38089281">next</a><span>|</span><label class="collapse" for="c-38092729">[-]</label><label class="expand" for="c-38092729">[6 more]</label></div><br/><div class="children"><div class="content">Why is there an 8x difference in price-per-search between Plus and Pro?<p>I always shy away from stuff like this because I view it as one of two things.  Either I&#x27;m getting ripped off if I pay for Plus, because 8x the cost to me means your margin is huge, or I&#x27;m getting subsidized by you with the Pro version which means I can&#x27;t rely on it lasting long term.<p>I also dislike daily limits for search.  My search usage isn&#x27;t uniform day-to-day.  I might go most of the month without searching for anything and then do a ton of searching over 2-3 days when I&#x27;m trying to learn something.  So I&#x27;ll be idle most of the month and then not have enough searches on the days I actually want to use it.<p>I prefer the model used by a lot of pre-paid services.  Let me deposit a chunk of money (ex: $20-50 minimum) and charge me per search until my money is gone.  That way I&#x27;m not &quot;losing out&quot; if I don&#x27;t use it every day and I can &quot;burst&quot; as high as I want when I&#x27;m trying to learn something.<p>If the pricing is based on a certain amount of loss (on my side) from the use-it-or-lose it model, I don&#x27;t like that.  I want simple, fair pricing, not a complex pricing scheme where the primary purpose is to get me to overpay for my usage.</div><br/><div id="38094009" class="c"><input type="checkbox" id="c-38094009" checked=""/><div class="controls bullet"><span class="by">ezekiel68</span><span>|</span><a href="#38092166">root</a><span>|</span><a href="#38092729">parent</a><span>|</span><a href="#38092783">next</a><span>|</span><label class="collapse" for="c-38094009">[-]</label><label class="expand" for="c-38094009">[1 more]</label></div><br/><div class="children"><div class="content">Plenty of people know their upper limit. The ability to pay 50% less if that limit applies is a feature, not a bug. (This applies to any service -- I am not affiliated with phind except as an occasional user).</div><br/></div></div><div id="38092783" class="c"><input type="checkbox" id="c-38092783" checked=""/><div class="controls bullet"><span class="by">rushingcreek</span><span>|</span><a href="#38092166">root</a><span>|</span><a href="#38092729">parent</a><span>|</span><a href="#38094009">prev</a><span>|</span><a href="#38089281">next</a><span>|</span><label class="collapse" for="c-38092783">[-]</label><label class="expand" for="c-38092783">[4 more]</label></div><br/><div class="children"><div class="content">Phind Plus is $15&#x2F;month and Phind Pro is $30&#x2F;month. There is a 2x price difference, not an 8x difference. And Phind Pro comes with (virtually) unlimited GPT-4 uses.<p>We understand that the incentives of setting daily limits for search aren&#x27;t great, which is why the Phind model is unlimited for free. GPT-4, however, is unfortunately too expensive for us not to charge past a certain usage threshold.</div><br/><div id="38092895" class="c"><input type="checkbox" id="c-38092895" checked=""/><div class="controls bullet"><span class="by">donmcronald</span><span>|</span><a href="#38092166">root</a><span>|</span><a href="#38092783">parent</a><span>|</span><a href="#38089281">next</a><span>|</span><label class="collapse" for="c-38092895">[-]</label><label class="expand" for="c-38092895">[3 more]</label></div><br/><div class="children"><div class="content">Plus costs $0.016 per search and Pro costs $0.002 per search.<p><a href="https:&#x2F;&#x2F;www.phind.com&#x2F;search?cache=wgyz13tg4jkbl9pklptmpds5">https:&#x2F;&#x2F;www.phind.com&#x2F;search?cache=wgyz13tg4jkbl9pklptmpds5</a></div><br/><div id="38093350" class="c"><input type="checkbox" id="c-38093350" checked=""/><div class="controls bullet"><span class="by">ojosilva</span><span>|</span><a href="#38092166">root</a><span>|</span><a href="#38092895">parent</a><span>|</span><a href="#38093400">next</a><span>|</span><label class="collapse" for="c-38093350">[-]</label><label class="expand" for="c-38093350">[1 more]</label></div><br/><div class="children"><div class="content">To me the $15&#x2F;mo plan is just bait so users pick the target $30&#x2F;mo month. Why would you pay $0.016&#x2F;search when you can pay 8x less and <i>feel smart about making that choice</i>?<p>edit: looking at it again, I think the $15&#x2F;mo is actually just for people who wants Phind &quot;private&quot;, so that their data is not used for training.</div><br/></div></div><div id="38093400" class="c"><input type="checkbox" id="c-38093400" checked=""/><div class="controls bullet"><span class="by">nick-sta</span><span>|</span><a href="#38092166">root</a><span>|</span><a href="#38092895">parent</a><span>|</span><a href="#38093350">prev</a><span>|</span><a href="#38089281">next</a><span>|</span><label class="collapse" for="c-38093400">[-]</label><label class="expand" for="c-38093400">[1 more]</label></div><br/><div class="children"><div class="content">Cost per search isn&#x27;t really a great metric. For me, I hit the cap of 30 searches&#x2F;day pretty easily, but 500 is pretty hard to hit. For me, its just a question of what tier matches my volume.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="38089281" class="c"><input type="checkbox" id="c-38089281" checked=""/><div class="controls bullet"><span class="by">BugsJustFindMe</span><span>|</span><a href="#38092166">prev</a><span>|</span><a href="#38089573">next</a><span>|</span><label class="collapse" for="c-38089281">[-]</label><label class="expand" for="c-38089281">[14 more]</label></div><br/><div class="children"><div class="content">&gt; <i>You can now get high quality answers for technical questions in 10 seconds instead of 50.</i><p>ChatGPT 4 does not take 50 seconds to answer, so I don&#x27;t understand this comparison.</div><br/><div id="38089344" class="c"><input type="checkbox" id="c-38089344" checked=""/><div class="controls bullet"><span class="by">bethekind</span><span>|</span><a href="#38089281">parent</a><span>|</span><a href="#38089642">next</a><span>|</span><label class="collapse" for="c-38089344">[-]</label><label class="expand" for="c-38089344">[10 more]</label></div><br/><div class="children"><div class="content">Recently I&#x27;ve used gpt 4 and yes it does take up to a minute even for easy questions.<p>I&#x27;ve asked it how to scp a file on Windows 11 and it&#x27;ll take a minute to tell me all the options possible.<p>If this takes 1&#x2F;5th the time for equivalent questions, I&#x27;d consider switching</div><br/><div id="38089415" class="c"><input type="checkbox" id="c-38089415" checked=""/><div class="controls bullet"><span class="by">joshspankit</span><span>|</span><a href="#38089281">root</a><span>|</span><a href="#38089344">parent</a><span>|</span><a href="#38089788">next</a><span>|</span><label class="collapse" for="c-38089415">[-]</label><label class="expand" for="c-38089415">[7 more]</label></div><br/><div class="children"><div class="content">Not my experience at all. Are you counting the entire answer in your time?<p>If so, consider adding one of the “just get to the point” prompts. GPT4’s defaults have been geared towards public acceptance through long-windedness which is imo entirely unnecessary when using it to do functional things like scp a file.</div><br/><div id="38089555" class="c"><input type="checkbox" id="c-38089555" checked=""/><div class="controls bullet"><span class="by">theWreckluse</span><span>|</span><a href="#38089281">root</a><span>|</span><a href="#38089415">parent</a><span>|</span><a href="#38089505">next</a><span>|</span><label class="collapse" for="c-38089555">[-]</label><label class="expand" for="c-38089555">[4 more]</label></div><br/><div class="children"><div class="content">LOL, it’s not just for “public acceptance”. Look up Chain of Thought. Asking it to get to the point typically reduces the accuracy.</div><br/><div id="38089810" class="c"><input type="checkbox" id="c-38089810" checked=""/><div class="controls bullet"><span class="by">freedomben</span><span>|</span><a href="#38089281">root</a><span>|</span><a href="#38089555">parent</a><span>|</span><a href="#38091256">next</a><span>|</span><label class="collapse" for="c-38089810">[-]</label><label class="expand" for="c-38089810">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>LOL, it’s not just for “public acceptance”. Look up Chain of Thought. Asking it to get to the point typically reduces the accuracy.</i><p>Just trying to provide helpful feedback for you, this would have been a great comment, except for the &quot;LOL&quot; at the beginning that was unnecesary and demeaning.</div><br/></div></div><div id="38091256" class="c"><input type="checkbox" id="c-38091256" checked=""/><div class="controls bullet"><span class="by">bigfudge</span><span>|</span><a href="#38089281">root</a><span>|</span><a href="#38089555">parent</a><span>|</span><a href="#38089810">prev</a><span>|</span><a href="#38089505">next</a><span>|</span><label class="collapse" for="c-38091256">[-]</label><label class="expand" for="c-38091256">[2 more]</label></div><br/><div class="children"><div class="content">You are being snarky but are right. I have scripts set up to auto summarise expansive answers.  I wish I could build this into the ChatGPT ui though.</div><br/><div id="38093085" class="c"><input type="checkbox" id="c-38093085" checked=""/><div class="controls bullet"><span class="by">idonotknowwhy</span><span>|</span><a href="#38089281">root</a><span>|</span><a href="#38091256">parent</a><span>|</span><a href="#38089505">next</a><span>|</span><label class="collapse" for="c-38093085">[-]</label><label class="expand" for="c-38093085">[1 more]</label></div><br/><div class="children"><div class="content">Try the custom instructions feature</div><br/></div></div></div></div></div></div><div id="38089505" class="c"><input type="checkbox" id="c-38089505" checked=""/><div class="controls bullet"><span class="by">londons_explore</span><span>|</span><a href="#38089281">root</a><span>|</span><a href="#38089415">parent</a><span>|</span><a href="#38089555">prev</a><span>|</span><a href="#38089473">next</a><span>|</span><label class="collapse" for="c-38089505">[-]</label><label class="expand" for="c-38089505">[1 more]</label></div><br/><div class="children"><div class="content">The words &quot;briefly&quot; or &quot;without explanation&quot; work well.<p>By keeping the prompt short, it starts generating output quicker too.</div><br/></div></div><div id="38089473" class="c"><input type="checkbox" id="c-38089473" checked=""/><div class="controls bullet"><span class="by">phillipcarter</span><span>|</span><a href="#38089281">root</a><span>|</span><a href="#38089415">parent</a><span>|</span><a href="#38089505">prev</a><span>|</span><a href="#38089788">next</a><span>|</span><label class="collapse" for="c-38089473">[-]</label><label class="expand" for="c-38089473">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, I would say this is a prompting problem and not a model problem. In a product area we&#x27;re building out right now with GPT-4, our prompt (more or less) tells it to provide exactly 3 values and it does that and only that. It&#x27;s quite fast.<p>Also, use case thing. It is very likely the case that for certain coding use cases, Phind will always be faster because it&#x27;s not designed to be general purpose.</div><br/></div></div></div></div><div id="38089788" class="c"><input type="checkbox" id="c-38089788" checked=""/><div class="controls bullet"><span class="by">furyofantares</span><span>|</span><a href="#38089281">root</a><span>|</span><a href="#38089344">parent</a><span>|</span><a href="#38089415">prev</a><span>|</span><a href="#38090054">next</a><span>|</span><label class="collapse" for="c-38089788">[-]</label><label class="expand" for="c-38089788">[1 more]</label></div><br/><div class="children"><div class="content">This isn&#x27;t a fair comparison because I have custom instructions that mention being brief but complete, but I did &quot;how to scp a file on Windows 11&quot;<p>ChatGPT4: 14 seconds<p>phind with &quot;pair programmer&quot; checked: 65 seconds<p>phind default: 16 seconds</div><br/></div></div><div id="38090054" class="c"><input type="checkbox" id="c-38090054" checked=""/><div class="controls bullet"><span class="by">BugsJustFindMe</span><span>|</span><a href="#38089281">root</a><span>|</span><a href="#38089344">parent</a><span>|</span><a href="#38089788">prev</a><span>|</span><a href="#38089642">next</a><span>|</span><label class="collapse" for="c-38090054">[-]</label><label class="expand" for="c-38090054">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>I&#x27;ve asked it how to scp a file on Windows 11 and it&#x27;ll take a minute</i><p><a href="https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;iqxOJUV" rel="nofollow noreferrer">https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;iqxOJUV</a> was 6.5 seconds.<p><a href="https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;pQFfWli" rel="nofollow noreferrer">https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;pQFfWli</a> was 15.<p>You can tell they&#x27;re GPT-4 because the logo is purple (the logo is green when using 3.5).</div><br/></div></div></div></div><div id="38089642" class="c"><input type="checkbox" id="c-38089642" checked=""/><div class="controls bullet"><span class="by">JoshGlazebrook</span><span>|</span><a href="#38089281">parent</a><span>|</span><a href="#38089344">prev</a><span>|</span><a href="#38089311">next</a><span>|</span><label class="collapse" for="c-38089642">[-]</label><label class="expand" for="c-38089642">[2 more]</label></div><br/><div class="children"><div class="content">ChatGPT4 is more often than not noticeably slow enough that I question why I pay for it.</div><br/><div id="38091248" class="c"><input type="checkbox" id="c-38091248" checked=""/><div class="controls bullet"><span class="by">shmoogy</span><span>|</span><a href="#38089281">root</a><span>|</span><a href="#38089642">parent</a><span>|</span><a href="#38089311">next</a><span>|</span><label class="collapse" for="c-38091248">[-]</label><label class="expand" for="c-38091248">[1 more]</label></div><br/><div class="children"><div class="content">Sometimes it&#x27;s insanely quick - like gpt3,5 turbo or a cached answer or something.</div><br/></div></div></div></div><div id="38089311" class="c"><input type="checkbox" id="c-38089311" checked=""/><div class="controls bullet"><span class="by">rushingcreek</span><span>|</span><a href="#38089281">parent</a><span>|</span><a href="#38089642">prev</a><span>|</span><a href="#38089573">next</a><span>|</span><label class="collapse" for="c-38089311">[-]</label><label class="expand" for="c-38089311">[1 more]</label></div><br/><div class="children"><div class="content">We find that it takes around a minute for a 1024-token answer. Answers to less complex questions will take less time, but Phind will still be 5x faster.</div><br/></div></div></div></div><div id="38089573" class="c"><input type="checkbox" id="c-38089573" checked=""/><div class="controls bullet"><span class="by">xrd</span><span>|</span><a href="#38089281">prev</a><span>|</span><a href="#38089329">next</a><span>|</span><label class="collapse" for="c-38089573">[-]</label><label class="expand" for="c-38089573">[14 more]</label></div><br/><div class="children"><div class="content">I know it isn&#x27;t popular, but I wish there was a way to use this inside Emacs. Or, vim. I just don&#x27;t want to use VS Code anymore.</div><br/><div id="38089762" class="c"><input type="checkbox" id="c-38089762" checked=""/><div class="controls bullet"><span class="by">freedomben</span><span>|</span><a href="#38089573">parent</a><span>|</span><a href="#38089725">next</a><span>|</span><label class="collapse" for="c-38089762">[-]</label><label class="expand" for="c-38089762">[4 more]</label></div><br/><div class="children"><div class="content">The standardizing on VS code is one of the saddest developments over the last several years IMHO.  I think it&#x27;s great that VS Code exists, but we&#x27;re headed for a world where you <i>have</i> to use VS Code if you want the best tooling because it won&#x27;t support other options.  The same thing happened with Java dev and IntelliJ, and IMHO it has been extremely unhealthy for the ecosystem.  I&#x27;m immensely glad that Copilot supports vim, but I&#x27;m fearful that it soon won&#x27;t.</div><br/><div id="38091532" class="c"><input type="checkbox" id="c-38091532" checked=""/><div class="controls bullet"><span class="by">papichulo2023</span><span>|</span><a href="#38089573">root</a><span>|</span><a href="#38089762">parent</a><span>|</span><a href="#38089908">next</a><span>|</span><label class="collapse" for="c-38091532">[-]</label><label class="expand" for="c-38091532">[2 more]</label></div><br/><div class="children"><div class="content">Didnt vscode standardise language servers making much easier for all  the rest text-editor-close-almost-ides to integrate? Is it really that sad?</div><br/><div id="38092177" class="c"><input type="checkbox" id="c-38092177" checked=""/><div class="controls bullet"><span class="by">freedomben</span><span>|</span><a href="#38089573">root</a><span>|</span><a href="#38091532">parent</a><span>|</span><a href="#38089908">next</a><span>|</span><label class="collapse" for="c-38092177">[-]</label><label class="expand" for="c-38092177">[1 more]</label></div><br/><div class="children"><div class="content">Very fair point. Vim has benefited tremendously from that effort.</div><br/></div></div></div></div><div id="38089908" class="c"><input type="checkbox" id="c-38089908" checked=""/><div class="controls bullet"><span class="by">FreezerburnV</span><span>|</span><a href="#38089573">root</a><span>|</span><a href="#38089762">parent</a><span>|</span><a href="#38091532">prev</a><span>|</span><a href="#38089725">next</a><span>|</span><label class="collapse" for="c-38089908">[-]</label><label class="expand" for="c-38089908">[1 more]</label></div><br/><div class="children"><div class="content">Same could have&#x2F;could be said about Jetbrains products. People are likely always going to use vim&#x2F;emacs and create tooling around whatever new hotness exists for them. And honestly? VS Code is just a new iteration on how vim&#x2F;emacs work in a lot of ways: Providing a place to edit text and then a bunch of plugins that do things with that text.<p>And if you want vim&#x2F;emacs to keep living, then you should spend time helping! Create your own extensions, maintain&#x2F;contribute to existing ones, etc. They will only die out when the last person actively contributing to them stops, so keep the chain of people going :)</div><br/></div></div></div></div><div id="38089725" class="c"><input type="checkbox" id="c-38089725" checked=""/><div class="controls bullet"><span class="by">Jeff_Brown</span><span>|</span><a href="#38089573">parent</a><span>|</span><a href="#38089762">prev</a><span>|</span><a href="#38089932">next</a><span>|</span><label class="collapse" for="c-38089725">[-]</label><label class="expand" for="c-38089725">[1 more]</label></div><br/><div class="children"><div class="content">If only the depth of our feelings for Emacs counted for more in the market.<p>There&#x27;s an argument that music and the arts are dumbed down by the fact that, for instance, making an album worth $10 to millions of people pays way better than making an album worth a million dollars to tens of people, since the album is going to get priced at $10 one way or the other. It only just now occurred to me that the same phenomenon applies to tools.</div><br/></div></div><div id="38089932" class="c"><input type="checkbox" id="c-38089932" checked=""/><div class="controls bullet"><span class="by">mg</span><span>|</span><a href="#38089573">parent</a><span>|</span><a href="#38089725">prev</a><span>|</span><a href="#38089709">next</a><span>|</span><label class="collapse" for="c-38089932">[-]</label><label class="expand" for="c-38089932">[2 more]</label></div><br/><div class="children"><div class="content">In Vim, I tried to assign a shortcut to send the selected text to Phind (or any other LLM) and came up with this:<p>:&#x27;&lt;,&#x27;&gt;y|call system(&#x27;firefox &lt;url&gt;?q=&#x27;.shellescape(@*).&#x27; &amp;&#x27;)<p>The only problem left is that the text is not urlencoded.<p>There probably is some elegant way to urlencode it. But I did not come up with one yet.</div><br/><div id="38091136" class="c"><input type="checkbox" id="c-38091136" checked=""/><div class="controls bullet"><span class="by">wizzwizz4</span><span>|</span><a href="#38089573">root</a><span>|</span><a href="#38089932">parent</a><span>|</span><a href="#38089709">next</a><span>|</span><label class="collapse" for="c-38091136">[-]</label><label class="expand" for="c-38091136">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;stackoverflow.com&#x2F;a&#x2F;76488059" rel="nofollow noreferrer">https:&#x2F;&#x2F;stackoverflow.com&#x2F;a&#x2F;76488059</a> claims to have one, though it&#x27;s not explained.</div><br/></div></div></div></div><div id="38089709" class="c"><input type="checkbox" id="c-38089709" checked=""/><div class="controls bullet"><span class="by">bigdict</span><span>|</span><a href="#38089573">parent</a><span>|</span><a href="#38089932">prev</a><span>|</span><a href="#38091989">next</a><span>|</span><label class="collapse" for="c-38089709">[-]</label><label class="expand" for="c-38089709">[2 more]</label></div><br/><div class="children"><div class="content">Pretty sure GitHub Copilot has emacs&#x2F;vim integration.</div><br/><div id="38089779" class="c"><input type="checkbox" id="c-38089779" checked=""/><div class="controls bullet"><span class="by">freedomben</span><span>|</span><a href="#38089573">root</a><span>|</span><a href="#38089709">parent</a><span>|</span><a href="#38091989">next</a><span>|</span><label class="collapse" for="c-38089779">[-]</label><label class="expand" for="c-38089779">[1 more]</label></div><br/><div class="children"><div class="content">It does, although not the most recent features.  I use the compatible features in Vim and I really like it.  Not enough to switch editors though.</div><br/></div></div></div></div><div id="38091989" class="c"><input type="checkbox" id="c-38091989" checked=""/><div class="controls bullet"><span class="by">accoil</span><span>|</span><a href="#38089573">parent</a><span>|</span><a href="#38089709">prev</a><span>|</span><a href="#38089875">next</a><span>|</span><label class="collapse" for="c-38091989">[-]</label><label class="expand" for="c-38091989">[1 more]</label></div><br/><div class="children"><div class="content">Maybe ellama[1] would work? It doesn&#x27;t support Phind yet, but a provider could be created for the underlying connection package llm[2].<p>[1]: <a href="https:&#x2F;&#x2F;github.com&#x2F;s-kostyaev&#x2F;ellama">https:&#x2F;&#x2F;github.com&#x2F;s-kostyaev&#x2F;ellama</a><p>[2]: <a href="https:&#x2F;&#x2F;github.com&#x2F;ahyatt&#x2F;llm">https:&#x2F;&#x2F;github.com&#x2F;ahyatt&#x2F;llm</a></div><br/></div></div><div id="38089875" class="c"><input type="checkbox" id="c-38089875" checked=""/><div class="controls bullet"><span class="by">notpublic</span><span>|</span><a href="#38089573">parent</a><span>|</span><a href="#38091989">prev</a><span>|</span><a href="#38089857">next</a><span>|</span><label class="collapse" for="c-38089875">[-]</label><label class="expand" for="c-38089875">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;github.com&#x2F;github&#x2F;copilot.vim">https:&#x2F;&#x2F;github.com&#x2F;github&#x2F;copilot.vim</a><p><a href="https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;llm.nvim">https:&#x2F;&#x2F;github.com&#x2F;huggingface&#x2F;llm.nvim</a></div><br/></div></div><div id="38089857" class="c"><input type="checkbox" id="c-38089857" checked=""/><div class="controls bullet"><span class="by">haarts</span><span>|</span><a href="#38089573">parent</a><span>|</span><a href="#38089875">prev</a><span>|</span><a href="#38090016">next</a><span>|</span><label class="collapse" for="c-38089857">[-]</label><label class="expand" for="c-38089857">[1 more]</label></div><br/><div class="children"><div class="content">You and me both brother. LSP integration seems the way forward.</div><br/></div></div><div id="38090016" class="c"><input type="checkbox" id="c-38090016" checked=""/><div class="controls bullet"><span class="by">fictorial</span><span>|</span><a href="#38089573">parent</a><span>|</span><a href="#38089857">prev</a><span>|</span><a href="#38089329">next</a><span>|</span><label class="collapse" for="c-38090016">[-]</label><label class="expand" for="c-38090016">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;github.com&#x2F;CoderCookE&#x2F;vim-chatgpt">https:&#x2F;&#x2F;github.com&#x2F;CoderCookE&#x2F;vim-chatgpt</a></div><br/></div></div></div></div><div id="38089329" class="c"><input type="checkbox" id="c-38089329" checked=""/><div class="controls bullet"><span class="by">eoinboylan</span><span>|</span><a href="#38089573">prev</a><span>|</span><a href="#38089704">next</a><span>|</span><label class="collapse" for="c-38089329">[-]</label><label class="expand" for="c-38089329">[5 more]</label></div><br/><div class="children"><div class="content">Ran a quick test with a Rust async code snippet that contains an error. Compared with GPT-4 its gives a far clearer solution, with linked sources to learn more! Super impressive!</div><br/><div id="38089337" class="c"><input type="checkbox" id="c-38089337" checked=""/><div class="controls bullet"><span class="by">rushingcreek</span><span>|</span><a href="#38089329">parent</a><span>|</span><a href="#38089704">next</a><span>|</span><label class="collapse" for="c-38089337">[-]</label><label class="expand" for="c-38089337">[4 more]</label></div><br/><div class="children"><div class="content">Amazing, that&#x27;s great to hear.</div><br/><div id="38089556" class="c"><input type="checkbox" id="c-38089556" checked=""/><div class="controls bullet"><span class="by">passion__desire</span><span>|</span><a href="#38089329">root</a><span>|</span><a href="#38089337">parent</a><span>|</span><a href="#38089704">next</a><span>|</span><label class="collapse" for="c-38089556">[-]</label><label class="expand" for="c-38089556">[3 more]</label></div><br/><div class="children"><div class="content">Is it possible to output all steps of solutions in a single copyable block? I don&#x27;t want to copy 4 separate blocks.</div><br/><div id="38094022" class="c"><input type="checkbox" id="c-38094022" checked=""/><div class="controls bullet"><span class="by">ezekiel68</span><span>|</span><a href="#38089329">root</a><span>|</span><a href="#38089556">parent</a><span>|</span><a href="#38090005">next</a><span>|</span><label class="collapse" for="c-38094022">[-]</label><label class="expand" for="c-38094022">[1 more]</label></div><br/><div class="children"><div class="content">When I use it I often give a final prompt like &quot;Now combine the above answers together into a function that accept the following arguments...&quot;.  This has worked well for my use cases.</div><br/></div></div><div id="38090005" class="c"><input type="checkbox" id="c-38090005" checked=""/><div class="controls bullet"><span class="by">rushingcreek</span><span>|</span><a href="#38089329">root</a><span>|</span><a href="#38089556">parent</a><span>|</span><a href="#38094022">prev</a><span>|</span><a href="#38089704">next</a><span>|</span><label class="collapse" for="c-38090005">[-]</label><label class="expand" for="c-38090005">[1 more]</label></div><br/><div class="children"><div class="content">You can tell it that in a followup. Or, configure an answer profile and tell it to use that style: <a href="https:&#x2F;&#x2F;phind.com&#x2F;profile">https:&#x2F;&#x2F;phind.com&#x2F;profile</a>.</div><br/></div></div></div></div></div></div></div></div><div id="38089704" class="c"><input type="checkbox" id="c-38089704" checked=""/><div class="controls bullet"><span class="by">buildbot</span><span>|</span><a href="#38089329">prev</a><span>|</span><a href="#38093835">next</a><span>|</span><label class="collapse" for="c-38089704">[-]</label><label class="expand" for="c-38089704">[4 more]</label></div><br/><div class="children"><div class="content">Well, neither GPT4 or this Phind model where able to answer my torture test:
&quot;Write amaranth code that can be used to control the readout of a frame from a kodak CCD with 4096 columns and 2048 rows.&quot;<p>Which yes, is missing a lot of detail (you could&#x2F;I have feed&#x2F;fed in a datasheet).<p>But Phind goes off on using pyserial (?!), and GPT4 assumes amaranth is a hypothetical CCD control library and makes a useless class control CCD using the hypothetical library.<p>Edit - Phind at least acknowledged that amaranth exists, unlike GPT4 with this prompt: 
&quot;Write amaranth code that can be used to control the readout of a frame from a kodak CCD using an lattice FPGA with 4096 columns and 2048 rows. Assume the design will be hooked up to a larger litex SoC &quot;</div><br/><div id="38089781" class="c"><input type="checkbox" id="c-38089781" checked=""/><div class="controls bullet"><span class="by">mensetmanusman</span><span>|</span><a href="#38089704">parent</a><span>|</span><a href="#38093143">next</a><span>|</span><label class="collapse" for="c-38089781">[-]</label><label class="expand" for="c-38089781">[2 more]</label></div><br/><div class="children"><div class="content">That’s torture for humans as well. The key to LLMs is communicating clearly to the information cloud.</div><br/><div id="38091541" class="c"><input type="checkbox" id="c-38091541" checked=""/><div class="controls bullet"><span class="by">buildbot</span><span>|</span><a href="#38089704">root</a><span>|</span><a href="#38089781">parent</a><span>|</span><a href="#38093143">next</a><span>|</span><label class="collapse" for="c-38091541">[-]</label><label class="expand" for="c-38091541">[1 more]</label></div><br/><div class="children"><div class="content">Sure, but a good example of how far certain domains have to go still. These datasheets should be in the models training data, at least one CCD datasheet, and verilog &amp; (migen | nmigen | amaranth) certainly are.<p>Controlling a CCD is actually pretty easy, I built (very simple, but working) controllers for several CCD chips in undergrad doing research for the ATLAS detector. You just clock a rows out basically, N columns times. Reset first. I&#x27;d expect an senior undergrad EE student to be able to design a simple core in a few class projects.</div><br/></div></div></div></div><div id="38093143" class="c"><input type="checkbox" id="c-38093143" checked=""/><div class="controls bullet"><span class="by">idonotknowwhy</span><span>|</span><a href="#38089704">parent</a><span>|</span><a href="#38089781">prev</a><span>|</span><a href="#38093835">next</a><span>|</span><label class="collapse" for="c-38093143">[-]</label><label class="expand" for="c-38093143">[1 more]</label></div><br/><div class="children"><div class="content">I have no idea what that means (even after googling it) lol.
This is how my local WizardLM-70B responded to your prompt.<p><a href="https:&#x2F;&#x2F;pastebin.com&#x2F;BCAthV8y" rel="nofollow noreferrer">https:&#x2F;&#x2F;pastebin.com&#x2F;BCAthV8y</a></div><br/></div></div></div></div><div id="38093835" class="c"><input type="checkbox" id="c-38093835" checked=""/><div class="controls bullet"><span class="by">rushingcreek</span><span>|</span><a href="#38089704">prev</a><span>|</span><a href="#38093285">next</a><span>|</span><label class="collapse" for="c-38093835">[-]</label><label class="expand" for="c-38093835">[4 more]</label></div><br/><div class="children"><div class="content">Phind co-founder here! Here&#x27;s a link to our blog post: <a href="https:&#x2F;&#x2F;www.phind.com&#x2F;blog&#x2F;phind-model-beats-gpt4-fast">https:&#x2F;&#x2F;www.phind.com&#x2F;blog&#x2F;phind-model-beats-gpt4-fast</a></div><br/><div id="38093845" class="c"><input type="checkbox" id="c-38093845" checked=""/><div class="controls bullet"><span class="by">archibaldJ</span><span>|</span><a href="#38093835">parent</a><span>|</span><a href="#38093285">next</a><span>|</span><label class="collapse" for="c-38093845">[-]</label><label class="expand" for="c-38093845">[3 more]</label></div><br/><div class="children"><div class="content">hi; great work. so is this more fine-tuning on Phind-CodeLlama-34B-v2?<p>will there be api access soon?<p>also: will it be open-source at some point? thanks</div><br/><div id="38093853" class="c"><input type="checkbox" id="c-38093853" checked=""/><div class="controls bullet"><span class="by">rushingcreek</span><span>|</span><a href="#38093835">root</a><span>|</span><a href="#38093845">parent</a><span>|</span><a href="#38093285">next</a><span>|</span><label class="collapse" for="c-38093853">[-]</label><label class="expand" for="c-38093853">[2 more]</label></div><br/><div class="children"><div class="content">Thank you. Yes, it is the 7th iteration that started with our open-source models. We do plan to open source this model as well down the road, once we&#x27;ve released a few more generations.<p>API access is on the roadmap but we have no time estimates for when we will build it. We&#x27;re trying to not get distracted from our main mission :)</div><br/><div id="38093933" class="c"><input type="checkbox" id="c-38093933" checked=""/><div class="controls bullet"><span class="by">archibaldJ</span><span>|</span><a href="#38093835">root</a><span>|</span><a href="#38093853">parent</a><span>|</span><a href="#38093285">next</a><span>|</span><label class="collapse" for="c-38093933">[-]</label><label class="expand" for="c-38093933">[1 more]</label></div><br/><div class="children"><div class="content">I see. Thanks!<p>So Phind&#x27;s main mission is to overtake Google right? ;)</div><br/></div></div></div></div></div></div></div></div><div id="38093285" class="c"><input type="checkbox" id="c-38093285" checked=""/><div class="controls bullet"><span class="by">elif</span><span>|</span><a href="#38093835">prev</a><span>|</span><a href="#38089246">next</a><span>|</span><label class="collapse" for="c-38093285">[-]</label><label class="expand" for="c-38093285">[2 more]</label></div><br/><div class="children"><div class="content">Hmm I wonder what kind of code quality can be accomplished by looping from phind-&gt;gpt-&gt;copilot for multiple iterations, asking for  criticisms of the code qualities then seeking code which addresses the ai-generated criticisms etc until it knocks out better than I would in a second.</div><br/><div id="38094840" class="c"><input type="checkbox" id="c-38094840" checked=""/><div class="controls bullet"><span class="by">sagarpatil</span><span>|</span><a href="#38093285">parent</a><span>|</span><a href="#38089246">next</a><span>|</span><label class="collapse" for="c-38094840">[-]</label><label class="expand" for="c-38094840">[1 more]</label></div><br/><div class="children"><div class="content">You could cook something like this using Microsoft Autogen. It does allow you to daisy chain the models.</div><br/></div></div></div></div><div id="38089246" class="c"><input type="checkbox" id="c-38089246" checked=""/><div class="controls bullet"><span class="by">tinco</span><span>|</span><a href="#38093285">prev</a><span>|</span><a href="#38089682">next</a><span>|</span><label class="collapse" for="c-38089246">[-]</label><label class="expand" for="c-38089246">[6 more]</label></div><br/><div class="children"><div class="content">Will you be offering the model as an API service? The product my team is working on would benefit from a significantly faster and possibly better performing model than GPT-4. If you&#x27;re planning on keeping pace with competitive models we&#x27;d love to integrate the use of your model into our service.</div><br/><div id="38089271" class="c"><input type="checkbox" id="c-38089271" checked=""/><div class="controls bullet"><span class="by">rushingcreek</span><span>|</span><a href="#38089246">parent</a><span>|</span><a href="#38089682">next</a><span>|</span><label class="collapse" for="c-38089271">[-]</label><label class="expand" for="c-38089271">[5 more]</label></div><br/><div class="children"><div class="content">If we get enough demand that&#x27;s definitely something we&#x27;ll consider. We&#x27;re still a small team, however, and we do everything in our power to not get distracted from our main mission.</div><br/><div id="38095696" class="c"><input type="checkbox" id="c-38095696" checked=""/><div class="controls bullet"><span class="by">halfjoking</span><span>|</span><a href="#38089246">root</a><span>|</span><a href="#38089271">parent</a><span>|</span><a href="#38091270">next</a><span>|</span><label class="collapse" for="c-38095696">[-]</label><label class="expand" for="c-38095696">[1 more]</label></div><br/><div class="children"><div class="content">If you offer an API you don&#x27;t have to maintain a Visual Studio plugin.  Trying to compete with tools like Cursor would be the real distraction.<p>And Cursor is just the start - there will be innovative workflows built on top of APIs you can&#x27;t predict.  You&#x27;re missing out not having developers build an ecosystem for you.</div><br/></div></div><div id="38091270" class="c"><input type="checkbox" id="c-38091270" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#38089246">root</a><span>|</span><a href="#38089271">parent</a><span>|</span><a href="#38095696">prev</a><span>|</span><a href="#38089358">next</a><span>|</span><label class="collapse" for="c-38091270">[-]</label><label class="expand" for="c-38091270">[1 more]</label></div><br/><div class="children"><div class="content">Please consider releasing an API. Having a faster alternative to GPT-4 would be amazing for so many use cases.<p>Especially for agents that do function calling.</div><br/></div></div><div id="38089358" class="c"><input type="checkbox" id="c-38089358" checked=""/><div class="controls bullet"><span class="by">tinco</span><span>|</span><a href="#38089246">root</a><span>|</span><a href="#38089271">parent</a><span>|</span><a href="#38091270">prev</a><span>|</span><a href="#38090629">next</a><span>|</span><label class="collapse" for="c-38089358">[-]</label><label class="expand" for="c-38089358">[1 more]</label></div><br/><div class="children"><div class="content">Makes sense, we&#x27;re also very small (pre-seed) so definitely no cash cow for you guys yet. We probably shouldn&#x27;t be prematurely optimizing our prompting performance as it&#x27;s not really a bottleneck, but a 4x improvement just by swapping an API would be too good not to act on.</div><br/></div></div><div id="38090629" class="c"><input type="checkbox" id="c-38090629" checked=""/><div class="controls bullet"><span class="by">mike_hearn</span><span>|</span><a href="#38089246">root</a><span>|</span><a href="#38089271">parent</a><span>|</span><a href="#38089358">prev</a><span>|</span><a href="#38089682">next</a><span>|</span><label class="collapse" for="c-38090629">[-]</label><label class="expand" for="c-38090629">[1 more]</label></div><br/><div class="children"><div class="content">If you offer an API then you can be used with tools like <a href="https:&#x2F;&#x2F;aider.chat&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;aider.chat&#x2F;</a>, which is the best way to use LLMs for coding. But if only available via the web it&#x27;s not possible. BTW this is the main reason I pay for the OpenAI API.</div><br/></div></div></div></div></div></div><div id="38089682" class="c"><input type="checkbox" id="c-38089682" checked=""/><div class="controls bullet"><span class="by">dontreact</span><span>|</span><a href="#38089246">prev</a><span>|</span><a href="#38094797">next</a><span>|</span><label class="collapse" for="c-38089682">[-]</label><label class="expand" for="c-38089682">[4 more]</label></div><br/><div class="children"><div class="content">I tried this question and GPT4 did way way better to getting closer to a final answer. Phind was horribly wrong. I can&#x27;t help but think something seems off with your eval given just how badly Phind did on this.<p>I want to make an interactive plot in Colab where I can show<p>X axis is interest rate of a 15 year mortgage.
Y axis is the relative advantage of buying a house vs. renting in terms of total net worth at 15 years.<p>Assume a monthly budget for renting + investing or buying a house of 10k<p>Plot different lines for a few different market returns.<p>Make a slider that controls the total size of the loan.</div><br/><div id="38089734" class="c"><input type="checkbox" id="c-38089734" checked=""/><div class="controls bullet"><span class="by">rushingcreek</span><span>|</span><a href="#38089682">parent</a><span>|</span><a href="#38094797">next</a><span>|</span><label class="collapse" for="c-38089734">[-]</label><label class="expand" for="c-38089734">[3 more]</label></div><br/><div class="children"><div class="content">Seemed to give plausible results for me: <a href="https:&#x2F;&#x2F;www.phind.com&#x2F;search?cache=lswmiuewv2l33jt337dgrsho">https:&#x2F;&#x2F;www.phind.com&#x2F;search?cache=lswmiuewv2l33jt337dgrsho</a></div><br/><div id="38091557" class="c"><input type="checkbox" id="c-38091557" checked=""/><div class="controls bullet"><span class="by">dontreact</span><span>|</span><a href="#38089682">root</a><span>|</span><a href="#38089734">parent</a><span>|</span><a href="#38094797">next</a><span>|</span><label class="collapse" for="c-38091557">[-]</label><label class="expand" for="c-38091557">[2 more]</label></div><br/><div class="children"><div class="content">def calculate_relative_advantage(interest_rate, loan_size, market_return):
   # Your calculation logic here
   pass<p>Chat gpt actually implements it</div><br/><div id="38095027" class="c"><input type="checkbox" id="c-38095027" checked=""/><div class="controls bullet"><span class="by">letitgo12345</span><span>|</span><a href="#38089682">root</a><span>|</span><a href="#38091557">parent</a><span>|</span><a href="#38094797">next</a><span>|</span><label class="collapse" for="c-38095027">[-]</label><label class="expand" for="c-38095027">[1 more]</label></div><br/><div class="children"><div class="content">Just prompt it to implement the function</div><br/></div></div></div></div></div></div></div></div><div id="38094797" class="c"><input type="checkbox" id="c-38094797" checked=""/><div class="controls bullet"><span class="by">sagarpatil</span><span>|</span><a href="#38089682">prev</a><span>|</span><a href="#38095221">next</a><span>|</span><label class="collapse" for="c-38094797">[-]</label><label class="expand" for="c-38094797">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been using GPT-4 for coding since it&#x27;s launched. I did try Phind when you launched it with GPT4 and found it useful but I use VS Code and hate having to switch. I see you do have a VS Code extension now (great job!). I tried the phind model and at first glance it definitely looks better than GPT-4 wrt coding. Do you have any plans to provide this model as an API?</div><br/></div></div><div id="38095221" class="c"><input type="checkbox" id="c-38095221" checked=""/><div class="controls bullet"><span class="by">smusamashah</span><span>|</span><a href="#38094797">prev</a><span>|</span><a href="#38095167">next</a><span>|</span><label class="collapse" for="c-38095221">[-]</label><label class="expand" for="c-38095221">[1 more]</label></div><br/><div class="children"><div class="content">Phind can be very good for general tech searches too. I spent long time looking for a way to disable my Pixel 4a from auto updating to android 14 (which it has already downloaded). With Google I only found one solution which disables update on restart. I asked the same in Phind and in one query I have got around 4 solutions [1].<p>[1]: <a href="https:&#x2F;&#x2F;www.phind.com&#x2F;search?cache=vtzigjx3rnruc9ltocv9gqi1">https:&#x2F;&#x2F;www.phind.com&#x2F;search?cache=vtzigjx3rnruc9ltocv9gqi1</a></div><br/></div></div><div id="38095167" class="c"><input type="checkbox" id="c-38095167" checked=""/><div class="controls bullet"><span class="by">throwaway4good</span><span>|</span><a href="#38095221">prev</a><span>|</span><a href="#38091440">next</a><span>|</span><label class="collapse" for="c-38095167">[-]</label><label class="expand" for="c-38095167">[1 more]</label></div><br/><div class="children"><div class="content">Playing on the other AI prompt thread (<a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38089247">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38089247</a>) here on HN I tried:<p>&quot;write me an angry birds clone using JavaScript and the matter physics library&quot;<p>And I really enjoyed the answer; in particular that it would show the sources for it - making it obvious how close its answer is openly available tutorials.<p>This is much better than a black box pretending to do black magic.</div><br/></div></div><div id="38091440" class="c"><input type="checkbox" id="c-38091440" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#38095167">prev</a><span>|</span><a href="#38091171">next</a><span>|</span><label class="collapse" for="c-38091440">[-]</label><label class="expand" for="c-38091440">[3 more]</label></div><br/><div class="children"><div class="content">Could you open source these great models? OK yes you need a competitive advantage. So maybe open source them when you are say 2 models ahead in production?<p>In any case I am happy there is some competition and that it has come from a more pragmatic scrappy space than one of the multiple billion dollar funded places.</div><br/><div id="38091514" class="c"><input type="checkbox" id="c-38091514" checked=""/><div class="controls bullet"><span class="by">sounds</span><span>|</span><a href="#38091440">parent</a><span>|</span><a href="#38092463">next</a><span>|</span><label class="collapse" for="c-38091514">[-]</label><label class="expand" for="c-38091514">[1 more]</label></div><br/><div class="children"><div class="content">Can we have a larger discussion about the tradeoffs that come with open sourcing a model?<p>When fb released Llama they obviously gained a huge amount of developer goodwill but it also required them to invest a serious amount of their own developer time to engage with the community.<p>I&#x27;m asking the community what it can offer the company? Or is this just self-abnegation by the company that releases the model?</div><br/></div></div><div id="38092463" class="c"><input type="checkbox" id="c-38092463" checked=""/><div class="controls bullet"><span class="by">poser-boy</span><span>|</span><a href="#38091440">parent</a><span>|</span><a href="#38091514">prev</a><span>|</span><a href="#38091171">next</a><span>|</span><label class="collapse" for="c-38092463">[-]</label><label class="expand" for="c-38092463">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know what model runs on Phind&#x27;s site right now, but in August Phind published a fine tune of CodeLlama 34B<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;Phind&#x2F;Phind-CodeLlama-34B-v2" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;Phind&#x2F;Phind-CodeLlama-34B-v2</a></div><br/></div></div></div></div><div id="38091171" class="c"><input type="checkbox" id="c-38091171" checked=""/><div class="controls bullet"><span class="by">lgkk</span><span>|</span><a href="#38091440">prev</a><span>|</span><a href="#38089772">next</a><span>|</span><label class="collapse" for="c-38091171">[-]</label><label class="expand" for="c-38091171">[1 more]</label></div><br/><div class="children"><div class="content">First off, congrats on building such a cool product. I love that I can just &quot;jump into it&quot; which is great.<p>Note that I&#x27;m not really a power user of these GPT style tools- here are my questions:<p>Is it possible to get right to the code without the ELI5 and general information?<p>Do you guys offer an API? I was browsing on my small iphone so maybe I missed this info.<p>Could you give an overview for someone like me how something like phind works technically? You mentioned those H100s, but at a very high level without revealing any &quot;secret sauce&quot; how does this GPT work from my input to getting a response?<p>Good luck!</div><br/></div></div><div id="38089772" class="c"><input type="checkbox" id="c-38089772" checked=""/><div class="controls bullet"><span class="by">mtkd</span><span>|</span><a href="#38091171">prev</a><span>|</span><a href="#38090391">next</a><span>|</span><label class="collapse" for="c-38089772">[-]</label><label class="expand" for="c-38089772">[1 more]</label></div><br/><div class="children"><div class="content">Been using Phind for a bit now and started paying for pro<p>They&#x27;re smashing it and can&#x27;t do enough if you report an issue, also they have started a weekly voice call to discuss algos and such with senior devs, like a surgery, only 10 people join at moment<p>Don&#x27;t think I&#x27;ve ever recommended anything as much as I have these guys in last couple of months</div><br/></div></div><div id="38090391" class="c"><input type="checkbox" id="c-38090391" checked=""/><div class="controls bullet"><span class="by">shazar</span><span>|</span><a href="#38089772">prev</a><span>|</span><a href="#38091537">next</a><span>|</span><label class="collapse" for="c-38090391">[-]</label><label class="expand" for="c-38090391">[8 more]</label></div><br/><div class="children"><div class="content">I gave it two tries, GPT-4 was much better in both cases. Tried with two Leetcode questions. It came back with an empty response for one, and provided a worse code (O(n2) solutions when it can be done with linear time) for the other one.<p>GPT-4 on the other hand provided a good answer for both questions. Also I guess the UI is buggy w.r.t code formatting, it things the following line is a code and switches to a code block.<p>```
You are given an array prices where prices[i] is the price of a given stock on the ith day.
```<p>The only downside for GPT-4 for me right now, is its slowness.</div><br/><div id="38091203" class="c"><input type="checkbox" id="c-38091203" checked=""/><div class="controls bullet"><span class="by">popularonion</span><span>|</span><a href="#38090391">parent</a><span>|</span><a href="#38090442">next</a><span>|</span><label class="collapse" for="c-38091203">[-]</label><label class="expand" for="c-38091203">[1 more]</label></div><br/><div class="children"><div class="content">GPT-4 has ingested all of Leetcode, you can literally just type &quot;leetcode 100 python&quot; and it will regurgitate a response for you.<p>Only exception I found is with some of the Leetcode Premium questions, you might have to actually type in the problem statement, but it&#x27;s still very likely that multiple solutions have been ingested from GitHub and elsewhere.</div><br/></div></div><div id="38090442" class="c"><input type="checkbox" id="c-38090442" checked=""/><div class="controls bullet"><span class="by">rushingcreek</span><span>|</span><a href="#38090391">parent</a><span>|</span><a href="#38091203">prev</a><span>|</span><a href="#38091537">next</a><span>|</span><label class="collapse" for="c-38090442">[-]</label><label class="expand" for="c-38090442">[6 more]</label></div><br/><div class="children"><div class="content">I suggest you try enabling &quot;Ignore search results&quot; from the model dropdown for these types of questions. The web results can be distracting for the model for Leetcode-type questions.</div><br/><div id="38090979" class="c"><input type="checkbox" id="c-38090979" checked=""/><div class="controls bullet"><span class="by">doctoboggan</span><span>|</span><a href="#38090391">root</a><span>|</span><a href="#38090442">parent</a><span>|</span><a href="#38091410">next</a><span>|</span><label class="collapse" for="c-38090979">[-]</label><label class="expand" for="c-38090979">[2 more]</label></div><br/><div class="children"><div class="content">I see you&#x27;ve had to suggest this a few times in this thread, and in my experience I would agree with the suggestion. I wonder if you can have a simple gpt model decide automatically when ignoring search results would improve the result and do it automatically.</div><br/><div id="38091555" class="c"><input type="checkbox" id="c-38091555" checked=""/><div class="controls bullet"><span class="by">rushingcreek</span><span>|</span><a href="#38090391">root</a><span>|</span><a href="#38090979">parent</a><span>|</span><a href="#38091410">next</a><span>|</span><label class="collapse" for="c-38091555">[-]</label><label class="expand" for="c-38091555">[1 more]</label></div><br/><div class="children"><div class="content">Interesting idea.</div><br/></div></div></div></div><div id="38091410" class="c"><input type="checkbox" id="c-38091410" checked=""/><div class="controls bullet"><span class="by">shazar</span><span>|</span><a href="#38090391">root</a><span>|</span><a href="#38090442">parent</a><span>|</span><a href="#38090979">prev</a><span>|</span><a href="#38091855">next</a><span>|</span><label class="collapse" for="c-38091410">[-]</label><label class="expand" for="c-38091410">[2 more]</label></div><br/><div class="children"><div class="content">I tried with that option enabled and now it can&#x27;t generate code at all. Here&#x27;s my prompt:<p>```
You are given an array prices where prices[i] is the price of a given stock on the ith day.<p>Find the maximum profit you can achieve. You may complete at most two transactions.<p>Note: You may not engage in multiple transactions simultaneously (i.e., you must sell the stock before you buy again).<p>Write Python code to solve this:
def maxProfit(self, prices: List[int]) -&gt; int:
```<p>Output:<p>```
It seems like you want to find the maximum profit that can be achieved by buying and selling stocks, with the constraint that you can only make at most two transactions. Is that correct?<p>Could you please provide some example input and output to help me better understand your requirements?
```<p>I also tried a more basic prompt, but the output is not what I&#x27;d consider good code.<p>Can you maybe share some examples where we can see how it exceeds GPT-4&#x27;s capabilities? Thanks!</div><br/><div id="38091543" class="c"><input type="checkbox" id="c-38091543" checked=""/><div class="controls bullet"><span class="by">rushingcreek</span><span>|</span><a href="#38090391">root</a><span>|</span><a href="#38091410">parent</a><span>|</span><a href="#38091855">next</a><span>|</span><label class="collapse" for="c-38091543">[-]</label><label class="expand" for="c-38091543">[1 more]</label></div><br/><div class="children"><div class="content">Seemed to work well just now: <a href="https:&#x2F;&#x2F;www.phind.com&#x2F;search?cache=w1jyatqyia1a8r3pfxlxqby6">https:&#x2F;&#x2F;www.phind.com&#x2F;search?cache=w1jyatqyia1a8r3pfxlxqby6</a></div><br/></div></div></div></div><div id="38091855" class="c"><input type="checkbox" id="c-38091855" checked=""/><div class="controls bullet"><span class="by">mediaman</span><span>|</span><a href="#38090391">root</a><span>|</span><a href="#38090442">parent</a><span>|</span><a href="#38091410">prev</a><span>|</span><a href="#38091537">next</a><span>|</span><label class="collapse" for="c-38091855">[-]</label><label class="expand" for="c-38091855">[1 more]</label></div><br/><div class="children"><div class="content">In my own RAG implementations in the industrial sector, I&#x27;ve found it effective to first have the AI decide whether it needs to search at all. If it doesn&#x27;t, the answers are much better.</div><br/></div></div></div></div></div></div><div id="38091537" class="c"><input type="checkbox" id="c-38091537" checked=""/><div class="controls bullet"><span class="by">notadev</span><span>|</span><a href="#38090391">prev</a><span>|</span><a href="#38091777">next</a><span>|</span><label class="collapse" for="c-38091537">[-]</label><label class="expand" for="c-38091537">[1 more]</label></div><br/><div class="children"><div class="content">I use Phind daily, including the VSCode extension, and I love it. Much better than anything ChatGPT is able to come up and the code it generates requires little-to-no modification to work properly. Very big fan!</div><br/></div></div><div id="38091777" class="c"><input type="checkbox" id="c-38091777" checked=""/><div class="controls bullet"><span class="by">schmorptron</span><span>|</span><a href="#38091537">prev</a><span>|</span><a href="#38091547">next</a><span>|</span><label class="collapse" for="c-38091777">[-]</label><label class="expand" for="c-38091777">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been a pretty heavy user of phind and have been very satisfied! Haven&#x27;t been using it to write code for me but to ask about features and docs and it&#x27;s been pretty incredible.</div><br/></div></div><div id="38091547" class="c"><input type="checkbox" id="c-38091547" checked=""/><div class="controls bullet"><span class="by">scarface_74</span><span>|</span><a href="#38091777">prev</a><span>|</span><a href="#38091600">next</a><span>|</span><label class="collapse" for="c-38091547">[-]</label><label class="expand" for="c-38091547">[1 more]</label></div><br/><div class="children"><div class="content">I was just discussing using ChatGPT to make working with deploying serverless code easier.<p>I gave this as an example<p>“create a CDK typescript app that deploys a lambda + API Gateway where the lambda works with Get request and a dynamodb table. The lambda should have permission to read and write to the Table”<p>It wrote the code perfectly.  I wanted to see if it was trained on the AWS APIs.</div><br/></div></div><div id="38091600" class="c"><input type="checkbox" id="c-38091600" checked=""/><div class="controls bullet"><span class="by">deegles</span><span>|</span><a href="#38091547">prev</a><span>|</span><label class="collapse" for="c-38091600">[-]</label><label class="expand" for="c-38091600">[1 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the best way to use an LLM with a large codebase that isn&#x27;t RAG? Ideally we could have the full source in the context or already trained into the model... I was thinking I could set something to fine tune a model overnight and every morning I&#x27;d have a fresh one ready. Any ideas?</div><br/></div></div></div></div></div></div></div></body></html>