<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1699434060476" as="style"/><link rel="stylesheet" href="styles.css?v=1699434060476"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.riverphillips.dev/blog/go-cfs/">Go, Containers, and the Linux Scheduler</a> <span class="domain">(<a href="https://www.riverphillips.dev">www.riverphillips.dev</a>)</span></div><div class="subtext"><span>rbanffy</span> | <span>104 comments</span></div><br/><div><div id="38187186" class="c"><input type="checkbox" id="c-38187186" checked=""/><div class="controls bullet"><span class="by">otterley</span><span>|</span><a href="#38182751">next</a><span>|</span><label class="collapse" for="c-38187186">[-]</label><label class="expand" for="c-38187186">[7 more]</label></div><br/><div class="children"><div class="content">This sort of tuning isn&#x27;t necessary if you use CPU reservations instead of limits, as you should: <a href="https:&#x2F;&#x2F;home.robusta.dev&#x2F;blog&#x2F;stop-using-cpu-limits" rel="nofollow noreferrer">https:&#x2F;&#x2F;home.robusta.dev&#x2F;blog&#x2F;stop-using-cpu-limits</a><p>CPU reservations <i>are</i> limits, just implicit ones and declared as guarantees.<p>So let the Go runtime use all the CPUs available, and let the Linux scheduler throttle according to your declared reservations if the CPU is contended for.</div><br/><div id="38187698" class="c"><input type="checkbox" id="c-38187698" checked=""/><div class="controls bullet"><span class="by">ithkuil</span><span>|</span><a href="#38187186">parent</a><span>|</span><a href="#38187302">next</a><span>|</span><label class="collapse" for="c-38187698">[-]</label><label class="expand" for="c-38187698">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t set limits because I&#x27;m afraid of how a pod is going to affect other pods. I set limits because I don&#x27;t want to get used to being able to tap on the excess CPU available because that&#x27;s not guaranteed to be available.<p>As the node fills up with more and more other pods, it&#x27;s possible that a pod that was running just fine a moment ago is crawling to a halt.<p>Limits allow me to simulate the same behavior and plan for it by doing the right capacity planning.<p>They are not the only way to approach it! But they are the simplest way to so it.</div><br/><div id="38187942" class="c"><input type="checkbox" id="c-38187942" checked=""/><div class="controls bullet"><span class="by">eloisant</span><span>|</span><a href="#38187186">root</a><span>|</span><a href="#38187698">parent</a><span>|</span><a href="#38187302">next</a><span>|</span><label class="collapse" for="c-38187942">[-]</label><label class="expand" for="c-38187942">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s why you use monitor and alerting, so you notice degraded performances before the pods is crawling to a halt.<p>You need to do it anyway because a service might progressively need more resources as it&#x27;s getting more traffic, even if you&#x27;re not adding any other pod.</div><br/></div></div></div></div><div id="38187302" class="c"><input type="checkbox" id="c-38187302" checked=""/><div class="controls bullet"><span class="by">yipbub</span><span>|</span><a href="#38187186">parent</a><span>|</span><a href="#38187698">prev</a><span>|</span><a href="#38187563">next</a><span>|</span><label class="collapse" for="c-38187302">[-]</label><label class="expand" for="c-38187302">[3 more]</label></div><br/><div class="children"><div class="content">Interesting. This is not true for Memory, correct? The OOMKiller might get you.<p>You also cannot achieve a QoS class of Guaranteed without both CPU and Memory limits, so the pod might be evicted at some point.</div><br/><div id="38187511" class="c"><input type="checkbox" id="c-38187511" checked=""/><div class="controls bullet"><span class="by">dilyevsky</span><span>|</span><a href="#38187186">root</a><span>|</span><a href="#38187302">parent</a><span>|</span><a href="#38187552">next</a><span>|</span><label class="collapse" for="c-38187511">[-]</label><label class="expand" for="c-38187511">[1 more]</label></div><br/><div class="children"><div class="content">Correct regarding memory - not true for memory because it&#x27;s non-fungible unlike CPU shares<p>&gt; You also cannot achieve a QoS class of Guaranteed without both CPU and Memory limits, so the pod might be evicted at some point.<p>Evicted due to node pressure - yes (but if all other pods also don&#x27;t have limits it doesn&#x27;t matter). For preemption QoS is not factored in the decision [0]<p>[0] - <a href="https:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;concepts&#x2F;scheduling-eviction&#x2F;pod-priority-preemption&#x2F;#interactions-of-pod-priority-and-qos" rel="nofollow noreferrer">https:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;concepts&#x2F;scheduling-eviction&#x2F;pod-...</a></div><br/></div></div><div id="38187552" class="c"><input type="checkbox" id="c-38187552" checked=""/><div class="controls bullet"><span class="by">iTokio</span><span>|</span><a href="#38187186">root</a><span>|</span><a href="#38187302">parent</a><span>|</span><a href="#38187511">prev</a><span>|</span><a href="#38187563">next</a><span>|</span><label class="collapse" for="c-38187552">[-]</label><label class="expand" for="c-38187552">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Memory is different because it is non-compressible - once you give memory you can&#x27;t take it away without killing the process</div><br/></div></div></div></div><div id="38187563" class="c"><input type="checkbox" id="c-38187563" checked=""/><div class="controls bullet"><span class="by">kubiton</span><span>|</span><a href="#38187186">parent</a><span>|</span><a href="#38187302">prev</a><span>|</span><a href="#38182751">next</a><span>|</span><label class="collapse" for="c-38187563">[-]</label><label class="expand" for="c-38187563">[1 more]</label></div><br/><div class="children"><div class="content">I run a few things on 128 core setups and I set CPU limits to much higher than request but still set them to make sure nothing runs ammok.<p>I would be curious to see this discussed but your article only states that people think you need limit to ensure CPU for all pods.</div><br/></div></div></div></div><div id="38182751" class="c"><input type="checkbox" id="c-38182751" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#38187186">prev</a><span>|</span><a href="#38182057">next</a><span>|</span><label class="collapse" for="c-38182751">[-]</label><label class="expand" for="c-38182751">[40 more]</label></div><br/><div class="children"><div class="content">The common problem I see across many languages is: applications detect machine cores by looking at &#x2F;proc&#x2F;cpuinfo.  However, in a docker container (or other container technology), that file looks the same as the container host (listing all cores, regardless of how few have been assigned to the container).<p>I wondered for a while if docker could make a fake &#x2F;proc&#x2F;cpuinfo that apps could parse that just listed &quot;docker cpus&quot; allocated to the job, but upon further reflection, that probably wouldn&#x27;t work for many reasons.</div><br/><div id="38182926" class="c"><input type="checkbox" id="c-38182926" checked=""/><div class="controls bullet"><span class="by">dharmab</span><span>|</span><a href="#38182751">parent</a><span>|</span><a href="#38185063">next</a><span>|</span><label class="collapse" for="c-38182926">[-]</label><label class="expand" for="c-38182926">[15 more]</label></div><br/><div class="children"><div class="content">Point of clarification: Containers, when using quota based limits, can use all of the CPU cores on the host. They&#x27;re limited in how much time they can spend using them.<p>(There are exceptions, such as documented here: <a href="https:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;tasks&#x2F;administer-cluster&#x2F;cpu-management-policies&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;tasks&#x2F;administer-cluster&#x2F;cpu-mana...</a>)</div><br/><div id="38183103" class="c"><input type="checkbox" id="c-38183103" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#38182751">root</a><span>|</span><a href="#38182926">parent</a><span>|</span><a href="#38185063">next</a><span>|</span><label class="collapse" for="c-38183103">[-]</label><label class="expand" for="c-38183103">[14 more]</label></div><br/><div class="children"><div class="content">Maybe I should be clearer:
Let&#x27;s say I have a 16 core host and I start a flask container with cpu=0.5 that forks and has a heavy post-fork initializer.<p>flask&#x2F;gunicorn will fork 16 processes (by reading &#x2F;proc&#x2F;cpuinfo and counting cores) all of which will try to share 0.5 cores worth of CPU power (maybe spread over many physical CPUs; I don&#x27;t really care about that).<p>I can solve this by passing a flag to my application; my complaint is more that apps shouldn&#x27;t consult &#x2F;proc&#x2F;cpuinfo, but have another standard interface to ask &quot;what should I set my max parallelism (NOT CONCURRENCY, ROB) so my worker threads get adequate CPU time so the framework doesn&#x27;t time out on startup.</div><br/><div id="38184174" class="c"><input type="checkbox" id="c-38184174" checked=""/><div class="controls bullet"><span class="by">Volundr</span><span>|</span><a href="#38182751">root</a><span>|</span><a href="#38183103">parent</a><span>|</span><a href="#38185824">next</a><span>|</span><label class="collapse" for="c-38184174">[-]</label><label class="expand" for="c-38184174">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not clear to me what the max parallelism should actually be on a container with a CPU limit of .5. To my understanding that limits CPU time the container can use within a certain time interval, but doesn&#x27;t actually limit the parallel processes an application can run. In other words that container with .5 on the CPU limit can indeed use all 16 physical cores of that machine. It&#x27;ll just burn through it&#x27;s budget 16x faster. If that&#x27;s desirable vs limiting itself to one process is going to be highly application dependent and not something kubernetes and docker can just tell you.</div><br/></div></div><div id="38185824" class="c"><input type="checkbox" id="c-38185824" checked=""/><div class="controls bullet"><span class="by">dharmab</span><span>|</span><a href="#38182751">root</a><span>|</span><a href="#38183103">parent</a><span>|</span><a href="#38184174">prev</a><span>|</span><a href="#38183445">next</a><span>|</span><label class="collapse" for="c-38185824">[-]</label><label class="expand" for="c-38185824">[1 more]</label></div><br/><div class="children"><div class="content">That interface partly exists. It&#x27;s &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;(cgroup here)&#x2F;cpu.max<p>I know the JVM automatically uses it, and there&#x27;s a popular library for Go that sets GONAXPROCS using it.</div><br/></div></div><div id="38183445" class="c"><input type="checkbox" id="c-38183445" checked=""/><div class="controls bullet"><span class="by">status_quo69</span><span>|</span><a href="#38182751">root</a><span>|</span><a href="#38183103">parent</a><span>|</span><a href="#38185824">prev</a><span>|</span><a href="#38184406">next</a><span>|</span><label class="collapse" for="c-38183445">[-]</label><label class="expand" for="c-38183445">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;65551215&#x2F;get-docker-cpu-memory-limit-inside-container" rel="nofollow noreferrer">https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;65551215&#x2F;get-docker-cpu-...</a><p>Been a bit but I do believe that dotnet does this exact behavior. Sounds like gunicorn needs a pr to mimic, if they want to replicate this.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;dotnet&#x2F;runtime&#x2F;issues&#x2F;8485">https:&#x2F;&#x2F;github.com&#x2F;dotnet&#x2F;runtime&#x2F;issues&#x2F;8485</a></div><br/></div></div><div id="38184406" class="c"><input type="checkbox" id="c-38184406" checked=""/><div class="controls bullet"><span class="by">vbezhenar</span><span>|</span><a href="#38182751">root</a><span>|</span><a href="#38183103">parent</a><span>|</span><a href="#38183445">prev</a><span>|</span><a href="#38185108">next</a><span>|</span><label class="collapse" for="c-38184406">[-]</label><label class="expand" for="c-38184406">[9 more]</label></div><br/><div class="children"><div class="content">You generally shouldn&#x27;t set CPU limits. You might want to configure CPU requests which is guaranteed chunk of CPU time that container will always receive. With CPU limits you&#x27;ll encounter situation when host CPU is not loaded, but your container workloaded is throttled at the same time, which is just waste of CPU resources.</div><br/><div id="38185575" class="c"><input type="checkbox" id="c-38185575" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#38182751">root</a><span>|</span><a href="#38184406">parent</a><span>|</span><a href="#38184815">next</a><span>|</span><label class="collapse" for="c-38185575">[-]</label><label class="expand" for="c-38185575">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s complicated.  I&#x27;ve worked on every kind of application in a container environment: ones that ran at ultra-low priority while declaring zero CPU <i>request</i> and infinite CPU <i>limit</i>.  I ran one or a few of these on nearly every machine in Google production for over a year, and could deliver over 1M xeon cores worth of throughput for embarassingly parallel jobs.  At other times, I ran jobs that asked for and used precisely all the cores on a machine (a TPU host), specifically setting limits and requests to get the most predictable behavior.<p>The true objective function I&#x27;m trying to optimize isn&#x27;t just &quot;save money&quot; or &quot;don&#x27;t waste CPU resources&quot;, but rather &quot;get a million different workloads to run smoothly on a large collection of resources, ensuring that revenue-critical jobs always can run, while any spare capacity is available for experimenters, up to some predefined limits determined by power capacity, staying within the overall budget, and not pissing off any really powerful users.&quot; (well, that&#x27;s really just a simplified approximation)</div><br/><div id="38185827" class="c"><input type="checkbox" id="c-38185827" checked=""/><div class="controls bullet"><span class="by">jeffbee</span><span>|</span><a href="#38182751">root</a><span>|</span><a href="#38185575">parent</a><span>|</span><a href="#38184815">next</a><span>|</span><label class="collapse" for="c-38185827">[-]</label><label class="expand" for="c-38185827">[2 more]</label></div><br/><div class="children"><div class="content">The problem is your experience involves a hacked up Linux that was far more suitable for doing this than is the upstream. Upstream scheduler can&#x27;t really deal with running a box hot with mixed batch and latency-sensitive workloads and intentionally abusive ones like yours ;-) That is partly why kubernetes doesn&#x27;t even really try.</div><br/><div id="38186935" class="c"><input type="checkbox" id="c-38186935" checked=""/><div class="controls bullet"><span class="by">dilyevsky</span><span>|</span><a href="#38182751">root</a><span>|</span><a href="#38185827">parent</a><span>|</span><a href="#38184815">next</a><span>|</span><label class="collapse" for="c-38186935">[-]</label><label class="expand" for="c-38186935">[1 more]</label></div><br/><div class="children"><div class="content">This. Some googlers forget there is a whole team of kernel devs in TI that are maintaining patched kernel (including patched CFS) specifically for Borg</div><br/></div></div></div></div></div></div><div id="38184815" class="c"><input type="checkbox" id="c-38184815" checked=""/><div class="controls bullet"><span class="by">lovasoa</span><span>|</span><a href="#38182751">root</a><span>|</span><a href="#38184406">parent</a><span>|</span><a href="#38185575">prev</a><span>|</span><a href="#38185108">next</a><span>|</span><label class="collapse" for="c-38184815">[-]</label><label class="expand" for="c-38184815">[5 more]</label></div><br/><div class="children"><div class="content">According to the article, this is not true. The limits become active only when the host cpu is under pressure.</div><br/><div id="38185059" class="c"><input type="checkbox" id="c-38185059" checked=""/><div class="controls bullet"><span class="by">vbezhenar</span><span>|</span><a href="#38182751">root</a><span>|</span><a href="#38184815">parent</a><span>|</span><a href="#38185108">next</a><span>|</span><label class="collapse" for="c-38185059">[-]</label><label class="expand" for="c-38185059">[4 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think that&#x27;s correct. --cpus is the same as --cpu-period which is cpu limit. You can easily check it yourself, just run docker container with --cpus set, run multi-core load there and check your activity monitor.</div><br/><div id="38185802" class="c"><input type="checkbox" id="c-38185802" checked=""/><div class="controls bullet"><span class="by">pbh101</span><span>|</span><a href="#38182751">root</a><span>|</span><a href="#38185059">parent</a><span>|</span><a href="#38185108">next</a><span>|</span><label class="collapse" for="c-38185802">[-]</label><label class="expand" for="c-38185802">[3 more]</label></div><br/><div class="children"><div class="content">CFS quotas only become active under contention and even then are relative: if you’re the only thing running on the box and want all the cores but only set one cpu, you get all of them anyway.<p>If you set cpus to 2 and another process sets to 1 and you both try to use all CPUs all out, you’ll get 66% and they’ll get 33%.<p>This isn’t the same as cpusets, which work differently.</div><br/><div id="38186383" class="c"><input type="checkbox" id="c-38186383" checked=""/><div class="controls bullet"><span class="by">ecnahc515</span><span>|</span><a href="#38182751">root</a><span>|</span><a href="#38185802">parent</a><span>|</span><a href="#38185108">next</a><span>|</span><label class="collapse" for="c-38186383">[-]</label><label class="expand" for="c-38186383">[2 more]</label></div><br/><div class="children"><div class="content">&gt; CFS quotas only become active under contention<p>That&#x27;s not true at all. Take a look at `cpu.cfs_quota_us` in <a href="https:&#x2F;&#x2F;kernel.googlesource.com&#x2F;pub&#x2F;scm&#x2F;linux&#x2F;kernel&#x2F;git&#x2F;glommer&#x2F;memcg&#x2F;+&#x2F;cpu_stat&#x2F;Documentation&#x2F;cgroups&#x2F;cpu.txt" rel="nofollow noreferrer">https:&#x2F;&#x2F;kernel.googlesource.com&#x2F;pub&#x2F;scm&#x2F;linux&#x2F;kernel&#x2F;git&#x2F;glo...</a><p>It&#x27;s a hard time limit. It doesn&#x27;t care about contention at all.<p>`cpu.shares` is relative, for choosing which process gets scheduled, and how often, but the CFS quota is a hard limit on runtime.</div><br/><div id="38187034" class="c"><input type="checkbox" id="c-38187034" checked=""/><div class="controls bullet"><span class="by">usr1106</span><span>|</span><a href="#38182751">root</a><span>|</span><a href="#38186383">parent</a><span>|</span><a href="#38185108">next</a><span>|</span><label class="collapse" for="c-38187034">[-]</label><label class="expand" for="c-38187034">[1 more]</label></div><br/><div class="children"><div class="content">Yes, there are hard limits in the CFS. I have used them for thermal reasons in the past, such that the system remained mostly idle although some threads would have had more work to do.<p>Not at my work environment right now, don&#x27;t remember the parameters I used.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="38185108" class="c"><input type="checkbox" id="c-38185108" checked=""/><div class="controls bullet"><span class="by">wutwutwat</span><span>|</span><a href="#38182751">root</a><span>|</span><a href="#38183103">parent</a><span>|</span><a href="#38184406">prev</a><span>|</span><a href="#38185063">next</a><span>|</span><label class="collapse" for="c-38185108">[-]</label><label class="expand" for="c-38185108">[1 more]</label></div><br/><div class="children"><div class="content">`gunicorn --workers $(nproc)`, see my comment on the parent</div><br/></div></div></div></div></div></div><div id="38185063" class="c"><input type="checkbox" id="c-38185063" checked=""/><div class="controls bullet"><span class="by">wutwutwat</span><span>|</span><a href="#38182751">parent</a><span>|</span><a href="#38182926">prev</a><span>|</span><a href="#38183006">next</a><span>|</span><label class="collapse" for="c-38185063">[-]</label><label class="expand" for="c-38185063">[3 more]</label></div><br/><div class="children"><div class="content">I only use `nproc` and see it used in other containers as well, ie `bundle install -j $(nproc)`. This honors cpu assignment and provides the functionality you&#x27;re seeking. Whether or not random application software uses nproc if available, idk<p>&gt; Print the number of processing units available to the current process, which may be less than the number of online processors. If this information is not accessible, then print the number of processors installed<p><a href="https:&#x2F;&#x2F;www.gnu.org&#x2F;software&#x2F;coreutils&#x2F;manual&#x2F;html_node&#x2F;nproc-invocation.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.gnu.org&#x2F;software&#x2F;coreutils&#x2F;manual&#x2F;html_node&#x2F;npro...</a><p><a href="https:&#x2F;&#x2F;www.flamingspork.com&#x2F;blog&#x2F;2020&#x2F;11&#x2F;25&#x2F;why-you-should-use-nproc-and-not-grep-proc-cpuinfo&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.flamingspork.com&#x2F;blog&#x2F;2020&#x2F;11&#x2F;25&#x2F;why-you-should-...</a></div><br/><div id="38185277" class="c"><input type="checkbox" id="c-38185277" checked=""/><div class="controls bullet"><span class="by">telotortium</span><span>|</span><a href="#38182751">root</a><span>|</span><a href="#38185063">parent</a><span>|</span><a href="#38183006">next</a><span>|</span><label class="collapse" for="c-38185277">[-]</label><label class="expand" for="c-38185277">[2 more]</label></div><br/><div class="children"><div class="content">This is not very robust. You probably should use the cgroup cpu limits where present, since `docker --cpus` uses a different way to set quota:<p><pre><code>    if [[ -e &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;cpu&#x2F;cpu.cfs_quota_us ]] &amp;&amp; [[ -e &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;cpu&#x2F;cpu.cfs_period_us ]]; then
        GOMAXPROCS=$(perl -e &#x27;use POSIX; printf &quot;%d\n&quot;, ceil($ARGV[0] &#x2F; $ARGV[1])&#x27; &quot;$(cat &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;cpu&#x2F;cpu.cfs_quota_us)&quot; &quot;$(cat &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;cpu&#x2F;cpu.cfs_period_us)&quot;)
    else
        GOMAXPROCS=$(nproc)
    fi
    export GOMAXPROCS
</code></pre>
This follows from how `docker --cpus` works (<a href="https:&#x2F;&#x2F;docs.docker.com&#x2F;config&#x2F;containers&#x2F;resource_constraints&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;docs.docker.com&#x2F;config&#x2F;containers&#x2F;resource_constrain...</a>), as well as <a href="https:&#x2F;&#x2F;stackoverflow.com&#x2F;a&#x2F;65554131&#x2F;207384" rel="nofollow noreferrer">https:&#x2F;&#x2F;stackoverflow.com&#x2F;a&#x2F;65554131&#x2F;207384</a> to get the &#x2F;sys paths to read from.<p>Or use <a href="https:&#x2F;&#x2F;github.com&#x2F;uber-go&#x2F;automaxprocs">https:&#x2F;&#x2F;github.com&#x2F;uber-go&#x2F;automaxprocs</a>, which is very comprehensive, but is a bunch of code for what should be a simple task.</div><br/><div id="38186603" class="c"><input type="checkbox" id="c-38186603" checked=""/><div class="controls bullet"><span class="by">fn-mote</span><span>|</span><a href="#38182751">root</a><span>|</span><a href="#38185277">parent</a><span>|</span><a href="#38183006">next</a><span>|</span><label class="collapse" for="c-38186603">[-]</label><label class="expand" for="c-38186603">[1 more]</label></div><br/><div class="children"><div class="content">A shell script that invokes perl to set an environment variable used by Go. Some days I feel like there is a lot of duct tape involved in these applications.</div><br/></div></div></div></div></div></div><div id="38183006" class="c"><input type="checkbox" id="c-38183006" checked=""/><div class="controls bullet"><span class="by">jeffbee</span><span>|</span><a href="#38182751">parent</a><span>|</span><a href="#38185063">prev</a><span>|</span><a href="#38185196">next</a><span>|</span><label class="collapse" for="c-38183006">[-]</label><label class="expand" for="c-38183006">[19 more]</label></div><br/><div class="children"><div class="content">That&#x27;s not what Go does though. Go looks at the population of the CPU mask at startup. It never looks again, which of problematic in K8s where the visible CPUs may change while your process runs.</div><br/><div id="38184713" class="c"><input type="checkbox" id="c-38184713" checked=""/><div class="controls bullet"><span class="by">sethammons</span><span>|</span><a href="#38182751">root</a><span>|</span><a href="#38183006">parent</a><span>|</span><a href="#38184958">next</a><span>|</span><label class="collapse" for="c-38184713">[-]</label><label class="expand" for="c-38184713">[2 more]</label></div><br/><div class="children"><div class="content">We use <a href="https:&#x2F;&#x2F;github.com&#x2F;uber-go&#x2F;automaxprocs">https:&#x2F;&#x2F;github.com&#x2F;uber-go&#x2F;automaxprocs</a> after we joyfully discovered that Go assumed we had the entire cluster&#x27;s cpu count on any particular pod. Made for some very strange performance characteristics in scheduling goroutines.</div><br/><div id="38185372" class="c"><input type="checkbox" id="c-38185372" checked=""/><div class="controls bullet"><span class="by">jeffbee</span><span>|</span><a href="#38182751">root</a><span>|</span><a href="#38184713">parent</a><span>|</span><a href="#38184958">next</a><span>|</span><label class="collapse" for="c-38185372">[-]</label><label class="expand" for="c-38185372">[1 more]</label></div><br/><div class="children"><div class="content">My opinion is that setting GOMAXPROCS that way is a quite poor idea. It tends to strand resources that <i>could</i> have been used to handle a stochastic burst of requests, which with a capped GOMAXPROCS will be converted directly into latency. I can think of no good reason why GOMAXPROCS needs to be 2 just because you expect the long-term CPU rate to be 2. That long-term quota is an artifact of capacity planning, while GOMAXPROCS is an artifact of process architecture.</div><br/></div></div></div></div><div id="38184958" class="c"><input type="checkbox" id="c-38184958" checked=""/><div class="controls bullet"><span class="by">n3t</span><span>|</span><a href="#38182751">root</a><span>|</span><a href="#38183006">parent</a><span>|</span><a href="#38184713">prev</a><span>|</span><a href="#38183070">next</a><span>|</span><label class="collapse" for="c-38184958">[-]</label><label class="expand" for="c-38184958">[8 more]</label></div><br/><div class="children"><div class="content">&gt; which of problematic in K8s where the visible CPUs may change while your process runs<p>This is new to me. What is this… behavior? What keywords should I use to find any details about it?<p>The only thing that rings a bell is requests&#x2F;limit parameters of a pod but you can&#x27;t change them on an existing pod AFAIK.</div><br/><div id="38185489" class="c"><input type="checkbox" id="c-38185489" checked=""/><div class="controls bullet"><span class="by">djbusby</span><span>|</span><a href="#38182751">root</a><span>|</span><a href="#38184958">parent</a><span>|</span><a href="#38185392">next</a><span>|</span><label class="collapse" for="c-38185489">[-]</label><label class="expand" for="c-38185489">[1 more]</label></div><br/><div class="children"><div class="content">Even way back in the day (1996) it was possible to hot-swap a CPU. Used to have this Sequent box, 96 Pentiums in there, 6 on a card. Could do some magic, pull the card and swap a new one in. Wild. And no processes died. Not sure if a process could lose a CPU then discover the new set.</div><br/></div></div><div id="38185392" class="c"><input type="checkbox" id="c-38185392" checked=""/><div class="controls bullet"><span class="by">jeffbee</span><span>|</span><a href="#38182751">root</a><span>|</span><a href="#38184958">parent</a><span>|</span><a href="#38185489">prev</a><span>|</span><a href="#38183070">next</a><span>|</span><label class="collapse" for="c-38185392">[-]</label><label class="expand" for="c-38185392">[6 more]</label></div><br/><div class="children"><div class="content">If you have one pod that has Burstable QoS, perhaps because it has a request and not a limit, its CPU mask will be populated by every CPU on the box, less one for the Kubelet and other node services, less all the CPUs requested by pods with Guaranteed QoS. Pods with Guaranteed QoS will have exactly the number of CPUs they asked for, no more or less, and consequently their GOMAXPROCS is consistent. Everyone else will see fewer or more CPUs as Guaranteed pods arrive and depart from the node.</div><br/><div id="38186564" class="c"><input type="checkbox" id="c-38186564" checked=""/><div class="controls bullet"><span class="by">n3t</span><span>|</span><a href="#38182751">root</a><span>|</span><a href="#38185392">parent</a><span>|</span><a href="#38183070">next</a><span>|</span><label class="collapse" for="c-38186564">[-]</label><label class="expand" for="c-38186564">[5 more]</label></div><br/><div class="children"><div class="content">If by &quot;CPU mask&quot; you refer to the `sched_getaffinity` syscall, I can&#x27;t reproduce this behavior.<p>What I tried: I created a &quot;Burstable&quot; Pod and run `nproc` [0] on it. It returned N CPUs (N &gt; 1).<p>Then I created a &quot;Guaranteed QoS&quot; Pod with both requests and limit set to 1 CPU. `nproc` returned N CPUs on it.<p>I went back to the &quot;Burstable&quot; Pod. It returned N.<p>I created a fresh &quot;Burstable&quot; Pod and run `nproc` on it, got N again. Please note that the &quot;Guaranteed QoS&quot; Pod is still running.<p>&gt; Pods with Guaranteed QoS will have exactly the number of CPUs they asked for, no more or less<p>Well, in my case I asked for 1 CPU and got more, i.e. N CPUs.<p>Also, please note that Pods might ask for fractional CPUs.<p>[0]: coreutils `nproc` program uses `sched_getaffinity` syscall under the hood, at least on my system. I&#x27;ve just checked it with `strace` to be sure.</div><br/><div id="38186615" class="c"><input type="checkbox" id="c-38186615" checked=""/><div class="controls bullet"><span class="by">jeffbee</span><span>|</span><a href="#38182751">root</a><span>|</span><a href="#38186564">parent</a><span>|</span><a href="#38183070">next</a><span>|</span><label class="collapse" for="c-38186615">[-]</label><label class="expand" for="c-38186615">[4 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know what nproc does. Consider `taskset`</div><br/><div id="38186656" class="c"><input type="checkbox" id="c-38186656" checked=""/><div class="controls bullet"><span class="by">n3t</span><span>|</span><a href="#38182751">root</a><span>|</span><a href="#38186615">parent</a><span>|</span><a href="#38183070">next</a><span>|</span><label class="collapse" for="c-38186656">[-]</label><label class="expand" for="c-38186656">[3 more]</label></div><br/><div class="children"><div class="content">I re-did the experiment again with `taskset` and got the same results, i.e. the mask is independent of creation of the &quot;Guaranteed QoS&quot; Pod.<p>FWIW, `taskset` uses the same syscall as `nproc` (according to `strace`).</div><br/><div id="38186686" class="c"><input type="checkbox" id="c-38186686" checked=""/><div class="controls bullet"><span class="by">jeffbee</span><span>|</span><a href="#38182751">root</a><span>|</span><a href="#38186656">parent</a><span>|</span><a href="#38183070">next</a><span>|</span><label class="collapse" for="c-38186686">[-]</label><label class="expand" for="c-38186686">[2 more]</label></div><br/><div class="children"><div class="content">Perhaps it is an artifact of your and my various container runtimes. For me, in a guaranteed qos pod, taskset shows just 1 visible CPU for a Guaranteed QoS pod with limit=request=1.<p><pre><code>  # taskset -c -p 1
  pid 1&#x27;s current affinity list: 1

  # nproc
  1
</code></pre>
I honestly do not see how it can work otherwise.</div><br/><div id="38186798" class="c"><input type="checkbox" id="c-38186798" checked=""/><div class="controls bullet"><span class="by">n3t</span><span>|</span><a href="#38182751">root</a><span>|</span><a href="#38186686">parent</a><span>|</span><a href="#38183070">next</a><span>|</span><label class="collapse" for="c-38186798">[-]</label><label class="expand" for="c-38186798">[1 more]</label></div><br/><div class="children"><div class="content">After reading <a href="https:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;tasks&#x2F;administer-cluster&#x2F;cpu-management-policies&#x2F;#cpu-management-policies" rel="nofollow noreferrer">https:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;tasks&#x2F;administer-cluster&#x2F;cpu-mana...</a>, I think we have different policies set for the CPU Manager.<p>In my case it&#x27;s `&quot;cpuManagerPolicy&quot;: &quot;none&quot;` and I suppose you&#x27;re using `&quot;static&quot;` policy.<p>Well, TIL. Thanks!</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="38183070" class="c"><input type="checkbox" id="c-38183070" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#38182751">root</a><span>|</span><a href="#38183006">parent</a><span>|</span><a href="#38184958">prev</a><span>|</span><a href="#38185196">next</a><span>|</span><label class="collapse" for="c-38183070">[-]</label><label class="expand" for="c-38183070">[8 more]</label></div><br/><div class="children"><div class="content">What is the population of the CPU mask at startup?
Is this a kernel call?  A &#x2F;proc file?  Some register?</div><br/><div id="38183092" class="c"><input type="checkbox" id="c-38183092" checked=""/><div class="controls bullet"><span class="by">EdSchouten</span><span>|</span><a href="#38182751">root</a><span>|</span><a href="#38183070">parent</a><span>|</span><a href="#38185196">next</a><span>|</span><label class="collapse" for="c-38183092">[-]</label><label class="expand" for="c-38183092">[7 more]</label></div><br/><div class="children"><div class="content">On Linux, it likely calls sched_getaffinity().</div><br/><div id="38183122" class="c"><input type="checkbox" id="c-38183122" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#38182751">root</a><span>|</span><a href="#38183092">parent</a><span>|</span><a href="#38185196">next</a><span>|</span><label class="collapse" for="c-38183122">[-]</label><label class="expand" for="c-38183122">[6 more]</label></div><br/><div class="children"><div class="content">hmm, I can see that as being useful but I also don&#x27;t see that as the way to determine &quot;how many worker threads I should start&quot;</div><br/><div id="38183174" class="c"><input type="checkbox" id="c-38183174" checked=""/><div class="controls bullet"><span class="by">jeffbee</span><span>|</span><a href="#38182751">root</a><span>|</span><a href="#38183122">parent</a><span>|</span><a href="#38185196">next</a><span>|</span><label class="collapse" for="c-38183174">[-]</label><label class="expand" for="c-38183174">[5 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not a bad way to guess, up to maybe 16 or so. Most Go server programs aren&#x27;t going to just scale up forever, so having 188 threads might be a waste.<p>Just setting it to 16 will satisfy 99% of users.</div><br/><div id="38183245" class="c"><input type="checkbox" id="c-38183245" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#38182751">root</a><span>|</span><a href="#38183174">parent</a><span>|</span><a href="#38185196">next</a><span>|</span><label class="collapse" for="c-38183245">[-]</label><label class="expand" for="c-38183245">[4 more]</label></div><br/><div class="children"><div class="content">There&#x27;s going to be a bunch of missing info, though, in some cases I can think of.  For example, more and more systems have asymmetric cores.  &#x2F;proc&#x2F;cpuinfo can expose that information in detail, including (current) clock speed, processor type, etc, while cpu_set is literally just a bitmask (if I read the man pages right) of system cores your process is allowed to schedule on.<p>Fundamentally, intelligent apps need to interrogate their environment to make concurrency decisions.  But I agree- Go would probably work best if it just picked a standard parallelism constant like 16 and just let users know that can be tuned if they have additional context.</div><br/><div id="38183510" class="c"><input type="checkbox" id="c-38183510" checked=""/><div class="controls bullet"><span class="by">jeffbee</span><span>|</span><a href="#38182751">root</a><span>|</span><a href="#38183245">parent</a><span>|</span><a href="#38185196">next</a><span>|</span><label class="collapse" for="c-38183510">[-]</label><label class="expand" for="c-38183510">[3 more]</label></div><br/><div class="children"><div class="content">Yes, running on a set of heterogenous CPUs presents further challenges, for the program and the thread scheduler. Happily there are no such systems in the cloud, yet.<p>Most people are running on systems where the CPU capacity varies and they haven&#x27;t even noticed. For example in EC2 there are 8 victim CPUs that handle all the network interrupts, so if you have an instance type with 32 CPUs, you already have 24 that are faster than the others. Practically nobody even notices this effect.</div><br/><div id="38184679" class="c"><input type="checkbox" id="c-38184679" checked=""/><div class="controls bullet"><span class="by">loxias</span><span>|</span><a href="#38182751">root</a><span>|</span><a href="#38183510">parent</a><span>|</span><a href="#38185196">next</a><span>|</span><label class="collapse" for="c-38184679">[-]</label><label class="expand" for="c-38184679">[2 more]</label></div><br/><div class="children"><div class="content">&gt; in EC2 there are 8 victim CPUs that handle all the network interrupts, so if you have an instance type with 32 CPUs, you already have 24 that are faster than the others<p>Fascinating.  Could you share any (all) more detail on this that you know?  Is it a specific instance type, only ones that use nitro?  (or only ones without?)   This might be related to a problem I&#x27;ve seen in the wild but never tracked down...</div><br/><div id="38185471" class="c"><input type="checkbox" id="c-38185471" checked=""/><div class="controls bullet"><span class="by">jeffbee</span><span>|</span><a href="#38182751">root</a><span>|</span><a href="#38184679">parent</a><span>|</span><a href="#38185196">next</a><span>|</span><label class="collapse" for="c-38185471">[-]</label><label class="expand" for="c-38185471">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve only observed it on Nitro, but I have also rarely used pre-Nitro instances.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="38185196" class="c"><input type="checkbox" id="c-38185196" checked=""/><div class="controls bullet"><span class="by">rrdharan</span><span>|</span><a href="#38182751">parent</a><span>|</span><a href="#38183006">prev</a><span>|</span><a href="#38182057">next</a><span>|</span><label class="collapse" for="c-38185196">[-]</label><label class="expand" for="c-38185196">[2 more]</label></div><br/><div class="children"><div class="content">Containers are a crappy abstraction and VMware fumbled the bag, is my takeaway from this comment…</div><br/><div id="38187713" class="c"><input type="checkbox" id="c-38187713" checked=""/><div class="controls bullet"><span class="by">sofixa</span><span>|</span><a href="#38182751">root</a><span>|</span><a href="#38185196">parent</a><span>|</span><a href="#38182057">next</a><span>|</span><label class="collapse" for="c-38187713">[-]</label><label class="expand" for="c-38187713">[1 more]</label></div><br/><div class="children"><div class="content">&gt; VMware fumbled the bag<p>Oh they did, they&#x27;re a modern day IBM.<p>&gt; Containers are a crappy abstraction<p>They&#x27;re one of the best abstractions we have (so far) because they contain only the application and what it needs.</div><br/></div></div></div></div></div></div><div id="38182057" class="c"><input type="checkbox" id="c-38182057" checked=""/><div class="controls bullet"><span class="by">dilyevsky</span><span>|</span><a href="#38182751">prev</a><span>|</span><a href="#38181807">next</a><span>|</span><label class="collapse" for="c-38182057">[-]</label><label class="expand" for="c-38182057">[18 more]</label></div><br/><div class="children"><div class="content">This is subtly incorrect - as far as Docker is concerned CFS cgroup extension has several knobs to tune - cfs_quota_us, cfs_period_us (typical default is 100ms not a second) and shares. When you set shares you get weighted proportional scheduling (but only when there&#x27;s contention). The former two enforce strict quota. Don&#x27;t use Docker&#x27;s --cpu flag and instead use --cpu-shares to avoid (mostly useless) quota enforcement.<p>From Linux docs:<p><pre><code>  - cpu.shares: The weight of each group living in the same hierarchy, that
    translates into the amount of CPU it is expected to get. Upon cgroup creation,
    each group gets assigned a default of 1024. The percentage of CPU assigned to
    the cgroup is the value of shares divided by the sum of all shares in all
    cgroups in the same level.
  - cpu.cfs_period_us: The duration in microseconds of each scheduler period, for
    bandwidth decisions. This defaults to 100000us or 100ms. Larger periods will
    improve throughput at the expense of latency, since the scheduler will be able
    to sustain a cpu-bound workload for longer. The opposite of true for smaller
    periods. Note that this only affects non-RT tasks that are scheduled by the
    CFS scheduler.
  - cpu.cfs_quota_us: The maximum time in microseconds during each cfs_period_us
    in for the current group will be allowed to run. For instance, if it is set to
    half of cpu_period_us, the cgroup will only be able to peak run for 50 % of
    the time. One should note that this represents aggregate time over all CPUs
    in the system. Therefore, in order to allow full usage of two CPUs, for
    instance, one should set this value to twice the value of cfs_period_us.</code></pre></div><br/><div id="38182754" class="c"><input type="checkbox" id="c-38182754" checked=""/><div class="controls bullet"><span class="by">cpuguy83</span><span>|</span><a href="#38182057">parent</a><span>|</span><a href="#38182546">next</a><span>|</span><label class="collapse" for="c-38182754">[-]</label><label class="expand" for="c-38182754">[5 more]</label></div><br/><div class="children"><div class="content">&gt; &quot;Don&#x27;t use Docker&#x27;s --cpu flag and instead use&quot;<p>This is rather strong language without any real qualifiers. It is definitely not &quot;mostly useless&quot;.
Shares and quotas are for different use-cases, that&#x27;s all.
Understand your use-case and choose accordingly.</div><br/><div id="38183660" class="c"><input type="checkbox" id="c-38183660" checked=""/><div class="controls bullet"><span class="by">dilyevsky</span><span>|</span><a href="#38182057">root</a><span>|</span><a href="#38182754">parent</a><span>|</span><a href="#38182546">next</a><span>|</span><label class="collapse" for="c-38183660">[-]</label><label class="expand" for="c-38183660">[4 more]</label></div><br/><div class="children"><div class="content">It doesn’t make any sense to me why —cpu flag is tweaking quota and not shares since quota is useful in tiny minority of usecases. A lot of people waste a ton of time debugging weird latency issues as a result of this decision</div><br/><div id="38184146" class="c"><input type="checkbox" id="c-38184146" checked=""/><div class="controls bullet"><span class="by">the8472</span><span>|</span><a href="#38182057">root</a><span>|</span><a href="#38183660">parent</a><span>|</span><a href="#38185539">next</a><span>|</span><label class="collapse" for="c-38184146">[-]</label><label class="expand" for="c-38184146">[2 more]</label></div><br/><div class="children"><div class="content">With shares you&#x27;re going to experience worse latency if all the containers on the system size their thread pool to the maximum that&#x27;s available during idle periods and then constantly context-switch due to oversubscription under load.
With quotas you can do fixed resource allocation and the runtimes (not Go apparently) can fit themselves into that and not try to service more requests than they can currently execute given those resources.</div><br/><div id="38185453" class="c"><input type="checkbox" id="c-38185453" checked=""/><div class="controls bullet"><span class="by">dilyevsky</span><span>|</span><a href="#38182057">root</a><span>|</span><a href="#38184146">parent</a><span>|</span><a href="#38185539">next</a><span>|</span><label class="collapse" for="c-38185453">[-]</label><label class="expand" for="c-38185453">[1 more]</label></div><br/><div class="children"><div class="content">And how is that different from worse latency due to cpu throttling from your users’ perspective?</div><br/></div></div></div></div><div id="38185539" class="c"><input type="checkbox" id="c-38185539" checked=""/><div class="controls bullet"><span class="by">cpuguy83</span><span>|</span><a href="#38182057">root</a><span>|</span><a href="#38183660">parent</a><span>|</span><a href="#38184146">prev</a><span>|</span><a href="#38182546">next</a><span>|</span><label class="collapse" for="c-38185539">[-]</label><label class="expand" for="c-38185539">[1 more]</label></div><br/><div class="children"><div class="content">These two options are not mutually exclusive.<p>When you want to limit the max CPU time available to a container use quotas (--cpus).
When you want to set relative priorities (compared to other containers&#x2F;processes), use shares.<p>These two options can be combined, it all depends on what you need.</div><br/></div></div></div></div></div></div><div id="38182546" class="c"><input type="checkbox" id="c-38182546" checked=""/><div class="controls bullet"><span class="by">mratsim</span><span>|</span><a href="#38182057">parent</a><span>|</span><a href="#38182754">prev</a><span>|</span><a href="#38182479">next</a><span>|</span><label class="collapse" for="c-38182546">[-]</label><label class="expand" for="c-38182546">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Don&#x27;t use Docker&#x27;s --cpu flag and instead use --cpu-shares to avoid (mostly useless) quota enforcement.<p>One caveat is that an application can detect when --cpu is used as I think it&#x27;s using cpuset. When quota are used it cannot detect and more threads than necessary will likely be spawned</div><br/><div id="38182676" class="c"><input type="checkbox" id="c-38182676" checked=""/><div class="controls bullet"><span class="by">cpuguy83</span><span>|</span><a href="#38182057">root</a><span>|</span><a href="#38182546">parent</a><span>|</span><a href="#38182804">next</a><span>|</span><label class="collapse" for="c-38182676">[-]</label><label class="expand" for="c-38182676">[1 more]</label></div><br/><div class="children"><div class="content">It is not using cpuset (there is a separate flag for this).
--cpus tweaks the cfs quota based on the number of cpus on the system and the requested amount.</div><br/></div></div><div id="38182804" class="c"><input type="checkbox" id="c-38182804" checked=""/><div class="controls bullet"><span class="by">dilyevsky</span><span>|</span><a href="#38182057">root</a><span>|</span><a href="#38182546">parent</a><span>|</span><a href="#38182676">prev</a><span>|</span><a href="#38182479">next</a><span>|</span><label class="collapse" for="c-38182804">[-]</label><label class="expand" for="c-38182804">[1 more]</label></div><br/><div class="children"><div class="content">—cpu sets the quota, there is is a —cpuset-cpu flag for cpuset and you can detect both by looking at the &#x2F;sys&#x2F;fs&#x2F;cgroup</div><br/></div></div></div></div><div id="38182479" class="c"><input type="checkbox" id="c-38182479" checked=""/><div class="controls bullet"><span class="by">riv991</span><span>|</span><a href="#38182057">parent</a><span>|</span><a href="#38182546">prev</a><span>|</span><a href="#38182286">next</a><span>|</span><label class="collapse" for="c-38182479">[-]</label><label class="expand" for="c-38182479">[1 more]</label></div><br/><div class="children"><div class="content">Hi I&#x27;m the blog author, thanks for the feedback<p>I&#x27;ll try and clarify this. I think this is how the sympton presents but I should be clearer.</div><br/></div></div><div id="38182286" class="c"><input type="checkbox" id="c-38182286" checked=""/><div class="controls bullet"><span class="by">Thaxll</span><span>|</span><a href="#38182057">parent</a><span>|</span><a href="#38182479">prev</a><span>|</span><a href="#38181807">next</a><span>|</span><label class="collapse" for="c-38182286">[-]</label><label class="expand" for="c-38182286">[8 more]</label></div><br/><div class="children"><div class="content">People using Kubernetes don&#x27;t tune or change those settings, it&#x27;s up to the app to behave properly.</div><br/><div id="38182385" class="c"><input type="checkbox" id="c-38182385" checked=""/><div class="controls bullet"><span class="by">dilyevsky</span><span>|</span><a href="#38182057">root</a><span>|</span><a href="#38182286">parent</a><span>|</span><a href="#38181807">next</a><span>|</span><label class="collapse" for="c-38182385">[-]</label><label class="expand" for="c-38182385">[7 more]</label></div><br/><div class="children"><div class="content">False. Kubernetes cpu request sets the shares, cpu limit sets the cfs quota</div><br/><div id="38182432" class="c"><input type="checkbox" id="c-38182432" checked=""/><div class="controls bullet"><span class="by">Thaxll</span><span>|</span><a href="#38182057">root</a><span>|</span><a href="#38182385">parent</a><span>|</span><a href="#38181807">next</a><span>|</span><label class="collapse" for="c-38182432">[-]</label><label class="expand" for="c-38182432">[6 more]</label></div><br/><div class="children"><div class="content">You said to change docker flags. Anyway your post is irrelevant, the goal is to let know the runtime about how many posix threads should it use.<p>If you set request &#x2F; limit to 1 core but you run on 64 cores node , then you runtime will see that which will bring performance down.</div><br/><div id="38182764" class="c"><input type="checkbox" id="c-38182764" checked=""/><div class="controls bullet"><span class="by">dilyevsky</span><span>|</span><a href="#38182057">root</a><span>|</span><a href="#38182432">parent</a><span>|</span><a href="#38181807">next</a><span>|</span><label class="collapse" for="c-38182764">[-]</label><label class="expand" for="c-38182764">[5 more]</label></div><br/><div class="children"><div class="content">Original article is about docker. That’s the point of my comment - dont set cpu limit</div><br/><div id="38182831" class="c"><input type="checkbox" id="c-38182831" checked=""/><div class="controls bullet"><span class="by">riv991</span><span>|</span><a href="#38182057">root</a><span>|</span><a href="#38182764">parent</a><span>|</span><a href="#38181807">next</a><span>|</span><label class="collapse" for="c-38182831">[-]</label><label class="expand" for="c-38182831">[4 more]</label></div><br/><div class="children"><div class="content">I intended it to be applicable to all containerised environments. Docker is just easiest on my local machine.<p>I still believe it&#x27;s best to set these variables regardless of cpu limits and&#x2F;or cpu shares</div><br/><div id="38183463" class="c"><input type="checkbox" id="c-38183463" checked=""/><div class="controls bullet"><span class="by">dilyevsky</span><span>|</span><a href="#38182057">root</a><span>|</span><a href="#38182831">parent</a><span>|</span><a href="#38181807">next</a><span>|</span><label class="collapse" for="c-38183463">[-]</label><label class="expand" for="c-38183463">[3 more]</label></div><br/><div class="children"><div class="content">All you did is kneecapped your app to have lower performance so it fits under your arbitrary limit. Hardly what most people describe as “best” - only useful in small percentage of usecases (like reselling compute)</div><br/><div id="38183795" class="c"><input type="checkbox" id="c-38183795" checked=""/><div class="controls bullet"><span class="by">riv991</span><span>|</span><a href="#38182057">root</a><span>|</span><a href="#38183463">parent</a><span>|</span><a href="#38181807">next</a><span>|</span><label class="collapse" for="c-38183795">[-]</label><label class="expand" for="c-38183795">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve seen significant performance gains from this in production.<p>Other people have encountered it too hence libraries like Automaxprocs existing and issues being open with Go for it.</div><br/><div id="38187518" class="c"><input type="checkbox" id="c-38187518" checked=""/><div class="controls bullet"><span class="by">dilyevsky</span><span>|</span><a href="#38182057">root</a><span>|</span><a href="#38183795">parent</a><span>|</span><a href="#38181807">next</a><span>|</span><label class="collapse" for="c-38187518">[-]</label><label class="expand" for="c-38187518">[1 more]</label></div><br/><div class="children"><div class="content">Gains by what metric? Are you sure you didn&#x27;t trade in better latency for worse overall throughput? Also, sure you didn&#x27;t hit one of many CFS overaccounting bugs which we&#x27;ve seen a few? Have you compared performance without the limit at all?</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="38181807" class="c"><input type="checkbox" id="c-38181807" checked=""/><div class="controls bullet"><span class="by">ntonozzi</span><span>|</span><a href="#38182057">prev</a><span>|</span><a href="#38182271">next</a><span>|</span><label class="collapse" for="c-38181807">[-]</label><label class="expand" for="c-38181807">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been bitten many times by the CFS scheduler while using containers and cgroups. What&#x27;s the new scheduler? Has anyone here tried it in a production cluster? We&#x27;re now going on two decades of wasted cores: <a href="https:&#x2F;&#x2F;people.ece.ubc.ca&#x2F;sasha&#x2F;papers&#x2F;eurosys16-final29.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;people.ece.ubc.ca&#x2F;sasha&#x2F;papers&#x2F;eurosys16-final29.pdf</a>.</div><br/><div id="38182206" class="c"><input type="checkbox" id="c-38182206" checked=""/><div class="controls bullet"><span class="by">donaldihunter</span><span>|</span><a href="#38181807">parent</a><span>|</span><a href="#38184004">next</a><span>|</span><label class="collapse" for="c-38182206">[-]</label><label class="expand" for="c-38182206">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;kernelnewbies.org&#x2F;Linux_6.6#New_task_scheduler:_EEVDF" rel="nofollow noreferrer">https:&#x2F;&#x2F;kernelnewbies.org&#x2F;Linux_6.6#New_task_scheduler:_EEVD...</a></div><br/></div></div><div id="38184004" class="c"><input type="checkbox" id="c-38184004" checked=""/><div class="controls bullet"><span class="by">the8472</span><span>|</span><a href="#38181807">parent</a><span>|</span><a href="#38182206">prev</a><span>|</span><a href="#38182271">next</a><span>|</span><label class="collapse" for="c-38184004">[-]</label><label class="expand" for="c-38184004">[1 more]</label></div><br/><div class="children"><div class="content">The problem here isn&#x27;t the scheduler. It&#x27;s resource restrictions imposed by the container but the containerized process (Go) not checking the OS features used to do that when calculating the available amount of parallelism.</div><br/></div></div></div></div><div id="38182271" class="c"><input type="checkbox" id="c-38182271" checked=""/><div class="controls bullet"><span class="by">rickette</span><span>|</span><a href="#38181807">prev</a><span>|</span><a href="#38182214">next</a><span>|</span><label class="collapse" for="c-38182271">[-]</label><label class="expand" for="c-38182271">[1 more]</label></div><br/><div class="children"><div class="content">Besides GOMAXPROCS there&#x27;s also GOMEMLIMIT in recent Go releases. You can use <a href="https:&#x2F;&#x2F;github.com&#x2F;KimMachineGun&#x2F;automemlimit">https:&#x2F;&#x2F;github.com&#x2F;KimMachineGun&#x2F;automemlimit</a> to automatically set this this limit, kinda like <a href="https:&#x2F;&#x2F;github.com&#x2F;uber-go&#x2F;automaxprocs">https:&#x2F;&#x2F;github.com&#x2F;uber-go&#x2F;automaxprocs</a>.</div><br/></div></div><div id="38182214" class="c"><input type="checkbox" id="c-38182214" checked=""/><div class="controls bullet"><span class="by">gregfurman</span><span>|</span><a href="#38182271">prev</a><span>|</span><a href="#38187192">next</a><span>|</span><label class="collapse" for="c-38182214">[-]</label><label class="expand" for="c-38182214">[3 more]</label></div><br/><div class="children"><div class="content">Discovered this sometime last year in my previous role as a platform engineer managing our on-prem kubernetes cluster as well as the CI&#x2F;CD pipeline infrastructure.<p>Although I saw this dissonance  between actual and assigned CPU causing issues, particularly CPU throttling, I struggled to find a scalable solution that would affect all Go deployments on the cluster.<p>Getting all devs to include that autoprocs dependency was not exactly an option for hundreds of projects. Alternatively, setting all CPU request&#x2F;limit to a whole number and then assigning that to a GOMAXPROCS environment variable in a k8s manifest was also clunky and infeasible.<p>I ended up just using this GOMAXPROCS variable for some of our more highly multithreaded applications which yielded some improvements but I’ve yet to find a solution that is applicable to all deployments in a microservices architecture with a high variability of CPU requirements for each project.</div><br/><div id="38183119" class="c"><input type="checkbox" id="c-38183119" checked=""/><div class="controls bullet"><span class="by">linuxftw</span><span>|</span><a href="#38182214">parent</a><span>|</span><a href="#38183037">next</a><span>|</span><label class="collapse" for="c-38183119">[-]</label><label class="expand" for="c-38183119">[1 more]</label></div><br/><div class="children"><div class="content">You could define a mutating webhook to inject GOMAXPROCS into all pod containers.</div><br/></div></div><div id="38183037" class="c"><input type="checkbox" id="c-38183037" checked=""/><div class="controls bullet"><span class="by">jeffbee</span><span>|</span><a href="#38182214">parent</a><span>|</span><a href="#38183119">prev</a><span>|</span><a href="#38187192">next</a><span>|</span><label class="collapse" for="c-38183037">[-]</label><label class="expand" for="c-38183037">[1 more]</label></div><br/><div class="children"><div class="content">There isn&#x27;t one answer for this. Capping GOMAXPROCS may cause severe latency problems if your process gets a burst of traffic and has naive queueing. It&#x27;s best really to set GOMAXPROCS to whatever the hardware offers regardless of your ideas about how much time the process will use on average.</div><br/></div></div></div></div><div id="38187192" class="c"><input type="checkbox" id="c-38187192" checked=""/><div class="controls bullet"><span class="by">irogers</span><span>|</span><a href="#38182214">prev</a><span>|</span><a href="#38182353">next</a><span>|</span><label class="collapse" for="c-38187192">[-]</label><label class="expand" for="c-38187192">[1 more]</label></div><br/><div class="children"><div class="content">There are also GC techniques to make the pause shorter, for example, doing the work for the pause concurrently and then repeating it in the safepoint. The hope is that the concurrent work will turn the safepoint work into a simpler check that no work is necessary. Doubling the work may hurt GC throughput.</div><br/></div></div><div id="38182353" class="c"><input type="checkbox" id="c-38182353" checked=""/><div class="controls bullet"><span class="by">ImJasonH</span><span>|</span><a href="#38187192">prev</a><span>|</span><a href="#38183125">next</a><span>|</span><label class="collapse" for="c-38182353">[-]</label><label class="expand" for="c-38182353">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for sharing this!<p>And as a maintainer of ko[1], it was a pleasant surprised to see ko mentioned briefly, so that&#x27;s for that too :)<p>1: <a href="https:&#x2F;&#x2F;ko.build" rel="nofollow noreferrer">https:&#x2F;&#x2F;ko.build</a></div><br/></div></div><div id="38183125" class="c"><input type="checkbox" id="c-38183125" checked=""/><div class="controls bullet"><span class="by">bruh2</span><span>|</span><a href="#38182353">prev</a><span>|</span><a href="#38183164">next</a><span>|</span><label class="collapse" for="c-38183125">[-]</label><label class="expand" for="c-38183125">[4 more]</label></div><br/><div class="children"><div class="content">As someone not that familiar with Docker or Go, is this behavior intentional? Could the Go team make it aware of the CGroups limit? Do other runtimes behave similarly?</div><br/><div id="38187979" class="c"><input type="checkbox" id="c-38187979" checked=""/><div class="controls bullet"><span class="by">eloisant</span><span>|</span><a href="#38183125">parent</a><span>|</span><a href="#38183742">next</a><span>|</span><label class="collapse" for="c-38187979">[-]</label><label class="expand" for="c-38187979">[1 more]</label></div><br/><div class="children"><div class="content">Yes, I&#x27;ve experienced the same problem with the JVM (in Scala).</div><br/></div></div><div id="38183742" class="c"><input type="checkbox" id="c-38183742" checked=""/><div class="controls bullet"><span class="by">yjftsjthsd-h</span><span>|</span><a href="#38183125">parent</a><span>|</span><a href="#38187979">prev</a><span>|</span><a href="#38183164">next</a><span>|</span><label class="collapse" for="c-38183742">[-]</label><label class="expand" for="c-38183742">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m fairly certain that that .net had to deal with it and Java had or still has a problem, I forget which. (Or did you mean runtimes like containerd?)</div><br/><div id="38186923" class="c"><input type="checkbox" id="c-38186923" checked=""/><div class="controls bullet"><span class="by">richdougherty</span><span>|</span><a href="#38183125">root</a><span>|</span><a href="#38183742">parent</a><span>|</span><a href="#38183164">next</a><span>|</span><label class="collapse" for="c-38186923">[-]</label><label class="expand" for="c-38186923">[1 more]</label></div><br/><div class="children"><div class="content">Supported in Java 10 (and backported to Java 8) since 2018. Not sure about .NET.<p>-
&quot;The JVM has been modified to be aware that it is running in a Docker container and will extract container specific configuration information instead of querying the operating system. The information being extracted is the number of CPUs and total memory that have been allocated to the container.&quot;
 <a href="https:&#x2F;&#x2F;www.oracle.com&#x2F;java&#x2F;technologies&#x2F;javase&#x2F;8u191-relnotes.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.oracle.com&#x2F;java&#x2F;technologies&#x2F;javase&#x2F;8u191-relnot...</a><p>-
Here&#x27;s a more detailed explanation and even a shared library that can be used to patch container unaware versions of Java. I wonder if the same could be done for Go?<p>&quot;LD_PRELOAD=&#x2F;path&#x2F;to&#x2F;libproccount.so java &lt;args&gt;&quot;<p><a href="https:&#x2F;&#x2F;stackoverflow.com&#x2F;a&#x2F;64271429" rel="nofollow noreferrer">https:&#x2F;&#x2F;stackoverflow.com&#x2F;a&#x2F;64271429</a><p><a href="https:&#x2F;&#x2F;gist.github.com&#x2F;apangin&#x2F;78d7e6f7402b1a5da0fa3abd93811f36" rel="nofollow noreferrer">https:&#x2F;&#x2F;gist.github.com&#x2F;apangin&#x2F;78d7e6f7402b1a5da0fa3abd9381...</a><p>-<p>There are more recent changes to Java container awareness as well:<p><a href="https:&#x2F;&#x2F;developers.redhat.com&#x2F;articles&#x2F;2022&#x2F;04&#x2F;19&#x2F;java-17-whats-new-openjdks-container-awareness#" rel="nofollow noreferrer">https:&#x2F;&#x2F;developers.redhat.com&#x2F;articles&#x2F;2022&#x2F;04&#x2F;19&#x2F;java-17-wh...</a></div><br/></div></div></div></div></div></div><div id="38183164" class="c"><input type="checkbox" id="c-38183164" checked=""/><div class="controls bullet"><span class="by">evntdrvn</span><span>|</span><a href="#38183125">prev</a><span>|</span><a href="#38185853">next</a><span>|</span><label class="collapse" for="c-38183164">[-]</label><label class="expand" for="c-38183164">[2 more]</label></div><br/><div class="children"><div class="content">I know that the .NET CLR team adjusted its behavior to address this scenario, fwiw!</div><br/><div id="38183976" class="c"><input type="checkbox" id="c-38183976" checked=""/><div class="controls bullet"><span class="by">the8472</span><span>|</span><a href="#38183164">parent</a><span>|</span><a href="#38185853">next</a><span>|</span><label class="collapse" for="c-38183976">[-]</label><label class="expand" for="c-38183976">[1 more]</label></div><br/><div class="children"><div class="content">So did OpenJDK and the Rust standard library.</div><br/></div></div></div></div><div id="38185853" class="c"><input type="checkbox" id="c-38185853" checked=""/><div class="controls bullet"><span class="by">alilleybrinker</span><span>|</span><a href="#38183164">prev</a><span>|</span><a href="#38182261">next</a><span>|</span><label class="collapse" for="c-38185853">[-]</label><label class="expand" for="c-38185853">[1 more]</label></div><br/><div class="children"><div class="content">I feel like this isn&#x27;t the first time I&#x27;ve read about issues with schedulers-in-schedulers, but I also can&#x27;t find any immediate references on hand for other examples. Anyone know of any?</div><br/></div></div><div id="38185977" class="c"><input type="checkbox" id="c-38185977" checked=""/><div class="controls bullet"><span class="by">WesternStar</span><span>|</span><a href="#38182261">prev</a><span>|</span><a href="#38182263">next</a><span>|</span><label class="collapse" for="c-38185977">[-]</label><label class="expand" for="c-38185977">[1 more]</label></div><br/><div class="children"><div class="content">I think this is a great article talking about a thorny point in Golang but boy do I wish I never read this article. I wish this article was never useful to anyone.</div><br/></div></div><div id="38182263" class="c"><input type="checkbox" id="c-38182263" checked=""/><div class="controls bullet"><span class="by">hiroshi3110</span><span>|</span><a href="#38185977">prev</a><span>|</span><a href="#38187602">next</a><span>|</span><label class="collapse" for="c-38182263">[-]</label><label class="expand" for="c-38182263">[1 more]</label></div><br/><div class="children"><div class="content">How about GKE and containerd?</div><br/></div></div><div id="38187602" class="c"><input type="checkbox" id="c-38187602" checked=""/><div class="controls bullet"><span class="by">goimonkeen</span><span>|</span><a href="#38182263">prev</a><span>|</span><a href="#38185778">next</a><span>|</span><label class="collapse" for="c-38187602">[-]</label><label class="expand" for="c-38187602">[1 more]</label></div><br/><div class="children"><div class="content">Ok</div><br/></div></div><div id="38185778" class="c"><input type="checkbox" id="c-38185778" checked=""/><div class="controls bullet"><span class="by">marcus_holmes</span><span>|</span><a href="#38187602">prev</a><span>|</span><label class="collapse" for="c-38185778">[-]</label><label class="expand" for="c-38185778">[18 more]</label></div><br/><div class="children"><div class="content">I still don&#x27;t get the benefit of running Go binaries in containers. Totally get it for Rails, Python, etc, where the program needs a consistent set of supporting libraries. But Go doesn&#x27;t need that. Especially now we can embed whole file systems into the actual binary.<p>I&#x27;ve been looking at going in the other direction and using a Go binary with a unikernel; the machine just runs the binary and nothing else. I haven&#x27;t got this working to my satisfaction yet - it works but there&#x27;s still too much infrastructure, and deployment is still &quot;fun&quot;. But I think this is way more interesting for Go deployments than using containers.</div><br/><div id="38185910" class="c"><input type="checkbox" id="c-38185910" checked=""/><div class="controls bullet"><span class="by">mholt</span><span>|</span><a href="#38185778">parent</a><span>|</span><a href="#38185980">next</a><span>|</span><label class="collapse" for="c-38185910">[-]</label><label class="expand" for="c-38185910">[3 more]</label></div><br/><div class="children"><div class="content">As the author of Caddy, I have often wondered why people run it in containers. The feedback I keep hearing is basically workflow&#x2F;ecosystem lock-in. Everything else uses containers, so the stuff that doesn&#x27;t need containers needs them now, too.</div><br/><div id="38186836" class="c"><input type="checkbox" id="c-38186836" checked=""/><div class="controls bullet"><span class="by">wonrax</span><span>|</span><a href="#38185778">root</a><span>|</span><a href="#38185910">parent</a><span>|</span><a href="#38186332">next</a><span>|</span><label class="collapse" for="c-38186836">[-]</label><label class="expand" for="c-38186836">[1 more]</label></div><br/><div class="children"><div class="content">For my personal site I run Caddy in a Docker container along with other containers using a compose file. By doing it this way, getting things up when moving to a new instance is as simple as running `docker compose up`. Also making changes to the config or upgrading Caddy version on deployments is the same as a other services since they&#x27;re all containers. So it&#x27;s easy to add CI&#x2F;CD and have it re-deploy Caddy whenever the config changes and there&#x27;s no need for extra GitHub Actions .yaml&#x27;s. Setup as code like this also documents all the dependencies and I think it might be helpful in the future.<p>Having said that, for serious business, this setup doesn&#x27;t make sense. It possibly takes more work to operate as container when the gateway runs on a dedicated instance.</div><br/></div></div><div id="38186332" class="c"><input type="checkbox" id="c-38186332" checked=""/><div class="controls bullet"><span class="by">marcus_holmes</span><span>|</span><a href="#38185778">root</a><span>|</span><a href="#38185910">parent</a><span>|</span><a href="#38186836">prev</a><span>|</span><a href="#38185980">next</a><span>|</span><label class="collapse" for="c-38186332">[-]</label><label class="expand" for="c-38186332">[1 more]</label></div><br/><div class="children"><div class="content">Love your work :) Good to know I&#x27;m not totally out there on this :)</div><br/></div></div></div></div><div id="38185980" class="c"><input type="checkbox" id="c-38185980" checked=""/><div class="controls bullet"><span class="by">wahern</span><span>|</span><a href="#38185778">parent</a><span>|</span><a href="#38185910">prev</a><span>|</span><a href="#38185896">next</a><span>|</span><label class="collapse" for="c-38185980">[-]</label><label class="expand" for="c-38185980">[2 more]</label></div><br/><div class="children"><div class="content">Go programs still make use of, e.g., &#x2F;etc&#x2F;resolv.conf and &#x2F;etc&#x2F;ssl&#x2F;certs&#x2F;ca-certificates.crt. These aren&#x27;t strictly necessary, but using them makes it easier--more consistent, more transparent--to configure these resource across different languages and frameworks.</div><br/><div id="38186393" class="c"><input type="checkbox" id="c-38186393" checked=""/><div class="controls bullet"><span class="by">marcus_holmes</span><span>|</span><a href="#38185778">root</a><span>|</span><a href="#38185980">parent</a><span>|</span><a href="#38185896">next</a><span>|</span><label class="collapse" for="c-38186393">[-]</label><label class="expand" for="c-38186393">[1 more]</label></div><br/><div class="children"><div class="content">I use the semi-official Acme package [0], which handles all that really well. I haven&#x27;t touched SSL or TLS config for years. I mean, I might be an outlier, but this seems pretty standard for Go deployments these days.<p>[0] <a href="https:&#x2F;&#x2F;pkg.go.dev&#x2F;golang.org&#x2F;x&#x2F;crypto&#x2F;acme" rel="nofollow noreferrer">https:&#x2F;&#x2F;pkg.go.dev&#x2F;golang.org&#x2F;x&#x2F;crypto&#x2F;acme</a></div><br/></div></div></div></div><div id="38185896" class="c"><input type="checkbox" id="c-38185896" checked=""/><div class="controls bullet"><span class="by">tail_exchange</span><span>|</span><a href="#38185778">parent</a><span>|</span><a href="#38185980">prev</a><span>|</span><a href="#38186045">next</a><span>|</span><label class="collapse" for="c-38185896">[-]</label><label class="expand" for="c-38185896">[3 more]</label></div><br/><div class="children"><div class="content">Containers can give you better isolation between the application and the host, as well as making horizontal scaling easier. It&#x27;s also a must if you are using a system like Kubernetes. If you are running multiple applications in the same host, you also have control over resource limits.</div><br/><div id="38185971" class="c"><input type="checkbox" id="c-38185971" checked=""/><div class="controls bullet"><span class="by">jeffbee</span><span>|</span><a href="#38185778">root</a><span>|</span><a href="#38185896">parent</a><span>|</span><a href="#38186045">next</a><span>|</span><label class="collapse" for="c-38185971">[-]</label><label class="expand" for="c-38185971">[2 more]</label></div><br/><div class="children"><div class="content">Kubernetes requires a platform runtime that answers its requests, but there&#x27;s no law enforcement agency that will prevent you from using a custom runtime that ignores the very existence of Linux control groups.</div><br/><div id="38186236" class="c"><input type="checkbox" id="c-38186236" checked=""/><div class="controls bullet"><span class="by">tail_exchange</span><span>|</span><a href="#38185778">root</a><span>|</span><a href="#38185971">parent</a><span>|</span><a href="#38186045">next</a><span>|</span><label class="collapse" for="c-38186236">[-]</label><label class="expand" for="c-38186236">[1 more]</label></div><br/><div class="children"><div class="content">Yes, that is true. Though if you are using Google&#x27;s or Amazon&#x27;s managed Kubernetes services, I think you need to use Docker.</div><br/></div></div></div></div></div></div><div id="38186045" class="c"><input type="checkbox" id="c-38186045" checked=""/><div class="controls bullet"><span class="by">cedws</span><span>|</span><a href="#38185778">parent</a><span>|</span><a href="#38185896">prev</a><span>|</span><a href="#38185982">next</a><span>|</span><label class="collapse" for="c-38186045">[-]</label><label class="expand" for="c-38186045">[1 more]</label></div><br/><div class="children"><div class="content">A lot of companies are Kubernetes based now so containers are the default delivery mechanism for all software.</div><br/></div></div><div id="38185982" class="c"><input type="checkbox" id="c-38185982" checked=""/><div class="controls bullet"><span class="by">throwaway892238</span><span>|</span><a href="#38185778">parent</a><span>|</span><a href="#38186045">prev</a><span>|</span><a href="#38186136">next</a><span>|</span><label class="collapse" for="c-38185982">[-]</label><label class="expand" for="c-38185982">[7 more]</label></div><br/><div class="children"><div class="content">Here&#x27;s what a container gives you:<p><pre><code>  - Isolation of networking
  - Isolation of process namespace
  - Isolation of filesystem namespace
  - CPU and Memory limit enforcement
  - A near-universal format for packaging, distributing, and running applications, with metadata, and support for multiple architectures
  - A simple method for declaring a multi-step build process
  - Other stuff I don&#x27;t remember
</code></pre>
You&#x27;re certainly welcome to go without them, but over time, everybody ends up needing at least one of the features containers bring. I get the aversion to adding more complexity, but complexity has this annoying habit of being useful.</div><br/><div id="38186322" class="c"><input type="checkbox" id="c-38186322" checked=""/><div class="controls bullet"><span class="by">marcus_holmes</span><span>|</span><a href="#38185778">root</a><span>|</span><a href="#38185982">parent</a><span>|</span><a href="#38186136">next</a><span>|</span><label class="collapse" for="c-38186322">[-]</label><label class="expand" for="c-38186322">[6 more]</label></div><br/><div class="children"><div class="content">A VM provides the first 4 anyway - if you&#x27;re deploying to a cloud instance then having these in the container is redundant. If you&#x27;re deploying to bare metal then it&#x27;s possibly useful, but only if you&#x27;re deploying multiple containers to the same machine.<p>Go doesn&#x27;t need a format for packaging - it&#x27;s one file. It&#x27;s becoming common practice to embed everything else into the binary. (side note: I haven&#x27;t done this with env files yet, and tend to deploy them separately, but I don&#x27;t see any reason why we don&#x27;t do this and produce binaries targetted at the specific deployment environments. I might give it a go).<p>I kinda prefer makefiles for the build stuff, or even just a script. The whole process of creating a Docker instance, pushing source files to it, trigging go build and then pulling back the binary seems redundant; there&#x27;s no advantage of doing this in a container over doing it on the local machine. And it&#x27;s a lot faster on the local machine.<p>Talking to people, it appears to be as mholt said: everyone just does everything in containers so apparently we do this too.</div><br/><div id="38186750" class="c"><input type="checkbox" id="c-38186750" checked=""/><div class="controls bullet"><span class="by">dilyevsky</span><span>|</span><a href="#38185778">root</a><span>|</span><a href="#38186322">parent</a><span>|</span><a href="#38186136">next</a><span>|</span><label class="collapse" for="c-38186750">[-]</label><label class="expand" for="c-38186750">[5 more]</label></div><br/><div class="children"><div class="content">Are you suggesting to setup a separate VM for each process that may only require like 0.25 cpu? Another thing you can’t do with VMs is oversubscribe (at least not with cloud ones)</div><br/><div id="38187598" class="c"><input type="checkbox" id="c-38187598" checked=""/><div class="controls bullet"><span class="by">marcus_holmes</span><span>|</span><a href="#38185778">root</a><span>|</span><a href="#38186750">parent</a><span>|</span><a href="#38187089">next</a><span>|</span><label class="collapse" for="c-38187598">[-]</label><label class="expand" for="c-38187598">[1 more]</label></div><br/><div class="children"><div class="content">Go binaries tend not to be that lightweight, because we have goroutines for that.<p>And yes, setting up a separate VM for each instance of a process is perfectly feasible. That&#x27;s what all this cloud business was about in the first place.</div><br/></div></div><div id="38187089" class="c"><input type="checkbox" id="c-38187089" checked=""/><div class="controls bullet"><span class="by">lmm</span><span>|</span><a href="#38185778">root</a><span>|</span><a href="#38186750">parent</a><span>|</span><a href="#38187598">prev</a><span>|</span><a href="#38186136">next</a><span>|</span><label class="collapse" for="c-38187089">[-]</label><label class="expand" for="c-38187089">[3 more]</label></div><br/><div class="children"><div class="content">You can&#x27;t have both oversubscription and isolation, almost by definition. If you want isolation, VMs are great. If you want oversubscription, OSes are still better than container runtimes at managing competing processes on the same host.</div><br/><div id="38187217" class="c"><input type="checkbox" id="c-38187217" checked=""/><div class="controls bullet"><span class="by">dilyevsky</span><span>|</span><a href="#38185778">root</a><span>|</span><a href="#38187089">parent</a><span>|</span><a href="#38186136">next</a><span>|</span><label class="collapse" for="c-38187217">[-]</label><label class="expand" for="c-38187217">[2 more]</label></div><br/><div class="children"><div class="content">OK but I can tho - oversub cpu, isolate memory, systems resources (like ports) etc.<p>&gt;  OSes are still better than container runtimes at managing competing processes on the same host.<p>OSes and container runtimes are the same thing</div><br/><div id="38187619" class="c"><input type="checkbox" id="c-38187619" checked=""/><div class="controls bullet"><span class="by">marcus_holmes</span><span>|</span><a href="#38185778">root</a><span>|</span><a href="#38187217">parent</a><span>|</span><a href="#38186136">next</a><span>|</span><label class="collapse" for="c-38187619">[-]</label><label class="expand" for="c-38187619">[1 more]</label></div><br/><div class="children"><div class="content">&gt; OSes and container runtimes are the same thing<p>For a subset of OSes</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>