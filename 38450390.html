<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1701248458725" as="style"/><link rel="stylesheet" href="styles.css?v=1701248458725"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://stability.ai/news/stability-ai-sdxl-turbo">SDXL Turbo: A Real-Time Text-to-Image Generation Model</a> <span class="domain">(<a href="https://stability.ai">stability.ai</a>)</span></div><div class="subtext"><span>minimaxir</span> | <span>96 comments</span></div><br/><div><div id="38454625" class="c"><input type="checkbox" id="c-38454625" checked=""/><div class="controls bullet"><span class="by">kmeisthax</span><span>|</span><a href="#38452820">next</a><span>|</span><label class="collapse" for="c-38454625">[-]</label><label class="expand" for="c-38454625">[22 more]</label></div><br/><div class="children"><div class="content">Noncommercial use - aside from being one of my licensing pet peeves - seems to indicate that the money is drying up. My guess is that the investors over at Stability are tired of subsidizing the part of the generative AI market that OpenAI refuses to touch[0].<p>The thing is, I&#x27;m not entirely sure there&#x27;s a paying portion of the market? Yes, I&#x27;ve heard of people paying for ChatGPT because it answers programming questions <i>really well</i>, but that&#x27;s individual users, who are cost sensitive. The real money was supposed to be selling these things as worker replacement, but Hollywood unions have (rightfully) shut AI companies out of the markets where a machine that can write endless slop might have made lots of money.<p>OpenAI can remain in Microsoft&#x27;s orbit for as long as the stupid altruist &#x2F; accelerationist doomer debate doesn&#x27;t tear them apart[1]. Google has at least a few more years of monopoly money before either the US breaks them up or the EU bankrupts them with fees. I don&#x27;t know who the hell is pumping more money into either Anthropic or Stability.<p>[0] Porn. It&#x27;s always porn. OpenAI doesn&#x27;t want to touch it for very obvious reasons.<p>[1] For what it&#x27;s worth, Microsoft has shown that all the AI safety guardrails can be ripped out by the money people at a moment&#x27;s notice, given how quickly they were able to make the OpenAI board blink.</div><br/><div id="38455314" class="c"><input type="checkbox" id="c-38455314" checked=""/><div class="controls bullet"><span class="by">Jackson__</span><span>|</span><a href="#38454625">parent</a><span>|</span><a href="#38456815">next</a><span>|</span><label class="collapse" for="c-38455314">[-]</label><label class="expand" for="c-38455314">[5 more]</label></div><br/><div class="children"><div class="content">The only truly successful commercial use of SDXL I know of is by NovelAI. Said company appears to have used an 256xH100 cluster to finetune it to produce anime art.<p>Open source efforts to produce a similar model seem to have failed due to the extreme compute requirements for finetuning. For example, Waifu Diffusion using 8XA40[0] have not managed to bend SDXL to their will after potentially months of training.<p>If you need 256xH100 to even finetune the model for your use case, what&#x27;s stopping you from just training your own base model? Not much, as it turns out. Developers of NovelAI have stated they&#x27;ll train from scratch for the next version of their model a few weeks ago.<p>So I agree, even with the licensing changes things might be looking somewhat dire for SAI.<p><a href="https:&#x2F;&#x2F;gist.github.com&#x2F;harubaru&#x2F;f727cedacae336d1f7877c4bbe2196e1" rel="nofollow noreferrer">https:&#x2F;&#x2F;gist.github.com&#x2F;harubaru&#x2F;f727cedacae336d1f7877c4bbe2...</a></div><br/><div id="38456535" class="c"><input type="checkbox" id="c-38456535" checked=""/><div class="controls bullet"><span class="by">DeathArrow</span><span>|</span><a href="#38454625">root</a><span>|</span><a href="#38455314">parent</a><span>|</span><a href="#38455439">next</a><span>|</span><label class="collapse" for="c-38456535">[-]</label><label class="expand" for="c-38456535">[2 more]</label></div><br/><div class="children"><div class="content">&gt;Open source efforts to produce a similar model seem to have failed due to the extreme compute requirements for finetuning.<p>A distributed computing project similar to SETI @ Moon wouldn&#x27;t help with training?</div><br/><div id="38456660" class="c"><input type="checkbox" id="c-38456660" checked=""/><div class="controls bullet"><span class="by">ozr</span><span>|</span><a href="#38454625">root</a><span>|</span><a href="#38456535">parent</a><span>|</span><a href="#38455439">next</a><span>|</span><label class="collapse" for="c-38456660">[-]</label><label class="expand" for="c-38456660">[1 more]</label></div><br/><div class="children"><div class="content">Not really with our current techniques.  The increased latency and low bandwidth between nodes makes it absurdly slow.</div><br/></div></div></div></div><div id="38455439" class="c"><input type="checkbox" id="c-38455439" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#38454625">root</a><span>|</span><a href="#38455314">parent</a><span>|</span><a href="#38456535">prev</a><span>|</span><a href="#38456815">next</a><span>|</span><label class="collapse" for="c-38455439">[-]</label><label class="expand" for="c-38455439">[2 more]</label></div><br/><div class="children"><div class="content">Is that the correct link? I&#x27;ve never heard of A40s, the link is to release notes from a year and two months ago, and SD XL just came out a month or two ago. Hard for me to get to &quot;SD XL cannot [be finetuned effectively]&quot; from there.</div><br/><div id="38455846" class="c"><input type="checkbox" id="c-38455846" checked=""/><div class="controls bullet"><span class="by">Jackson__</span><span>|</span><a href="#38454625">root</a><span>|</span><a href="#38455439">parent</a><span>|</span><a href="#38456815">next</a><span>|</span><label class="collapse" for="c-38455846">[-]</label><label class="expand" for="c-38455846">[1 more]</label></div><br/><div class="children"><div class="content">A40: <a href="https:&#x2F;&#x2F;www.techpowerup.com&#x2F;gpu-specs&#x2F;a40-pcie.c3700" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.techpowerup.com&#x2F;gpu-specs&#x2F;a40-pcie.c3700</a><p>I have not heard about the team upgrading or downgrading from the hardware mentioned there, so I assumed it&#x27;s still the same hardware they use.<p>&gt;SD XL just came out a month or two ago<p>About 4.5 months actually.<p>For the SDXL cannot be finetuned efficiently claim, an attempt at a finetune was released here: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;hakurei&#x2F;waifu-diffusion-xl" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;hakurei&#x2F;waifu-diffusion-xl</a><p>The team was given early access by StabilityAI to SDXL0.9 for this. You&#x27;ll have to test it out for yourself, if you&#x27;re interested in comparing. From my experience, it is a world of a difference between the NovelAI and WaifuDiffusion models in both quality and prompt understanding.<p>Note, the very baseline I set for the WaifuDiffusionSDXL model was to beat their SD2.1 based model[0], which it did not in my opinion.<p>[0] <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;hakurei&#x2F;waifu-diffusion-v1-4" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;hakurei&#x2F;waifu-diffusion-v1-4</a></div><br/></div></div></div></div></div></div><div id="38456815" class="c"><input type="checkbox" id="c-38456815" checked=""/><div class="controls bullet"><span class="by">PeterStuer</span><span>|</span><a href="#38454625">parent</a><span>|</span><a href="#38455314">prev</a><span>|</span><a href="#38456076">next</a><span>|</span><label class="collapse" for="c-38456815">[-]</label><label class="expand" for="c-38456815">[1 more]</label></div><br/><div class="children"><div class="content">Are you suggesting the only use for locally run free SD derived models is porn?<p>Creating illustrations for articles&#x2F;presentations and stock photo alteratives are huge!<p>The ability to run for free on your local machine allows for far more iterations than using SaaS, and the checkpoint&#x2F;finetune ecosystem the openness sprouted has created models performing way better for these use cases than standard SD.</div><br/></div></div><div id="38456076" class="c"><input type="checkbox" id="c-38456076" checked=""/><div class="controls bullet"><span class="by">dreampen</span><span>|</span><a href="#38454625">parent</a><span>|</span><a href="#38456815">prev</a><span>|</span><a href="#38455522">next</a><span>|</span><label class="collapse" for="c-38456076">[-]</label><label class="expand" for="c-38456076">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Porn. It&#x27;s always porn.<p>I posted about my AI porn site pornpen.ai here last year and it reached the top of the front page. And yes, it&#x27;s still going strong :D (and we&#x27;ve integrated SDXL and videos recently)</div><br/></div></div><div id="38455522" class="c"><input type="checkbox" id="c-38455522" checked=""/><div class="controls bullet"><span class="by">orbital-decay</span><span>|</span><a href="#38454625">parent</a><span>|</span><a href="#38456076">prev</a><span>|</span><a href="#38456627">next</a><span>|</span><label class="collapse" for="c-38455522">[-]</label><label class="expand" for="c-38455522">[1 more]</label></div><br/><div class="children"><div class="content">Emad just tweeted about the future monetization of their core models. Seems they want to use the Unity model - the original one, not the recent trick Unity pulled. AKA free to use until you make lots of money with it.<p><a href="https:&#x2F;&#x2F;x.com&#x2F;EMostaque&#x2F;status&#x2F;1729609312601887109" rel="nofollow noreferrer">https:&#x2F;&#x2F;x.com&#x2F;EMostaque&#x2F;status&#x2F;1729609312601887109</a></div><br/></div></div><div id="38456627" class="c"><input type="checkbox" id="c-38456627" checked=""/><div class="controls bullet"><span class="by">torginus</span><span>|</span><a href="#38454625">parent</a><span>|</span><a href="#38455522">prev</a><span>|</span><a href="#38455021">next</a><span>|</span><label class="collapse" for="c-38456627">[-]</label><label class="expand" for="c-38456627">[2 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t literally every imagegen AI that&#x27;s not DALL-E or Midjourney based on Stable Diffusion?</div><br/><div id="38457121" class="c"><input type="checkbox" id="c-38457121" checked=""/><div class="controls bullet"><span class="by">cubefox</span><span>|</span><a href="#38454625">root</a><span>|</span><a href="#38456627">parent</a><span>|</span><a href="#38455021">next</a><span>|</span><label class="collapse" for="c-38457121">[-]</label><label class="expand" for="c-38457121">[1 more]</label></div><br/><div class="children"><div class="content">There are exceptions, e.g. <a href="https:&#x2F;&#x2F;generated.photos&#x2F;human-generator" rel="nofollow noreferrer">https:&#x2F;&#x2F;generated.photos&#x2F;human-generator</a> uses a GAN based model.</div><br/></div></div></div></div><div id="38455021" class="c"><input type="checkbox" id="c-38455021" checked=""/><div class="controls bullet"><span class="by">fnordpiglet</span><span>|</span><a href="#38454625">parent</a><span>|</span><a href="#38456627">prev</a><span>|</span><a href="#38455341">next</a><span>|</span><label class="collapse" for="c-38455021">[-]</label><label class="expand" for="c-38455021">[5 more]</label></div><br/><div class="children"><div class="content">Open AI has a pretty robust and profitable business without Microsoft. In every enterprise I’ve been involved with over the last few years we have had some incredibly material and important use cases of OpenAI LLMs (as well as Claude). They aren’t spewing slop or whatever, they’re genuinely achieving valuable and foundational business outcomes. I’ve been a bit stunned at how fast we’ve achieved these things and it tells me that the AI hype isn’t hype, and that if we have done these things in a year, it’s hard to estimate how much impact the technologies will have in five but I think it’s substantial. So is our spend with OpenAI. Or rather, with Azure on OpenAI products. The only value from our experiences Microsoft offers is IAM - which is sufficient frankly.<p>Stability and Midjourney are also making money, but it’s largely with amateurs and people prototyping content for their own creations. A lot of single person Indy game developers are using these tools to generate assets, or at minimum first pass assets. I think a lot of media companies are producing the art for their articles or news letters etc using these tools. Whether this is enough, I don’t know.</div><br/><div id="38455421" class="c"><input type="checkbox" id="c-38455421" checked=""/><div class="controls bullet"><span class="by">__loam</span><span>|</span><a href="#38454625">root</a><span>|</span><a href="#38455021">parent</a><span>|</span><a href="#38455341">next</a><span>|</span><label class="collapse" for="c-38455421">[-]</label><label class="expand" for="c-38455421">[4 more]</label></div><br/><div class="children"><div class="content"><i>Sigh</i> I&#x27;m really tired of seeing people assume OpenAI is profitable. We have no idea of they are or not and have some indication that they&#x27;re incinerating money on chatgpt to the point that they&#x27;re turning off sign ups because they&#x27;re out of compute.</div><br/><div id="38455971" class="c"><input type="checkbox" id="c-38455971" checked=""/><div class="controls bullet"><span class="by">fnordpiglet</span><span>|</span><a href="#38454625">root</a><span>|</span><a href="#38455421">parent</a><span>|</span><a href="#38455466">next</a><span>|</span><label class="collapse" for="c-38455971">[-]</label><label class="expand" for="c-38455971">[1 more]</label></div><br/><div class="children"><div class="content">My understanding, which I can’t prove other than to say it comes from folks affiliated with OpenAI, is that chatgpt doesn’t make money but also doesn’t lose money (in aggregate, some accounts use way more than others but many accounts are fairly idle), and their API business is profitable and accounts for most of their GPU utilization. I have no insight into why they would turn off signups for chatgpt other than they may need the capacity for their enterprise customers, where they make a decent margin.</div><br/></div></div><div id="38455466" class="c"><input type="checkbox" id="c-38455466" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#38454625">root</a><span>|</span><a href="#38455421">parent</a><span>|</span><a href="#38455971">prev</a><span>|</span><a href="#38455341">next</a><span>|</span><label class="collapse" for="c-38455466">[-]</label><label class="expand" for="c-38455466">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not sure that&#x27;s a useful argument insomuch as A) its unfalsifiable until they IPO (reporting indicates they are very, very profitable) and B) running out of GPUs seems like an odd thing to name as an indicator you&#x27;re _losing_ money.<p>People are genuinely way, way, way underestimating how intense the GPU shortage is.</div><br/><div id="38455525" class="c"><input type="checkbox" id="c-38455525" checked=""/><div class="controls bullet"><span class="by">__loam</span><span>|</span><a href="#38454625">root</a><span>|</span><a href="#38455466">parent</a><span>|</span><a href="#38455341">next</a><span>|</span><label class="collapse" for="c-38455525">[-]</label><label class="expand" for="c-38455525">[1 more]</label></div><br/><div class="children"><div class="content">I haven&#x27;t seen any reporting about their costs. I&#x27;m sure they&#x27;re making bonkers numbers in revenue, but that doesn&#x27;t mean they&#x27;re profitable if they&#x27;re losing money on every gpt call. You&#x27;re saying my claim that they&#x27;re unprofitable is unfalsifiable. The same is true for claims that they&#x27;re wildly profitable. We just don&#x27;t know the financial state of the company because they&#x27;re not public.</div><br/></div></div></div></div></div></div></div></div><div id="38455341" class="c"><input type="checkbox" id="c-38455341" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#38454625">parent</a><span>|</span><a href="#38455021">prev</a><span>|</span><a href="#38455224">next</a><span>|</span><label class="collapse" for="c-38455341">[-]</label><label class="expand" for="c-38455341">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Porn. It&#x27;s always porn.<p>I&#x27;ve been surprised at the explosion of porn. Well, not actually. Automatic1111 made that easy and anyone that CivitAI knows all too well what those models are being used for. I mean when you give teenagers the ability to undress their crushes[0] what do you think is going to happen (do laws adequately protect people (kids)? Can they? Will this force a shift towards actually chasing producers, distributors, and diddlers?)?<p>Porn is clearly an in demand market. But what does surprise me is that there&#x27;s been a lot of work in depth maps and 3D rendering from 2D images in the past few years so I&#x27;m a bit surprised that given how popular VR headsets are (according to today&#x27;s LTT episode, half as many Meta Quest 2s as PS5s have been sold?!). I mean if VR headsets are actually that prolific it seems like there&#x27;d be a good market for even just turning a bunch of videos into VR videos, not to mention porn (I don&#x27;t have a VR headset, but I hear a lot of porn is watched. No Linus, I&#x27;m not going to buy second hand...). I think all it takes is for some group to optimize these models like they have for LLaMA and SD (because as a researcher I can sure tell you, we&#x27;re not the optimizers. Use our work as ballpark figures (e.g. GANs 10x Diffusion) but there&#x27;s a lot of performance on the table). You could definitely convert video frames to 3D on prosumer grade hardware (say a 90 minute movie in &lt;8hrs? Aka: while you sleep).<p>There are a lot of wild things that I think AI is going to change that I&#x27;m not sure people are really considering (average people anyways or at least stuff that&#x27;s not making it into popular conversation). ResNet-50 is still probably the most used model btw. Not sure why, but just about every project I see that&#x27;s not diffusion of an LLM is using this as a backbone despite research models that are smaller, faster, and better (at least on ImageNet-22k and COCO).<p>[0] <a href="https:&#x2F;&#x2F;www.bbc.co.uk&#x2F;news&#x2F;world-europe-66877718" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.bbc.co.uk&#x2F;news&#x2F;world-europe-66877718</a></div><br/><div id="38455529" class="c"><input type="checkbox" id="c-38455529" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#38454625">root</a><span>|</span><a href="#38455341">parent</a><span>|</span><a href="#38455923">next</a><span>|</span><label class="collapse" for="c-38455529">[-]</label><label class="expand" for="c-38455529">[2 more]</label></div><br/><div class="children"><div class="content">SDXL and ControlNet are already optimized, if thats what you mean: <a href="https:&#x2F;&#x2F;github.com&#x2F;chengzeyi&#x2F;stable-fast">https:&#x2F;&#x2F;github.com&#x2F;chengzeyi&#x2F;stable-fast</a><p>(Note the links to various SD compilers).<p>But the whole field is moving so fast that people aren&#x27;t even <i>adopting</i> the compilers and optimized implementations at large.</div><br/><div id="38455813" class="c"><input type="checkbox" id="c-38455813" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#38454625">root</a><span>|</span><a href="#38455529">parent</a><span>|</span><a href="#38455923">next</a><span>|</span><label class="collapse" for="c-38455813">[-]</label><label class="expand" for="c-38455813">[1 more]</label></div><br/><div class="children"><div class="content">Not really what I mean. I mean TensorRT is faster than that according to their README. By optimized I&#x27;m specifically pointing to Llama cpp because 1) it&#x27;s in C, 2) using quantized models, 3) there&#x27;s a hell of a lot of optimizations in there. The thing runs on a raspberry pi! I mean not well but damn. SD is still pushing my 3080Ti for comparison.<p>But I wasn&#x27;t thinking diffusion. Models are big and slow. GANs still reign in terms of speed and model sizes. I mean the StyleGAN-T model is 75M params (lightweight) or 1bn (full) (with 123M for text). That paper notes that the 56 images they use in the Fig 2 takes 6 seconds on a 3090 at 512 resolution. I have a 3080Ti and I can tell you that&#x27;s about how long it takes for me to generate a batch size of 4 with an optimized TensorRT model. That&#x27;s a big difference, especially considering those are done with interpolations. I mean the GAN vs Diffusion debate is often a little silly as realistically it is more a matter of application. I&#x27;ll take diffusion in my photoshop but I&#x27;ll take StyleGAN for my real time video upscaling.<p>But yes, I do understand how fast the field is moving. You can check my comment history to verify if register isn&#x27;t sufficient indication.</div><br/></div></div></div></div><div id="38455923" class="c"><input type="checkbox" id="c-38455923" checked=""/><div class="controls bullet"><span class="by">kmeisthax</span><span>|</span><a href="#38454625">root</a><span>|</span><a href="#38455341">parent</a><span>|</span><a href="#38455529">prev</a><span>|</span><a href="#38455224">next</a><span>|</span><label class="collapse" for="c-38455923">[-]</label><label class="expand" for="c-38455923">[1 more]</label></div><br/><div class="children"><div class="content">&gt;do laws adequately protect people (kids)? Can they? Will this force a shift towards actually chasing producers, distributors, and diddlers?<p>It&#x27;s extremely complicated. Actual CSAM is very illegal, and for good reason. However, artistic depictions of such are... protected 1st Amendment expression[0]. So there&#x27;s an argument - and I really hate that I&#x27;m even saying this - that AI generated CSAM is not prosecutable, as if the law works on SCP-096 rules or something. Furthermore, that&#x27;s just a subset of all revenge porn, itself a subset of nonconsensual porn. In the US, there&#x27;s no specific law banning this behavior unless children are involved. The EU doesn&#x27;t have one either. A specific law targeted at nonconsensual porn is <i>drastically</i> needed, but people keep failing to draft one that isn&#x27;t either a generalized censorship device or a damp squib.<p>You <i>can</i> cobble together other laws to target specific behavior - for example, there was a wave of women in the US copyrighting their nudes so they could file DMCA 512 takedown requests at Facebook. But that&#x27;s got problems - first off, you have to put your nudes in the Library of Congress, which is an own goal; and it only works for revenge porn that the (adult) victim originally made, not all nonconsensual porn. I imagine EU GDPR <i>might</i> be usable for getting nonconsensual porn removed from online platforms, but I haven&#x27;t seen this tried yet.<p>I&#x27;m disgusted, but not surprised, that teenage kids are generating CSAM like this. Even before we had diffusion models, we had GANs and deepfakes, which were almost immediately used for generating shittons of nonconsensual porn[1].<p>[0] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Ashcroft_v._Free_Speech_Coalition" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Ashcroft_v._Free_Speech_Coalit...</a> and the later <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;United_States_v._Handley" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;United_States_v._Handley</a><p>[1] <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=OCLaeBAkFAY" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=OCLaeBAkFAY</a></div><br/></div></div></div></div><div id="38455224" class="c"><input type="checkbox" id="c-38455224" checked=""/><div class="controls bullet"><span class="by">adventured</span><span>|</span><a href="#38454625">parent</a><span>|</span><a href="#38455341">prev</a><span>|</span><a href="#38452820">next</a><span>|</span><label class="collapse" for="c-38455224">[-]</label><label class="expand" for="c-38455224">[2 more]</label></div><br/><div class="children"><div class="content">The EU views Google as a tobacco company. The last thing they want to do is bankrupt them with fees. They want to milk Google - big tech generally - for tax revenue. And besides, it&#x27;d take $100 billion per year in fees, which is never going to happen. Meanwhile Google keeps getting bigger year after year (they have nearly doubled in size in four years, up to $300b in sales now) and Bing has made zero headway despite the AI-angled efforts (BingChat etc). Maybe the mainstream adoption of GPT (or similar) would severely damage Google, there&#x27;s still a lot of time left for Google to take their shot at getting out in front of that outcome.<p>The US breaking Google up also won&#x27;t end the monopoly money. More likely one of the children will spin out with an even better margin business.</div><br/><div id="38455404" class="c"><input type="checkbox" id="c-38455404" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#38454625">root</a><span>|</span><a href="#38455224">parent</a><span>|</span><a href="#38452820">next</a><span>|</span><label class="collapse" for="c-38455404">[-]</label><label class="expand" for="c-38455404">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The US breaking Google up also won&#x27;t end the monopoly money.<p>It&#x27;s also weird because modern economics with tech has created a space that creates a lot of natural monopolies. Momentum is very powerful and it&#x27;s the reason silicon valley companies will run at a loss for years creating a userbase. Trick is to keep them (or sell before buyer starts charging). There&#x27;s what, 2 map companies and only one of them is in high usage because it&#x27;s a default? Same with browsers. You can&#x27;t compete because making a better product isn&#x27;t enough in a system where network effects dominate the economics. Hardware companies are seeing that (along with patent issues, especially across borders). I have no idea how to think about this tbh because it is weird. In some cases breaking these companies up ultimately destroys them while in other cases, exactly what you say.</div><br/></div></div></div></div></div></div><div id="38452820" class="c"><input type="checkbox" id="c-38452820" checked=""/><div class="controls bullet"><span class="by">thot_experiment</span><span>|</span><a href="#38454625">prev</a><span>|</span><a href="#38453091">next</a><span>|</span><label class="collapse" for="c-38452820">[-]</label><label class="expand" for="c-38452820">[7 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been mucking with this stuff again and the SDXL + LCM sampling &amp; LoRA makes 1280x800 images in like 2 second, so about a ~5x speed increase for me (so this would be roughly 2x faster than LCM (??, napkin math)). I&#x27;ve found that the method isn&#x27;t as good at complex prompts. They claim here this can outperform SDXL 1.0 WRT prompt alignment, but I&#x27;m curious what their test methodology is. I searched the paper and I couldn&#x27;t immediately find how it was evaluated. I think these sorts of subjective measurements are fiendishly hard to quantify given the infinity of possible prompts. Still, exciting stuff always happening here, what a time to be alive.</div><br/><div id="38452887" class="c"><input type="checkbox" id="c-38452887" checked=""/><div class="controls bullet"><span class="by">htrp</span><span>|</span><a href="#38452820">parent</a><span>|</span><a href="#38453091">next</a><span>|</span><label class="collapse" for="c-38452887">[-]</label><label class="expand" for="c-38452887">[6 more]</label></div><br/><div class="children"><div class="content">the use case here is really for segmented inpainting.<p>don&#x27;t like a part of an image? replace it instantly</div><br/><div id="38452913" class="c"><input type="checkbox" id="c-38452913" checked=""/><div class="controls bullet"><span class="by">thot_experiment</span><span>|</span><a href="#38452820">root</a><span>|</span><a href="#38452887">parent</a><span>|</span><a href="#38456481">next</a><span>|</span><label class="collapse" for="c-38452913">[-]</label><label class="expand" for="c-38452913">[4 more]</label></div><br/><div class="children"><div class="content">There are SO many use cases! I maintain we&#x27;re not even scratching the surface here. You could programmatically reskin video based on crowd participation, you could re-texture VR spaces on the fly. The space of cool shit that you can do with this stuff is growing far faster than we&#x27;re able to explore it right now.</div><br/></div></div><div id="38456481" class="c"><input type="checkbox" id="c-38456481" checked=""/><div class="controls bullet"><span class="by">zorgmonkey</span><span>|</span><a href="#38452820">root</a><span>|</span><a href="#38452887">parent</a><span>|</span><a href="#38452913">prev</a><span>|</span><a href="#38453091">next</a><span>|</span><label class="collapse" for="c-38456481">[-]</label><label class="expand" for="c-38456481">[1 more]</label></div><br/><div class="children"><div class="content">It has been a while since I last tried, but I never had very good results when I tried inpainting with SDXL in comparison to the SD1.5 inpainting models.</div><br/></div></div></div></div></div></div><div id="38453091" class="c"><input type="checkbox" id="c-38453091" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#38452820">prev</a><span>|</span><a href="#38457035">next</a><span>|</span><label class="collapse" for="c-38453091">[-]</label><label class="expand" for="c-38453091">[2 more]</label></div><br/><div class="children"><div class="content">Hugging Face released a Colab Notebook for generation from SDXL Turbo using the diffusers library: <a href="https:&#x2F;&#x2F;colab.research.google.com&#x2F;drive&#x2F;1yRC3Z2bWQOeM4z0FeJ0rF6fnDTTSdnAJ" rel="nofollow noreferrer">https:&#x2F;&#x2F;colab.research.google.com&#x2F;drive&#x2F;1yRC3Z2bWQOeM4z0FeJ0...</a><p>Playing around with the generation params a bit, Colab&#x27;s T4 GPU can batch-generate up to 6 images at a time at roughly the same speed as one.</div><br/><div id="38456636" class="c"><input type="checkbox" id="c-38456636" checked=""/><div class="controls bullet"><span class="by">3abiton</span><span>|</span><a href="#38453091">parent</a><span>|</span><a href="#38457035">next</a><span>|</span><label class="collapse" for="c-38456636">[-]</label><label class="expand" for="c-38456636">[1 more]</label></div><br/><div class="children"><div class="content">So the new SD model requires higher end hardware compare to the rest?</div><br/></div></div></div></div><div id="38457035" class="c"><input type="checkbox" id="c-38457035" checked=""/><div class="controls bullet"><span class="by">joelthelion</span><span>|</span><a href="#38453091">prev</a><span>|</span><a href="#38453788">next</a><span>|</span><label class="collapse" for="c-38457035">[-]</label><label class="expand" for="c-38457035">[1 more]</label></div><br/><div class="children"><div class="content">What are the use cases for this?</div><br/></div></div><div id="38453788" class="c"><input type="checkbox" id="c-38453788" checked=""/><div class="controls bullet"><span class="by">Severian</span><span>|</span><a href="#38457035">prev</a><span>|</span><a href="#38450403">next</a><span>|</span><label class="collapse" for="c-38453788">[-]</label><label class="expand" for="c-38453788">[11 more]</label></div><br/><div class="children"><div class="content">Works with Automatic111. Generated 20 512x512 on a lowly RTS 2070S with 8GB RAM.<p>Prompt: a man<p>Steps: 1, Sampler: Euler a, CFG scale: 1, Seed: -1, Size: 512x512, Model hash: e869ac7d69, Model: sd_xl_turbo_1.0_fp16, Clip skip: 2, RNG: NV, Version: v1.6.0<p>Examples:
<a href="https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;UuuT9qu" rel="nofollow noreferrer">https:&#x2F;&#x2F;imgur.com&#x2F;a&#x2F;UuuT9qu</a></div><br/><div id="38454116" class="c"><input type="checkbox" id="c-38454116" checked=""/><div class="controls bullet"><span class="by">thot_experiment</span><span>|</span><a href="#38453788">parent</a><span>|</span><a href="#38453982">next</a><span>|</span><label class="collapse" for="c-38454116">[-]</label><label class="expand" for="c-38454116">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve done a bit of fiddling around with it and definitely holding back judgement for now, seems like the 1 and 2 step images are WAY more coherent than LCM, but the images are kinda trash for any kind of prompt complexity so you start to have to use more steps, and since the individual steps take the same amount of time (I think there&#x27;s a specific sampler for this which may be faster &amp; better?) by the time you start prompting details you end up using 4 steps and the perf is about the same as LCM, and that breaks down the same way as you start going for more complexity (text, coherent bg details etc) because you end up needing 10-15 steps and at that point you&#x27;re going to get a much better result from full-fat SDXL x dpmpp3msdee (lol)<p>Curious to see the bigbrain people tackle this over the next few days and wring all the perf out of it, maybe samplers tailored to this model will give a notable boost.</div><br/><div id="38454667" class="c"><input type="checkbox" id="c-38454667" checked=""/><div class="controls bullet"><span class="by">radicality</span><span>|</span><a href="#38453788">root</a><span>|</span><a href="#38454116">parent</a><span>|</span><a href="#38453982">next</a><span>|</span><label class="collapse" for="c-38454667">[-]</label><label class="expand" for="c-38454667">[3 more]</label></div><br/><div class="children"><div class="content">Are you finding dpm++ 3M SDE better than dpm++ 2M SDE in sdxl?<p>Afaik the second order (2M) version is the recommended one to use for guided sampling vs the 3rd order one.<p>From here: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;docs&#x2F;diffusers&#x2F;v0.23.1&#x2F;en&#x2F;api&#x2F;schedulers&#x2F;multistep_dpm_solver#diffusers.DPMSolverMultistepScheduler" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;docs&#x2F;diffusers&#x2F;v0.23.1&#x2F;en&#x2F;api&#x2F;schedul...</a><p>&gt; It is recommended to set solver_order to 2 for guide sampling, and solver_order=3 for unconditional sampling.</div><br/><div id="38454902" class="c"><input type="checkbox" id="c-38454902" checked=""/><div class="controls bullet"><span class="by">Filligree</span><span>|</span><a href="#38453788">root</a><span>|</span><a href="#38454667">parent</a><span>|</span><a href="#38455028">next</a><span>|</span><label class="collapse" for="c-38454902">[-]</label><label class="expand" for="c-38454902">[1 more]</label></div><br/><div class="children"><div class="content">It might be placebo, but I find 3M better for upscaling, when I usually set CFG quite low and use a generic prompt that doesn&#x27;t describe any localised element of the picture.<p>Which is what it&#x27;s meant for, I suppose.</div><br/></div></div><div id="38455028" class="c"><input type="checkbox" id="c-38455028" checked=""/><div class="controls bullet"><span class="by">thot_experiment</span><span>|</span><a href="#38453788">root</a><span>|</span><a href="#38454667">parent</a><span>|</span><a href="#38454902">prev</a><span>|</span><a href="#38453982">next</a><span>|</span><label class="collapse" for="c-38455028">[-]</label><label class="expand" for="c-38455028">[1 more]</label></div><br/><div class="children"><div class="content">In my tests it&#x27;s basically been 50&#x2F;50, i probably did ~40 or so comparisons when i was testing samplers and i felt like there were a couple that seemed really good on the 3rd order one, but idfk, it was very very close, I don&#x27;t know if I saw a single gen where one of the two was bad but the other wasn&#x27;t.</div><br/></div></div></div></div></div></div><div id="38453982" class="c"><input type="checkbox" id="c-38453982" checked=""/><div class="controls bullet"><span class="by">techbro92</span><span>|</span><a href="#38453788">parent</a><span>|</span><a href="#38454116">prev</a><span>|</span><a href="#38450403">next</a><span>|</span><label class="collapse" for="c-38453982">[-]</label><label class="expand" for="c-38453982">[6 more]</label></div><br/><div class="children"><div class="content">I just want to point out that I’ve noticed sdxl isn’t good at producing images that are 512x512 for some reason. It works much better with at least 768x768 resolution.</div><br/><div id="38454071" class="c"><input type="checkbox" id="c-38454071" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#38453788">root</a><span>|</span><a href="#38453982">parent</a><span>|</span><a href="#38454103">next</a><span>|</span><label class="collapse" for="c-38454071">[-]</label><label class="expand" for="c-38454071">[4 more]</label></div><br/><div class="children"><div class="content">Normal SDXL requires 1024x1024 output or the quality degrades significantly.</div><br/><div id="38454578" class="c"><input type="checkbox" id="c-38454578" checked=""/><div class="controls bullet"><span class="by">doctorhandshake</span><span>|</span><a href="#38453788">root</a><span>|</span><a href="#38454071">parent</a><span>|</span><a href="#38454103">next</a><span>|</span><label class="collapse" for="c-38454578">[-]</label><label class="expand" for="c-38454578">[3 more]</label></div><br/><div class="children"><div class="content">Hi Max! Thanks for all the tuts! Small correction - SDXL wants ~1 megapixel resolutions at a variety of aspect ratios.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;lllyasviel&#x2F;Fooocus&#x2F;issues&#x2F;24">https:&#x2F;&#x2F;github.com&#x2F;lllyasviel&#x2F;Fooocus&#x2F;issues&#x2F;24</a></div><br/><div id="38454759" class="c"><input type="checkbox" id="c-38454759" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#38453788">root</a><span>|</span><a href="#38454578">parent</a><span>|</span><a href="#38454103">next</a><span>|</span><label class="collapse" for="c-38454759">[-]</label><label class="expand" for="c-38454759">[2 more]</label></div><br/><div class="children"><div class="content">Fair point, although in my experience SDXL still isn&#x27;t great at non-square ratios. I end up just cropping them to the ratio I want.</div><br/><div id="38455179" class="c"><input type="checkbox" id="c-38455179" checked=""/><div class="controls bullet"><span class="by">doctorhandshake</span><span>|</span><a href="#38453788">root</a><span>|</span><a href="#38454759">parent</a><span>|</span><a href="#38454103">next</a><span>|</span><label class="collapse" for="c-38455179">[-]</label><label class="expand" for="c-38455179">[1 more]</label></div><br/><div class="children"><div class="content">What have you seen to be the issue? Composition? Realism? Prompt adherence? I’m just finishing a project having generated tons of images at a mix of ~6 aspect ratios and I haven’t noticed any difference.</div><br/></div></div></div></div></div></div></div></div><div id="38454103" class="c"><input type="checkbox" id="c-38454103" checked=""/><div class="controls bullet"><span class="by">Severian</span><span>|</span><a href="#38453788">root</a><span>|</span><a href="#38453982">parent</a><span>|</span><a href="#38454071">prev</a><span>|</span><a href="#38450403">next</a><span>|</span><label class="collapse" for="c-38454103">[-]</label><label class="expand" for="c-38454103">[1 more]</label></div><br/><div class="children"><div class="content">Indeed, however one of the listed limitations is:
The generated images are of a fixed resolution (512x512 pix), and the model does not achieve perfect photorealism.</div><br/></div></div></div></div></div></div><div id="38450403" class="c"><input type="checkbox" id="c-38450403" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#38453788">prev</a><span>|</span><a href="#38453620">next</a><span>|</span><label class="collapse" for="c-38450403">[-]</label><label class="expand" for="c-38450403">[9 more]</label></div><br/><div class="children"><div class="content">The license is non-commercial, but:<p>&gt; For clarity, Derivative Works do not include the output of any Model.<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;stabilityai&#x2F;sdxl-turbo&#x2F;blob&#x2F;main&#x2F;LICENSE.TXT" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;stabilityai&#x2F;sdxl-turbo&#x2F;blob&#x2F;main&#x2F;LICE...</a><p>Doesn&#x27;t that mean that generated images from it should be fine for commercial?</div><br/><div id="38453291" class="c"><input type="checkbox" id="c-38453291" checked=""/><div class="controls bullet"><span class="by">nperez</span><span>|</span><a href="#38450403">parent</a><span>|</span><a href="#38454523">next</a><span>|</span><label class="collapse" for="c-38453291">[-]</label><label class="expand" for="c-38453291">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not a lawyer, but I believe there was a court ruling that says AI generated works cannot be copyrighted. So you could use them, but couldn&#x27;t stop anyone else from doing what they want with them</div><br/><div id="38453727" class="c"><input type="checkbox" id="c-38453727" checked=""/><div class="controls bullet"><span class="by">yreg</span><span>|</span><a href="#38450403">root</a><span>|</span><a href="#38453291">parent</a><span>|</span><a href="#38454523">next</a><span>|</span><label class="collapse" for="c-38453727">[-]</label><label class="expand" for="c-38453727">[2 more]</label></div><br/><div class="children"><div class="content">&gt; there was a court ruling<p>What jurisdiction? USA?</div><br/><div id="38455292" class="c"><input type="checkbox" id="c-38455292" checked=""/><div class="controls bullet"><span class="by">adventured</span><span>|</span><a href="#38450403">root</a><span>|</span><a href="#38453727">parent</a><span>|</span><a href="#38454523">next</a><span>|</span><label class="collapse" for="c-38455292">[-]</label><label class="expand" for="c-38455292">[1 more]</label></div><br/><div class="children"><div class="content">Yes -<p><a href="https:&#x2F;&#x2F;www.reuters.com&#x2F;legal&#x2F;ai-generated-art-cannot-receive-copyrights-us-court-says-2023-08-21&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.reuters.com&#x2F;legal&#x2F;ai-generated-art-cannot-receiv...</a></div><br/></div></div></div></div></div></div><div id="38454523" class="c"><input type="checkbox" id="c-38454523" checked=""/><div class="controls bullet"><span class="by">kmeisthax</span><span>|</span><a href="#38450403">parent</a><span>|</span><a href="#38453291">prev</a><span>|</span><a href="#38455130">next</a><span>|</span><label class="collapse" for="c-38454523">[-]</label><label class="expand" for="c-38454523">[4 more]</label></div><br/><div class="children"><div class="content">No, it just means that if they sue you, they&#x27;re pre-committing to not try and foreclose on your own generated outputs by claiming they&#x27;re derivatives that they would then own.<p>Of course, this is a water sandwich. If model outputs are derivatives of the model, it&#x27;d be difficult to argue that the model itself isn&#x27;t a derivative of all the training data, most of which isn&#x27;t licensed. So if anything, this covers Stability&#x27;s ass, not yours. There&#x27;s also the related question of if AI models - not their outputs, just the models themselves - have any copyright at all. The logic behind the non-copyrightability of AI art would also apply to the AI training process, so the only way you could get copyright would be a particularly creative way of organizing and compiling the training dataset.<p>Remember: while the &quot;AI art isn&#x27;t art&quot; argument reeks to high heavens of artistic snobbery, it&#x27;s not entirely wrong. There isn&#x27;t a lot of creative control in the process. Furthermore, we don&#x27;t give copyright to monkeys[2], so why should we give it to AI models?<p>&quot;Noncommercial&quot; isn&#x27;t actually a thing in copyright law. Copyrighted works are inherently commercial artifacts[0], so if you just say &quot;noncommercial use is fine&quot;, you&#x27;ve said nothing - and you&#x27;ve invited the legal equivalent of nasal goblins[1] into the courtroom. Creative Commons gets around this by defining their own concept of NonCommercial use. So what did Stability&#x27;s lawyers cook up?<p>&gt; “Non-Commercial Uses” means exercising any of the rights granted herein for the purpose of research or non-commercial purposes. Non-Commercial Uses does not include any production use of the Software Products or any Derivative Works.<p>Uh... yeah. That&#x27;s replacing a meaningless phrase with a tautology. Fun. The only concrete <i>grant</i> of rights is research use, and they categorically reject &quot;any production use&quot;, which is awfully close to all uses. Even using this to generate funny fanart mashups for your own personal enjoyment could be construed as a &#x27;production use&#x27;. Stability could actually sue you for that (however unlikely that would be).<p>[0] In the eyes of the law. I actually hate this opinion, but it&#x27;s the opinion the law takes.<p>[1] Under ISO 9899, it is entirely legal for C programs with undefined behavior to make goblins fly out of your nose.<p>[2] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Monkey_selfie_copyright_dispute" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Monkey_selfie_copyright_disput...</a></div><br/><div id="38455501" class="c"><input type="checkbox" id="c-38455501" checked=""/><div class="controls bullet"><span class="by">__loam</span><span>|</span><a href="#38450403">root</a><span>|</span><a href="#38454523">parent</a><span>|</span><a href="#38455130">next</a><span>|</span><label class="collapse" for="c-38455501">[-]</label><label class="expand" for="c-38455501">[3 more]</label></div><br/><div class="children"><div class="content">I think the artists get to say whatever they want about this bullshit considering it&#x27;s entirely dependent on their work to even function. I don&#x27;t think the AI community gets to call the people who make the models possible at all snobs or anything else. The AI people didn&#x27;t make shit. They should have some respect for the people that do.</div><br/><div id="38455581" class="c"><input type="checkbox" id="c-38455581" checked=""/><div class="controls bullet"><span class="by">senseiV</span><span>|</span><a href="#38450403">root</a><span>|</span><a href="#38455501">parent</a><span>|</span><a href="#38455130">next</a><span>|</span><label class="collapse" for="c-38455581">[-]</label><label class="expand" for="c-38455581">[2 more]</label></div><br/><div class="children"><div class="content">The so called &quot;AI People&quot; built the entire architecture, something people didn&#x27;t think was possible at the scale and quality a year ago, and the matter of &quot;artists should get whatever they want&quot; because it trained on their works isn&#x27;t the point. Diffusion Models don&#x27;t rip parts of pictures together, they happen to be trained to make art out of noise, finding patterns in art. same things happening with LLM&#x27;s in court with the LLama model and book authors claiming it only makes books.<p>I still remember that piece of art that was submitted to an art contest and won, only to be announced an SD prompt later</div><br/><div id="38456626" class="c"><input type="checkbox" id="c-38456626" checked=""/><div class="controls bullet"><span class="by">__loam</span><span>|</span><a href="#38450403">root</a><span>|</span><a href="#38455581">parent</a><span>|</span><a href="#38455130">next</a><span>|</span><label class="collapse" for="c-38456626">[-]</label><label class="expand" for="c-38456626">[1 more]</label></div><br/><div class="children"><div class="content">These models can&#x27;t exist without the training sets. Their value is <i>entirely</i> derived from existing data. The ml architecture does not matter at all. Sure, throw enough compute and data at a problem, do a little parallelization, and you can extract plenty of patterns. Does that mean the ml engineers understand art? Or are they just using glorified brute force to alienate people who actually make things from their labor? No, I have very little respect for the AI people. Once you get over the novelty, their creations inspire little else beside disgust. They seem to take pride in how little they understand about the models they create.</div><br/></div></div></div></div></div></div></div></div><div id="38455130" class="c"><input type="checkbox" id="c-38455130" checked=""/><div class="controls bullet"><span class="by">esjeon</span><span>|</span><a href="#38450403">parent</a><span>|</span><a href="#38454523">prev</a><span>|</span><a href="#38453620">next</a><span>|</span><label class="collapse" for="c-38455130">[-]</label><label class="expand" for="c-38455130">[1 more]</label></div><br/><div class="children"><div class="content">IANAL but it sounds more like SAI is not responsible for any outputs generated using their models and how those are used.</div><br/></div></div></div></div><div id="38453620" class="c"><input type="checkbox" id="c-38453620" checked=""/><div class="controls bullet"><span class="by">liuliu</span><span>|</span><a href="#38450403">prev</a><span>|</span><a href="#38453592">next</a><span>|</span><label class="collapse" for="c-38453620">[-]</label><label class="expand" for="c-38453620">[1 more]</label></div><br/><div class="children"><div class="content">Instruction here for how to use it on iPhone &#x2F; iPad &#x2F; Mac today: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;drawthingsapp&#x2F;status&#x2F;1729633231526404400" rel="nofollow noreferrer">https:&#x2F;&#x2F;twitter.com&#x2F;drawthingsapp&#x2F;status&#x2F;1729633231526404400</a> (note that you need to convert 8-bit version yourself if you want to use it on &lt; 8GiB devices).</div><br/></div></div><div id="38453592" class="c"><input type="checkbox" id="c-38453592" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#38453620">prev</a><span>|</span><a href="#38456617">next</a><span>|</span><label class="collapse" for="c-38453592">[-]</label><label class="expand" for="c-38453592">[5 more]</label></div><br/><div class="children"><div class="content">Does anyone have any idea of when&#x2F;if there will be a simple way to get commercial access in one step? Like an API or something? Or if they want to charge for it, then maybe a web check out?<p>It&#x27;s interesting that they finally decided to try to make something commercially restricted.<p>Do they have a watermark or anything that they can use to track down people who use without a license?<p>Also, is there anything like an open source (commercial allowed) effort at reproducing this?</div><br/><div id="38454622" class="c"><input type="checkbox" id="c-38454622" checked=""/><div class="controls bullet"><span class="by">zaptrem</span><span>|</span><a href="#38453592">parent</a><span>|</span><a href="#38454672">next</a><span>|</span><label class="collapse" for="c-38454622">[-]</label><label class="expand" for="c-38454622">[2 more]</label></div><br/><div class="children"><div class="content">Open version predates this one: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;lcm_lora" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;blog&#x2F;lcm_lora</a></div><br/><div id="38455690" class="c"><input type="checkbox" id="c-38455690" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#38453592">root</a><span>|</span><a href="#38454622">parent</a><span>|</span><a href="#38454672">next</a><span>|</span><label class="collapse" for="c-38455690">[-]</label><label class="expand" for="c-38455690">[1 more]</label></div><br/><div class="children"><div class="content">Yes but Turbo is 4 times faster.</div><br/></div></div></div></div><div id="38454672" class="c"><input type="checkbox" id="c-38454672" checked=""/><div class="controls bullet"><span class="by">fernly</span><span>|</span><a href="#38453592">parent</a><span>|</span><a href="#38454622">prev</a><span>|</span><a href="#38456617">next</a><span>|</span><label class="collapse" for="c-38454672">[-]</label><label class="expand" for="c-38454672">[2 more]</label></div><br/><div class="children"><div class="content">about my 8th prompt, it told me I had used up my usage and offered an option to sign up for &quot;pro&quot;.</div><br/><div id="38455687" class="c"><input type="checkbox" id="c-38455687" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#38453592">root</a><span>|</span><a href="#38454672">parent</a><span>|</span><a href="#38456617">next</a><span>|</span><label class="collapse" for="c-38455687">[-]</label><label class="expand" for="c-38455687">[1 more]</label></div><br/><div class="children"><div class="content">Me too, but I need an API.</div><br/></div></div></div></div></div></div><div id="38456617" class="c"><input type="checkbox" id="c-38456617" checked=""/><div class="controls bullet"><span class="by">DeathArrow</span><span>|</span><a href="#38453592">prev</a><span>|</span><a href="#38455598">next</a><span>|</span><label class="collapse" for="c-38456617">[-]</label><label class="expand" for="c-38456617">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s really great, we can generate porn faster and better. :)</div><br/></div></div><div id="38455598" class="c"><input type="checkbox" id="c-38455598" checked=""/><div class="controls bullet"><span class="by">iAkashPaul</span><span>|</span><a href="#38456617">prev</a><span>|</span><a href="#38453870">next</a><span>|</span><label class="collapse" for="c-38455598">[-]</label><label class="expand" for="c-38455598">[1 more]</label></div><br/><div class="children"><div class="content">From testing out the 1024px outputs it seems to work okay for text2img tasks only, other tasks get deepfried results. Also got it working using SDXL-LCM with similar results.</div><br/></div></div><div id="38453870" class="c"><input type="checkbox" id="c-38453870" checked=""/><div class="controls bullet"><span class="by">Filligree</span><span>|</span><a href="#38455598">prev</a><span>|</span><a href="#38452985">next</a><span>|</span><label class="collapse" for="c-38453870">[-]</label><label class="expand" for="c-38453870">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s... fast, yeah, but the quality is low. Not sure what I&#x27;d be using this for.</div><br/></div></div><div id="38452985" class="c"><input type="checkbox" id="c-38452985" checked=""/><div class="controls bullet"><span class="by">lemoncookiechip</span><span>|</span><a href="#38453870">prev</a><span>|</span><a href="#38453531">next</a><span>|</span><label class="collapse" for="c-38452985">[-]</label><label class="expand" for="c-38452985">[11 more]</label></div><br/><div class="children"><div class="content">the clipdrop demo doesn&#x27;t inspire much confidence with how bad the generations are, we&#x27;re talking 2021 levels, not to mention everything is NSFW somehow, should probably work on those filters.</div><br/><div id="38453136" class="c"><input type="checkbox" id="c-38453136" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#38452985">parent</a><span>|</span><a href="#38453187">next</a><span>|</span><label class="collapse" for="c-38453136">[-]</label><label class="expand" for="c-38453136">[5 more]</label></div><br/><div class="children"><div class="content">I tested a bit and the quality for photorealistic images is surprisingly bad, and definitely worse than LCM and of course normal SDXL. For more artistic images, SDXL Turbo fares better.<p>Unlike normal SDXL, you&#x27;re required here to use the old-fashioned syntatic sugar like &quot;8k hd&quot; and &quot;hyperrealistic&quot; to align things.</div><br/><div id="38453285" class="c"><input type="checkbox" id="c-38453285" checked=""/><div class="controls bullet"><span class="by">jauntywundrkind</span><span>|</span><a href="#38452985">root</a><span>|</span><a href="#38453136">parent</a><span>|</span><a href="#38453187">next</a><span>|</span><label class="collapse" for="c-38453285">[-]</label><label class="expand" for="c-38453285">[4 more]</label></div><br/><div class="children"><div class="content">Are there any good resources for learning a lot of &quot;syntactic sugar&quot; terms? This is new to me, but I&#x27;d love to know more.</div><br/><div id="38454420" class="c"><input type="checkbox" id="c-38454420" checked=""/><div class="controls bullet"><span class="by">genewitch</span><span>|</span><a href="#38452985">root</a><span>|</span><a href="#38453285">parent</a><span>|</span><a href="#38454001">next</a><span>|</span><label class="collapse" for="c-38454420">[-]</label><label class="expand" for="c-38454420">[1 more]</label></div><br/><div class="children"><div class="content">It is completely dependent on the model. Civit dot ai has model showcases as well as fine-tune showcases, and you can click any image or press the (i) to see the generation info.<p>Some models like natural language prompts - &quot;draw me a pterodactyl tanning at a beach&quot;, some prefer shorthand (danbooru style clip) - &quot;1man, professor, classroom, chalkboard, white_hair, suit&quot;, and some work with a mixture of the above as well as the syntactical sugar -&quot;masterpiece, 8k, trending on artstation, space image, a man floating next to a spaceship in space, bokeh, rim lighting, cinematic lighting, Nikon D60, f &#x2F; 2&quot;<p>Fine-tuning models - LoRA, etc, allow one to convert prompts from one style to another if they wish, but usually it&#x27;s to compress an idea, style, person, object, etc in to a single &quot;token&quot;, so you can work on other aspects of the image.<p>Check out civit AI and you can sort of get an idea of the cargo cultism as well as what sort of keywords actually make a difference.</div><br/></div></div><div id="38454001" class="c"><input type="checkbox" id="c-38454001" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#38452985">root</a><span>|</span><a href="#38453285">parent</a><span>|</span><a href="#38454420">prev</a><span>|</span><a href="#38453187">next</a><span>|</span><label class="collapse" for="c-38454001">[-]</label><label class="expand" for="c-38454001">[2 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;github.com&#x2F;swyxio&#x2F;ai-notes&#x2F;blob&#x2F;main&#x2F;IMAGE_PROMPTS.md">https:&#x2F;&#x2F;github.com&#x2F;swyxio&#x2F;ai-notes&#x2F;blob&#x2F;main&#x2F;IMAGE_PROMPTS.m...</a></div><br/><div id="38456285" class="c"><input type="checkbox" id="c-38456285" checked=""/><div class="controls bullet"><span class="by">jauntywundrkind</span><span>|</span><a href="#38452985">root</a><span>|</span><a href="#38454001">parent</a><span>|</span><a href="#38453187">next</a><span>|</span><label class="collapse" for="c-38456285">[-]</label><label class="expand" for="c-38456285">[1 more]</label></div><br/><div class="children"><div class="content">#LearningInPublic strikes again! I love you swyx!</div><br/></div></div></div></div></div></div></div></div><div id="38453187" class="c"><input type="checkbox" id="c-38453187" checked=""/><div class="controls bullet"><span class="by">Jackson__</span><span>|</span><a href="#38452985">parent</a><span>|</span><a href="#38453136">prev</a><span>|</span><a href="#38453531">next</a><span>|</span><label class="collapse" for="c-38453187">[-]</label><label class="expand" for="c-38453187">[5 more]</label></div><br/><div class="children"><div class="content">Yeah, it looks like the enshittification of StabilityAI is in full force by now. Especially considering the continually worse licensing.<p>I expect if they ever manage to release an image gen model that&#x27;s an objective improvement, lets say 80% as good as dalle3, it will be subscription API only.</div><br/><div id="38453381" class="c"><input type="checkbox" id="c-38453381" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#38452985">root</a><span>|</span><a href="#38453187">parent</a><span>|</span><a href="#38453531">next</a><span>|</span><label class="collapse" for="c-38453381">[-]</label><label class="expand" for="c-38453381">[4 more]</label></div><br/><div class="children"><div class="content">Are you serious?<p>I&#x27;m using Stability in production: they kept their SDXL beta model which was capable of SDXL 1.0 level prompt adherence at a fraction of the cost up for months after was reasonable for a one-off undocumented beta, and it was a huge boon to my product.<p>Then a few weeks back they went and quietly cut costs to 1&#x2F;5th or so what they were for SDXL and released a model that produced similar quality outputs to SDXL for my specific usecase in a fraction of the time (SD 1.6)<p>They&#x27;re on fire as far as I&#x27;m concerned, just quietly making their product cheaper and faster.<p>—<p>Also Dalle 3 is in a very awkward place for programmatic access, so awkward I wouldn&#x27;t call them competitive to SD for many usecases: It&#x27;s got a layer of prompt interference baked in, it&#x27;s expensive, latency is not very consistent. Text is a cool trick but it&#x27;s still not reliable enough to expose as a core part of the generation for an end user.</div><br/><div id="38453626" class="c"><input type="checkbox" id="c-38453626" checked=""/><div class="controls bullet"><span class="by">ShamelessC</span><span>|</span><a href="#38452985">root</a><span>|</span><a href="#38453381">parent</a><span>|</span><a href="#38453531">next</a><span>|</span><label class="collapse" for="c-38453626">[-]</label><label class="expand" for="c-38453626">[3 more]</label></div><br/><div class="children"><div class="content">Sounds like they’re doing the same thing OpenAI is doing. Claiming to favor open models but the reality is they’re pumping growth by reducing costs and this lowering prices. They want a massive chunk of this new market, all of it if they can get it. Their perceived valuation then becomes a matter of how many eyeballs they have looking at segments of their website to advertise to, or how many data points they can collect on their users to sell to advertisers. It’s unlikely they can capture the whole market and still make a chunky enough profit to satisfy investors if they also intend to keep prices high enough without needing to resort to enshitification.</div><br/><div id="38453843" class="c"><input type="checkbox" id="c-38453843" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#38452985">root</a><span>|</span><a href="#38453626">parent</a><span>|</span><a href="#38453531">next</a><span>|</span><label class="collapse" for="c-38453843">[-]</label><label class="expand" for="c-38453843">[2 more]</label></div><br/><div class="children"><div class="content">This would be a lot more pithy if it weren&#x27;t in the comment section of a post that showcases <i>exactly</i> how they were likely able to make 1.6 cheaper, and open sources the underlying tech.<p>There couldn&#x27;t be a more perfect rebuttal to this theory than the post you decided to leave it under.</div><br/><div id="38453926" class="c"><input type="checkbox" id="c-38453926" checked=""/><div class="controls bullet"><span class="by">ShamelessC</span><span>|</span><a href="#38452985">root</a><span>|</span><a href="#38453843">parent</a><span>|</span><a href="#38453531">next</a><span>|</span><label class="collapse" for="c-38453926">[-]</label><label class="expand" for="c-38453926">[1 more]</label></div><br/><div class="children"><div class="content">&gt; the reality is they’re pumping growth by reducing costs and this lowering prices<p>At what point did I indicate they weren&#x27;t making it cheaper? Also their licensing isn&#x27;t really in the spirit of what open source originally described, which is what I meant.<p>Sorry if that didn&#x27;t come across, I guess. I was being intentionally pithy but largely related to standard practices for VC funded startups.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="38453531" class="c"><input type="checkbox" id="c-38453531" checked=""/><div class="controls bullet"><span class="by">smusamashah</span><span>|</span><a href="#38452985">prev</a><span>|</span><a href="#38454240">next</a><span>|</span><label class="collapse" for="c-38453531">[-]</label><label class="expand" for="c-38453531">[6 more]</label></div><br/><div class="children"><div class="content">SDXL is already very very slow when compared to SD 1.5. They are claiming 200ms for 512x512 image in SDXL on A100. We need SD 1.5 turbo for even faster generation.</div><br/><div id="38453622" class="c"><input type="checkbox" id="c-38453622" checked=""/><div class="controls bullet"><span class="by">Sharlin</span><span>|</span><a href="#38453531">parent</a><span>|</span><a href="#38453752">next</a><span>|</span><label class="collapse" for="c-38453622">[-]</label><label class="expand" for="c-38453622">[2 more]</label></div><br/><div class="children"><div class="content">I haven&#x27;t found SDXL to be inherently much slower than 1.5, besides the obvious 4x slowdown from having twice the linear resolution.</div><br/><div id="38453834" class="c"><input type="checkbox" id="c-38453834" checked=""/><div class="controls bullet"><span class="by">thot_experiment</span><span>|</span><a href="#38453531">root</a><span>|</span><a href="#38453622">parent</a><span>|</span><a href="#38453752">next</a><span>|</span><label class="collapse" for="c-38453834">[-]</label><label class="expand" for="c-38453834">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, in my experience it&#x27;s actually FASTER because I was already doing high res gens, and that required 2 or 3 passes previously.</div><br/></div></div></div></div><div id="38453752" class="c"><input type="checkbox" id="c-38453752" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#38453531">parent</a><span>|</span><a href="#38453622">prev</a><span>|</span><a href="#38453995">next</a><span>|</span><label class="collapse" for="c-38453752">[-]</label><label class="expand" for="c-38453752">[1 more]</label></div><br/><div class="children"><div class="content">SDXL Turbo also uses a distilled version of SDXL so it gets a speed bonus from that too.</div><br/></div></div><div id="38453995" class="c"><input type="checkbox" id="c-38453995" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#38453531">parent</a><span>|</span><a href="#38453752">prev</a><span>|</span><a href="#38453565">next</a><span>|</span><label class="collapse" for="c-38453995">[-]</label><label class="expand" for="c-38453995">[1 more]</label></div><br/><div class="children"><div class="content">is it known how much larger SDXL is compared to SD1 or SD2?</div><br/></div></div><div id="38453565" class="c"><input type="checkbox" id="c-38453565" checked=""/><div class="controls bullet"><span class="by">ShamelessC</span><span>|</span><a href="#38453531">parent</a><span>|</span><a href="#38453995">prev</a><span>|</span><a href="#38454240">next</a><span>|</span><label class="collapse" for="c-38453565">[-]</label><label class="expand" for="c-38453565">[1 more]</label></div><br/><div class="children"><div class="content">It _would_ be nice if they offered 512&#x2F;768&#x2F;1024 px variants of the models. I frequently don’t actually need the full 1024 px as it just needs to look good enough for a chat thumbnail. Then I could upscale it manually later. There’s other models like Kandinsky but it’s not super convenient to use multiple models with different code and what not.</div><br/></div></div></div></div><div id="38454240" class="c"><input type="checkbox" id="c-38454240" checked=""/><div class="controls bullet"><span class="by">smcleod</span><span>|</span><a href="#38453531">prev</a><span>|</span><a href="#38452974">next</a><span>|</span><label class="collapse" for="c-38454240">[-]</label><label class="expand" for="c-38454240">[1 more]</label></div><br/><div class="children"><div class="content">Very impressive, I&#x27;m generating some really nice quality images at 1024x1024 in under 1 second on a 3090 in InvokeAI!</div><br/></div></div><div id="38452974" class="c"><input type="checkbox" id="c-38452974" checked=""/><div class="controls bullet"><span class="by">xena</span><span>|</span><a href="#38454240">prev</a><span>|</span><a href="#38452271">next</a><span>|</span><label class="collapse" for="c-38452974">[-]</label><label class="expand" for="c-38452974">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m gonna wait until this is released as a model. This is really cool though!</div><br/><div id="38453104" class="c"><input type="checkbox" id="c-38453104" checked=""/><div class="controls bullet"><span class="by">SushiHippie</span><span>|</span><a href="#38452974">parent</a><span>|</span><a href="#38452271">next</a><span>|</span><label class="collapse" for="c-38453104">[-]</label><label class="expand" for="c-38453104">[2 more]</label></div><br/><div class="children"><div class="content">From TFA:<p>&gt; Download the model weights and code on Hugging Face[0], currently being released under a non-commercial research license that permits personal, non-commercial use.<p>[0] <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;stabilityai&#x2F;sdxl-turbo" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;stabilityai&#x2F;sdxl-turbo</a></div><br/><div id="38453417" class="c"><input type="checkbox" id="c-38453417" checked=""/><div class="controls bullet"><span class="by">xena</span><span>|</span><a href="#38452974">root</a><span>|</span><a href="#38453104">parent</a><span>|</span><a href="#38452271">next</a><span>|</span><label class="collapse" for="c-38453417">[-]</label><label class="expand" for="c-38453417">[1 more]</label></div><br/><div class="children"><div class="content">I am fucking blind, thanks!</div><br/></div></div></div></div></div></div><div id="38452271" class="c"><input type="checkbox" id="c-38452271" checked=""/><div class="controls bullet"><span class="by">rkwasny</span><span>|</span><a href="#38452974">prev</a><span>|</span><a href="#38452624">next</a><span>|</span><label class="collapse" for="c-38452271">[-]</label><label class="expand" for="c-38452271">[7 more]</label></div><br/><div class="children"><div class="content">Awesome! so where is the API I can try?</div><br/><div id="38452330" class="c"><input type="checkbox" id="c-38452330" checked=""/><div class="controls bullet"><span class="by">dreadlordbone</span><span>|</span><a href="#38452271">parent</a><span>|</span><a href="#38452624">next</a><span>|</span><label class="collapse" for="c-38452330">[-]</label><label class="expand" for="c-38452330">[6 more]</label></div><br/><div class="children"><div class="content">They have a web demo here: <a href="https:&#x2F;&#x2F;clipdrop.co&#x2F;stable-diffusion-turbo" rel="nofollow noreferrer">https:&#x2F;&#x2F;clipdrop.co&#x2F;stable-diffusion-turbo</a></div><br/><div id="38452535" class="c"><input type="checkbox" id="c-38452535" checked=""/><div class="controls bullet"><span class="by">KennyBlanken</span><span>|</span><a href="#38452271">root</a><span>|</span><a href="#38452330">parent</a><span>|</span><a href="#38452624">next</a><span>|</span><label class="collapse" for="c-38452535">[-]</label><label class="expand" for="c-38452535">[5 more]</label></div><br/><div class="children"><div class="content">...which requires you to sign in. That nice little text box invites you until you actually click to enter some text and get a registration box thrust at you<p>People that design a UX where the user tricked into a registration &#x27;ambush&#x27; need to be punched in the face.</div><br/><div id="38453300" class="c"><input type="checkbox" id="c-38453300" checked=""/><div class="controls bullet"><span class="by">meeech</span><span>|</span><a href="#38452271">root</a><span>|</span><a href="#38452535">parent</a><span>|</span><a href="#38455046">next</a><span>|</span><label class="collapse" for="c-38453300">[-]</label><label class="expand" for="c-38453300">[1 more]</label></div><br/><div class="children"><div class="content">A totally reasonable response to a minor inconvenience.</div><br/></div></div><div id="38455046" class="c"><input type="checkbox" id="c-38455046" checked=""/><div class="controls bullet"><span class="by">Tadpole9181</span><span>|</span><a href="#38452271">root</a><span>|</span><a href="#38452535">parent</a><span>|</span><a href="#38453300">prev</a><span>|</span><a href="#38452596">next</a><span>|</span><label class="collapse" for="c-38455046">[-]</label><label class="expand" for="c-38455046">[1 more]</label></div><br/><div class="children"><div class="content">The level of entitlement HN posters have these days is insane. They&#x27;re giving you free compute resources on the order of an entire $800 GPU. If you don&#x27;t like it, run the damn thing yourself!<p>UX designers doing their job and programmers protecting their <i>free product</i> from being abused are not the people deserving abuse here.</div><br/></div></div><div id="38452596" class="c"><input type="checkbox" id="c-38452596" checked=""/><div class="controls bullet"><span class="by">LordDragonfang</span><span>|</span><a href="#38452271">root</a><span>|</span><a href="#38452535">parent</a><span>|</span><a href="#38455046">prev</a><span>|</span><a href="#38453912">next</a><span>|</span><label class="collapse" for="c-38452596">[-]</label><label class="expand" for="c-38452596">[1 more]</label></div><br/><div class="children"><div class="content">Luckily this one accepts burner emails just fine, without any intrusive other data collection (name, etc)<p><a href="http:&#x2F;&#x2F;grr.la" rel="nofollow noreferrer">http:&#x2F;&#x2F;grr.la</a></div><br/></div></div><div id="38453912" class="c"><input type="checkbox" id="c-38453912" checked=""/><div class="controls bullet"><span class="by">simbolit</span><span>|</span><a href="#38452271">root</a><span>|</span><a href="#38452535">parent</a><span>|</span><a href="#38452596">prev</a><span>|</span><a href="#38452624">next</a><span>|</span><label class="collapse" for="c-38453912">[-]</label><label class="expand" for="c-38453912">[1 more]</label></div><br/><div class="children"><div class="content">me reading your comment: yeah, also hated that, yes, yes, yes<p>your last four words: nope, hard no, no, we are not friends</div><br/></div></div></div></div></div></div></div></div><div id="38452624" class="c"><input type="checkbox" id="c-38452624" checked=""/><div class="controls bullet"><span class="by">LordDragonfang</span><span>|</span><a href="#38452271">prev</a><span>|</span><a href="#38455146">next</a><span>|</span><label class="collapse" for="c-38452624">[-]</label><label class="expand" for="c-38452624">[3 more]</label></div><br/><div class="children"><div class="content">Based on the demo, that&#x27;s... incredibly fast. Literally generating images faster than I can type a prompt. They&#x27;ve clearly got a set seed, so they&#x27;re probably caching request, but even with prompts that they couldn&#x27;t possibly have cached it&#x27;s within a second or so.</div><br/><div id="38453678" class="c"><input type="checkbox" id="c-38453678" checked=""/><div class="controls bullet"><span class="by">gremlinsinc</span><span>|</span><a href="#38452624">parent</a><span>|</span><a href="#38455146">next</a><span>|</span><label class="collapse" for="c-38453678">[-]</label><label class="expand" for="c-38453678">[2 more]</label></div><br/><div class="children"><div class="content">I hope playgroundai.com adopts this asap, but not sure they can with that non-commercial bit...</div><br/></div></div></div></div><div id="38455146" class="c"><input type="checkbox" id="c-38455146" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#38452624">prev</a><span>|</span><label class="collapse" for="c-38455146">[-]</label><label class="expand" for="c-38455146">[3 more]</label></div><br/><div class="children"><div class="content">&gt; A Real-Time Text-to-Image Generation Model<p>&gt; On an A100, SDXL Turbo generates a 512x512 image in 207ms (prompt encoding + a single denoising step + decoding, fp16), where 67ms are accounted for by a single UNet forward evaluation.<p>Okay... so what part of this is real time? 207ms is 4.8Hz. 67ms is 14.9Hz. Isn&#x27;t &quot;real time&quot; in graphics considered to be at least 30Hz (33ms)? And by today&#x27;s standards at minimum 60Hz (16ms) if not 144Hz (7ms)? I&#x27;m lost at what part of this is real time? I&#x27;m not sure it even would get there with an H100. Maybe an H100 and everything is TensorRT?</div><br/><div id="38455163" class="c"><input type="checkbox" id="c-38455163" checked=""/><div class="controls bullet"><span class="by">LelouBil</span><span>|</span><a href="#38455146">parent</a><span>|</span><label class="collapse" for="c-38455163">[-]</label><label class="expand" for="c-38455163">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s &quot;real-time&quot; as in &quot;I finished typing and the image is already there&quot;</div><br/><div id="38455451" class="c"><input type="checkbox" id="c-38455451" checked=""/><div class="controls bullet"><span class="by">godelski</span><span>|</span><a href="#38455146">root</a><span>|</span><a href="#38455163">parent</a><span>|</span><label class="collapse" for="c-38455451">[-]</label><label class="expand" for="c-38455451">[1 more]</label></div><br/><div class="children"><div class="content">I think it is still a but deceptive to say this considering a well know and high usage alternative definition exists. Plus that&#x27;s on commercial grade hardware.</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>