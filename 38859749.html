<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1704358854790" as="style"/><link rel="stylesheet" href="styles.css?v=1704358854790"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/DLYuanGod/TinyGPT-V">TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>T-A</span> | <span>22 comments</span></div><br/><div><div id="38859789" class="c"><input type="checkbox" id="c-38859789" checked=""/><div class="controls bullet"><span class="by">T-A</span><span>|</span><a href="#38864473">next</a><span>|</span><label class="collapse" for="c-38859789">[-]</label><label class="expand" for="c-38859789">[1 more]</label></div><br/><div class="children"><div class="content">From the paper&#x27;s abstract [1]:<p><i>It stands out by requiring merely a 24G GPU for training and an 8G GPU or CPU for inference. Built upon Phi-2, TinyGPT-V couples an effective language backbone with pre-trained vision modules from BLIP-2 or CLIP. TinyGPT-V&#x27;s 2.8B parameters can undergo a unique quantisation process, suitable for local deployment and inference tasks on 8G various devices.</i><p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2312.16862" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2312.16862</a></div><br/></div></div><div id="38862107" class="c"><input type="checkbox" id="c-38862107" checked=""/><div class="controls bullet"><span class="by">infotainment</span><span>|</span><a href="#38864473">prev</a><span>|</span><a href="#38860785">next</a><span>|</span><label class="collapse" for="c-38862107">[-]</label><label class="expand" for="c-38862107">[3 more]</label></div><br/><div class="children"><div class="content">While the non-commercial Phi2 license continues to be a letdown, I&#x27;m excited to see additional development in the space of these ultracompact LLMs. On-device AI excites me far more than relying on yet another cloud-based API.<p>On my M1 Macbook Air, current LLMs can run surprisingly quickly locally already.</div><br/><div id="38862876" class="c"><input type="checkbox" id="c-38862876" checked=""/><div class="controls bullet"><span class="by">rablackburn</span><span>|</span><a href="#38862107">parent</a><span>|</span><a href="#38860785">next</a><span>|</span><label class="collapse" for="c-38862876">[-]</label><label class="expand" for="c-38862876">[2 more]</label></div><br/><div class="children"><div class="content">&gt; On-device AI excites me far more than relying on yet another cloud-based API.<p>Agreed. The huge leap-forward in capability already boggles my mind, the fact that I can run it _on my desktop cpu_ and have an interactive, natural-language oracle of the internet in a mere ~11GB file.<p>Everyone seems to be chasing the network-accessible API approach because lock-in is easy and if you&#x27;re at the bleeding-edge (training) you&#x27;ve got the compute for running it anyway.<p>But now with accessible, local models my bet is on bored young hackers coming up with the best use cases and killer apps, not Microsoft.</div><br/><div id="38863362" class="c"><input type="checkbox" id="c-38863362" checked=""/><div class="controls bullet"><span class="by">triyambakam</span><span>|</span><a href="#38862107">root</a><span>|</span><a href="#38862876">parent</a><span>|</span><a href="#38860785">next</a><span>|</span><label class="collapse" for="c-38863362">[-]</label><label class="expand" for="c-38863362">[1 more]</label></div><br/><div class="children"><div class="content">Which local models and CPU are you using?</div><br/></div></div></div></div></div></div><div id="38860785" class="c"><input type="checkbox" id="c-38860785" checked=""/><div class="controls bullet"><span class="by">justinl33</span><span>|</span><a href="#38862107">prev</a><span>|</span><a href="#38860424">next</a><span>|</span><label class="collapse" for="c-38860785">[-]</label><label class="expand" for="c-38860785">[4 more]</label></div><br/><div class="children"><div class="content">&gt; <i>You need to execute the above code 17 times to complete the first stage of training.</i><p>Am I missing something here? Did the authors forget about for loops? What happens if you only do it 16 times?</div><br/><div id="38861023" class="c"><input type="checkbox" id="c-38861023" checked=""/><div class="controls bullet"><span class="by">sjwhevvvvvsj</span><span>|</span><a href="#38860785">parent</a><span>|</span><a href="#38860424">next</a><span>|</span><label class="collapse" for="c-38861023">[-]</label><label class="expand" for="c-38861023">[3 more]</label></div><br/><div class="children"><div class="content">Ever feed a Gremlin after midnight?  Same thing.</div><br/><div id="38863778" class="c"><input type="checkbox" id="c-38863778" checked=""/><div class="controls bullet"><span class="by">RonnieOwnsLexus</span><span>|</span><a href="#38860785">root</a><span>|</span><a href="#38861023">parent</a><span>|</span><a href="#38860424">next</a><span>|</span><label class="collapse" for="c-38863778">[-]</label><label class="expand" for="c-38863778">[2 more]</label></div><br/><div class="children"><div class="content">please explain this reference for non US&#x2F;western hemisphere folks.</div><br/><div id="38864302" class="c"><input type="checkbox" id="c-38864302" checked=""/><div class="controls bullet"><span class="by">jl6</span><span>|</span><a href="#38860785">root</a><span>|</span><a href="#38863778">parent</a><span>|</span><a href="#38860424">next</a><span>|</span><label class="collapse" for="c-38864302">[-]</label><label class="expand" for="c-38864302">[1 more]</label></div><br/><div class="children"><div class="content">In the 1984 movie Gremlins, the protagonist is warned not to feed the titular creatures (a type of transformer) after midnight, otherwise they promptly lose alignment and begin to hallucinate.</div><br/></div></div></div></div></div></div></div></div><div id="38860424" class="c"><input type="checkbox" id="c-38860424" checked=""/><div class="controls bullet"><span class="by">fabmilo</span><span>|</span><a href="#38860785">prev</a><span>|</span><a href="#38863418">next</a><span>|</span><label class="collapse" for="c-38860424">[-]</label><label class="expand" for="c-38860424">[2 more]</label></div><br/><div class="children"><div class="content">Funny I was just looking for something to substitute GPT4V as they are bounding the API usage to few request per day. Sadly this project is built on top of phi-2 that has the non-commercial friendly Microsoft research license.</div><br/><div id="38863820" class="c"><input type="checkbox" id="c-38863820" checked=""/><div class="controls bullet"><span class="by">ashvardanian</span><span>|</span><a href="#38860424">parent</a><span>|</span><a href="#38863418">next</a><span>|</span><label class="collapse" for="c-38863820">[-]</label><label class="expand" for="c-38863820">[1 more]</label></div><br/><div class="children"><div class="content">We have released an even-smaller model - UForm-Gen a couple of weeks ago. It&#x27;s fully open-source, so it may help, but I wouldn&#x27;t expect results anywhere close to the GPT4V, assuming there is over 1000x difference in model size and consumed resources.</div><br/></div></div></div></div><div id="38863418" class="c"><input type="checkbox" id="c-38863418" checked=""/><div class="controls bullet"><span class="by">jn2clark</span><span>|</span><a href="#38860424">prev</a><span>|</span><a href="#38861311">next</a><span>|</span><label class="collapse" for="c-38863418">[-]</label><label class="expand" for="c-38863418">[1 more]</label></div><br/><div class="children"><div class="content">Can anyone comment on an open source multi-modal LLM that can produce structured outputs based on an image? I have not found a good open source one yet (this included), seems to be only closed source that can do this reliably well. Any suggestions are very welcome!</div><br/></div></div><div id="38861311" class="c"><input type="checkbox" id="c-38861311" checked=""/><div class="controls bullet"><span class="by">smpanaro</span><span>|</span><a href="#38863418">prev</a><span>|</span><a href="#38860331">next</a><span>|</span><label class="collapse" for="c-38861311">[-]</label><label class="expand" for="c-38861311">[1 more]</label></div><br/><div class="children"><div class="content">MobileVLM [1] is another recent small multimodal model. They trained their own 1.4B&#x2F;2.7B LLaMa from scratch using RedPajama and Vicuna instead of leveraging Phi-2.<p>The papers only have one common benchmark (GQA, MobileVLM scores better) so hard to say how they compare otherwise.<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2312.16886" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2312.16886</a></div><br/></div></div><div id="38860331" class="c"><input type="checkbox" id="c-38860331" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#38861311">prev</a><span>|</span><a href="#38862170">next</a><span>|</span><label class="collapse" for="c-38860331">[-]</label><label class="expand" for="c-38860331">[1 more]</label></div><br/><div class="children"><div class="content">Their results seem comparable to BLIP-2, shifted over in the diagram.</div><br/></div></div><div id="38862170" class="c"><input type="checkbox" id="c-38862170" checked=""/><div class="controls bullet"><span class="by">tbalsam</span><span>|</span><a href="#38860331">prev</a><span>|</span><a href="#38860599">next</a><span>|</span><label class="collapse" for="c-38862170">[-]</label><label class="expand" for="c-38862170">[4 more]</label></div><br/><div class="children"><div class="content">Doesn&#x27;t Phi-2 have testing data contamination, hence why it&#x27;s performing well on these benchmarks?<p>Most professionals in the field that I&#x27;m near would not touch that model with a 10 foot pole. We desperately need better validation&#x2F;data contamination detection methods.</div><br/><div id="38863374" class="c"><input type="checkbox" id="c-38863374" checked=""/><div class="controls bullet"><span class="by">triyambakam</span><span>|</span><a href="#38862170">parent</a><span>|</span><a href="#38863611">next</a><span>|</span><label class="collapse" for="c-38863374">[-]</label><label class="expand" for="c-38863374">[2 more]</label></div><br/><div class="children"><div class="content">Can you explain why you wouldn&#x27;t want to use it, i.e. what data contamination is?</div><br/><div id="38863421" class="c"><input type="checkbox" id="c-38863421" checked=""/><div class="controls bullet"><span class="by">0xDEADFED5</span><span>|</span><a href="#38862170">root</a><span>|</span><a href="#38863374">parent</a><span>|</span><a href="#38863611">next</a><span>|</span><label class="collapse" for="c-38863421">[-]</label><label class="expand" for="c-38863421">[1 more]</label></div><br/><div class="children"><div class="content">contamination:  training on the questions&#x2F;answers used for LLM benchmarks.<p>the top of the HuggingFace Open LLM leaderboard was pretty meaningless for a bit because many of the top scores were achieved by training on the evaluation data</div><br/></div></div></div></div></div></div><div id="38860599" class="c"><input type="checkbox" id="c-38860599" checked=""/><div class="controls bullet"><span class="by">samstave</span><span>|</span><a href="#38862170">prev</a><span>|</span><label class="collapse" for="c-38860599">[-]</label><label class="expand" for="c-38860599">[3 more]</label></div><br/><div class="children"><div class="content">I really want to understand this post, but I can&#x27;t, may you please direct me, and those noobs like me - to resources to be able to read this? (help anyone to climb the knowledge ladder) ELI3<p>-<p>EDIT - GPT helped me understand better:<p>--<p>&gt;&gt;&gt;  &quot;<i>This model is special because it can do similar tasks as the big models but requires much less computational power1. It’s like having a small but powerful engine that can do the work of a big one. This makes it more accessible for more people to use it&quot;</i><p>---<p>&gt;&gt;&gt;  <i>&quot;TinyGPT-V is built on another model called Phi-2 and uses pre-trained vision modules from BLIP-2 or CLIP1. It has 2.8 billion parameters (these are like the model’s brain cells) and can be further compressed to fit on devices with 8GB memory1. This means you could potentially run this model on your personal computer or even some high-end smartphones1&quot;</i><p>----<p>&gt;&gt;&gt;  <i>&quot;In summary, TinyGPT-V is a step towards making powerful AI models more accessible and efficient, which could lead to their use in a wide range of real-world applications1. The authors have also shared their code and training weights for others to use and learn from1&quot;</i><p>-----<p>This is really interesting if you fan out implications over N time?<p>Here is my thinking:<p>Assume this paper results in a way of &quot;compression-alyzed vision&quot; into a model (a tiny compressed view into a model)<p>Then one, in a few years can imagine &quot;laser views&quot; - that slice through fractals of models to find the result. Resulting in tiny agents that have a heat-seeking-fractal-laser that can navigate giant data based on a method of knowing instantaneously what to <i>exclude</i> (meaning the path is defined by the walls that you already know you do not want to hit, so your steps are always that which helps you forward)<p>--<p>Or am I stating something obvious to all you brainiacs?<p>(no shame, I like thinking out loud)</div><br/><div id="38861052" class="c"><input type="checkbox" id="c-38861052" checked=""/><div class="controls bullet"><span class="by">ricopags</span><span>|</span><a href="#38860599">parent</a><span>|</span><a href="#38861515">next</a><span>|</span><label class="collapse" for="c-38861052">[-]</label><label class="expand" for="c-38861052">[1 more]</label></div><br/><div class="children"><div class="content">I am no brainiac, and it isn&#x27;t super clear from your post what you&#x27;re describing, but here is some info that might help you better convey your question:<p>This is a neural net built by conjoining Phi-2 [the best available small LLM, if you&#x27;ll pardon the contradiction in terms] with pre-trained vision models like BLIP or CLIP. Models are piles of weights[&#x2F;parameters] that are generated by training on datasets.<p>Already work has shown that training a multi-model model from the start results in smaller, more effective model. If you want to know more, check out recent work from CVPR [a vision machine learning conference] from &#x27;23[0] and upcoming work for this year[1]<p>edit to add:<p>The work of MS researcher Chunyuan Li[2] is worth keeping an eye on, particularly recent work like LLaVA-Interactive[3], a multi-model multi-task AI system, might be what you&#x27;re trying to describe with your laser&#x2F;fractal view phrasing.<p>[0]<a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;@VLPTutorial" rel="nofollow">https:&#x2F;&#x2F;www.youtube.com&#x2F;@VLPTutorial</a><p>[1]<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;search&#x2F;?query=cvpr+2024&amp;searchtype=all&amp;source=header" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;search&#x2F;?query=cvpr+2024&amp;searchtype=all&amp;sou...</a><p>[2]<a href="https:&#x2F;&#x2F;chunyuan.li&#x2F;" rel="nofollow">https:&#x2F;&#x2F;chunyuan.li&#x2F;</a><p>[3]<a href="https:&#x2F;&#x2F;llava-vl.github.io&#x2F;llava-interactive&#x2F;" rel="nofollow">https:&#x2F;&#x2F;llava-vl.github.io&#x2F;llava-interactive&#x2F;</a></div><br/></div></div><div id="38861515" class="c"><input type="checkbox" id="c-38861515" checked=""/><div class="controls bullet"><span class="by">conacts</span><span>|</span><a href="#38860599">parent</a><span>|</span><a href="#38861052">prev</a><span>|</span><label class="collapse" for="c-38861515">[-]</label><label class="expand" for="c-38861515">[1 more]</label></div><br/><div class="children"><div class="content">thanks for sharing this!</div><br/></div></div></div></div></div></div></div></div></div></body></html>