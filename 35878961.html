<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1683709263271" as="style"/><link rel="stylesheet" href="styles.css?v=1683709263271"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://jack-vanlightly.com/blog/2023/5/9/is-sequential-io-dead-in-the-era-of-the-nvme-drive">Is sequential IO dead in the era of the NVMe drive?</a> <span class="domain">(<a href="https://jack-vanlightly.com">jack-vanlightly.com</a>)</span></div><div class="subtext"><span>eatonphil</span> | <span>130 comments</span></div><br/><div><div id="35885099" class="c"><input type="checkbox" id="c-35885099" checked=""/><div class="controls bullet"><span class="by">jillesvangurp</span><span>|</span><a href="#35879440">next</a><span>|</span><label class="collapse" for="c-35885099">[-]</label><label class="expand" for="c-35885099">[1 more]</label></div><br/><div class="children"><div class="content">I think append only type logs like Kafka are more important from a semantic point of view than from an IO point of view. The importance of having an immutable, append only data store is that it is immutable. That gives you some nice properties with consistency in especially distributed setups and synchronizing state between different nodes.<p>The IO write speed is a bit of a secondary concern. And since the data is immutable, you don&#x27;t actually overwrite it all the time. And of course, SSDs have been around for quite long and spinning disk was already on the way out over a decade ago on high end servers. So, things like Kafka and event sourcing mostly became popular after that started happening; not before. This was never really about SSDs vs. spinning disks. Instead it is about the guarantees that come with having immutable data in a distributed setup. You see that in the Elasticsearch world as well (lucene uses append only data structures). Elasticsearch emerged around 2010. Almost from the beginning the consensus was that ssd was vastly preferable to spinning disks for scaling. Not because of the writes but for reads. Of course lots of people still used spinning disk around the time as well.<p>So, sequential reads and writes are two things. Elasticsearch writes sequentially; but reading is random access. Which is why SSDs are nice to have for Elasticsearch clusters even though it uses append only storage.</div><br/></div></div><div id="35879440" class="c"><input type="checkbox" id="c-35879440" checked=""/><div class="controls bullet"><span class="by">cperciva</span><span>|</span><a href="#35885099">prev</a><span>|</span><a href="#35881242">next</a><span>|</span><label class="collapse" for="c-35879440">[-]</label><label class="expand" for="c-35879440">[38 more]</label></div><br/><div class="children"><div class="content">Moving from spinning rust to solid-state storage dramatically improved the performance of random <i>reads</i>, but random <i>writes</i> still carry a penalty compared to sequential writes.  So sequential I&#x2F;O is perhaps half-dead; there&#x27;s no need to optimize layouts for sequential reads any more.</div><br/><div id="35879918" class="c"><input type="checkbox" id="c-35879918" checked=""/><div class="controls bullet"><span class="by">jasonwatkinspdx</span><span>|</span><a href="#35879440">parent</a><span>|</span><a href="#35880881">next</a><span>|</span><label class="collapse" for="c-35879918">[-]</label><label class="expand" for="c-35879918">[24 more]</label></div><br/><div class="children"><div class="content">Yes. And the Flash Translation Layer inside modern drives are quite sophisticated. This means you could start out with a write pattern that&#x27;s causing a lot of copying and reorganization of pages in the back ground, typically because it&#x27;s doing a lot of unaligned random writes smaller than the erase block size, and initially the performance will be fine. But as the drive wears in, and the capacity fills up, the FTL has less extra space to work with, forcing this garbage collection activity to become increasingly common. So you get performance degradation and premature wearing of the drive in a way that&#x27;s opaque.</div><br/><div id="35880915" class="c"><input type="checkbox" id="c-35880915" checked=""/><div class="controls bullet"><span class="by">bcrl</span><span>|</span><a href="#35879440">root</a><span>|</span><a href="#35879918">parent</a><span>|</span><a href="#35880881">next</a><span>|</span><label class="collapse" for="c-35880915">[-]</label><label class="expand" for="c-35880915">[23 more]</label></div><br/><div class="children"><div class="content">Short stroking SSDs is a commonly used trick in the toolbag of wise sysadmins for dealing with this kind of workload.</div><br/><div id="35883343" class="c"><input type="checkbox" id="c-35883343" checked=""/><div class="controls bullet"><span class="by">usernew</span><span>|</span><a href="#35879440">root</a><span>|</span><a href="#35880915">parent</a><span>|</span><a href="#35881247">next</a><span>|</span><label class="collapse" for="c-35883343">[-]</label><label class="expand" for="c-35883343">[6 more]</label></div><br/><div class="children"><div class="content">Umm sorry, no. I&#x27;m a storage engineer with one of the leading enterprise vendors.  SSDs are already &quot;short-stroked.&quot;  They have much more space available internally than you can see through the controller externally.  The more &quot;enterprisey&quot; the drive, the more of that hidden space there is.<p>A wise sysadmin buys a drive with more of that hidden internal space, instead of getting a larger drive with less hidden space.  The logic inside the drive is much better at zeroing&#x2F;out and background trim on that internal hidden space, than host-addressable blocks that are currently unused.  That&#x27;s because the drive has no idea if you&#x27;re about to use them, while the hidden space is guaranteed to be always free.<p>In fact - a fun fact.  Flash has been used as a Write buffer on storage arrays for a couple of decades.</div><br/><div id="35885044" class="c"><input type="checkbox" id="c-35885044" checked=""/><div class="controls bullet"><span class="by">marginalia_nu</span><span>|</span><a href="#35879440">root</a><span>|</span><a href="#35883343">parent</a><span>|</span><a href="#35883943">next</a><span>|</span><label class="collapse" for="c-35885044">[-]</label><label class="expand" for="c-35885044">[1 more]</label></div><br/><div class="children"><div class="content">Most of this hides the problem. It still exists though.<p>I have a piece of logic in my indexing code that essentially transposes an ~100 Gb multi-value dictionary on disk. If you do this the naive way with random writes, the write amplification makes it a complete non-starter. All the caching layers and buffers fill up with completely disjointed 8 byte writes and it takes <i>ages</i> to write.<p>What I&#x27;ve ended up doing is to in an intermediate stage write the data to be written into a series of files, containing pairs of offsets and data (up to like 100Mb each); and then going over the files one by one and essentially evaluating them as assembly instructions.<p>Both passes have relatively good data locality, and despite essentially writing 2.5X as much shit to disk, it takes hours rather than weeks to do this operation.</div><br/></div></div><div id="35883943" class="c"><input type="checkbox" id="c-35883943" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#35879440">root</a><span>|</span><a href="#35883343">parent</a><span>|</span><a href="#35885044">prev</a><span>|</span><a href="#35881247">next</a><span>|</span><label class="collapse" for="c-35883943">[-]</label><label class="expand" for="c-35883943">[4 more]</label></div><br/><div class="children"><div class="content"><i>A wise sysadmin buys a drive with more of that hidden internal space, instead of getting a larger drive with less hidden space.</i><p>The pricing of enterprise SSDs is... not exactly fair for the performance you get.</div><br/><div id="35884195" class="c"><input type="checkbox" id="c-35884195" checked=""/><div class="controls bullet"><span class="by">usernew</span><span>|</span><a href="#35879440">root</a><span>|</span><a href="#35883943">parent</a><span>|</span><a href="#35881247">next</a><span>|</span><label class="collapse" for="c-35884195">[-]</label><label class="expand" for="c-35884195">[3 more]</label></div><br/><div class="children"><div class="content">that&#x27;s like complaining a bentley is too expensive and not good at moving your piano.<p>when you get fined $1mil&#x2F;min by the SEC for downtime, and data loss or corruption can cost you in the hundreds of millions, or someone dying at a hospital, enterprise gear is cheap. reliability is the key and for what you are paying. now yes, there may be a specific consumer drive more reliable than a specific enterprise drive.  so which do you buy?  well the enterprise array vendor tested the crap out of everything in every combination and workload and environment, and picked one for you, and it comes with full support and SLAs that you can use to meet gov regulations.<p>performance for an enterprise drive is not something you usually consider, at all.  in fact, did you know that when I quote a storage array, I can&#x27;t even specify the drive type of vendor, and who knows what will get shipped? Ionly specify drive size.<p>your performance comes from all your workloads clumped together, spread over a thousand of these drives connected with infiniband, with dedupe and compression on the backend, and hundreds of terabytes of RAM. the perf of an individual drive is not relevant.<p>but yes, sticking it into an AMD server you bought on newegg when they had a sale is not its purpose, and is a very bad deal.<p>now when we talk about wise sysadmins, we&#x27;re talking about guys who know their stuff, and do &quot;important big stuff.&quot; not a guy at a small business ordering from newegg. and that guy - he shouldn&#x27;t be coming up with any storage policies because he lacks the needed large-scale experience.<p>I sold a 1PB usable-effective (after 3x dedupe&#x2F;compression) space storage array last year. It was $2mil, after a 65% discount.  If one time in its 5year lifecycle the &quot;enterprisey&quot; stuff on that array prevents about 10 seconds of downtime, it paid for itself.</div><br/><div id="35884528" class="c"><input type="checkbox" id="c-35884528" checked=""/><div class="controls bullet"><span class="by">wongarsu</span><span>|</span><a href="#35879440">root</a><span>|</span><a href="#35884195">parent</a><span>|</span><a href="#35881247">next</a><span>|</span><label class="collapse" for="c-35884528">[-]</label><label class="expand" for="c-35884528">[2 more]</label></div><br/><div class="children"><div class="content">Sure, and judging by System Z sales there&#x27;s still a demand for earthquake-resistant machines with hot-swappable CPUs.<p>But for <i>most</i> of us, infrequent downtime is acceptable, and most single-machine downtimes are either automatically mitigated or have very limited impact. In that scenario, getting a prosumer SSD and over-provisioning it can be a sensible choice.</div><br/><div id="35885060" class="c"><input type="checkbox" id="c-35885060" checked=""/><div class="controls bullet"><span class="by">geraldhh</span><span>|</span><a href="#35879440">root</a><span>|</span><a href="#35884528">parent</a><span>|</span><a href="#35881247">next</a><span>|</span><label class="collapse" for="c-35885060">[-]</label><label class="expand" for="c-35885060">[1 more]</label></div><br/><div class="children"><div class="content">* Hetzner joined the chat</div><br/></div></div></div></div></div></div></div></div></div></div><div id="35881247" class="c"><input type="checkbox" id="c-35881247" checked=""/><div class="controls bullet"><span class="by">oakwhiz</span><span>|</span><a href="#35879440">root</a><span>|</span><a href="#35880915">parent</a><span>|</span><a href="#35883343">prev</a><span>|</span><a href="#35884572">next</a><span>|</span><label class="collapse" for="c-35881247">[-]</label><label class="expand" for="c-35881247">[3 more]</label></div><br/><div class="children"><div class="content">I wish that it was easier to trim unused unpartitioned space. I find myself fumbling on BSD and Linux trying to remember how to get the exact offsets for that. You could write zeroes but there isn&#x27;t a guarantee that the FTL will interpret that as unused.</div><br/><div id="35881527" class="c"><input type="checkbox" id="c-35881527" checked=""/><div class="controls bullet"><span class="by">mindslight</span><span>|</span><a href="#35879440">root</a><span>|</span><a href="#35881247">parent</a><span>|</span><a href="#35881897">next</a><span>|</span><label class="collapse" for="c-35881527">[-]</label><label class="expand" for="c-35881527">[1 more]</label></div><br/><div class="children"><div class="content">Just make another partition for the unused space, and run blkdiscard on it?</div><br/></div></div></div></div><div id="35884572" class="c"><input type="checkbox" id="c-35884572" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#35879440">root</a><span>|</span><a href="#35880915">parent</a><span>|</span><a href="#35881247">prev</a><span>|</span><a href="#35881153">next</a><span>|</span><label class="collapse" for="c-35884572">[-]</label><label class="expand" for="c-35884572">[3 more]</label></div><br/><div class="children"><div class="content">Why not just buy SLC?</div><br/><div id="35884706" class="c"><input type="checkbox" id="c-35884706" checked=""/><div class="controls bullet"><span class="by">snovv_crash</span><span>|</span><a href="#35879440">root</a><span>|</span><a href="#35884572">parent</a><span>|</span><a href="#35884652">next</a><span>|</span><label class="collapse" for="c-35884706">[-]</label><label class="expand" for="c-35884706">[1 more]</label></div><br/><div class="children"><div class="content">SLC still has a minimum block size that gets used during writes.</div><br/></div></div><div id="35884652" class="c"><input type="checkbox" id="c-35884652" checked=""/><div class="controls bullet"><span class="by">LinAGKar</span><span>|</span><a href="#35879440">root</a><span>|</span><a href="#35884572">parent</a><span>|</span><a href="#35884706">prev</a><span>|</span><a href="#35881153">next</a><span>|</span><label class="collapse" for="c-35884652">[-]</label><label class="expand" for="c-35884652">[1 more]</label></div><br/><div class="children"><div class="content">Does that even exist anymore?</div><br/></div></div></div></div><div id="35881153" class="c"><input type="checkbox" id="c-35881153" checked=""/><div class="controls bullet"><span class="by">Waterluvian</span><span>|</span><a href="#35879440">root</a><span>|</span><a href="#35880915">parent</a><span>|</span><a href="#35884572">prev</a><span>|</span><a href="#35880881">next</a><span>|</span><label class="collapse" for="c-35881153">[-]</label><label class="expand" for="c-35881153">[10 more]</label></div><br/><div class="children"><div class="content">I’m guessing that means pretending they’re smaller than they really are?</div><br/><div id="35881224" class="c"><input type="checkbox" id="c-35881224" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#35879440">root</a><span>|</span><a href="#35881153">parent</a><span>|</span><a href="#35881185">next</a><span>|</span><label class="collapse" for="c-35881224">[-]</label><label class="expand" for="c-35881224">[1 more]</label></div><br/><div class="children"><div class="content">Yes. The term dates back to hard drives, where using only a fraction of their capacity would minimize the worst-case distance the heads needed to travel, reducing the portion of seek latency due to head movements (but not helping with rotational latency).<p>On most hard drives, the beginning of the logical block address space corresponds to the outer edges of the platters where the bits are going past heads with the highest linear velocity, so sequential throughput is higher than elsewhere on the disk.</div><br/></div></div><div id="35881185" class="c"><input type="checkbox" id="c-35881185" checked=""/><div class="controls bullet"><span class="by">colordrops</span><span>|</span><a href="#35879440">root</a><span>|</span><a href="#35881153">parent</a><span>|</span><a href="#35881224">prev</a><span>|</span><a href="#35880881">next</a><span>|</span><label class="collapse" for="c-35881185">[-]</label><label class="expand" for="c-35881185">[8 more]</label></div><br/><div class="children"><div class="content">Looks like that&#x27;s the case based on a cursory google search.  Seems analogous to only charging batteries up to 80%.  I wish companies would factor these issues into their products and leave invisible buffers, but I guess making them cheaper is more important than being consistent and reliable.</div><br/><div id="35884613" class="c"><input type="checkbox" id="c-35884613" checked=""/><div class="controls bullet"><span class="by">wongarsu</span><span>|</span><a href="#35879440">root</a><span>|</span><a href="#35881185">parent</a><span>|</span><a href="#35881295">next</a><span>|</span><label class="collapse" for="c-35884613">[-]</label><label class="expand" for="c-35884613">[1 more]</label></div><br/><div class="children"><div class="content">Most consumers make purchasing decisions against these buffers.<p>If you have two identical electric cars for the same price, one with a range of 350 miles and one with a range of 280 miles, which one would you buy?<p>If you have two identical phones, one with a 4300 mAh rated battery and another with a 3400 mAh battery, which one would you buy?<p>If you have two equally priced SSDs, one with 500GB capacity and one with 480GB, which one would you buy?<p>To enterprise customers the sales rep will explain &quot;look, this device has a bit less space, but the write endurance number quoted here is much better, over 5 years and 5000 drives this will lower your TCO by X&quot;. In the consumer space you build your entire brand around one price-reliability tradeoff and stick to it. Usually the cheaper one with the bigger headline stat sells the most, and everyone else has to make up the lost volume with margin.</div><br/></div></div><div id="35881295" class="c"><input type="checkbox" id="c-35881295" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#35879440">root</a><span>|</span><a href="#35881185">parent</a><span>|</span><a href="#35884613">prev</a><span>|</span><a href="#35881235">next</a><span>|</span><label class="collapse" for="c-35881295">[-]</label><label class="expand" for="c-35881295">[5 more]</label></div><br/><div class="children"><div class="content">They <i>do</i> leave invisible buffers, commonly referred to as over provisioning. The discrepancy between GB and GiB is ~7% hiding in plain sight, and then more expensive enterprise drives are commonly sold in (usable) capacities like 960GB and 7.68TB rather than 1TB and 8TB.</div><br/><div id="35881695" class="c"><input type="checkbox" id="c-35881695" checked=""/><div class="controls bullet"><span class="by">andrecarini</span><span>|</span><a href="#35879440">root</a><span>|</span><a href="#35881295">parent</a><span>|</span><a href="#35882665">next</a><span>|</span><label class="collapse" for="c-35881695">[-]</label><label class="expand" for="c-35881695">[2 more]</label></div><br/><div class="children"><div class="content">1 TB = 930 GiB<p>The value difference is not because of invisible buffers! Marketing material and usable OS space are measured in different units, but the usable amount of bits in each value is exactly the same.</div><br/><div id="35881750" class="c"><input type="checkbox" id="c-35881750" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#35879440">root</a><span>|</span><a href="#35881695">parent</a><span>|</span><a href="#35882665">next</a><span>|</span><label class="collapse" for="c-35881750">[-]</label><label class="expand" for="c-35881750">[1 more]</label></div><br/><div class="children"><div class="content">No, there really are invisible buffers resulting from this discrepancy. The raw flash chips have nominal capacities that align with power of two sizes, but the drives use decimal-based units. So a drive advertised as 1TB will have a usable capacity of approximately 1TB, but is assembled from flash chips with a total capacity of at least 1 TiB.</div><br/></div></div></div></div><div id="35882665" class="c"><input type="checkbox" id="c-35882665" checked=""/><div class="controls bullet"><span class="by">Osiris</span><span>|</span><a href="#35879440">root</a><span>|</span><a href="#35881295">parent</a><span>|</span><a href="#35881695">prev</a><span>|</span><a href="#35881235">next</a><span>|</span><label class="collapse" for="c-35882665">[-]</label><label class="expand" for="c-35882665">[2 more]</label></div><br/><div class="children"><div class="content">Some flash based drives will even report this value in SMART, the amount of the over-provisioned space that has been used.<p>Modern QLC has very low write endurance so those drives need to have spare space to use when it starts to wear out.</div><br/><div id="35883297" class="c"><input type="checkbox" id="c-35883297" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#35879440">root</a><span>|</span><a href="#35882665">parent</a><span>|</span><a href="#35881235">next</a><span>|</span><label class="collapse" for="c-35883297">[-]</label><label class="expand" for="c-35883297">[1 more]</label></div><br/><div class="children"><div class="content">I think there are two things you may be referring to. First, some drives support thin provisioning of namespaces, and can report the current amount of storage actually being used to back the namespace(s), relative to the maximum user-accessible capacity.<p>Second, in the SMART&#x2F;drive health data, there&#x27;s usually a counter tracking how many reserve&#x2F;spare blocks remain usable as in not worn out and retired. That&#x27;s a one-way counter; deleting data won&#x27;t un-retire defective or worn-out blocks.<p>It&#x27;s almost unheard-of for a drive to directly expose a realtime counter of how many blocks are currently unallocated, not being used to store data, and in the erased state making them available to accept new writes.</div><br/></div></div></div></div></div></div><div id="35881235" class="c"><input type="checkbox" id="c-35881235" checked=""/><div class="controls bullet"><span class="by">Waterluvian</span><span>|</span><a href="#35879440">root</a><span>|</span><a href="#35881185">parent</a><span>|</span><a href="#35881295">prev</a><span>|</span><a href="#35880881">next</a><span>|</span><label class="collapse" for="c-35881235">[-]</label><label class="expand" for="c-35881235">[1 more]</label></div><br/><div class="children"><div class="content">Sales and marketing will never leave anything on the table that might make their job easier.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="35880881" class="c"><input type="checkbox" id="c-35880881" checked=""/><div class="controls bullet"><span class="by">loeg</span><span>|</span><a href="#35879440">parent</a><span>|</span><a href="#35879918">prev</a><span>|</span><a href="#35884677">next</a><span>|</span><label class="collapse" for="c-35880881">[-]</label><label class="expand" for="c-35880881">[1 more]</label></div><br/><div class="children"><div class="content">&gt; So sequential I&#x2F;O is perhaps half-dead; there&#x27;s no need to optimize layouts for sequential reads any more.<p>Yes and no.  Large contiguous IOs are still faster to read than a bunch of random small sectors.  You generally want your frequently read files&#x2F;blobs to be split into as few operations as possible.<p><a href="https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;abs&#x2F;10.1145&#x2F;3477132.3483593" rel="nofollow">https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;abs&#x2F;10.1145&#x2F;3477132.3483593</a></div><br/></div></div><div id="35884677" class="c"><input type="checkbox" id="c-35884677" checked=""/><div class="controls bullet"><span class="by">NovaDudely</span><span>|</span><a href="#35879440">parent</a><span>|</span><a href="#35880881">prev</a><span>|</span><a href="#35880136">next</a><span>|</span><label class="collapse" for="c-35884677">[-]</label><label class="expand" for="c-35884677">[3 more]</label></div><br/><div class="children"><div class="content">It was Nautilus file manager in Ubuntu that used to drag if you opened a folder with a few thousand files. It was it was essentially doing at least one request for each file when you opened a folder and this was limited by the spin rate of the HDD. So if you had 5,400 files and a 5400RPM drive, yep that would take a full minute to resolve.<p>Eventually the solution really just become - get an SSD because you could throw thousand of requests and the results was fast enough.</div><br/><div id="35884945" class="c"><input type="checkbox" id="c-35884945" checked=""/><div class="controls bullet"><span class="by">vistu</span><span>|</span><a href="#35879440">root</a><span>|</span><a href="#35884677">parent</a><span>|</span><a href="#35880136">next</a><span>|</span><label class="collapse" for="c-35884945">[-]</label><label class="expand" for="c-35884945">[2 more]</label></div><br/><div class="children"><div class="content">While I do not doubt that Nautilus did that, I don&#x27;t think it&#x27;s a given that a 5400 RPM drive will take exactly a minute to scan through 5400 files.</div><br/><div id="35885075" class="c"><input type="checkbox" id="c-35885075" checked=""/><div class="controls bullet"><span class="by">geraldhh</span><span>|</span><a href="#35879440">root</a><span>|</span><a href="#35884945">parent</a><span>|</span><a href="#35880136">next</a><span>|</span><label class="collapse" for="c-35885075">[-]</label><label class="expand" for="c-35885075">[1 more]</label></div><br/><div class="children"><div class="content">a sensible conclusion</div><br/></div></div></div></div></div></div><div id="35880136" class="c"><input type="checkbox" id="c-35880136" checked=""/><div class="controls bullet"><span class="by">ilyt</span><span>|</span><a href="#35879440">parent</a><span>|</span><a href="#35884677">prev</a><span>|</span><a href="#35880967">next</a><span>|</span><label class="collapse" for="c-35880136">[-]</label><label class="expand" for="c-35880136">[1 more]</label></div><br/><div class="children"><div class="content">Sequential reads will still generally be faster at least for small ones, just that NVMe are fast enough that this rarely matters</div><br/></div></div><div id="35880967" class="c"><input type="checkbox" id="c-35880967" checked=""/><div class="controls bullet"><span class="by">xenadu02</span><span>|</span><a href="#35879440">parent</a><span>|</span><a href="#35880136">prev</a><span>|</span><a href="#35880923">next</a><span>|</span><label class="collapse" for="c-35880967">[-]</label><label class="expand" for="c-35880967">[3 more]</label></div><br/><div class="children"><div class="content">Flash controllers provide an abstraction layer that should turn any write workload into a sequential one, freeing software developers from having to care about it.</div><br/><div id="35881376" class="c"><input type="checkbox" id="c-35881376" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#35879440">root</a><span>|</span><a href="#35880967">parent</a><span>|</span><a href="#35880923">next</a><span>|</span><label class="collapse" for="c-35881376">[-]</label><label class="expand" for="c-35881376">[2 more]</label></div><br/><div class="children"><div class="content">That abstraction layer cannot always avoid creating write amplification when you send it writes that are too small.</div><br/><div id="35881483" class="c"><input type="checkbox" id="c-35881483" checked=""/><div class="controls bullet"><span class="by">dsr_</span><span>|</span><a href="#35879440">root</a><span>|</span><a href="#35881376">parent</a><span>|</span><a href="#35880923">next</a><span>|</span><label class="collapse" for="c-35881483">[-]</label><label class="expand" for="c-35881483">[1 more]</label></div><br/><div class="children"><div class="content">Which is why having a nice chunk of RAM on the disk itself is useful. 2x write queue size x block size is nice.</div><br/></div></div></div></div></div></div><div id="35880887" class="c"><input type="checkbox" id="c-35880887" checked=""/><div class="controls bullet"><span class="by">almog</span><span>|</span><a href="#35879440">parent</a><span>|</span><a href="#35880923">prev</a><span>|</span><a href="#35879698">next</a><span>|</span><label class="collapse" for="c-35880887">[-]</label><label class="expand" for="c-35880887">[1 more]</label></div><br/><div class="children"><div class="content">To add to that, the penalty for random writes is paid not only with performance but also in durability of the drive.</div><br/></div></div><div id="35879698" class="c"><input type="checkbox" id="c-35879698" checked=""/><div class="controls bullet"><span class="by">GauntletWizard</span><span>|</span><a href="#35879440">parent</a><span>|</span><a href="#35880887">prev</a><span>|</span><a href="#35880373">next</a><span>|</span><label class="collapse" for="c-35879698">[-]</label><label class="expand" for="c-35879698">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s still a ton of performance benefit that can be gained by keeping your read pipeline full, and you can most easily keep your read pipeline full by making it guessable by doing readahead. It doesn&#x27;t need to be strictly disk-layout sequential, file-layout sequential will usually do.</div><br/></div></div><div id="35879709" class="c"><input type="checkbox" id="c-35879709" checked=""/><div class="controls bullet"><span class="by">Proven</span><span>|</span><a href="#35879440">parent</a><span>|</span><a href="#35880373">prev</a><span>|</span><a href="#35881242">next</a><span>|</span><label class="collapse" for="c-35879709">[-]</label><label class="expand" for="c-35879709">[1 more]</label></div><br/><div class="children"><div class="content">Far from it, sequential IO optimization isn&#x27;t dead.<p>Sequential reads need to be optimized to produce optimal performance for selected workload. That usually means applying a lower level of inline compression.<p>In some cases deduplication works better before, in other after, compression. Sometimes post-process dedupe is more suitable than inline.<p>Then there&#x27;s erasure coding and data protection methods that are still being optimized for NVMe with sequential workloads, including random workloads which are being sequentialized to work better with latest flash media.<p>I would even say developments in sequential IO are becoming more important than random IO.</div><br/></div></div></div></div><div id="35881242" class="c"><input type="checkbox" id="c-35881242" checked=""/><div class="controls bullet"><span class="by">jrockway</span><span>|</span><a href="#35879440">prev</a><span>|</span><a href="#35880523">next</a><span>|</span><label class="collapse" for="c-35881242">[-]</label><label class="expand" for="c-35881242">[17 more]</label></div><br/><div class="children"><div class="content">I always worry that hardware &quot;locks in&quot; software.  SSDs showed up after everyone wrote their databases to be optimal on spinning rust, so SSDs were built to look at the flow of requests, assume that the application was written for spinning rust, and optimize the data layout accordingly.  Whether or not this is as good as brand-new software that managed the layout itself is debatable, and nobody will write that software anyway because they won&#x27;t have users that have SSDs that let the database manage the disk in full.  So the software has to tune it for one manufacturer&#x27;s reorganization algorithm and hope for the best.  The path to a &quot;global maximum&quot; remains unclear because of this.<p>CPUs are similar.  People started writing programs in C, so CPUs started optimizing for the outputs of C compilers.  If someone were to invent CPUs and compilers today, the list of optimizations would probably be different, and the performance characteristics of real-world software would probably be different.  (It&#x27;s not just C; people wanted virtual address spaces, and that was slow in software, so now there is a TLB, etc.)<p>Actually, it goes even deeper.  Half the startups I see on HN related to software operations have a quickstart like &quot;don&#x27;t worry, you don&#x27;t have to rewrite your code, we&#x27;ll magically do everything for you&quot;.  Why not just refactor the code?  It would take about an hour, and then you don&#x27;t need a crazy Rube Goldberg machine to achieve your desired results.  (If you want specifics, think service meshes, and how they now detect what language your app is written in so they can rewrite your HTTP handling functions to pass around a trace ID.  Back in the day, you just set outgoing.Headers[&quot;x-trace-id&quot;] to incoming.Headers[&quot;x-trace-id&quot;].  Now we have layers and layers of OS-level machinery that still don&#x27;t do as good a job as spending half a day adjusting your codebase.  Billions of dollars invested in saving half a developer day!  Wow!)</div><br/><div id="35881336" class="c"><input type="checkbox" id="c-35881336" checked=""/><div class="controls bullet"><span class="by">paulmd</span><span>|</span><a href="#35881242">parent</a><span>|</span><a href="#35881454">next</a><span>|</span><label class="collapse" for="c-35881336">[-]</label><label class="expand" for="c-35881336">[5 more]</label></div><br/><div class="children"><div class="content">Probably the biggest ground-up rethink of SSDs that I&#x27;ve seen is the Samsung SSDs with a key-value store mode.<p>Like, think about it, a filesystem is really a database, right?  Copy-on-write even makes the &quot;transactionality&quot; explicit.  And a lot of high-performance databases will go farther and skip the filesystem and treat the device as block storage... which it is.  The filesystem is a leaky abstraction with filesystem blocks and flash pages and flash block erasure, etc.<p>Well, what if the SSD was just an object store?  In that model you slice away a bunch of layers of abstraction and just let the SSD worry about all that.  SSD says it&#x27;s committed?  Alright then, guess it is.<p>Obviously you are very much at the mercy of the SSD to implement ACID correctly however...</div><br/><div id="35881513" class="c"><input type="checkbox" id="c-35881513" checked=""/><div class="controls bullet"><span class="by">r1ch</span><span>|</span><a href="#35881242">root</a><span>|</span><a href="#35881336">parent</a><span>|</span><a href="#35881823">next</a><span>|</span><label class="collapse" for="c-35881513">[-]</label><label class="expand" for="c-35881513">[2 more]</label></div><br/><div class="children"><div class="content">Optane was also a contender for the biggest fundamental shift - byte addressable persistent memory with insane endurance is simply in a different class to every other SSD technology out there. No need for a FTL. No need for a DRAM &#x2F; SLC cache. Unfortunately no software really took advantage of it, OSes still provisioned it as 512&#x2F;4096 byte sectors, etc. so in standard benchmarks it never really looked that compelling over a regular NVME drive. But if the access patterns align, nothing can come close. Truly a technology ahead of its time.</div><br/><div id="35881670" class="c"><input type="checkbox" id="c-35881670" checked=""/><div class="controls bullet"><span class="by">MichaelZuo</span><span>|</span><a href="#35881242">root</a><span>|</span><a href="#35881513">parent</a><span>|</span><a href="#35881823">next</a><span>|</span><label class="collapse" for="c-35881670">[-]</label><label class="expand" for="c-35881670">[1 more]</label></div><br/><div class="children"><div class="content">It always made me surprised that Intel didn&#x27;t market it better, such as forking linux and writing some custom additions that could allow the Optane drives to demonstrate real world 10x improvements.</div><br/></div></div></div></div><div id="35881823" class="c"><input type="checkbox" id="c-35881823" checked=""/><div class="controls bullet"><span class="by">throwawaylinux</span><span>|</span><a href="#35881242">root</a><span>|</span><a href="#35881336">parent</a><span>|</span><a href="#35881513">prev</a><span>|</span><a href="#35881454">next</a><span>|</span><label class="collapse" for="c-35881823">[-]</label><label class="expand" for="c-35881823">[2 more]</label></div><br/><div class="children"><div class="content">Key value FTLs have been around for about as long as FTLs. The logical block address -&gt; NAND page forward map is just a key value store after all. It can potentially take one layer of indirection out of applications but you still have one there (the FTL). Or alternatively exposing the raw NAND and having the software manage it is another common way to go (indirection is in software now instead of hardware&#x2F;firmware).<p>When I worked on FTLs it was always a thing executives and product managers liked to talk about but didn&#x27;t seem to provide incredible results and people generally didn&#x27;t like using them because it was just harder &#x2F; different to manage. Has that changed?</div><br/><div id="35881911" class="c"><input type="checkbox" id="c-35881911" checked=""/><div class="controls bullet"><span class="by">anyfoo</span><span>|</span><a href="#35881242">root</a><span>|</span><a href="#35881823">parent</a><span>|</span><a href="#35881454">next</a><span>|</span><label class="collapse" for="c-35881911">[-]</label><label class="expand" for="c-35881911">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Key value FTLs have been around for about as long as FTLs.<p>Is this similar to what IBM had since the 60s already on their hard disks (which they call &quot;DASD&quot;), namely &quot;CKD&quot; (count-key-data) storage?</div><br/></div></div></div></div></div></div><div id="35881454" class="c"><input type="checkbox" id="c-35881454" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#35881242">parent</a><span>|</span><a href="#35881336">prev</a><span>|</span><a href="#35881302">next</a><span>|</span><label class="collapse" for="c-35881454">[-]</label><label class="expand" for="c-35881454">[1 more]</label></div><br/><div class="children"><div class="content">Typical SSDs all assume your IO will be in 4kB-aligned chunks even though the storage is denominated in 512-byte sectors for compatibility reasons. So we have managed to move past the legacy of early hard drives to some degree, and are now being held back by assumptions that derive more from x86 page sizes.</div><br/></div></div><div id="35881302" class="c"><input type="checkbox" id="c-35881302" checked=""/><div class="controls bullet"><span class="by">JoeAltmaier</span><span>|</span><a href="#35881242">parent</a><span>|</span><a href="#35881454">prev</a><span>|</span><a href="#35883732">next</a><span>|</span><label class="collapse" for="c-35881302">[-]</label><label class="expand" for="c-35881302">[4 more]</label></div><br/><div class="children"><div class="content">Deeper than that! Such fundamentals as &#x27;the stack is accessible by code&#x27; goes back to Fortran. C doesn&#x27;t need that. The call stack and the &#x27;display&#x27; stack could be different things and no C programmer need care.<p>This is the root of attacks that rewrite the return address of a kernel call. If the return address were managed&#x2F;protected outside normal data operation this would not exist.</div><br/><div id="35881468" class="c"><input type="checkbox" id="c-35881468" checked=""/><div class="controls bullet"><span class="by">pklausler</span><span>|</span><a href="#35881242">root</a><span>|</span><a href="#35881302">parent</a><span>|</span><a href="#35883732">next</a><span>|</span><label class="collapse" for="c-35881468">[-]</label><label class="expand" for="c-35881468">[3 more]</label></div><br/><div class="children"><div class="content">Could you explain this in more detail, please?  Fortran standards didn&#x27;t support recursion until Fortran &#x27;90 and it wasn&#x27;t easy to get stack allocation in F&#x27;77 compilers from vendors until the mid-80&#x27;s or so, both long after Algol-like languages had them.</div><br/><div id="35881487" class="c"><input type="checkbox" id="c-35881487" checked=""/><div class="controls bullet"><span class="by">rtomanek</span><span>|</span><a href="#35881242">root</a><span>|</span><a href="#35881468">parent</a><span>|</span><a href="#35883732">next</a><span>|</span><label class="collapse" for="c-35881487">[-]</label><label class="expand" for="c-35881487">[2 more]</label></div><br/><div class="children"><div class="content">Wondering if they meant Forth rather than Fortran.</div><br/><div id="35884884" class="c"><input type="checkbox" id="c-35884884" checked=""/><div class="controls bullet"><span class="by">int_19h</span><span>|</span><a href="#35881242">root</a><span>|</span><a href="#35881487">parent</a><span>|</span><a href="#35883732">next</a><span>|</span><label class="collapse" for="c-35884884">[-]</label><label class="expand" for="c-35884884">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s even less true for Forth, given that it has separate data and return address stacks, and neither is addressable. There is some dedicated Forth hardware that relies on this fact to separate the stacks physically in hardware and e.g. use SRAM for them.</div><br/></div></div></div></div></div></div></div></div><div id="35883732" class="c"><input type="checkbox" id="c-35883732" checked=""/><div class="controls bullet"><span class="by">sebazzz</span><span>|</span><a href="#35881242">parent</a><span>|</span><a href="#35881302">prev</a><span>|</span><a href="#35881589">next</a><span>|</span><label class="collapse" for="c-35883732">[-]</label><label class="expand" for="c-35883732">[3 more]</label></div><br/><div class="children"><div class="content">Didn&#x27;t or do still some CPUs have instructions that can aid the JVM?</div><br/><div id="35885119" class="c"><input type="checkbox" id="c-35885119" checked=""/><div class="controls bullet"><span class="by">geraldhh</span><span>|</span><a href="#35881242">root</a><span>|</span><a href="#35883732">parent</a><span>|</span><a href="#35884014">next</a><span>|</span><label class="collapse" for="c-35885119">[-]</label><label class="expand" for="c-35885119">[1 more]</label></div><br/><div class="children"><div class="content">&quot;aid&quot; is very broad, so i&#x27;d say &quot;yes&quot;<p>but recently there was an article on hn that talked about an arm-instruction for some js stuff.</div><br/></div></div><div id="35884014" class="c"><input type="checkbox" id="c-35884014" checked=""/><div class="controls bullet"><span class="by">saagarjha</span><span>|</span><a href="#35881242">root</a><span>|</span><a href="#35883732">parent</a><span>|</span><a href="#35885119">prev</a><span>|</span><a href="#35881589">next</a><span>|</span><label class="collapse" for="c-35884014">[-]</label><label class="expand" for="c-35884014">[1 more]</label></div><br/><div class="children"><div class="content">Jazelle is generally deprecated.</div><br/></div></div></div></div><div id="35881589" class="c"><input type="checkbox" id="c-35881589" checked=""/><div class="controls bullet"><span class="by">throwawaylinux</span><span>|</span><a href="#35881242">parent</a><span>|</span><a href="#35883732">prev</a><span>|</span><a href="#35880523">next</a><span>|</span><label class="collapse" for="c-35881589">[-]</label><label class="expand" for="c-35881589">[3 more]</label></div><br/><div class="children"><div class="content">&gt; I always worry that hardware &quot;locks in&quot; software. SSDs showed up after everyone wrote their databases to be optimal on spinning rust, so SSDs were built to look at the flow of requests, assume that the application was written for spinning rust, and optimize the data layout accordingly.<p>It&#x27;s not really that so much as contiguous or nearby access <i>is</i> faster on NAND, just like it&#x27;s faster on hard disks, so software optimized layout and access patterns can look similar for both.<p>&gt; CPUs are similar. People started writing programs in C, so CPUs started optimizing for the outputs of C compilers.<p>This didn&#x27;t start at a one-way street. Actually CPUs were around first, so the first C compiler optimized for the CPU it generated code for. And from that point onward, C compilers optimize for the CPU.<p>&gt; If someone were to invent CPUs and compilers today, the list of optimizations would probably be different, and the performance characteristics of real-world software would probably be different. (It&#x27;s not just C; people wanted virtual address spaces, and that was slow in software, so now there is a TLB, etc.)<p>Unlikely. At the periphery yes, but there are fundamentally difficult things for CPUs and compilers to do which shape the solutions. Before somebody says &quot;those fundamental difficulties might be different if we invented things differently&quot; - the story of CPUs and of compiler optimization is about inventing ways to make those fundamental difficulties less difficult.<p>There wasn&#x27;t one day somebody invented the CPU and that was that, millions of people around the world have worked countless hours inventing and improving parts of CPUs and compilers for the past 70 years or so. There is inertia, but new ideas that are good enough can catch on even if it means entirely new models and ecosystems have to be invented. That&#x27;s how we got multiprocessors, clusters, SIMD, GPUs.</div><br/><div id="35882717" class="c"><input type="checkbox" id="c-35882717" checked=""/><div class="controls bullet"><span class="by">jrockway</span><span>|</span><a href="#35881242">root</a><span>|</span><a href="#35881589">parent</a><span>|</span><a href="#35880523">next</a><span>|</span><label class="collapse" for="c-35882717">[-]</label><label class="expand" for="c-35882717">[2 more]</label></div><br/><div class="children"><div class="content">&gt; It&#x27;s not really that so much as contiguous or nearby access is faster on NAND, just like it&#x27;s faster on hard disks, so software optimized layout and access patterns can look similar for both.<p>I worry about the pathological cases.  Imagine you have an append-only log, and you write and fsync() one byte at a time.  Each time you write a byte, the entire flash block (are they still 4KB these days?) has to be erased.  So you end up chewing through 4000 durability cycles on your SSD, whereas if you had waited to write an exact 4KB block, then you&#x27;d use only 1.<p>A hard drive is fine with this access pattern, though you&#x27;d probably be doing a lot of seeking to update fs metadata and it would be so slow that you&#x27;d come up with some other way.  So maybe this pathology only exists in my mind.</div><br/><div id="35884030" class="c"><input type="checkbox" id="c-35884030" checked=""/><div class="controls bullet"><span class="by">throwawaylinux</span><span>|</span><a href="#35881242">root</a><span>|</span><a href="#35882717">parent</a><span>|</span><a href="#35880523">next</a><span>|</span><label class="collapse" for="c-35884030">[-]</label><label class="expand" for="c-35884030">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I worry about the pathological cases. Imagine you have an append-only log, and you write and fsync() one byte at a time. Each time you write a byte, the entire flash block (are they still 4KB these days?) has to be erased. So you end up chewing through 4000 durability cycles on your SSD, whereas if you had waited to write an exact 4KB block, then you&#x27;d use only 1.<p>NAND pages are what, something around 16-64kB these days, and block sizes are 10s of MB. In SSDs the logical size of those things may end up being larger at the FTL level if they are ganged together, but that&#x27;s about your minimum.<p>Those are program and erase units respectively. With NAND, you do not erase a block to write. You can program pages in a block incrementally, and then you have to erase the entire block before reprogramming any.<p>Flash translation layers have to make this look like a disk. To do that they will do something like gather writes into a page size chunk in a small cache that is non-volatile or can persist itself on power failure. Then that chunk is programmed out to a free page. A mapping structure records the new NAND location of the logical block addresses you wrote. And a garbage collector comes along behind and compacts and frees data in block size units, erases them, and puts them on the free list.<p>It&#x27;s a log structured filesystem with one file (the block device), if you&#x27;ve read any of those papers.<p>If you write+fsync to sector 0 of your disk 500 times, that data will get stored at 500 different places on the NAND (ignoring larger persistent caches in front of the NAND that some devices have). Selecting what pages to use and what to garbage collect etc is all part of wear leveling that is intended to prolong live of the drive. That&#x27;s why endurance ratings tend to be in total writes to the drive, not writes to any particular sector.<p>To handwave the numbers, if you have a 100GB SSD that might be implemented with 105GB of NAND. Then if you had a program&#x2F;erase endurance of 1,000 cycles you will be able to write that block 25.6 billion times. Your drive can do about 20-30 thousand QD1 writes per second, so about 12 days of that write+fsync block workload. Again assuming no front end cache on it.<p>You definitely can wear out NAND drives if you write a lot to them (large streaming writes would be easier to do it with), but regardless of what you do in the software layer, the drive will (should) last for its rated endurance no matter what kind of write patterns your application does. A block is a block as far as the NAND sees.<p>For consumer stuff you generally have to be pretty extreme or have malfunctioning software to wear them out as far as I&#x27;ve seen.<p>&gt; A hard drive is fine with this access pattern, though you&#x27;d probably be doing a lot of seeking to update fs metadata and it would be so slow that you&#x27;d come up with some other way. So maybe this pathology only exists in my mind.</div><br/></div></div></div></div></div></div></div></div><div id="35880523" class="c"><input type="checkbox" id="c-35880523" checked=""/><div class="controls bullet"><span class="by">rektide</span><span>|</span><a href="#35881242">prev</a><span>|</span><a href="#35884347">next</a><span>|</span><label class="collapse" for="c-35880523">[-]</label><label class="expand" for="c-35880523">[13 more]</label></div><br/><div class="children"><div class="content">It&#x27;s weird to me that there&#x27;s so many promising reliefs that seem near at hand, but which have simply never been delivered. Zoned Storage has existed for a while, for Shingled Magnetic Recording &amp; tape, and there was much hubbub in 2018-2020 about NVMe getting Zoned Namespaces, which happened. <a href="https:&#x2F;&#x2F;zonedstorage.io&#x2F;" rel="nofollow">https:&#x2F;&#x2F;zonedstorage.io&#x2F;</a><p>And there&#x27;s now a bunch of fs implementations. f2fs and btfs both have some support.<p>But there&#x27;s still no products actually available to buy. We could be getting so much better at NVMe, so systematically making things better. But we&#x27;ve kind of been stalled out for a while, after a bunch of ceremony figuring out what we wanted to do.</div><br/><div id="35880888" class="c"><input type="checkbox" id="c-35880888" checked=""/><div class="controls bullet"><span class="by">bcrl</span><span>|</span><a href="#35880523">parent</a><span>|</span><a href="#35881182">next</a><span>|</span><label class="collapse" for="c-35880888">[-]</label><label class="expand" for="c-35880888">[7 more]</label></div><br/><div class="children"><div class="content">SMR and zoned storage offer compete and total utter crap performance in the real world.  They have all the glass jaws of early flash firmware (garbage collection that took seconds to complete during which time no additional I&#x2F;Os completed) while running on a medium that has many orders of magnitude higher latency than flash.  Take 2008 era USB flash drives and that&#x27;s about what you&#x27;ll get out of an SMR HDD when using a write workload that isn&#x27;t 100% large sequential writes.<p>SMR might have become more relevant if it had provided a useful increase in density, but that never materialized as was originally promised.  Retail prices for SMR drives were never much better than CMR.  The largest generally available HDDs are available in CMR flavours because that&#x27;s what&#x27;s needed in real world servers.  There&#x27;s no 10x density improvement (or even 2x).  Meanwhile flash is marching relentlessly down the cost reduction path afford by Moore&#x27;s law + 3D layer stacking.  Really fast 1TB NVMe SSDs are under $100 now, and it doesn&#x27;t look like flash cost reduction is going to slow down any time soon.</div><br/><div id="35880999" class="c"><input type="checkbox" id="c-35880999" checked=""/><div class="controls bullet"><span class="by">rektide</span><span>|</span><a href="#35880523">root</a><span>|</span><a href="#35880888">parent</a><span>|</span><a href="#35881964">next</a><span>|</span><label class="collapse" for="c-35880999">[-]</label><label class="expand" for="c-35880999">[5 more]</label></div><br/><div class="children"><div class="content">I wasn&#x27;t really talking about SMR though, just some protocols they had that are now being re-used. I&#x27;m talking about Zoned Namespace SSDs (ZNS).<p>Let&#x27;s go back in time almost a decade. A bunch of smart people had figured out that the cumbersome Flash Translation Layer (FTL) on SSDs made it really hard to get expectable &amp; consistent performance. They were building possible specs to try to directly access each flash block, to be able to fill them up as they saw fit &amp; clear them out as they saw fit. They wanted direct access, with far less work juggling complex data-mappings done on the SSD itself. Two examples of Open Channel SSD works: <a href="https:&#x2F;&#x2F;openchannelssd.readthedocs.io&#x2F;en&#x2F;latest&#x2F;" rel="nofollow">https:&#x2F;&#x2F;openchannelssd.readthedocs.io&#x2F;en&#x2F;latest&#x2F;</a> <a href="http:&#x2F;&#x2F;lightnvm.io&#x2F;" rel="nofollow">http:&#x2F;&#x2F;lightnvm.io&#x2F;</a><p>Note the huge banner on the second: &quot;<i>Zoned Namespace (ZNS) SSDs have replaced the work on OCSSDs, and is now a standardized interface.</i>&quot; Everyone realized that the protocols we had for Zoned Storage were basically adequate to get us what we wanted. We&#x27;d just make a lot of 128kB or whatever sized zones on the SSD, and let people manage them themselves.<p>Currently this means opening blocks &amp; then appending to them. There&#x27;s been outstanding hope, in the future, that we might go further and allow more random write patterns, but still with the same contract of no over-writes (just clears).<p>That work seemed like it was ready to go in 2020 &amp; 2021. SSDs were supposedly sampling&#x2F;becoming available. <a href="https:&#x2F;&#x2F;blog.westerndigital.com&#x2F;zns-ssd-ultrastar-dc-zn540-sampling&#x2F;" rel="nofollow">https:&#x2F;&#x2F;blog.westerndigital.com&#x2F;zns-ssd-ultrastar-dc-zn540-s...</a>  <a href="https:&#x2F;&#x2F;semiconductor.samsung.com&#x2F;newsroom&#x2F;news&#x2F;samsung-introduces-its-first-zns-ssd-with-maximized-user-capacity-and-enhanced-lifespan&#x2F;" rel="nofollow">https:&#x2F;&#x2F;semiconductor.samsung.com&#x2F;newsroom&#x2F;news&#x2F;samsung-intr...</a> And that was reaffirmed again a year latter. <a href="https:&#x2F;&#x2F;www.tomshardware.com&#x2F;news&#x2F;samsung-and-western-digital-team-up-for-zns-ssds" rel="nofollow">https:&#x2F;&#x2F;www.tomshardware.com&#x2F;news&#x2F;samsung-and-western-digita...</a><p>But here we are in 2023 &amp; there&#x27;s still no Zoned Namespace SSDs (ZNS) one can purchase.</div><br/><div id="35881498" class="c"><input type="checkbox" id="c-35881498" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#35880523">root</a><span>|</span><a href="#35880999">parent</a><span>|</span><a href="#35881964">next</a><span>|</span><label class="collapse" for="c-35881498">[-]</label><label class="expand" for="c-35881498">[4 more]</label></div><br/><div class="children"><div class="content">You <i>can</i> purchase them, just not from retailers. These drive break compatibility in annoying ways (eg. cannot boot a system off one), so it&#x27;s totally reasonable that drive vendors would not be offering them through channels where unsuspecting customers could so easily buy something they cannot use.</div><br/><div id="35881645" class="c"><input type="checkbox" id="c-35881645" checked=""/><div class="controls bullet"><span class="by">Dylan16807</span><span>|</span><a href="#35880523">root</a><span>|</span><a href="#35881498">parent</a><span>|</span><a href="#35881964">next</a><span>|</span><label class="collapse" for="c-35881645">[-]</label><label class="expand" for="c-35881645">[3 more]</label></div><br/><div class="children"><div class="content">They didn&#x27;t need to completely remove compatibility modes.<p>And I&#x27;m not asking to get one off a shelf but not only can I not buy one on amazon or newegg I can&#x27;t even find a price when I look for a specific model!<p>I think it&#x27;s fair to say that to a first approximation I, as a person, can&#x27;t get one.<p>Edit: I found one site with a price on a WD drive, but they only have refurbished drives and the price is 25% off of $2200 for 1TB, so I&#x27;m going to ignore that site.</div><br/><div id="35881776" class="c"><input type="checkbox" id="c-35881776" checked=""/><div class="controls bullet"><span class="by">rektide</span><span>|</span><a href="#35880523">root</a><span>|</span><a href="#35881645">parent</a><span>|</span><a href="#35881734">next</a><span>|</span><label class="collapse" for="c-35881776">[-]</label><label class="expand" for="c-35881776">[1 more]</label></div><br/><div class="children"><div class="content">I really wouldn&#x27;t mind buying special purpose hardware. In fact, I think I prefer it.<p>The whole story here is that modern SSD controllers tend to embed a bunch of very fast very expensive data-processing cores to quickly do a ton of fancy mapping. We don&#x27;t hear it quite as clearly these days, but for example for a while higher end Samsung SSD controllers were announced as penta-core ARM Cortex-R[1] systems, and I think those Cortex-Rs are what most folks do. (It was groundbreaking news that WD released an open source &quot;swerv&quot; RISC-V core which they&#x27;d experimented with using instead, <a href="https:&#x2F;&#x2F;blog.westerndigital.com&#x2F;risc-v-swerv-core-open-source&#x2F;" rel="nofollow">https:&#x2F;&#x2F;blog.westerndigital.com&#x2F;risc-v-swerv-core-open-sourc...</a> .)<p>Ideally, the fantasy is, we can build much cheaper &amp; faster controllers that do far less. Zoned Namespace drives should ideally have enterprise grade, fast, dual-port access, but in many ways, I feel like they should ideally be far cheaper than DRAM-less drives. They should leave even more up to the host. They should be incredibly dumb &amp; simple drives. They should be lower power, by far. They should eskew DRAM. Drop all the legacy baggage &amp; expose what you are, un-intermediated, so we can be fast &amp; use it well. Which you, the drive, with all purpose tasks, could never do.<p>It&#x27;s not worth it, to me, to make a drive that still keeps the ancillary old baggage of convention. If you want to make a good ZNS drive, make a good ZNS drive.<p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;ARM_Cortex-R" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;ARM_Cortex-R</a></div><br/></div></div><div id="35881734" class="c"><input type="checkbox" id="c-35881734" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#35880523">root</a><span>|</span><a href="#35881645">parent</a><span>|</span><a href="#35881776">prev</a><span>|</span><a href="#35881964">next</a><span>|</span><label class="collapse" for="c-35881734">[-]</label><label class="expand" for="c-35881734">[1 more]</label></div><br/><div class="children"><div class="content">&gt;  I can&#x27;t even find a price when I look for a specific model!<p>That&#x27;s true of most enterprise&#x2F;server components. At most you&#x27;ll find a laughably inflated list price that hardly anyone actually pays.</div><br/></div></div></div></div></div></div></div></div><div id="35881964" class="c"><input type="checkbox" id="c-35881964" checked=""/><div class="controls bullet"><span class="by">tremon</span><span>|</span><a href="#35880523">root</a><span>|</span><a href="#35880888">parent</a><span>|</span><a href="#35880999">prev</a><span>|</span><a href="#35881182">next</a><span>|</span><label class="collapse" for="c-35881964">[-]</label><label class="expand" for="c-35881964">[1 more]</label></div><br/><div class="children"><div class="content"><i>SMR and zoned storage offer compete and total utter crap performance in the real world</i><p>Yes, that&#x27;s because we&#x27;re in the drive-managed transition phase of zoned storage (not clear when -if ever- we&#x27;ll leave that phase). The first zoned disks needed to present themselves as a dumb unit to the OS, because none of the operating systems had any support for it.<p>Now that at least some OS&#x27;es have native support for zoned storage, we might finally see host-managed zoned storage on the market, but as you say -- the technology has not delivered on the projected storage benefits, so the value-add of CMR disks is very much an open question.<p>And there&#x27;s also the consumer confusion angle: how to market a device that can physically connect to their computer, but might logically not work at all? So I&#x27;d suspect to see the first host-managed drives in enterprise datacenters (think Amazon Glacier) -- but FAFAIK they&#x27;re nowhere to be found.</div><br/></div></div></div></div><div id="35881182" class="c"><input type="checkbox" id="c-35881182" checked=""/><div class="controls bullet"><span class="by">kijiki</span><span>|</span><a href="#35880523">parent</a><span>|</span><a href="#35880888">prev</a><span>|</span><a href="#35880894">next</a><span>|</span><label class="collapse" for="c-35881182">[-]</label><label class="expand" for="c-35881182">[1 more]</label></div><br/><div class="children"><div class="content">Google, Facebook, Microsoft, Amazon and so forth buy most of the spinning rust, and have for years. Each of them have their own extremely proprietary storage layer that knows how to manage Shingled Magnetic Recording.<p>So when SMR got released into the open market, it was exclusively drive managed, and mainly on the low-end, as a cost reduction strategy. With all the performance downsides you&#x27;d expect.<p>It is a real shame that host managed SMR isn&#x27;t available outside the mega-scale corps. It would be nice to have access to an intelligent storage stack mixing flash and SMR drives without having to go work for one of the big N companies.</div><br/></div></div><div id="35880894" class="c"><input type="checkbox" id="c-35880894" checked=""/><div class="controls bullet"><span class="by">b33j0r</span><span>|</span><a href="#35880523">parent</a><span>|</span><a href="#35881182">prev</a><span>|</span><a href="#35884347">next</a><span>|</span><label class="collapse" for="c-35880894">[-]</label><label class="expand" for="c-35880894">[4 more]</label></div><br/><div class="children"><div class="content">The thing that still bothers me is that the read-write cycles and lifetime are sort of unpredictable to me personally.<p>For an analogy, I only buy LED lightbulbs, love that tech. When they first became widely-available, my house had a few incandescent bulbs on the porch that I probably just left on for 5 years.<p>The supposedly superior led bulbs in that adoption period often died surprisingly fast, compared to the claims.<p>To bring it back to storage. I use as much NVMe as I can, but I’m still a bit uncertain on how much a risk I’m taking with my filesystem choice and access patterns.<p>My answer: buy more SSDs when they go on sale! I don’t have intuition about lifetime even now.<p>I know, HDDs have equivalent classes of worse problems. I just learned to expect them to reliably fail, like a few days after a build ;)</div><br/><div id="35884560" class="c"><input type="checkbox" id="c-35884560" checked=""/><div class="controls bullet"><span class="by">the_pwner224</span><span>|</span><a href="#35880523">root</a><span>|</span><a href="#35880894">parent</a><span>|</span><a href="#35881046">next</a><span>|</span><label class="collapse" for="c-35884560">[-]</label><label class="expand" for="c-35884560">[2 more]</label></div><br/><div class="children"><div class="content"><p><pre><code>  $ sudo smartctl -A &#x2F;dev&#x2F;nvme0n1

  Available Spare:                    100%
  Available Spare Threshold:          10%
  Percentage Used:                    0%
  Data Units Read:                    626,936 [320 GB]
  Data Units Written:                 22,262,139 [11.3 TB]
</code></pre>
This is on a few months old PC which is why percentage used is still 0%. The number does slowly go up over time especially if you do heavy writing.<p>As with LED lightbulbs, the consumer industry is focused on costcutting to deliver shitty products at low prices. Any high quality SSD is more than adequate for consumer use. Main thing to look at is how many layers the flash has. SLC is pretty much not available in consumer products. Triple layer is very adequate. All the cheap products have four layer, which you don&#x27;t want unless you know you&#x27;re not going to be writing often.<p>Firmware bugs or bad controllers are still a potential issue. I have a 1.5 year old drive which is at something like 65% percentage used, it&#x27;s a big drive with a high TBW rating and only a moderate amount of data written to it, but the excessive wear is known issue with that model. It&#x27;ll get replaced under warranty but that&#x27;s only kicking the can down the road since the replacement will have the same issue.</div><br/><div id="35884907" class="c"><input type="checkbox" id="c-35884907" checked=""/><div class="controls bullet"><span class="by">b33j0r</span><span>|</span><a href="#35880523">root</a><span>|</span><a href="#35884560">parent</a><span>|</span><a href="#35881046">next</a><span>|</span><label class="collapse" for="c-35884907">[-]</label><label class="expand" for="c-35884907">[1 more]</label></div><br/><div class="children"><div class="content">That was a very well-thought out comment, and I learned new things! Thanks!<p>The strange thing to me that gives me weird feels despite the science is that I’m never had an ssd die that I installed. And I’m no Linus of LTT here.<p>Plenty of apple machines and etc have needed a catastrophic replacement. That just throws off my ability to make rational decisions about it at times.<p>“Dude! That SSD was premium priced, what the heck? Thermal failure?”<p>(Seems likely, but ugh. Note: Opinion was formed a few years back, so it’s not something I’m actively diagnosing.)</div><br/></div></div></div></div><div id="35881046" class="c"><input type="checkbox" id="c-35881046" checked=""/><div class="controls bullet"><span class="by">macjohnmcc</span><span>|</span><a href="#35880523">root</a><span>|</span><a href="#35880894">parent</a><span>|</span><a href="#35884560">prev</a><span>|</span><a href="#35884347">next</a><span>|</span><label class="collapse" for="c-35881046">[-]</label><label class="expand" for="c-35881046">[1 more]</label></div><br/><div class="children"><div class="content">The recent issues with Samsung&#x27;s SSD&#x27;s don&#x27;t help with those feelings I&#x27;m sure. I have a WD-Black 1TB SSD in my system that I built in 2018 and I used it heavily all day long and I have not suffered any issues from the SSD. I also used a MacBook Pro from early 2013 and only recently sold it still going strong with it&#x27;s SSD (proprietary). I read recently that some HDD were starting to have reliability problems after a few years.</div><br/></div></div></div></div></div></div><div id="35884347" class="c"><input type="checkbox" id="c-35884347" checked=""/><div class="controls bullet"><span class="by">bullen</span><span>|</span><a href="#35880523">prev</a><span>|</span><a href="#35884951">next</a><span>|</span><label class="collapse" for="c-35884347">[-]</label><label class="expand" for="c-35884347">[1 more]</label></div><br/><div class="children"><div class="content">Seems SSDs are reaching the end game with ~8TB on a 2.5&quot; platter.<p>Now you just need to nail the peak of $&#x2F;GB, I&#x27;m looking at both 870 EVO &#x2F; QVO, they have 4TB versions for $300.<p>For &quot;write once&quot; or append only data these could work well, think user registry or sequential log.<p>But you need to offload the active data on SLC drives or take the risk of the wearing driver firmware in the exact devices you buy&#x2F;patch.<p>The OS needs to be on SLC anyhow.</div><br/></div></div><div id="35884951" class="c"><input type="checkbox" id="c-35884951" checked=""/><div class="controls bullet"><span class="by">_joel</span><span>|</span><a href="#35884347">prev</a><span>|</span><a href="#35880528">next</a><span>|</span><label class="collapse" for="c-35884951">[-]</label><label class="expand" for="c-35884951">[1 more]</label></div><br/><div class="children"><div class="content">Definitely not, random access on PCIe5 is not too different (from user experience) from PCIe4 and not that stellar compared to sequential throughput of various block sizes.</div><br/></div></div><div id="35880528" class="c"><input type="checkbox" id="c-35880528" checked=""/><div class="controls bullet"><span class="by">COGlory</span><span>|</span><a href="#35884951">prev</a><span>|</span><a href="#35880204">next</a><span>|</span><label class="collapse" for="c-35880528">[-]</label><label class="expand" for="c-35880528">[8 more]</label></div><br/><div class="children"><div class="content">Not to harp too much on the title (I read the article and found it very interesting), but are there any industries storing their data on all-flash? My intuition is that most institutions still have a giant array of spinning rust somewhere underpinning everything they do.</div><br/><div id="35880598" class="c"><input type="checkbox" id="c-35880598" checked=""/><div class="controls bullet"><span class="by">birdman3131</span><span>|</span><a href="#35880528">parent</a><span>|</span><a href="#35881022">next</a><span>|</span><label class="collapse" for="c-35880598">[-]</label><label class="expand" for="c-35880598">[4 more]</label></div><br/><div class="children"><div class="content">Outside of backups I would expect many business to be all SSD. Maybe not the majority but for many use cases I see no good reason to go HDD for most small businesses.<p>I know I went pretty much all SSD ages ago. I do run backups to a few 14TB externals but nothing outside of that is spinning rust.</div><br/><div id="35880882" class="c"><input type="checkbox" id="c-35880882" checked=""/><div class="controls bullet"><span class="by">girvo</span><span>|</span><a href="#35880528">root</a><span>|</span><a href="#35880598">parent</a><span>|</span><a href="#35881327">next</a><span>|</span><label class="collapse" for="c-35880882">[-]</label><label class="expand" for="c-35880882">[1 more]</label></div><br/><div class="children"><div class="content">I ditched my final spinning rust (a WD Black 2.5” 7200RPM drive I had jammed into my home server) last night actually. It’s all SSDs in my server, and it’s all NVMe in my gaming desktop. My Macs and my partner iPad are all flash too obviously.</div><br/></div></div><div id="35881327" class="c"><input type="checkbox" id="c-35881327" checked=""/><div class="controls bullet"><span class="by">kccqzy</span><span>|</span><a href="#35880528">root</a><span>|</span><a href="#35880598">parent</a><span>|</span><a href="#35880882">prev</a><span>|</span><a href="#35881308">next</a><span>|</span><label class="collapse" for="c-35881327">[-]</label><label class="expand" for="c-35881327">[1 more]</label></div><br/><div class="children"><div class="content">I do not think it is feasible to store all of YouTube videos on SSD. Google must be using spinning rust for the long tail of videos.</div><br/></div></div></div></div><div id="35881022" class="c"><input type="checkbox" id="c-35881022" checked=""/><div class="controls bullet"><span class="by">tatersolid</span><span>|</span><a href="#35880528">parent</a><span>|</span><a href="#35880598">prev</a><span>|</span><a href="#35880893">next</a><span>|</span><label class="collapse" for="c-35881022">[-]</label><label class="expand" for="c-35881022">[1 more]</label></div><br/><div class="children"><div class="content">Our datacenter at $dayjob retired our last spinning rust drive way back in 2017.
Backups are done over the wire to dedupe-enabled targets 600km away; dedupe restore times are shit unless you use all-SSD there as well.</div><br/></div></div><div id="35880893" class="c"><input type="checkbox" id="c-35880893" checked=""/><div class="controls bullet"><span class="by">loeg</span><span>|</span><a href="#35880528">parent</a><span>|</span><a href="#35881022">prev</a><span>|</span><a href="#35880846">next</a><span>|</span><label class="collapse" for="c-35880893">[-]</label><label class="expand" for="c-35880893">[1 more]</label></div><br/><div class="children"><div class="content">Gobs.  This is Pure Storage&#x27;s entire market.</div><br/></div></div><div id="35880846" class="c"><input type="checkbox" id="c-35880846" checked=""/><div class="controls bullet"><span class="by">rch</span><span>|</span><a href="#35880528">parent</a><span>|</span><a href="#35880893">prev</a><span>|</span><a href="#35880204">next</a><span>|</span><label class="collapse" for="c-35880846">[-]</label><label class="expand" for="c-35880846">[1 more]</label></div><br/><div class="children"><div class="content">I see a fair number of all SSD clusters, but most have a mix of drive types. Interestingly, the ratio of drive types is aligned with optimal performance rather than cost.</div><br/></div></div></div></div><div id="35880204" class="c"><input type="checkbox" id="c-35880204" checked=""/><div class="controls bullet"><span class="by">issafram</span><span>|</span><a href="#35880528">prev</a><span>|</span><a href="#35881036">next</a><span>|</span><label class="collapse" for="c-35880204">[-]</label><label class="expand" for="c-35880204">[15 more]</label></div><br/><div class="children"><div class="content">I&#x27;m still of the opinion that even with SSD&#x2F;NVMe, the random reads&#x2F;writes are still very slow.</div><br/><div id="35880546" class="c"><input type="checkbox" id="c-35880546" checked=""/><div class="controls bullet"><span class="by">exabrial</span><span>|</span><a href="#35880204">parent</a><span>|</span><a href="#35880710">next</a><span>|</span><label class="collapse" for="c-35880546">[-]</label><label class="expand" for="c-35880546">[4 more]</label></div><br/><div class="children"><div class="content">In my experience, depends on the SSD. Consumer grade ones appear really fast because they have a DRAM cache, and as soon as you start having a bunch of cache-misses you realize how slow they actually are.<p>We got a bunch of enterprise grade Samsung SSDs and there&#x27;a large difference in sustained I&#x2F;O. It&#x27;s not &quot;instant&quot; by any means, but it there are other things slowing I&#x2F;O.</div><br/><div id="35881107" class="c"><input type="checkbox" id="c-35881107" checked=""/><div class="controls bullet"><span class="by">zamadatix</span><span>|</span><a href="#35880204">root</a><span>|</span><a href="#35880546">parent</a><span>|</span><a href="#35880710">next</a><span>|</span><label class="collapse" for="c-35881107">[-]</label><label class="expand" for="c-35881107">[3 more]</label></div><br/><div class="children"><div class="content">Only the very best drives hit &gt;200 MB&#x2F;s in 4k write (even with DRAM as cache) and for 4k mixed not even Optane drives get that high.</div><br/><div id="35881594" class="c"><input type="checkbox" id="c-35881594" checked=""/><div class="controls bullet"><span class="by">wtallis</span><span>|</span><a href="#35880204">root</a><span>|</span><a href="#35881107">parent</a><span>|</span><a href="#35880710">next</a><span>|</span><label class="collapse" for="c-35881594">[-]</label><label class="expand" for="c-35881594">[2 more]</label></div><br/><div class="children"><div class="content">Did you forget to stipulate that you&#x27;re only talking about workloads that issue IO requests serially with zero parallelism?</div><br/><div id="35884307" class="c"><input type="checkbox" id="c-35884307" checked=""/><div class="controls bullet"><span class="by">zamadatix</span><span>|</span><a href="#35880204">root</a><span>|</span><a href="#35881594">parent</a><span>|</span><a href="#35880710">next</a><span>|</span><label class="collapse" for="c-35884307">[-]</label><label class="expand" for="c-35884307">[1 more]</label></div><br/><div class="children"><div class="content">No, I just didn&#x27;t pick QD32T8 to pump the numbers either. Even in such workloads the gap to sequential speed is a decent ratio, just not painfully so as in most other mid to singular workloads.</div><br/></div></div></div></div></div></div></div></div><div id="35880710" class="c"><input type="checkbox" id="c-35880710" checked=""/><div class="controls bullet"><span class="by">95014_refugee</span><span>|</span><a href="#35880204">parent</a><span>|</span><a href="#35880546">prev</a><span>|</span><a href="#35880350">next</a><span>|</span><label class="collapse" for="c-35880710">[-]</label><label class="expand" for="c-35880710">[1 more]</label></div><br/><div class="children"><div class="content">This obviously depends substantially on the implementation, but (especially for standalone devices) there tends to be metadata access associated with translating a given LBA to a physical page. This brings cache locality of reference into the picture, and any workload that is &quot;random enough&quot; such that the metadata working set exceeds the cache will suffer...</div><br/></div></div><div id="35880350" class="c"><input type="checkbox" id="c-35880350" checked=""/><div class="controls bullet"><span class="by">marginalia_nu</span><span>|</span><a href="#35880204">parent</a><span>|</span><a href="#35880710">prev</a><span>|</span><a href="#35880698">next</a><span>|</span><label class="collapse" for="c-35880350">[-]</label><label class="expand" for="c-35880350">[1 more]</label></div><br/><div class="children"><div class="content">With SSDs it&#x27;s more like a discount on sequential&#x2F;adjacent reads within a physical block that is of a size that varies with model and manufacturer.</div><br/></div></div><div id="35880698" class="c"><input type="checkbox" id="c-35880698" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#35880204">parent</a><span>|</span><a href="#35880350">prev</a><span>|</span><a href="#35880877">next</a><span>|</span><label class="collapse" for="c-35880698">[-]</label><label class="expand" for="c-35880698">[1 more]</label></div><br/><div class="children"><div class="content">Relative to what?</div><br/></div></div><div id="35880877" class="c"><input type="checkbox" id="c-35880877" checked=""/><div class="controls bullet"><span class="by">thelastparadise</span><span>|</span><a href="#35880204">parent</a><span>|</span><a href="#35880698">prev</a><span>|</span><a href="#35880557">next</a><span>|</span><label class="collapse" for="c-35880877">[-]</label><label class="expand" for="c-35880877">[6 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t understand why so much emphasis is put on optimizing for sequential reads and writes. Sequential IO is not something that happens in the real world except in rare circumstances. In most applications IO is random, even if it&#x27;s a log file.</div><br/><div id="35881640" class="c"><input type="checkbox" id="c-35881640" checked=""/><div class="controls bullet"><span class="by">acscott</span><span>|</span><a href="#35880204">root</a><span>|</span><a href="#35880877">parent</a><span>|</span><a href="#35881084">next</a><span>|</span><label class="collapse" for="c-35881640">[-]</label><label class="expand" for="c-35881640">[1 more]</label></div><br/><div class="children"><div class="content">When writing to a log file, isn&#x27;t that sequential write?</div><br/></div></div><div id="35881084" class="c"><input type="checkbox" id="c-35881084" checked=""/><div class="controls bullet"><span class="by">dmitrygr</span><span>|</span><a href="#35880204">root</a><span>|</span><a href="#35880877">parent</a><span>|</span><a href="#35881640">prev</a><span>|</span><a href="#35880557">next</a><span>|</span><label class="collapse" for="c-35881084">[-]</label><label class="expand" for="c-35881084">[4 more]</label></div><br/><div class="children"><div class="content">&gt; In most applications IO is random, even if it&#x27;s a log file.<p>I am all ears... how are log writes random?</div><br/><div id="35881866" class="c"><input type="checkbox" id="c-35881866" checked=""/><div class="controls bullet"><span class="by">JdeBP</span><span>|</span><a href="#35880204">root</a><span>|</span><a href="#35881084">parent</a><span>|</span><a href="#35880557">next</a><span>|</span><label class="collapse" for="c-35881866">[-]</label><label class="expand" for="c-35881866">[3 more]</label></div><br/><div class="children"><div class="content">I <i>think</i> that I see what thelastparadise is getting at.  At base, most logfile output is a read-modify-write of usually no more than the final block of the file, whereas the sequential writes that are generally optimized for by filesystem and disc drivers are multiple-block writes sent as single output requests.<p>The twain are similar, but aren&#x27;t exactly the same.  Optimize for the cases where you are writing multiple blocks in a chunk, with contiguous free block allocation policies and write behind and whatnot, and you haven&#x27;t in reality <i>quite</i> optimized for the case where what you&#x27;re actually doing is overwriting the tail block in the file repeatedly, oftentimes changing as few as mere 10s of bytes in that block with each read-modify-write request.<p>(And yes, logging mechanisms often do like to ensure that logs are synchronously flushed to disc line by line, effectively and intentionally defeating write caching and write behind optimizations.)</div><br/><div id="35882811" class="c"><input type="checkbox" id="c-35882811" checked=""/><div class="controls bullet"><span class="by">justsomehnguy</span><span>|</span><a href="#35880204">root</a><span>|</span><a href="#35881866">parent</a><span>|</span><a href="#35880557">next</a><span>|</span><label class="collapse" for="c-35882811">[-]</label><label class="expand" for="c-35882811">[2 more]</label></div><br/><div class="children"><div class="content">I would add what it is very rare to have 100MiB+ writes to a log file and anything less would be concurrent with the other operations, which would make &#x27;sequential&#x27; here a moot point.<p>&gt;  At base, most logfile output is a read-modify-write<p>It&#x27;s worse, actually, as you never can tell if this would be RMW to the same block or to the other free block. In the former you waste the writes for the whole blocks. In the latter you can hit a write amplification despite never have been writing any meaningful amount of data.</div><br/><div id="35883137" class="c"><input type="checkbox" id="c-35883137" checked=""/><div class="controls bullet"><span class="by">JdeBP</span><span>|</span><a href="#35880204">root</a><span>|</span><a href="#35882811">parent</a><span>|</span><a href="#35880557">next</a><span>|</span><label class="collapse" for="c-35883137">[-]</label><label class="expand" for="c-35883137">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s worse for an even simpler reason, which I thought about mentioning but it probably wasn&#x27;t what thelastparadise was talking about.  The line by line synchronization and flushing also causes i-node updates.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="35880557" class="c"><input type="checkbox" id="c-35880557" checked=""/><div class="controls bullet"><span class="by">willis936</span><span>|</span><a href="#35880204">parent</a><span>|</span><a href="#35880877">prev</a><span>|</span><a href="#35881036">next</a><span>|</span><label class="collapse" for="c-35880557">[-]</label><label class="expand" for="c-35880557">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s why all of my minecraft servers are on RAM drives.</div><br/></div></div></div></div><div id="35881036" class="c"><input type="checkbox" id="c-35881036" checked=""/><div class="controls bullet"><span class="by">numpad0</span><span>|</span><a href="#35880204">prev</a><span>|</span><a href="#35882609">next</a><span>|</span><label class="collapse" for="c-35881036">[-]</label><label class="expand" for="c-35881036">[3 more]</label></div><br/><div class="children"><div class="content">Completely tangential, but Yangtze Memory&#x2F;YMTC 232-layer NAND flash that was supposed to be used in iPhone 14 is on fire sale due to US import restrictions, to the point that some speculates that YMTC will be gone soon.<p>The point is, DRAM-less 2TB M.2 NVMe disks are at $80 right now in select non-US markets. This is mere ~5x HDD, and <i>slow</i> as 4Gbps on writes at worst conditions.</div><br/><div id="35881069" class="c"><input type="checkbox" id="c-35881069" checked=""/><div class="controls bullet"><span class="by">zamadatix</span><span>|</span><a href="#35881036">parent</a><span>|</span><a href="#35882609">next</a><span>|</span><label class="collapse" for="c-35881069">[-]</label><label class="expand" for="c-35881069">[2 more]</label></div><br/><div class="children"><div class="content">DRAM-less 2 TB m.2 NVMe drives are &lt;$80 in the US market as well <a href="https:&#x2F;&#x2F;www.amazon.com&#x2F;dp&#x2F;B08CDM2HSS" rel="nofollow">https:&#x2F;&#x2F;www.amazon.com&#x2F;dp&#x2F;B08CDM2HSS</a>. There are a couple such drives in this price range, even from companies like Intel with their 670p.</div><br/><div id="35881213" class="c"><input type="checkbox" id="c-35881213" checked=""/><div class="controls bullet"><span class="by">rektide</span><span>|</span><a href="#35881036">root</a><span>|</span><a href="#35881069">parent</a><span>|</span><a href="#35882609">next</a><span>|</span><label class="collapse" for="c-35881213">[-]</label><label class="expand" for="c-35881213">[1 more]</label></div><br/><div class="children"><div class="content">Minor detail, Intel sold their SSD business to SK Hynix for $7b. SK Hynix created a new company Solidigm out of the acquisition. So it is now a Solidigm 670p. <a href="https:&#x2F;&#x2F;www.solidigm.com&#x2F;products&#x2F;client&#x2F;d6&#x2F;670p.html" rel="nofollow">https:&#x2F;&#x2F;www.solidigm.com&#x2F;products&#x2F;client&#x2F;d6&#x2F;670p.html</a><p>Personally it really felt a bit rotten to me that Intel had created drives like the 670p that would simply stop working after a designated amount of writes. On the one hand, predictable life-span is kind of of a good thing, but sending drives to the landfill before they&#x27;re actually done seems monstrous. Anyways though, yeah, DRAM-less drives are available for incredible prices.</div><br/></div></div></div></div></div></div><div id="35882609" class="c"><input type="checkbox" id="c-35882609" checked=""/><div class="controls bullet"><span class="by">hakunin</span><span>|</span><a href="#35881036">prev</a><span>|</span><a href="#35880191">next</a><span>|</span><label class="collapse" for="c-35882609">[-]</label><label class="expand" for="c-35882609">[1 more]</label></div><br/><div class="children"><div class="content">We had a use case at one of former jobs where we had to lookup entries in hundreds of megs of static key&#x2F;value data (vendored with the app), but it didn&#x27;t have to be super fast and we didn&#x27;t want to waste any RAM on it. Since everything is on SSDs, I wrote a lib[0] that was just seeking sorted data in files with pread and binary search. Worked perfectly for our needs.<p>[0]: <a href="https:&#x2F;&#x2F;github.com&#x2F;maxim&#x2F;wordmap">https:&#x2F;&#x2F;github.com&#x2F;maxim&#x2F;wordmap</a></div><br/></div></div><div id="35880191" class="c"><input type="checkbox" id="c-35880191" checked=""/><div class="controls bullet"><span class="by">hawk_</span><span>|</span><a href="#35882609">prev</a><span>|</span><a href="#35880861">next</a><span>|</span><label class="collapse" for="c-35880191">[-]</label><label class="expand" for="c-35880191">[5 more]</label></div><br/><div class="children"><div class="content">&gt; There are different techniques for implementing OP (Over provisioning) yourself. You can simply leave a portion of the drive unpartitioned<p>This doesn&#x27;t sound right. Does this mean that the drive decides to arbitrarily move blocks between your partition and unpartioned spaces to manage GC?</div><br/><div id="35880421" class="c"><input type="checkbox" id="c-35880421" checked=""/><div class="controls bullet"><span class="by">Dalewyn</span><span>|</span><a href="#35880191">parent</a><span>|</span><a href="#35880387">prev</a><span>|</span><a href="#35880861">next</a><span>|</span><label class="collapse" for="c-35880421">[-]</label><label class="expand" for="c-35880421">[3 more]</label></div><br/><div class="children"><div class="content">Unlike hard drives of yore where ordering the drive to write a bit at a given location results in the drive writing a bit at a given location, SSDs abstract it away as a matter of their design.<p>An SSD will still provide a virtual bitmap so file systems can still order it to write a bit at a given location like ye olde days, but the SSD controller ultimately decides where to actually write that bit which more than likely has no relation to the specified location.<p>Incidentally, yes: This also means defragmenting an SSD is not only harmful, it is outright misleading because the state of the file system is not related to the state of the bits on the drive.</div><br/><div id="35882536" class="c"><input type="checkbox" id="c-35882536" checked=""/><div class="controls bullet"><span class="by">rasz</span><span>|</span><a href="#35880191">root</a><span>|</span><a href="#35880421">parent</a><span>|</span><a href="#35880861">next</a><span>|</span><label class="collapse" for="c-35882536">[-]</label><label class="expand" for="c-35882536">[2 more]</label></div><br/><div class="children"><div class="content">Mechanical HDDs have same mechanisms available, its just used for defects  <a href="https:&#x2F;&#x2F;www.dataclinic.co.uk&#x2F;hard-drive-defects-table&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.dataclinic.co.uk&#x2F;hard-drive-defects-table&#x2F;</a></div><br/><div id="35883153" class="c"><input type="checkbox" id="c-35883153" checked=""/><div class="controls bullet"><span class="by">Dalewyn</span><span>|</span><a href="#35880191">root</a><span>|</span><a href="#35882536">parent</a><span>|</span><a href="#35880861">next</a><span>|</span><label class="collapse" for="c-35883153">[-]</label><label class="expand" for="c-35883153">[1 more]</label></div><br/><div class="children"><div class="content">Indeed! HDDs remap sectors to reserve sectors if their controller detects the original sectors have become unwritable. The result is quite similar to what SSDs do more broadly; the file system remains as-is, but the workings behind the scene differ from it.</div><br/></div></div></div></div></div></div></div></div><div id="35880861" class="c"><input type="checkbox" id="c-35880861" checked=""/><div class="controls bullet"><span class="by">loeg</span><span>|</span><a href="#35880191">prev</a><span>|</span><a href="#35880914">next</a><span>|</span><label class="collapse" for="c-35880861">[-]</label><label class="expand" for="c-35880861">[1 more]</label></div><br/><div class="children"><div class="content">Even setting aside internal GC, there is read-side benefit to having fewer contiguous sections: your large read request needs to be split into fewer IOs.  This matters if you&#x27;re reading the output of this workload.  <a href="https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;abs&#x2F;10.1145&#x2F;3477132.3483593" rel="nofollow">https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;abs&#x2F;10.1145&#x2F;3477132.3483593</a> discusses it in a sciencey way.</div><br/></div></div><div id="35880914" class="c"><input type="checkbox" id="c-35880914" checked=""/><div class="controls bullet"><span class="by">hot_gril</span><span>|</span><a href="#35880861">prev</a><span>|</span><a href="#35881202">next</a><span>|</span><label class="collapse" for="c-35880914">[-]</label><label class="expand" for="c-35880914">[1 more]</label></div><br/><div class="children"><div class="content">No, because flash memory (and so-called RAM) still do sequential reads and writes faster, but for sure I&#x27;ll tune a default Postgres config to give <i>less</i> of a penalty to random reads.</div><br/></div></div><div id="35879951" class="c"><input type="checkbox" id="c-35879951" checked=""/><div class="controls bullet"><span class="by">antx</span><span>|</span><a href="#35881202">prev</a><span>|</span><a href="#35880088">next</a><span>|</span><label class="collapse" for="c-35879951">[-]</label><label class="expand" for="c-35879951">[6 more]</label></div><br/><div class="children"><div class="content">&quot;Writes to the WAL are purely sequential and writes to long-term storage are purely sequential.&quot;<p>Is that simply a typo or am I missing something? Shouldn&#x27;t it be mostly random for long-term storage?</div><br/><div id="35880453" class="c"><input type="checkbox" id="c-35880453" checked=""/><div class="controls bullet"><span class="by">jasonwatkinspdx</span><span>|</span><a href="#35879951">parent</a><span>|</span><a href="#35880088">next</a><span>|</span><label class="collapse" for="c-35880453">[-]</label><label class="expand" for="c-35880453">[5 more]</label></div><br/><div class="children"><div class="content">No. In WAL based systems new writes are appended to the write ahead log, and the dirty database pages held in ram. Periodically, a checkpoint process writes out all those dirty pages to long term storage, then truncates the write ahead log. So both these writes can be sequential. This is one of the reasons databases have the performance they do vs just using lots of little files like a database naively.</div><br/><div id="35880799" class="c"><input type="checkbox" id="c-35880799" checked=""/><div class="controls bullet"><span class="by">eatonphil</span><span>|</span><a href="#35879951">root</a><span>|</span><a href="#35880453">parent</a><span>|</span><a href="#35880088">next</a><span>|</span><label class="collapse" for="c-35880799">[-]</label><label class="expand" for="c-35880799">[4 more]</label></div><br/><div class="children"><div class="content">&gt; So both these writes can be sequential.<p>In the worst case though, the long-term storage can still have random writes though no (for btrees anyway)? I figure LSM tree writes can always be sequential, even during compaction. But btrees can only do solely sequential writes if the entire tree needs to be rewritten. And you don&#x27;t want to always do that if only parts of the tree need to be updated?</div><br/><div id="35881183" class="c"><input type="checkbox" id="c-35881183" checked=""/><div class="controls bullet"><span class="by">saltcured</span><span>|</span><a href="#35879951">root</a><span>|</span><a href="#35880799">parent</a><span>|</span><a href="#35881558">next</a><span>|</span><label class="collapse" for="c-35881183">[-]</label><label class="expand" for="c-35881183">[1 more]</label></div><br/><div class="children"><div class="content">I would take it as a simplification to really mean &quot;sequential enough&quot; when writing out the reorganized copy to the long-term storage.<p>With all the different layers in modern storage, I think you can get asymptotically close to the sequential rate once you start hitting some of these other multi-block granularities, i.e. around the block size for flash erasure, encryption, redundancy coding, etc.  These don&#x27;t have to be that big, i.e. several megabytes.</div><br/></div></div><div id="35881558" class="c"><input type="checkbox" id="c-35881558" checked=""/><div class="controls bullet"><span class="by">jasonwatkinspdx</span><span>|</span><a href="#35879951">root</a><span>|</span><a href="#35880799">parent</a><span>|</span><a href="#35881183">prev</a><span>|</span><a href="#35880088">next</a><span>|</span><label class="collapse" for="c-35881558">[-]</label><label class="expand" for="c-35881558">[2 more]</label></div><br/><div class="children"><div class="content">The dirty pages can be written out in physical address order no matter what indexing structure is used. What real databases do involves a bunch of complicated choices, but basically there&#x27;s ways you can make checkpoints incremental such that it&#x27;s nearly always large sequential writes.</div><br/><div id="35881584" class="c"><input type="checkbox" id="c-35881584" checked=""/><div class="controls bullet"><span class="by">eatonphil</span><span>|</span><a href="#35879951">root</a><span>|</span><a href="#35881558">parent</a><span>|</span><a href="#35880088">next</a><span>|</span><label class="collapse" for="c-35881584">[-]</label><label class="expand" for="c-35881584">[1 more]</label></div><br/><div class="children"><div class="content">Sure it can be written in order but it still can involve non-consecutive writes which I assumed is closer to random writes from the disk&#x27;s perspective.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="35880088" class="c"><input type="checkbox" id="c-35880088" checked=""/><div class="controls bullet"><span class="by">xmonkee</span><span>|</span><a href="#35879951">prev</a><span>|</span><a href="#35879915">next</a><span>|</span><label class="collapse" for="c-35880088">[-]</label><label class="expand" for="c-35880088">[1 more]</label></div><br/><div class="children"><div class="content">What a great and clear write-up. Thanks to the author.</div><br/></div></div><div id="35879915" class="c"><input type="checkbox" id="c-35879915" checked=""/><div class="controls bullet"><span class="by">revelio</span><span>|</span><a href="#35880088">prev</a><span>|</span><a href="#35881131">next</a><span>|</span><label class="collapse" for="c-35879915">[-]</label><label class="expand" for="c-35879915">[7 more]</label></div><br/><div class="children"><div class="content">Amazing how much faster f2fs is than btrfs. Now I have some buyer&#x27;s remorse over using btrfs...</div><br/><div id="35880146" class="c"><input type="checkbox" id="c-35880146" checked=""/><div class="controls bullet"><span class="by">ilyt</span><span>|</span><a href="#35879915">parent</a><span>|</span><a href="#35880532">next</a><span>|</span><label class="collapse" for="c-35880146">[-]</label><label class="expand" for="c-35880146">[2 more]</label></div><br/><div class="children"><div class="content">It was always slow-ish compared to traditional ones, same with ZFS in some workloads</div><br/><div id="35880503" class="c"><input type="checkbox" id="c-35880503" checked=""/><div class="controls bullet"><span class="by">thewataccount</span><span>|</span><a href="#35879915">root</a><span>|</span><a href="#35880146">parent</a><span>|</span><a href="#35880532">next</a><span>|</span><label class="collapse" for="c-35880503">[-]</label><label class="expand" for="c-35880503">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s because of the COW I think right?</div><br/></div></div></div></div><div id="35880532" class="c"><input type="checkbox" id="c-35880532" checked=""/><div class="controls bullet"><span class="by">rektide</span><span>|</span><a href="#35879915">parent</a><span>|</span><a href="#35880146">prev</a><span>|</span><a href="#35880522">next</a><span>|</span><label class="collapse" for="c-35880532">[-]</label><label class="expand" for="c-35880532">[2 more]</label></div><br/><div class="children"><div class="content">What are you citing or talking about? Neither f2fs nor btrfs show up in the article.</div><br/><div id="35880632" class="c"><input type="checkbox" id="c-35880632" checked=""/><div class="controls bullet"><span class="by">kardos</span><span>|</span><a href="#35879915">root</a><span>|</span><a href="#35880532">parent</a><span>|</span><a href="#35880522">next</a><span>|</span><label class="collapse" for="c-35880632">[-]</label><label class="expand" for="c-35880632">[1 more]</label></div><br/><div class="children"><div class="content">See the reproduced Figure 9</div><br/></div></div></div></div><div id="35880522" class="c"><input type="checkbox" id="c-35880522" checked=""/><div class="controls bullet"><span class="by">infamouscow</span><span>|</span><a href="#35879915">parent</a><span>|</span><a href="#35880532">prev</a><span>|</span><a href="#35881131">next</a><span>|</span><label class="collapse" for="c-35880522">[-]</label><label class="expand" for="c-35880522">[2 more]</label></div><br/><div class="children"><div class="content">If you like f2fs, you might also be excited about ssdfs: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=34939248" rel="nofollow">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=34939248</a></div><br/><div id="35880930" class="c"><input type="checkbox" id="c-35880930" checked=""/><div class="controls bullet"><span class="by">bcrl</span><span>|</span><a href="#35879915">root</a><span>|</span><a href="#35880522">parent</a><span>|</span><a href="#35881131">next</a><span>|</span><label class="collapse" for="c-35880930">[-]</label><label class="expand" for="c-35880930">[1 more]</label></div><br/><div class="children"><div class="content">Or bcachefs...</div><br/></div></div></div></div></div></div><div id="35881131" class="c"><input type="checkbox" id="c-35881131" checked=""/><div class="controls bullet"><span class="by">gfody</span><span>|</span><a href="#35879915">prev</a><span>|</span><a href="#35881293">next</a><span>|</span><label class="collapse" for="c-35881131">[-]</label><label class="expand" for="c-35881131">[1 more]</label></div><br/><div class="children"><div class="content">you&#x27;d need something byte addressable for access pattern not to matter.</div><br/></div></div><div id="35881293" class="c"><input type="checkbox" id="c-35881293" checked=""/><div class="controls bullet"><span class="by">jonstewart</span><span>|</span><a href="#35881131">prev</a><span>|</span><a href="#35881116">next</a><span>|</span><label class="collapse" for="c-35881293">[-]</label><label class="expand" for="c-35881293">[1 more]</label></div><br/><div class="children"><div class="content">The interesting question —- I’d appreciate any links —- is how to change our programming patterns to get the most performance out of NVMe SSDs. I’ve seen before that it takes a lot of parallel requests to saturate a fast NVMe drive —- but how to change our application architectures to generate such a workload?</div><br/></div></div><div id="35881116" class="c"><input type="checkbox" id="c-35881116" checked=""/><div class="controls bullet"><span class="by">jscipione</span><span>|</span><a href="#35881293">prev</a><span>|</span><a href="#35880004">next</a><span>|</span><label class="collapse" for="c-35881116">[-]</label><label class="expand" for="c-35881116">[1 more]</label></div><br/><div class="children"><div class="content">COW file systems are pretty amazing, especially on flash memory-based storage.</div><br/></div></div><div id="35880518" class="c"><input type="checkbox" id="c-35880518" checked=""/><div class="controls bullet"><span class="by">layer8</span><span>|</span><a href="#35880004">prev</a><span>|</span><a href="#35880644">next</a><span>|</span><label class="collapse" for="c-35880518">[-]</label><label class="expand" for="c-35880518">[1 more]</label></div><br/><div class="children"><div class="content">Tldr: No – “the benefits of sequential IO are alive and well, even in this new era of NAND flash.”<p>(Betteridge&#x27;s law of headlines strikes again.)</div><br/></div></div><div id="35880644" class="c"><input type="checkbox" id="c-35880644" checked=""/><div class="controls bullet"><span class="by">sylware</span><span>|</span><a href="#35880518">prev</a><span>|</span><label class="collapse" for="c-35880644">[-]</label><label class="expand" for="c-35880644">[4 more]</label></div><br/><div class="children"><div class="content">Huh?<p>PCIE is serial.</div><br/><div id="35880712" class="c"><input type="checkbox" id="c-35880712" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#35880644">parent</a><span>|</span><a href="#35881082">next</a><span>|</span><label class="collapse" for="c-35880712">[-]</label><label class="expand" for="c-35880712">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s true(ish) but it has nothing to do with sequential access patterns in LBA space.</div><br/></div></div><div id="35881082" class="c"><input type="checkbox" id="c-35881082" checked=""/><div class="controls bullet"><span class="by">dboreham</span><span>|</span><a href="#35880644">parent</a><span>|</span><a href="#35880712">prev</a><span>|</span><a href="#35881139">next</a><span>|</span><label class="collapse" for="c-35881082">[-]</label><label class="expand" for="c-35881082">[1 more]</label></div><br/><div class="children"><div class="content">Everything is serial.</div><br/></div></div><div id="35881139" class="c"><input type="checkbox" id="c-35881139" checked=""/><div class="controls bullet"><span class="by">sylware</span><span>|</span><a href="#35880644">parent</a><span>|</span><a href="#35881082">prev</a><span>|</span><label class="collapse" for="c-35881139">[-]</label><label class="expand" for="c-35881139">[1 more]</label></div><br/><div class="children"><div class="content">At least you could have said they are several lanes, a bit of &#x2F;&#x2F;.</div><br/></div></div></div></div></div></div></div></div></div></body></html>