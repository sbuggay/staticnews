<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1688720460405" as="style"/><link rel="stylesheet" href="styles.css?v=1688720460405"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://openai.com/blog/gpt-4-api-general-availability">GPT-4 API General Availability</a> <span class="domain">(<a href="https://openai.com">openai.com</a>)</span></div><div class="subtext"><span>mfiguiere</span> | <span>427 comments</span></div><br/><div><div id="36623570" class="c"><input type="checkbox" id="c-36623570" checked=""/><div class="controls bullet"><span class="by">PostOnce</span><span>|</span><a href="#36623213">next</a><span>|</span><label class="collapse" for="c-36623570">[-]</label><label class="expand" for="c-36623570">[75 more]</label></div><br/><div class="children"><div class="content">Promote and proliferate local LLMs.<p>If you use GPT, you&#x27;re giving OpenAI money to lobby the government so they&#x27;ll have no competitors, ultimately screwing yourself, your wallet, and the rest of us too.<p>OpenAI has no moat, unless you give them money to write legislation.<p>I can currently run some scary smart and fast LLMs on a 5 year old laptop with no GPU. The future is, at least, interesting.</div><br/><div id="36624911" class="c"><input type="checkbox" id="c-36624911" checked=""/><div class="controls bullet"><span class="by">andai</span><span>|</span><a href="#36623570">parent</a><span>|</span><a href="#36628952">next</a><span>|</span><label class="collapse" for="c-36624911">[-]</label><label class="expand" for="c-36624911">[11 more]</label></div><br/><div class="children"><div class="content">Can you elaborate on scary smart and fast?<p>It&#x27;s been a month or two since I&#x27;ve tried but the results were depressingly slow and useless for more or less every task I tried.<p>Every time a model is claimed to be &quot;90% of GPT-3&quot; I get excited and every time it&#x27;s very disappointing.<p>(On that note, after using GPT-4, GPT-3 now seems disappointing almost every time I interact with it.)</div><br/><div id="36626541" class="c"><input type="checkbox" id="c-36626541" checked=""/><div class="controls bullet"><span class="by">PostOnce</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36624911">parent</a><span>|</span><a href="#36628784">next</a><span>|</span><label class="collapse" for="c-36626541">[-]</label><label class="expand" for="c-36626541">[7 more]</label></div><br/><div class="children"><div class="content">Different quantizations can give you a big speedup if you&#x27;ve had &quot;depressingly slow&quot; issues. Even the slowest ones (that fit in RAM) will run at basically interactive speed, not instant, but also not &quot;email speed&quot;. I have a laptop with a 2018 CPU and I&#x27;m working with them just fine.<p>Text generation style instead of chat style is another avenue that makes the feedback time not so annoying for a developer.<p>at 100ms&#x2F;token, it&#x27;s faster than most people type, I think. That&#x27;s what you might get on an old laptop with a 7B model.<p>There&#x27;s a useful leaderboard here to help you pick a model: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;HuggingFaceH4&#x2F;open_llm_leaderboard" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;HuggingFaceH4&#x2F;open_llm_leaderb...</a><p>It really depends on your task, lots and lots of natural language type tasks give great results, the models seem to have extensive knowledge of many fields. So for some kinds of Q&amp;A bot (technical or not), for copy blurbs, for fiction, game NPCs, etc, the models (especially 13B and up) can be breathtaking, even moreso considering they run on bottom-dollar consumer hardware (I paid $250 for the laptop I&#x27;m developing on).<p>There are of course some things that neither the local LLMs nor GPT4 can do, like create useful OpenSCAD models :)<p>Things keep getting better, newer quantization methods give you more smarts in the same amount of RAM at basically the same speed -- the models are getting better, there are more permissively licensed ones now.</div><br/><div id="36627511" class="c"><input type="checkbox" id="c-36627511" checked=""/><div class="controls bullet"><span class="by">RossBencina</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36626541">parent</a><span>|</span><a href="#36628193">next</a><span>|</span><label class="collapse" for="c-36627511">[-]</label><label class="expand" for="c-36627511">[1 more]</label></div><br/><div class="children"><div class="content">How are you running inference? GPU or CPU? I&#x27;m trying to use GPT4All (ggml-based) on 32 cores of E5-v3 hardware and even the 4GB models are depressingly slow as far as I&#x27;m concerned (i.e. slower than the GPT4 API, which is barely usable for interactive work). I&#x27;d be much obliged if you could point me at a specific quantized model on HF that you think is &quot;fast&quot; and I&#x27;ll download it and try it out.</div><br/></div></div><div id="36628193" class="c"><input type="checkbox" id="c-36628193" checked=""/><div class="controls bullet"><span class="by">wokwokwok</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36626541">parent</a><span>|</span><a href="#36627511">prev</a><span>|</span><a href="#36628784">next</a><span>|</span><label class="collapse" for="c-36628193">[-]</label><label class="expand" for="c-36628193">[5 more]</label></div><br/><div class="children"><div class="content">Whaaaaat, how are you getting 100ms per token on an 5 year old potato without a graphics card?<p>Like, not vaguely hand wavey stuff, specifically, what model and what inference code?<p>I get nothing like that performance for the 7B models, forget the larger models, using llama.cpp on a pc without an nvidia GPU.</div><br/><div id="36628402" class="c"><input type="checkbox" id="c-36628402" checked=""/><div class="controls bullet"><span class="by">pbmonster</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36628193">parent</a><span>|</span><a href="#36628303">next</a><span>|</span><label class="collapse" for="c-36628402">[-]</label><label class="expand" for="c-36628402">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m running TheBlokes wizard-vicuna-13b-superhot-8k.ggmlv3 with 4-bit quantization on a Ryzen 5 that&#x27;s probably older than OPs laptop.<p>I get around 5 tokens a second using the webui that comes with oogabooga using default settings. If I understand correctly, this does not get me 8k context length yet, because oogabooga doesn&#x27;t have NTK-aware scaled RoPE implemented yet.<p>Using the same model with the newest kobold.cpp release should provide 8k context, but runs significantly slower.<p>Note that this model is great at creative writing, and sounding smart when talking about tech stuff, but it sucks horribly at stuff like logic puzzles or (re-)producing factually correct in-depth answers about any topic I&#x27;m an expert in. Still at least an order of magnitude below GPT4.<p>The model is also uncensored, which is amusing after using GPT4. It will happily elaborate on how to mix explosives and it has a dirty mouth.<p>Interestingly, the model speaks at least half a dozen languages much better than I do, and is proficient at translating between them (far worse than deepL, of course). Which is mindblowing for a 8GByte binary. It&#x27;s actual black magic.</div><br/></div></div><div id="36628303" class="c"><input type="checkbox" id="c-36628303" checked=""/><div class="controls bullet"><span class="by">Rastonbury</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36628193">parent</a><span>|</span><a href="#36628402">prev</a><span>|</span><a href="#36628723">next</a><span>|</span><label class="collapse" for="c-36628303">[-]</label><label class="expand" for="c-36628303">[2 more]</label></div><br/><div class="children"><div class="content">This is a exactly a case in point why people decide to pay OpenAI instead of rolling their own. I&#x27;m non-technical but have setup an image gen app based  custom SD model using diffusers, so not entirely clueless.<p>But for LLM I have no where idea where to start quickly. Finding a model on a leaderboard, download and setup then customising it and benchmarking is way too much time for me, I&#x27;ll just pay for GPT4 if ever need to instead of chasing and troubleshooting to get some magical result. It&#x27;ll be easier in the future I&#x27;m sure when an open model merges as the SD1.5 of LLM</div><br/><div id="36628828" class="c"><input type="checkbox" id="c-36628828" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36628303">parent</a><span>|</span><a href="#36628723">next</a><span>|</span><label class="collapse" for="c-36628828">[-]</label><label class="expand" for="c-36628828">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve found <a href="https:&#x2F;&#x2F;gpt4all.io&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;gpt4all.io&#x2F;</a> to be the fastest way to get started. I&#x27;ve also started moving my notes to <a href="https:&#x2F;&#x2F;llm-tracker.info&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;llm-tracker.info&#x2F;</a> which should help make it easier for people getting started: <a href="https:&#x2F;&#x2F;llm-tracker.info&#x2F;books&#x2F;howto-guides&#x2F;page&#x2F;getting-started" rel="nofollow noreferrer">https:&#x2F;&#x2F;llm-tracker.info&#x2F;books&#x2F;howto-guides&#x2F;page&#x2F;getting-sta...</a></div><br/></div></div></div></div><div id="36628723" class="c"><input type="checkbox" id="c-36628723" checked=""/><div class="controls bullet"><span class="by">marci</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36628193">parent</a><span>|</span><a href="#36628303">prev</a><span>|</span><a href="#36628784">next</a><span>|</span><label class="collapse" for="c-36628723">[-]</label><label class="expand" for="c-36628723">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m on a thinkpad with a 2016 CPU (i5-7300U) running ubuntu.<p>I don&#x27;t know anything so I left default settings.<p>I get about 450ms&#x2F;t with airoboros-7b and 350ms&#x2F;t with orca-mini-3b.<p>edit: with oobabooga webui</div><br/></div></div></div></div></div></div><div id="36628784" class="c"><input type="checkbox" id="c-36628784" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36624911">parent</a><span>|</span><a href="#36626541">prev</a><span>|</span><a href="#36626877">next</a><span>|</span><label class="collapse" for="c-36628784">[-]</label><label class="expand" for="c-36628784">[1 more]</label></div><br/><div class="children"><div class="content">In terms of speed, we&#x27;re talking about 140t&#x2F;s for 7B models, and 40t&#x2F;s for 33B models on a 3090&#x2F;4090 now.[1] (1 token ~= 0.75 word) It&#x27;s quite zippy. llama.cpp performs close on Nvidia GPUs now (but they don&#x27;t have a handy chart) and you can get decent performance on 13B models on M1&#x2F;M2 Macs.<p>You can take a look at a list of evals here: <a href="https:&#x2F;&#x2F;llm-tracker.info&#x2F;books&#x2F;evals&#x2F;page&#x2F;list-of-evals" rel="nofollow noreferrer">https:&#x2F;&#x2F;llm-tracker.info&#x2F;books&#x2F;evals&#x2F;page&#x2F;list-of-evals</a> - for general usage, I think home-rolled evals like llm-jeopardy [2] and local-llm-comparison [3] by hobbyists are more useful than most of the benchmark rankings.<p>That being said, personally I mostly use GPT-4 for code assistance to that&#x27;s what I&#x27;m most interested in, and the latest code assistants are scoring quite well: <a href="https:&#x2F;&#x2F;github.com&#x2F;abacaj&#x2F;code-eval">https:&#x2F;&#x2F;github.com&#x2F;abacaj&#x2F;code-eval</a> - a recent replit-3b fine tune the human-eval results for open models (as a point of reference, GPT-3.5 gets 60.4 on pass@1 and 68.9 on pass@10 [4]) - I&#x27;ve only just started playing around with it since replit model tooling is not as good as llamas (doc here: <a href="https:&#x2F;&#x2F;llm-tracker.info&#x2F;books&#x2F;howto-guides&#x2F;page&#x2F;replit-models" rel="nofollow noreferrer">https:&#x2F;&#x2F;llm-tracker.info&#x2F;books&#x2F;howto-guides&#x2F;page&#x2F;replit-mode...</a>).<p>I&#x27;m interested in potentially applying reflexion or some of the other techniques that have been tried to even further increase coding abilities. (InterCode in particular has caught my eye <a href="https:&#x2F;&#x2F;intercode-benchmark.github.io&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;intercode-benchmark.github.io&#x2F;</a>)<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;turboderp&#x2F;exllama#results-so-far">https:&#x2F;&#x2F;github.com&#x2F;turboderp&#x2F;exllama#results-so-far</a><p>[2] <a href="https:&#x2F;&#x2F;github.com&#x2F;aigoopy&#x2F;llm-jeopardy">https:&#x2F;&#x2F;github.com&#x2F;aigoopy&#x2F;llm-jeopardy</a><p>[3] <a href="https:&#x2F;&#x2F;github.com&#x2F;Troyanovsky&#x2F;Local-LLM-comparison&#x2F;tree&#x2F;main">https:&#x2F;&#x2F;github.com&#x2F;Troyanovsky&#x2F;Local-LLM-comparison&#x2F;tree&#x2F;mai...</a><p>[4] <a href="https:&#x2F;&#x2F;github.com&#x2F;nlpxucan&#x2F;WizardLM&#x2F;tree&#x2F;main&#x2F;WizardCoder">https:&#x2F;&#x2F;github.com&#x2F;nlpxucan&#x2F;WizardLM&#x2F;tree&#x2F;main&#x2F;WizardCoder</a></div><br/></div></div><div id="36626877" class="c"><input type="checkbox" id="c-36626877" checked=""/><div class="controls bullet"><span class="by">ApolloFortyNine</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36624911">parent</a><span>|</span><a href="#36628784">prev</a><span>|</span><a href="#36625603">next</a><span>|</span><label class="collapse" for="c-36626877">[-]</label><label class="expand" for="c-36626877">[1 more]</label></div><br/><div class="children"><div class="content">Save for coding they&#x27;ve been pretty good in my experience.<p>There&#x27;s definitely some prompt magic openai does behind the scenes that helps beat the raw style local llms usually go for. With proper prompting you can get chatgpt like answers.</div><br/></div></div><div id="36625603" class="c"><input type="checkbox" id="c-36625603" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36624911">parent</a><span>|</span><a href="#36626877">prev</a><span>|</span><a href="#36628952">next</a><span>|</span><label class="collapse" for="c-36625603">[-]</label><label class="expand" for="c-36625603">[1 more]</label></div><br/><div class="children"><div class="content">i think the falcon instruct is considered pretty good but if you are expectation set by gpt4 it still will not compare</div><br/></div></div></div></div><div id="36628952" class="c"><input type="checkbox" id="c-36628952" checked=""/><div class="controls bullet"><span class="by">Jzush</span><span>|</span><a href="#36623570">parent</a><span>|</span><a href="#36624911">prev</a><span>|</span><a href="#36626943">next</a><span>|</span><label class="collapse" for="c-36628952">[-]</label><label class="expand" for="c-36628952">[1 more]</label></div><br/><div class="children"><div class="content">I’m currently using the free tier ChatGPT web interface to help me with mundane coding tasks like JavaScript, php or css.<p>Is there a local solution that is at least as intelligent as GPT 3.5 in that regard that I can run in a container?</div><br/></div></div><div id="36626943" class="c"><input type="checkbox" id="c-36626943" checked=""/><div class="controls bullet"><span class="by">bredren</span><span>|</span><a href="#36623570">parent</a><span>|</span><a href="#36628952">prev</a><span>|</span><a href="#36628471">next</a><span>|</span><label class="collapse" for="c-36626943">[-]</label><label class="expand" for="c-36626943">[2 more]</label></div><br/><div class="children"><div class="content">Running an LLM locally and paying for access to OpenAI are two separate concerns.<p>But to address both: is it very relevant what LLM you use right now? Local or hosted, openAI or other?<p>It seems like the interface has converged around chat-based prompts.<p>New ideas for tuning or improving the efficiency of foundational models are published almost every week.<p>If one wants to build a product on top of of generative AI, why not simply start with what’s free or works with one’s dev environment?<p>Presumably, the interaction with or API to text-based gen AI will be very similar no matter what engine is best for your use case at any given time.<p>This would imply these backends will be swappable, the way web services are that copy AWS S3 APIs.<p>So, to return to the point, can’t people just build their product with openAI or other and plan to move away based on the cost and fit for their circumstances?<p>Couldn’t someone say prototype the entire product on some lower-quality LLM and occasionally pass requests to GPT4 to validate behavior?<p>It seems far-fetched to believe this tech can be constrained by legislation.<p>OpenAI can lobby all they want, it won’t necessarily buy them anything.  Look what happened with FTX.<p>Since LLMs can be run locally and the engines be black boxes to the user, how could a legislative act really prevent them from being everywhere—-especially given the public utility.</div><br/><div id="36627737" class="c"><input type="checkbox" id="c-36627737" checked=""/><div class="controls bullet"><span class="by">ignoramous</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36626943">parent</a><span>|</span><a href="#36628471">next</a><span>|</span><label class="collapse" for="c-36627737">[-]</label><label class="expand" for="c-36627737">[1 more]</label></div><br/><div class="children"><div class="content">&gt; <i>Couldn’t someone say prototype the entire product on some lower-quality LLM and occasionally pass requests to GPT4 to validate behavior?</i><p>This, infact, might be a better way to do inference anyway: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;Francis_YAO_&#x2F;status&#x2F;1675967988925710338" rel="nofollow noreferrer">https:&#x2F;&#x2F;twitter.com&#x2F;Francis_YAO_&#x2F;status&#x2F;1675967988925710338</a><p>&gt; <i>So, to return to the point, can’t people just build their product with openAI or other and plan to move away based on the cost and fit for their circumstances?</i><p>Depends. There are signs that folks are buying into GPT-specific APIs (like function calls) which may not be as easy to migrate away from.</div><br/></div></div></div></div><div id="36628471" class="c"><input type="checkbox" id="c-36628471" checked=""/><div class="controls bullet"><span class="by">Obscurity4340</span><span>|</span><a href="#36623570">parent</a><span>|</span><a href="#36626943">prev</a><span>|</span><a href="#36623605">next</a><span>|</span><label class="collapse" for="c-36628471">[-]</label><label class="expand" for="c-36628471">[2 more]</label></div><br/><div class="children"><div class="content">Where can we aquire or access these local LLMs? How much space and specs does it actually require?</div><br/><div id="36628826" class="c"><input type="checkbox" id="c-36628826" checked=""/><div class="controls bullet"><span class="by">dagaci</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36628471">parent</a><span>|</span><a href="#36623605">next</a><span>|</span><label class="collapse" for="c-36628826">[-]</label><label class="expand" for="c-36628826">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;gpt4all.io&#x2F;index.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;gpt4all.io&#x2F;index.html</a> is a good place to start, you can literally download one of the many recommended models.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;imartinez&#x2F;privateGPT">https:&#x2F;&#x2F;github.com&#x2F;imartinez&#x2F;privateGPT</a> is great if you want do it with code.</div><br/></div></div></div></div><div id="36623605" class="c"><input type="checkbox" id="c-36623605" checked=""/><div class="controls bullet"><span class="by">john2x</span><span>|</span><a href="#36623570">parent</a><span>|</span><a href="#36628471">prev</a><span>|</span><a href="#36624589">next</a><span>|</span><label class="collapse" for="c-36623605">[-]</label><label class="expand" for="c-36623605">[27 more]</label></div><br/><div class="children"><div class="content">Care to share some links? My lack of GPU is the main blocker for me from playing with local-only options.<p>I have an old laptop with 16GB RAM and no GPU. Can I run these models?</div><br/><div id="36623680" class="c"><input type="checkbox" id="c-36623680" checked=""/><div class="controls bullet"><span class="by">PostOnce</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36623605">parent</a><span>|</span><a href="#36623809">next</a><span>|</span><label class="collapse" for="c-36623680">[-]</label><label class="expand" for="c-36623680">[16 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp</a><p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;TheBloke" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;TheBloke</a><p>There&#x27;s a LocalLLaMA subreddit, irc channels, and a whole big community around the web working on it on GitHub nd elsewhere.<p>edit: I forgot to directly answer you: <i>yes</i> you can run these models. 16GB of plenty. Different quantizations give you different amounts of smarts and speed. There are tables that tell you how much RAM is needed per which quantization you choose, as well as how fast it can produce results (ms per token). e.g. <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp#quantization">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp#quantization</a>  where RAM required a little more than the file size, but there are tables that list it explicitly which I don&#x27;t have immediately at hand.</div><br/><div id="36624132" class="c"><input type="checkbox" id="c-36624132" checked=""/><div class="controls bullet"><span class="by">tensor</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36623680">parent</a><span>|</span><a href="#36623809">next</a><span>|</span><label class="collapse" for="c-36624132">[-]</label><label class="expand" for="c-36624132">[15 more]</label></div><br/><div class="children"><div class="content">A reminder that llama isn&#x27;t legal for the vast majority of use cases. Unless you signed their contract and then you can use it only for research purposes.</div><br/><div id="36624522" class="c"><input type="checkbox" id="c-36624522" checked=""/><div class="controls bullet"><span class="by">PostOnce</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36624132">parent</a><span>|</span><a href="#36624178">next</a><span>|</span><label class="collapse" for="c-36624522">[-]</label><label class="expand" for="c-36624522">[4 more]</label></div><br/><div class="children"><div class="content">OpenLLaMA is though. <a href="https:&#x2F;&#x2F;github.com&#x2F;openlm-research&#x2F;open_llama">https:&#x2F;&#x2F;github.com&#x2F;openlm-research&#x2F;open_llama</a><p>All of these are surmountable problems.<p>We can beat OpenAI.<p>We can drain their moat.</div><br/><div id="36626853" class="c"><input type="checkbox" id="c-36626853" checked=""/><div class="controls bullet"><span class="by">donw</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36624522">parent</a><span>|</span><a href="#36627247">next</a><span>|</span><label class="collapse" for="c-36626853">[-]</label><label class="expand" for="c-36626853">[1 more]</label></div><br/><div class="children"><div class="content">For the above, are the RAM figures system RAM or GPU?</div><br/></div></div><div id="36627247" class="c"><input type="checkbox" id="c-36627247" checked=""/><div class="controls bullet"><span class="by">kordlessagain</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36624522">parent</a><span>|</span><a href="#36626853">prev</a><span>|</span><a href="#36624178">next</a><span>|</span><label class="collapse" for="c-36627247">[-]</label><label class="expand" for="c-36627247">[2 more]</label></div><br/><div class="children"><div class="content">&gt; We can drain their moat.<p>I&#x27;ve got an AI powered sump pump if you need it.</div><br/><div id="36627764" class="c"><input type="checkbox" id="c-36627764" checked=""/><div class="controls bullet"><span class="by">ignoramous</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36627247">parent</a><span>|</span><a href="#36624178">next</a><span>|</span><label class="collapse" for="c-36627764">[-]</label><label class="expand" for="c-36627764">[1 more]</label></div><br/><div class="children"><div class="content">They most certainly don&#x27;t need &#x2F; deserve the snark, to be sure, on <i>hacker</i> news of all places.</div><br/></div></div></div></div></div></div><div id="36624178" class="c"><input type="checkbox" id="c-36624178" checked=""/><div class="controls bullet"><span class="by">rvcdbn</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36624132">parent</a><span>|</span><a href="#36624522">prev</a><span>|</span><a href="#36628197">next</a><span>|</span><label class="collapse" for="c-36624178">[-]</label><label class="expand" for="c-36624178">[9 more]</label></div><br/><div class="children"><div class="content">We don’t actually know that it’s not legal. The copyrightability of model weights is an open legal question right now afaik.</div><br/><div id="36624325" class="c"><input type="checkbox" id="c-36624325" checked=""/><div class="controls bullet"><span class="by">tensor</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36624178">parent</a><span>|</span><a href="#36628197">next</a><span>|</span><label class="collapse" for="c-36624325">[-]</label><label class="expand" for="c-36624325">[8 more]</label></div><br/><div class="children"><div class="content">It doesn&#x27;t have to be copyrightable to be intellectual property.</div><br/><div id="36625011" class="c"><input type="checkbox" id="c-36625011" checked=""/><div class="controls bullet"><span class="by">twbarr</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36624325">parent</a><span>|</span><a href="#36624415">next</a><span>|</span><label class="collapse" for="c-36625011">[-]</label><label class="expand" for="c-36625011">[5 more]</label></div><br/><div class="children"><div class="content">No, but what is it? Not your lawyer, not legal advice, but it&#x27;s not a trade secret, they&#x27;ve given it to researchers. It&#x27;s not a trademark because it&#x27;s not an origin identifier. The structure might be patentable, but the weights won&#x27;t be. It&#x27;s certainly not a mask work.<p>It might have been a contract violation for the guy who redistributed it, but I&#x27;m not a party to that contract.</div><br/><div id="36628963" class="c"><input type="checkbox" id="c-36628963" checked=""/><div class="controls bullet"><span class="by">andsoitis</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36625011">parent</a><span>|</span><a href="#36626469">next</a><span>|</span><label class="collapse" for="c-36628963">[-]</label><label class="expand" for="c-36628963">[1 more]</label></div><br/><div class="children"><div class="content">&gt; It might have been a contract violation for the guy who redistributed it, but I&#x27;m not a party to that contract.<p>Wouldn’t that violate the <i>Nemo dat quod non habet</i> legal principle and so you cannot hide behind the claim that you weren’t party to the contact?<p><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Nemo_dat_quod_non_habet" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Nemo_dat_quod_non_habet</a></div><br/></div></div><div id="36626469" class="c"><input type="checkbox" id="c-36626469" checked=""/><div class="controls bullet"><span class="by">Art9681</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36625011">parent</a><span>|</span><a href="#36628963">prev</a><span>|</span><a href="#36624415">next</a><span>|</span><label class="collapse" for="c-36626469">[-]</label><label class="expand" for="c-36626469">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m going to play devil&#x27;s advocate and state that a lot of what you mentioned will be relevant to a tiny part of the world that has the means to enforce this. The law will be forced to change as a response to AI. Many debates will be had. Many crap laws will be made by people grasping at straws but it&#x27;s too late. Putting red tape around this technology puts that nation at a technological disadvantage. I would go as far as labeling a national security threat.<p>I&#x27;m calling it now. Based on what I see today. Europe will position itself as a leader in AI legislation, and its economy will give way to the nations that want to enter the race and grab a chunk of the new economy.<p>It&#x27;s a Catch 22. You either gimp your own technological progress, or start a war with a nation that does not. Pretty sure Russia and China don&#x27;t really care about the ethics behind it. There are plenty of nations capable enough in the same boat.<p>Now what? OK, so in some hypothetical future China has an uncensored model with free reign over the internet. The US and Europe has banned this. What&#x27;s stopping anyone from running the Chinese model? There isn&#x27;t enough money in the world to enforce software laws.<p>How long have they tried to take down The Pirate Bay? Pretty much every permutation of every software that&#x27;s ever been banned can be found and ran with impunity if you have the technical knowledge to do so. No law exists that can prevent that.<p>If it did, OpenAI wouldn&#x27;t exist.</div><br/><div id="36627294" class="c"><input type="checkbox" id="c-36627294" checked=""/><div class="controls bullet"><span class="by">rileymat2</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36626469">parent</a><span>|</span><a href="#36624415">next</a><span>|</span><label class="collapse" for="c-36627294">[-]</label><label class="expand" for="c-36627294">[2 more]</label></div><br/><div class="children"><div class="content">&gt; How long have they tried to take down The Pirate Bay? Pretty much every permutation of every software that&#x27;s ever been banned can be found and ran with impunity if you have the technical knowledge to do so. No law exists that can prevent that.<p>Forms of this argument get tossed out a lot. Laws don’t prevent, they hopefully limit. Murder has been illegal for a long time, it still happens.</div><br/><div id="36628797" class="c"><input type="checkbox" id="c-36628797" checked=""/><div class="controls bullet"><span class="by">szundi</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36627294">parent</a><span>|</span><a href="#36624415">next</a><span>|</span><label class="collapse" for="c-36628797">[-]</label><label class="expand" for="c-36628797">[1 more]</label></div><br/><div class="children"><div class="content">You missed the point: these laws are not limiting other countries, only those who introduce them. Self-limiting, giving advantage to others.</div><br/></div></div></div></div></div></div></div></div><div id="36624415" class="c"><input type="checkbox" id="c-36624415" checked=""/><div class="controls bullet"><span class="by">actionfromafar</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36624325">parent</a><span>|</span><a href="#36625011">prev</a><span>|</span><a href="#36628197">next</a><span>|</span><label class="collapse" for="c-36624415">[-]</label><label class="expand" for="c-36624415">[2 more]</label></div><br/><div class="children"><div class="content">Patents? Trademark? What do you mean?</div><br/><div id="36626311" class="c"><input type="checkbox" id="c-36626311" checked=""/><div class="controls bullet"><span class="by">Rexxar</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36624415">parent</a><span>|</span><a href="#36628197">next</a><span>|</span><label class="collapse" for="c-36626311">[-]</label><label class="expand" for="c-36626311">[1 more]</label></div><br/><div class="children"><div class="content">Maybe this: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Database_right" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Database_right</a> but it doesn&#x27;t exist in every countries.</div><br/></div></div></div></div></div></div></div></div><div id="36628197" class="c"><input type="checkbox" id="c-36628197" checked=""/><div class="controls bullet"><span class="by">niemandhier</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36624132">parent</a><span>|</span><a href="#36624178">prev</a><span>|</span><a href="#36623809">next</a><span>|</span><label class="collapse" for="c-36628197">[-]</label><label class="expand" for="c-36628197">[1 more]</label></div><br/><div class="children"><div class="content">It’s not clear if their license terms would hold, for the moment just act and worry later.<p>Update:
That is only true for the legal system I am currently residing in. No idea about e.g. the US.</div><br/></div></div></div></div></div></div><div id="36623809" class="c"><input type="checkbox" id="c-36623809" checked=""/><div class="controls bullet"><span class="by">jstummbillig</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36623605">parent</a><span>|</span><a href="#36623680">prev</a><span>|</span><a href="#36624356">next</a><span>|</span><label class="collapse" for="c-36623809">[-]</label><label class="expand" for="c-36623809">[4 more]</label></div><br/><div class="children"><div class="content">Just a heads up: If you are more interested in being effective than being an evangelist, beware.<p>While you can run all kinds of GPTs locally, GPT-4 still smokes everything right now – and even it is not actually good enough to not be a lynchpin for a lot of cases yet.</div><br/><div id="36625811" class="c"><input type="checkbox" id="c-36625811" checked=""/><div class="controls bullet"><span class="by">slaymaker1907</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36623809">parent</a><span>|</span><a href="#36624356">next</a><span>|</span><label class="collapse" for="c-36625811">[-]</label><label class="expand" for="c-36625811">[3 more]</label></div><br/><div class="children"><div class="content">I guess ignoring copyright and treating the whole internet as your training data does have its advantages.</div><br/><div id="36626532" class="c"><input type="checkbox" id="c-36626532" checked=""/><div class="controls bullet"><span class="by">dcow</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36625811">parent</a><span>|</span><a href="#36624356">next</a><span>|</span><label class="collapse" for="c-36626532">[-]</label><label class="expand" for="c-36626532">[2 more]</label></div><br/><div class="children"><div class="content">Yes? That’s the point. Who cares about an outdated concept that has no digital analog? All the artists have moved on already #midjourney.</div><br/><div id="36629019" class="c"><input type="checkbox" id="c-36629019" checked=""/><div class="controls bullet"><span class="by">bombolo</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36626532">parent</a><span>|</span><a href="#36624356">next</a><span>|</span><label class="collapse" for="c-36629019">[-]</label><label class="expand" for="c-36629019">[1 more]</label></div><br/><div class="children"><div class="content">When mirosoft will open up all of their source code, I will agree with you.</div><br/></div></div></div></div></div></div></div></div><div id="36624356" class="c"><input type="checkbox" id="c-36624356" checked=""/><div class="controls bullet"><span class="by">tudorw</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36623605">parent</a><span>|</span><a href="#36623809">prev</a><span>|</span><a href="#36624549">next</a><span>|</span><label class="collapse" for="c-36624356">[-]</label><label class="expand" for="c-36624356">[2 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;gpt4all.io&#x2F;index.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;gpt4all.io&#x2F;index.html</a></div><br/><div id="36628093" class="c"><input type="checkbox" id="c-36628093" checked=""/><div class="controls bullet"><span class="by">yard2010</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36624356">parent</a><span>|</span><a href="#36624549">next</a><span>|</span><label class="collapse" for="c-36628093">[-]</label><label class="expand" for="c-36628093">[1 more]</label></div><br/><div class="children"><div class="content">Keep in mind it doesn&#x27;t relate to GPT4, the 4 in the name is for, not four. But I should try it. TBH openAI shady practices and MS behind them is just an anti trust waiting to happen and I don&#x27;t want a part in this dystopia</div><br/></div></div></div></div><div id="36624549" class="c"><input type="checkbox" id="c-36624549" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36623605">parent</a><span>|</span><a href="#36624356">prev</a><span>|</span><a href="#36624589">next</a><span>|</span><label class="collapse" for="c-36624549">[-]</label><label class="expand" for="c-36624549">[4 more]</label></div><br/><div class="children"><div class="content">16GB of RAM can fit a 5 bit 13B model at best, they&#x27;re second dumbest class of LLama model. If Open Orca turns out any good than that might be enough for the time being, but you&#x27;ll need more RAM to use anything serious.<p>Here&#x27;s a handy model comparison chart (this is a coding benchmark, so coding-only models tend to rank higher): <a href="https:&#x2F;&#x2F;i.imgur.com&#x2F;AqSjjj2.jpeg" rel="nofollow noreferrer">https:&#x2F;&#x2F;i.imgur.com&#x2F;AqSjjj2.jpeg</a></div><br/><div id="36624633" class="c"><input type="checkbox" id="c-36624633" checked=""/><div class="controls bullet"><span class="by">PostOnce</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36624549">parent</a><span>|</span><a href="#36624589">next</a><span>|</span><label class="collapse" for="c-36624633">[-]</label><label class="expand" for="c-36624633">[3 more]</label></div><br/><div class="children"><div class="content">Your benchmark lacks the current #2 <a href="https:&#x2F;&#x2F;github.com&#x2F;nlpxucan&#x2F;WizardLM&#x2F;tree&#x2F;main&#x2F;WizardCoder">https:&#x2F;&#x2F;github.com&#x2F;nlpxucan&#x2F;WizardLM&#x2F;tree&#x2F;main&#x2F;WizardCoder</a><p>It beats Claude and Bard.<p>You could probably get a 4bit 15B model going in 16GB of RAM and be approaching GPT4 in capability.<p><i>...on an old laptop, lol</i><p>Let&#x27;s eat OpenAI&#x27;s lunch! They deserve it for trying to steal this tech by &quot;privatizing&quot; a charity, hiding scientific data that was supposed to be shared with us by said charity whose purpose was to help us all, and dishonestly trying to persuade the government not to let us compete with them.</div><br/><div id="36625145" class="c"><input type="checkbox" id="c-36625145" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36624633">parent</a><span>|</span><a href="#36624589">next</a><span>|</span><label class="collapse" for="c-36625145">[-]</label><label class="expand" for="c-36625145">[2 more]</label></div><br/><div class="children"><div class="content">Yeah I mean I wouldn&#x27;t really include coding models in this list since they&#x27;re not general purpose models and have an obvious fine tuning edge compared to the rest. But WizardCoder is definitely something to look at as a Copilot replacement.<p>I&#x27;d post a more well rounded benchmark but the problem is that all non-coding benchmarks are currently more or less complete garbage, especially the Vicuna benchmark that rates everything as 99.7% GPT 3.5 lol.</div><br/><div id="36625770" class="c"><input type="checkbox" id="c-36625770" checked=""/><div class="controls bullet"><span class="by">PostOnce</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36625145">parent</a><span>|</span><a href="#36624589">next</a><span>|</span><label class="collapse" for="c-36625770">[-]</label><label class="expand" for="c-36625770">[1 more]</label></div><br/><div class="children"><div class="content">The benchmark you linked was to &quot;programming performance&quot;, not generic LLM &quot;intelligence&quot;.<p>The situation for the little guy is <i>wildly</i> better than most people imagine.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36624589" class="c"><input type="checkbox" id="c-36624589" checked=""/><div class="controls bullet"><span class="by">joeythedolphin</span><span>|</span><a href="#36623570">parent</a><span>|</span><a href="#36623605">prev</a><span>|</span><a href="#36625249">next</a><span>|</span><label class="collapse" for="c-36624589">[-]</label><label class="expand" for="c-36624589">[7 more]</label></div><br/><div class="children"><div class="content">Great point -- I was thinking of renewing my $20&#x2F;subscription but I will keep it cancelled. We must not fund AI propaganda machines.</div><br/><div id="36626792" class="c"><input type="checkbox" id="c-36626792" checked=""/><div class="controls bullet"><span class="by">ed_mercer</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36624589">parent</a><span>|</span><a href="#36625249">next</a><span>|</span><label class="collapse" for="c-36626792">[-]</label><label class="expand" for="c-36626792">[6 more]</label></div><br/><div class="children"><div class="content">Forgive me as I’m out of the loop. What propaganda are you referring to?</div><br/><div id="36627252" class="c"><input type="checkbox" id="c-36627252" checked=""/><div class="controls bullet"><span class="by">joeythedolphin</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36626792">parent</a><span>|</span><a href="#36627152">next</a><span>|</span><label class="collapse" for="c-36627252">[-]</label><label class="expand" for="c-36627252">[3 more]</label></div><br/><div class="children"><div class="content">Sam tells Congress that AI is so dangerous it will extinct humanity. Why? So Congress can license him and only his buddies. Then get goes to euro and speaks with world leaders to remove consumer protection. Why? So he can mine data without any consequences. He is a narcissistic CEO who lies to win. If you are tired of the past decade of electronic corporate tyranny, abuse, manipulation and lies, then boycott OpenAi (should be named ClosedAi) and support open source, or ethical companies (if there are any).</div><br/><div id="36628390" class="c"><input type="checkbox" id="c-36628390" checked=""/><div class="controls bullet"><span class="by">concordDance</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36627252">parent</a><span>|</span><a href="#36627152">next</a><span>|</span><label class="collapse" for="c-36628390">[-]</label><label class="expand" for="c-36628390">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Sam tells Congress that AI is so dangerous it will extinct humanity. Why? So Congress can license him and only his buddies.<p>No, he says it because its true and concerning.<p>However, just because AGI has a good chance of making humanity extinct does not mean we&#x27;re anywhere close to making AIs that capable. LLMs seem like a dead end.</div><br/><div id="36628752" class="c"><input type="checkbox" id="c-36628752" checked=""/><div class="controls bullet"><span class="by">kaba0</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36628390">parent</a><span>|</span><a href="#36627152">next</a><span>|</span><label class="collapse" for="c-36628752">[-]</label><label class="expand" for="c-36628752">[1 more]</label></div><br/><div class="children"><div class="content">&gt; However, just because AGI has a good chance of making humanity extinct<p>How? I mean surely it will lead humanity down some chaotic path, but I would fear climate catastrophe much much more than anything AI-related.</div><br/></div></div></div></div></div></div><div id="36627152" class="c"><input type="checkbox" id="c-36627152" checked=""/><div class="controls bullet"><span class="by">NortySpock</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36626792">parent</a><span>|</span><a href="#36627252">prev</a><span>|</span><a href="#36625249">next</a><span>|</span><label class="collapse" for="c-36627152">[-]</label><label class="expand" for="c-36627152">[2 more]</label></div><br/><div class="children"><div class="content">These ChatGPT tools allow anyone to write short marketing and propaganda prompts. They can then take the resulting paragraphs of puffery and post them using bots or sock puppets to whatever target community to create the illusion of action, consensus, conflict, discussion or dissention.<p>It used to be this took a few people to come up with writing actual responses to forum posts all day, or marketing operations plans, or pro- or anti-thing propaganda plans.<p>But now, you could astroturf a movement with a GPU, a ChatGPT clone, some bots and vpns hosted from a single computer, a cron job,  and one human running it.<p>If you thought disinformation was bad 2 years ago, get ready for fully automated disinformation that can be targeted down to an online community or specific user in an online community...</div><br/><div id="36628764" class="c"><input type="checkbox" id="c-36628764" checked=""/><div class="controls bullet"><span class="by">kaba0</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36627152">parent</a><span>|</span><a href="#36625249">next</a><span>|</span><label class="collapse" for="c-36628764">[-]</label><label class="expand" for="c-36628764">[1 more]</label></div><br/><div class="children"><div class="content">I believe a new wave of authentication might come out of this, where it is tied to citizenship for example (or something related to physical reality). Otherwise we will find ourselves in a truly chaotic situation.</div><br/></div></div></div></div></div></div></div></div><div id="36625249" class="c"><input type="checkbox" id="c-36625249" checked=""/><div class="controls bullet"><span class="by">kristianp</span><span>|</span><a href="#36623570">parent</a><span>|</span><a href="#36624589">prev</a><span>|</span><a href="#36624933">next</a><span>|</span><label class="collapse" for="c-36625249">[-]</label><label class="expand" for="c-36625249">[5 more]</label></div><br/><div class="children"><div class="content">Gpt-4 runs on 8 x 220B params[1] and gpt is about 220B params(?).   Local LLMs can be good for some tasks, but they are much slower and less capable than the size of model and hardware that openai brings to their apis. Even running a 7B model on the CPU in ggml is much slower than the gpt-3-turbo api, in my experience with a 12th gen i7 intel laptop.<p>[1] GPT4 is 8 x 220B params = 1.7T params: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36413296">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36413296</a></div><br/><div id="36626564" class="c"><input type="checkbox" id="c-36626564" checked=""/><div class="controls bullet"><span class="by">Art9681</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36625249">parent</a><span>|</span><a href="#36626575">next</a><span>|</span><label class="collapse" for="c-36626564">[-]</label><label class="expand" for="c-36626564">[3 more]</label></div><br/><div class="children"><div class="content">It&#x27;s been well documented by now that the number of parameters does not necessarily translate to a better model. My guess is that OpenAI has learned a thing or two from the endless papers published daily that your &quot;instance&quot; of the model is not what it seems. They likely have a workflow that picks the best model suitable for your prompt. Some people may get a 13B permutation because it is &quot;good enough&quot; to produce a common answer to a common prompt. Why waste precious compute resources on a prompt that is common? Would it not be feasible to collect the data of the top worldwide prompts and produce a small model that can answer those? Why would OpenAI spend precious compute time on the typical user&#x27;s &quot;write a short story of...&quot;.<p>I would guesstimate that the great majority of prompts are trash. People playing with a toy and amusing themselves. The platform sends those to the trash models.<p>For the other tiny percentage that produces a prompt the size of a paragraph, using the techniques published by OpenAI themselves, they likely get the higher tier models. This is also why I believe many are recently complaining about the quality of the outputs. When your chat history is filled with &quot;have waifu pretend to be my girlfriend&quot; then whatever memory the model is maintaining will be poisoned by the quality of your past prompts.<p>Garbage in, garbage out. I am certain that the #1 priority for OpenAI&#x2F;Microsoft is lowering the cost of each prompt while satisfying the majority.<p>The majority is not in HN.</div><br/><div id="36629056" class="c"><input type="checkbox" id="c-36629056" checked=""/><div class="controls bullet"><span class="by">nethdeco</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36626564">parent</a><span>|</span><a href="#36628742">next</a><span>|</span><label class="collapse" for="c-36629056">[-]</label><label class="expand" for="c-36629056">[1 more]</label></div><br/><div class="children"><div class="content">Picking the  best model based on the prompt seems to be the best way to simplify the task they are doing.</div><br/></div></div><div id="36628742" class="c"><input type="checkbox" id="c-36628742" checked=""/><div class="controls bullet"><span class="by">ranguna</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36626564">parent</a><span>|</span><a href="#36629056">prev</a><span>|</span><a href="#36626575">next</a><span>|</span><label class="collapse" for="c-36628742">[-]</label><label class="expand" for="c-36628742">[1 more]</label></div><br/><div class="children"><div class="content">&gt; It&#x27;s been well documented by now that the number of parameters does not necessarily translate to a better model.<p>That&#x27;s certainly true, but it&#x27;s hard to deny the quality of gpt 4. If the issue is the training data, let&#x27;s just use their training data, it&#x27;s not like they had to close up shop because of using restricted data.<p>I think the issue is more on the financial side, it must have been extremely expensive to train gpt 4. Open source models don&#x27;t have that kind of money right now.<p>I&#x27;ll finance open source models once they are actually good, or show realistic promises of reaching that level of quality on consumer hardware. Until then, open source will open source.<p>I&#x27;ve never bought any kind of subscription or paid api costs to openai, but if gpt 4 finally reached the point where I feel like it&#x27;s a lot better than just good enough, I&#x27;ll happily pay for it (while still being on the lookout for open source models that fit my hardware).</div><br/></div></div></div></div><div id="36626575" class="c"><input type="checkbox" id="c-36626575" checked=""/><div class="controls bullet"><span class="by">PostOnce</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36625249">parent</a><span>|</span><a href="#36626564">prev</a><span>|</span><a href="#36624933">next</a><span>|</span><label class="collapse" for="c-36626575">[-]</label><label class="expand" for="c-36626575">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;mpost.io&#x2F;phi-1-a-compact-language-model-outpaces-gpt-in-efficient-code-generation&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;mpost.io&#x2F;phi-1-a-compact-language-model-outpaces-gpt...</a><p>a 1billion parameter model beats 175billion parameter GPT3.5<p>OpenAI wants us all to drink the kool-aid.</div><br/></div></div></div></div><div id="36624933" class="c"><input type="checkbox" id="c-36624933" checked=""/><div class="controls bullet"><span class="by">andai</span><span>|</span><a href="#36623570">parent</a><span>|</span><a href="#36625249">prev</a><span>|</span><a href="#36628370">next</a><span>|</span><label class="collapse" for="c-36624933">[-]</label><label class="expand" for="c-36624933">[3 more]</label></div><br/><div class="children"><div class="content">Which models are you using and for which tasks? I have found local models largely a waste of time (except for very simple tasks with very heavy prompting). But perhaps there are some recent breakthroughs I haven&#x27;t seen yet.</div><br/><div id="36626409" class="c"><input type="checkbox" id="c-36626409" checked=""/><div class="controls bullet"><span class="by">PostOnce</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36624933">parent</a><span>|</span><a href="#36628370">next</a><span>|</span><label class="collapse" for="c-36626409">[-]</label><label class="expand" for="c-36626409">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m using a variety of 7 and 13B models (and a 3B one for fast feedback loop debugging) at between 8bit and 4_K_M quantizations.<p>Depending on your pre-prompt, your fine-tune (i.e. which model you downloaded), and your specific task, the results can be startlingly good, it&#x27;s crazy that you can do this on a $250 laptop. I stay up nights working on it lately, it&#x27;s so interesting.<p>More importantly, things change by the day. New models, new methods, new software, new interfaces... the possibilities are endless... unless we let OpenAI corrupt our government(s).</div><br/><div id="36628560" class="c"><input type="checkbox" id="c-36628560" checked=""/><div class="controls bullet"><span class="by">redox99</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36626409">parent</a><span>|</span><a href="#36628370">next</a><span>|</span><label class="collapse" for="c-36628560">[-]</label><label class="expand" for="c-36628560">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m surprised you&#x27;re having such a good time with 7B and 13B models. I find anything below 33B to be almost useless. And only 65B is close to GPT 3.5.</div><br/></div></div></div></div></div></div><div id="36628370" class="c"><input type="checkbox" id="c-36628370" checked=""/><div class="controls bullet"><span class="by">concordDance</span><span>|</span><a href="#36623570">parent</a><span>|</span><a href="#36624933">prev</a><span>|</span><a href="#36627983">next</a><span>|</span><label class="collapse" for="c-36628370">[-]</label><label class="expand" for="c-36628370">[2 more]</label></div><br/><div class="children"><div class="content">&gt; OpenAI has no moat, unless you give them money to write legislation.<p>Their moat is that they had access to data sources which have since been clamped down on, eg reddit and twitter apis.</div><br/><div id="36628454" class="c"><input type="checkbox" id="c-36628454" checked=""/><div class="controls bullet"><span class="by">travisjungroth</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36628370">parent</a><span>|</span><a href="#36627983">next</a><span>|</span><label class="collapse" for="c-36628454">[-]</label><label class="expand" for="c-36628454">[1 more]</label></div><br/><div class="children"><div class="content">You can still download Reddit archives with the same data they used.</div><br/></div></div></div></div><div id="36627983" class="c"><input type="checkbox" id="c-36627983" checked=""/><div class="controls bullet"><span class="by">two_in_one</span><span>|</span><a href="#36623570">parent</a><span>|</span><a href="#36628370">prev</a><span>|</span><a href="#36625299">next</a><span>|</span><label class="collapse" for="c-36627983">[-]</label><label class="expand" for="c-36627983">[2 more]</label></div><br/><div class="children"><div class="content">I see no moral problems paying OpenAI for GPT Plus. it helps a lot in development. Their free  speech-to-text &#x27;whisper&#x27; is really good too. I&#x27;m going to use it + small local GPT for voice control.<p>&gt; I can currently run some scary smart and fast LLMs on a 5 year old laptop with no GPU.<p>And, something useful or just playing? I played with local models, and will keep  playing, training, experimenting. It&#x27;s interesting, but not  a solution, not yet.</div><br/><div id="36628253" class="c"><input type="checkbox" id="c-36628253" checked=""/><div class="controls bullet"><span class="by">two_in_one</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36627983">parent</a><span>|</span><a href="#36625299">next</a><span>|</span><label class="collapse" for="c-36628253">[-]</label><label class="expand" for="c-36628253">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ll take downvote as a sign you have nothing to say :) Just one warning, bad karma will be hard to fix.</div><br/></div></div></div></div><div id="36625299" class="c"><input type="checkbox" id="c-36625299" checked=""/><div class="controls bullet"><span class="by">willsmith72</span><span>|</span><a href="#36623570">parent</a><span>|</span><a href="#36627983">prev</a><span>|</span><a href="#36626923">next</a><span>|</span><label class="collapse" for="c-36625299">[-]</label><label class="expand" for="c-36625299">[3 more]</label></div><br/><div class="children"><div class="content">My laptop already works too hard doing development and having chrome open, it&#x27;s just not feasible. A good hosted alternative, sure, but local is not going to scale to the masses.</div><br/><div id="36626397" class="c"><input type="checkbox" id="c-36626397" checked=""/><div class="controls bullet"><span class="by">PostOnce</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36625299">parent</a><span>|</span><a href="#36626923">next</a><span>|</span><label class="collapse" for="c-36626397">[-]</label><label class="expand" for="c-36626397">[2 more]</label></div><br/><div class="children"><div class="content">I have a Dell 7490 (intel 8350u cpu) I paid $250 for and I have no trouble running 13B models through a custom interactive interface I wrote as a hobby project in an afternoon. It can still get a lot better. I made it async the following day and its even more fun.<p>Most of peoples&#x27; problem is watching the AI type, it&#x27;s not instant, but then not all (or even most) applications need to be instant. You can also avoid that by having it return everything at once instead of streaming style.<p>Local absolutely can scale. All kinds of fun things can be done on a machine with 16GB of RAM, or 8GB if you work harder.</div><br/><div id="36628113" class="c"><input type="checkbox" id="c-36628113" checked=""/><div class="controls bullet"><span class="by">DJHenk</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36626397">parent</a><span>|</span><a href="#36626923">next</a><span>|</span><label class="collapse" for="c-36628113">[-]</label><label class="expand" for="c-36628113">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Most of peoples&#x27; problem is watching the AI type, it&#x27;s not instant, but then not all (or even most) applications need to be instant. You can also avoid that by having it return everything at once instead of streaming style.<p>Funny, for me it is the complete opposite. I created an interface in Matrix that does just that: return everything at once. But the lag annoys me more than the slow typing in the regular chat interface. The slow typing helps me keep me focused on the conversation. Without it, my mind starts wandering while it waits.</div><br/></div></div></div></div></div></div><div id="36626923" class="c"><input type="checkbox" id="c-36626923" checked=""/><div class="controls bullet"><span class="by">klysm</span><span>|</span><a href="#36623570">parent</a><span>|</span><a href="#36625299">prev</a><span>|</span><a href="#36624663">next</a><span>|</span><label class="collapse" for="c-36626923">[-]</label><label class="expand" for="c-36626923">[1 more]</label></div><br/><div class="children"><div class="content">Not as good as chatgpt 4 unfortunately, and they do have a moat. You could argue the most will fall in time but I’m not seeing chatgpt4 equivalents at the moment</div><br/></div></div><div id="36624663" class="c"><input type="checkbox" id="c-36624663" checked=""/><div class="controls bullet"><span class="by">musha68k</span><span>|</span><a href="#36623570">parent</a><span>|</span><a href="#36626923">prev</a><span>|</span><a href="#36627424">next</a><span>|</span><label class="collapse" for="c-36624663">[-]</label><label class="expand" for="c-36624663">[4 more]</label></div><br/><div class="children"><div class="content">One has to give them credit for what must be the most grandiose stunt actually landed. And on so many angles! “It just works” - they even got the scientists fully aligned! Fiercely smart industriousness.<p><a href="https:&#x2F;&#x2F;youtu.be&#x2F;P_ACcQxJIsg?t=5946" rel="nofollow noreferrer">https:&#x2F;&#x2F;youtu.be&#x2F;P_ACcQxJIsg?t=5946</a></div><br/><div id="36624729" class="c"><input type="checkbox" id="c-36624729" checked=""/><div class="controls bullet"><span class="by">jonplackett</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36624663">parent</a><span>|</span><a href="#36627424">next</a><span>|</span><label class="collapse" for="c-36624729">[-]</label><label class="expand" for="c-36624729">[3 more]</label></div><br/><div class="children"><div class="content">No equity? For real? He really does need an agent if that&#x27;s the case.</div><br/><div id="36625611" class="c"><input type="checkbox" id="c-36625611" checked=""/><div class="controls bullet"><span class="by">whimsicalism</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36624729">parent</a><span>|</span><a href="#36624857">next</a><span>|</span><label class="collapse" for="c-36625611">[-]</label><label class="expand" for="c-36625611">[1 more]</label></div><br/><div class="children"><div class="content">If you listen to him talk at any point, you can see him explain why.</div><br/></div></div><div id="36624857" class="c"><input type="checkbox" id="c-36624857" checked=""/><div class="controls bullet"><span class="by">darkerside</span><span>|</span><a href="#36623570">root</a><span>|</span><a href="#36624729">parent</a><span>|</span><a href="#36625611">prev</a><span>|</span><a href="#36627424">next</a><span>|</span><label class="collapse" for="c-36624857">[-]</label><label class="expand" for="c-36624857">[1 more]</label></div><br/><div class="children"><div class="content">Wow, under penalty of perjury</div><br/></div></div></div></div></div></div><div id="36627424" class="c"><input type="checkbox" id="c-36627424" checked=""/><div class="controls bullet"><span class="by">marinhero</span><span>|</span><a href="#36623570">parent</a><span>|</span><a href="#36624663">prev</a><span>|</span><a href="#36628677">next</a><span>|</span><label class="collapse" for="c-36627424">[-]</label><label class="expand" for="c-36627424">[1 more]</label></div><br/><div class="children"><div class="content">Make a tutorial?</div><br/></div></div><div id="36628677" class="c"><input type="checkbox" id="c-36628677" checked=""/><div class="controls bullet"><span class="by">lynx23</span><span>|</span><a href="#36623570">parent</a><span>|</span><a href="#36627424">prev</a><span>|</span><a href="#36624314">next</a><span>|</span><label class="collapse" for="c-36628677">[-]</label><label class="expand" for="c-36628677">[1 more]</label></div><br/><div class="children"><div class="content">I tried, and decided it is not worth it.  llama.cpp with a 13B model fit into RAM of my laptop, but pushes CPU temperature to 95 degrees within a few seconds, and mightily sucks the battery dry.  Besides, the results were slow and rather useless.  GPT is the first cloud application I deliberately use to push off computing and energy consumption to an external host which is clearly more capable of handling the request then my local hardware.<p>I sympathize with the idea of wanting to run a local LLM, but IMO, this would require building a desktop with a GPU and plenty of horsepower + silent cooling and put it somewhere in a closet in my apartment.  Running LLMs on my laptop is (to me) clearly a waste of my time and its battery&#x2F;cooling.</div><br/></div></div><div id="36624314" class="c"><input type="checkbox" id="c-36624314" checked=""/><div class="controls bullet"><span class="by">gowld</span><span>|</span><a href="#36623570">parent</a><span>|</span><a href="#36628677">prev</a><span>|</span><a href="#36624659">next</a><span>|</span><label class="collapse" for="c-36624314">[-]</label><label class="expand" for="c-36624314">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s no need to run locally if you aren&#x27;t utilizing 8 hrs&#x2F;day.<p>You can rent time on a hosted GPU, sharing a hosted model with others.</div><br/></div></div><div id="36624659" class="c"><input type="checkbox" id="c-36624659" checked=""/><div class="controls bullet"><span class="by">dimgl</span><span>|</span><a href="#36623570">parent</a><span>|</span><a href="#36624314">prev</a><span>|</span><a href="#36623213">next</a><span>|</span><label class="collapse" for="c-36624659">[-]</label><label class="expand" for="c-36624659">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d love to get into AI and AI development. Where can I start?</div><br/></div></div></div></div><div id="36623213" class="c"><input type="checkbox" id="c-36623213" checked=""/><div class="controls bullet"><span class="by">alpark3</span><span>|</span><a href="#36623570">prev</a><span>|</span><a href="#36621961">next</a><span>|</span><label class="collapse" for="c-36623213">[-]</label><label class="expand" for="c-36623213">[37 more]</label></div><br/><div class="children"><div class="content">&gt;Developers wishing to continue using their fine-tuned models beyond January 4, 2024 will need to fine-tune replacements atop the new base GPT-3 models (ada-002, babbage-002, curie-002, davinci-002), or newer models (gpt-3.5-turbo, gpt-4). Once this feature is available later this year, we will give priority access to GPT-3.5 Turbo and GPT-4 fine-tuning to users who previously fine-tuned older models. We acknowledge that migrating off of models that are fine-tuned on your own data is challenging. We will be providing support to users who previously fine-tuned models to make this transition as smooth as possible.<p>Wait, they&#x27;re not letting you use your own fine-tuned models anymore? So anybody who paid for a fine-tuned model is just forced to repay the training tokens to fine-tune on top of the new censored models? Maybe I&#x27;m misunderstanding it.</div><br/><div id="36625082" class="c"><input type="checkbox" id="c-36625082" checked=""/><div class="controls bullet"><span class="by">npew</span><span>|</span><a href="#36623213">parent</a><span>|</span><a href="#36623932">next</a><span>|</span><label class="collapse" for="c-36625082">[-]</label><label class="expand" for="c-36625082">[6 more]</label></div><br/><div class="children"><div class="content">(I work at OpenAI) We&#x27;re planning to cover the cost for fine-tuning replacement models. We&#x27;re still working through the exact mechanics that will work best for customers, and will be reaching out to customers to get feedback on different approaches in the next few weeks.</div><br/><div id="36627084" class="c"><input type="checkbox" id="c-36627084" checked=""/><div class="controls bullet"><span class="by">extr</span><span>|</span><a href="#36623213">root</a><span>|</span><a href="#36625082">parent</a><span>|</span><a href="#36626609">next</a><span>|</span><label class="collapse" for="c-36627084">[-]</label><label class="expand" for="c-36627084">[1 more]</label></div><br/><div class="children"><div class="content">Please fix the phone verification system. I created two personal accounts a long time ago with the same phone number, and now I can&#x27;t create a work account with the same number, even if I delete one of them. Being able to change the email associated with an account would also work. This is causing issues with adoption in my workplace.</div><br/></div></div><div id="36626609" class="c"><input type="checkbox" id="c-36626609" checked=""/><div class="controls bullet"><span class="by">ShadowBanThis01</span><span>|</span><a href="#36623213">root</a><span>|</span><a href="#36625082">parent</a><span>|</span><a href="#36627084">prev</a><span>|</span><a href="#36623932">next</a><span>|</span><label class="collapse" for="c-36626609">[-]</label><label class="expand" for="c-36626609">[4 more]</label></div><br/><div class="children"><div class="content">Why does OpenAI demand your phone number, and a particular KIND of phone number at that? For example they won&#x27;t accept VOIP numbers. I&#x27;m not about to give them my real phone number.<p>It&#x27;s a deal-breaker for many.</div><br/><div id="36626635" class="c"><input type="checkbox" id="c-36626635" checked=""/><div class="controls bullet"><span class="by">jeromegv</span><span>|</span><a href="#36623213">root</a><span>|</span><a href="#36626609">parent</a><span>|</span><a href="#36627221">next</a><span>|</span><label class="collapse" for="c-36626635">[-]</label><label class="expand" for="c-36626635">[1 more]</label></div><br/><div class="children"><div class="content">Seems clear that it’s for bots. And they refuse voip numbers because it’s a hell of a lot easier to buy and generate hundreds of voip numbers.</div><br/></div></div><div id="36627221" class="c"><input type="checkbox" id="c-36627221" checked=""/><div class="controls bullet"><span class="by">trolan</span><span>|</span><a href="#36623213">root</a><span>|</span><a href="#36626609">parent</a><span>|</span><a href="#36626635">prev</a><span>|</span><a href="#36623932">next</a><span>|</span><label class="collapse" for="c-36627221">[-]</label><label class="expand" for="c-36627221">[2 more]</label></div><br/><div class="children"><div class="content">I signed up under my .edu to use the $18 credit for a school project and the phone # was all it took to know I was the same person.</div><br/><div id="36627228" class="c"><input type="checkbox" id="c-36627228" checked=""/><div class="controls bullet"><span class="by">ShadowBanThis01</span><span>|</span><a href="#36623213">root</a><span>|</span><a href="#36627221">parent</a><span>|</span><a href="#36623932">next</a><span>|</span><label class="collapse" for="c-36627228">[-]</label><label class="expand" for="c-36627228">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s fine. The question stands though.</div><br/></div></div></div></div></div></div></div></div><div id="36623932" class="c"><input type="checkbox" id="c-36623932" checked=""/><div class="controls bullet"><span class="by">meghan_rain</span><span>|</span><a href="#36623213">parent</a><span>|</span><a href="#36625082">prev</a><span>|</span><a href="#36624518">next</a><span>|</span><label class="collapse" for="c-36623932">[-]</label><label class="expand" for="c-36623932">[3 more]</label></div><br/><div class="children"><div class="content">not your weights, not your bitcoins</div><br/><div id="36624801" class="c"><input type="checkbox" id="c-36624801" checked=""/><div class="controls bullet"><span class="by">viksit</span><span>|</span><a href="#36623213">root</a><span>|</span><a href="#36623932">parent</a><span>|</span><a href="#36624518">next</a><span>|</span><label class="collapse" for="c-36624801">[-]</label><label class="expand" for="c-36624801">[2 more]</label></div><br/><div class="children"><div class="content">now its 18. iykyk</div><br/><div id="36625866" class="c"><input type="checkbox" id="c-36625866" checked=""/><div class="controls bullet"><span class="by">oars</span><span>|</span><a href="#36623213">root</a><span>|</span><a href="#36624801">parent</a><span>|</span><a href="#36624518">next</a><span>|</span><label class="collapse" for="c-36625866">[-]</label><label class="expand" for="c-36625866">[1 more]</label></div><br/><div class="children"><div class="content">care to explain?</div><br/></div></div></div></div></div></div><div id="36624518" class="c"><input type="checkbox" id="c-36624518" checked=""/><div class="controls bullet"><span class="by">londons_explore</span><span>|</span><a href="#36623213">parent</a><span>|</span><a href="#36623932">prev</a><span>|</span><a href="#36623325">next</a><span>|</span><label class="collapse" for="c-36624518">[-]</label><label class="expand" for="c-36624518">[6 more]</label></div><br/><div class="children"><div class="content">This tells me that either there were very few commercial users of finetuned models, or they need to decommission the infrastructure to free up GPU&#x27;s for more valuable projects.</div><br/><div id="36624559" class="c"><input type="checkbox" id="c-36624559" checked=""/><div class="controls bullet"><span class="by">eli</span><span>|</span><a href="#36623213">root</a><span>|</span><a href="#36624518">parent</a><span>|</span><a href="#36625891">next</a><span>|</span><label class="collapse" for="c-36624559">[-]</label><label class="expand" for="c-36624559">[4 more]</label></div><br/><div class="children"><div class="content">The former seems very believable. And I bet a lot of the fine tuned models that are active are still part of prototypes or experiments.<p>I assume if you reach out they throw some credits at you</div><br/><div id="36624627" class="c"><input type="checkbox" id="c-36624627" checked=""/><div class="controls bullet"><span class="by">londons_explore</span><span>|</span><a href="#36623213">root</a><span>|</span><a href="#36624559">parent</a><span>|</span><a href="#36625891">next</a><span>|</span><label class="collapse" for="c-36624627">[-]</label><label class="expand" for="c-36624627">[3 more]</label></div><br/><div class="children"><div class="content">If it really was a tiny number of users, they would publically make a really good offer - for example:  &quot;Unfortunately, you will need to retune your models on top of GPT-4.   OpenAI will do this for you for free, <i>and</i> refund all money you paid tuning your original model, <i>and</i> offer the new model for the same price as the original model.&quot;<p>The extra trust gained by seeing another customer treated that way easily pays for a few credits for a small number of users.</div><br/><div id="36624732" class="c"><input type="checkbox" id="c-36624732" checked=""/><div class="controls bullet"><span class="by">lolinder</span><span>|</span><a href="#36623213">root</a><span>|</span><a href="#36624627">parent</a><span>|</span><a href="#36625891">next</a><span>|</span><label class="collapse" for="c-36624732">[-]</label><label class="expand" for="c-36624732">[2 more]</label></div><br/><div class="children"><div class="content">OpenAI probably doesn&#x27;t feel the need to pay to win publicity right now—they&#x27;ve been in the spotlight for as long as LLMs have been a thing, and GPT-4 is far ahead of competitors&#x27; offerings.</div><br/><div id="36625955" class="c"><input type="checkbox" id="c-36625955" checked=""/><div class="controls bullet"><span class="by">nimithryn</span><span>|</span><a href="#36623213">root</a><span>|</span><a href="#36624732">parent</a><span>|</span><a href="#36625891">next</a><span>|</span><label class="collapse" for="c-36625955">[-]</label><label class="expand" for="c-36625955">[1 more]</label></div><br/><div class="children"><div class="content">It’s about trust - not publicity. Trust is hard to earn back once broken, and there will be multiple offerings eventually.<p>For example, AWS was one of the first cloud providers. Now there are alternatives, but I still pick AWS because I trust them not to break my dependencies way more than, say, Google</div><br/></div></div></div></div></div></div></div></div><div id="36625891" class="c"><input type="checkbox" id="c-36625891" checked=""/><div class="controls bullet"><span class="by">kristjansson</span><span>|</span><a href="#36623213">root</a><span>|</span><a href="#36624518">parent</a><span>|</span><a href="#36624559">prev</a><span>|</span><a href="#36623325">next</a><span>|</span><label class="collapse" for="c-36625891">[-]</label><label class="expand" for="c-36625891">[1 more]</label></div><br/><div class="children"><div class="content">There’s also the possibility that they weren’t seeing lots of ongoing usage of existing fine tuned models e.g. users tuning, running some batch of inputs, then abandoning the fine tuned weights.</div><br/></div></div></div></div><div id="36623325" class="c"><input type="checkbox" id="c-36623325" checked=""/><div class="controls bullet"><span class="by">fnordpiglet</span><span>|</span><a href="#36623213">parent</a><span>|</span><a href="#36624518">prev</a><span>|</span><a href="#36623359">next</a><span>|</span><label class="collapse" for="c-36623325">[-]</label><label class="expand" for="c-36623325">[14 more]</label></div><br/><div class="children"><div class="content">If you don’t own the weights you don’t own anything. This is why open models are so crucial. I don’t understand any business who is building fine tuned models against closed models.</div><br/><div id="36623363" class="c"><input type="checkbox" id="c-36623363" checked=""/><div class="controls bullet"><span class="by">reaperman</span><span>|</span><a href="#36623213">root</a><span>|</span><a href="#36623325">parent</a><span>|</span><a href="#36624225">next</a><span>|</span><label class="collapse" for="c-36623363">[-]</label><label class="expand" for="c-36623363">[7 more]</label></div><br/><div class="children"><div class="content">Right now the closed models are incredibly higher quality than the open models. They&#x27;re useful as a stopgap for 1-2 years in hopes&#x2F;expectation of open models reaching a point where they can be swapped in. It burns cash now, but in exchange you can grab more market share sooner while you&#x27;re stuck using the expensive but high quality OpenAI models.<p>It&#x27;s not cost-effective, but it may be part of a valid business plan.</div><br/><div id="36623405" class="c"><input type="checkbox" id="c-36623405" checked=""/><div class="controls bullet"><span class="by">ronsor</span><span>|</span><a href="#36623213">root</a><span>|</span><a href="#36623363">parent</a><span>|</span><a href="#36623586">next</a><span>|</span><label class="collapse" for="c-36623405">[-]</label><label class="expand" for="c-36623405">[5 more]</label></div><br/><div class="children"><div class="content">If you&#x27;re finetuning your own model, the closed models being &quot;incredibly higher quality&quot; is probably less relevant.</div><br/><div id="36623482" class="c"><input type="checkbox" id="c-36623482" checked=""/><div class="controls bullet"><span class="by">claytonjy</span><span>|</span><a href="#36623213">root</a><span>|</span><a href="#36623405">parent</a><span>|</span><a href="#36624327">next</a><span>|</span><label class="collapse" for="c-36623482">[-]</label><label class="expand" for="c-36623482">[3 more]</label></div><br/><div class="children"><div class="content">That&#x27;s how we all want it to work, but the reality today is that GPT-4 is better at almost anything than a fine-tuned version of any other model.<p>It&#x27;s somewhat rare to have a task and good enough dataset that you can finetune something else to be close enough in quality to GPT-4 for your task.</div><br/><div id="36627633" class="c"><input type="checkbox" id="c-36627633" checked=""/><div class="controls bullet"><span class="by">zirgs</span><span>|</span><a href="#36623213">root</a><span>|</span><a href="#36623482">parent</a><span>|</span><a href="#36624327">next</a><span>|</span><label class="collapse" for="c-36627633">[-]</label><label class="expand" for="c-36627633">[2 more]</label></div><br/><div class="children"><div class="content">GPT-4 is still heavily censored and will simply refuse to talk about many &quot;problematic&quot; things. How is that better than a completely uncensored model?</div><br/><div id="36628307" class="c"><input type="checkbox" id="c-36628307" checked=""/><div class="controls bullet"><span class="by">Veen</span><span>|</span><a href="#36623213">root</a><span>|</span><a href="#36627633">parent</a><span>|</span><a href="#36624327">next</a><span>|</span><label class="collapse" for="c-36628307">[-]</label><label class="expand" for="c-36628307">[1 more]</label></div><br/><div class="children"><div class="content">Depends what you’re using it for. For many use cases, the censorship is irrelevant.</div><br/></div></div></div></div></div></div><div id="36624327" class="c"><input type="checkbox" id="c-36624327" checked=""/><div class="controls bullet"><span class="by">wongarsu</span><span>|</span><a href="#36623213">root</a><span>|</span><a href="#36623405">parent</a><span>|</span><a href="#36623482">prev</a><span>|</span><a href="#36623586">next</a><span>|</span><label class="collapse" for="c-36624327">[-]</label><label class="expand" for="c-36624327">[1 more]</label></div><br/><div class="children"><div class="content">Finetuning a better model still yields better results than finetuning a worse model.</div><br/></div></div></div></div><div id="36623586" class="c"><input type="checkbox" id="c-36623586" checked=""/><div class="controls bullet"><span class="by">fnordpiglet</span><span>|</span><a href="#36623213">root</a><span>|</span><a href="#36623363">parent</a><span>|</span><a href="#36623405">prev</a><span>|</span><a href="#36624225">next</a><span>|</span><label class="collapse" for="c-36623586">[-]</label><label class="expand" for="c-36623586">[1 more]</label></div><br/><div class="children"><div class="content">That should be a wake up call to every corporation pinning their business on OAI models.  My experience thus far is no one is seeing a need to plan an exit from OAI, and the perception is “AI is magic and we aren’t magicians.”  There needs to be a concerted effort to finance and tune high quality freely available models and tool chains asap.<p>That said I think efficiencies will dramatically improve over the next few years and over investing now probably captures very little value beyond building internal <i>competency</i> - which doesn’t grow with anything but time and practice. The longer you depend on OAI, the longer you will depend on OAI past your point of profound regret.</div><br/></div></div></div></div><div id="36624225" class="c"><input type="checkbox" id="c-36624225" checked=""/><div class="controls bullet"><span class="by">r3trohack3r</span><span>|</span><a href="#36623213">root</a><span>|</span><a href="#36623325">parent</a><span>|</span><a href="#36623363">prev</a><span>|</span><a href="#36624925">next</a><span>|</span><label class="collapse" for="c-36624225">[-]</label><label class="expand" for="c-36624225">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I don’t understand any business who is building fine tuned models against closed models<p>Do you have any recommendations for good open models that businesses could use today?<p>From what I&#x27;ve seen in the space, I suspect businesses are building fine tuned models against closed models because those are the only viable models to build a business model on top of. The quality of open models isn&#x27;t competitive.</div><br/><div id="36626308" class="c"><input type="checkbox" id="c-36626308" checked=""/><div class="controls bullet"><span class="by">cj</span><span>|</span><a href="#36623213">root</a><span>|</span><a href="#36624225">parent</a><span>|</span><a href="#36624925">next</a><span>|</span><label class="collapse" for="c-36626308">[-]</label><label class="expand" for="c-36626308">[1 more]</label></div><br/><div class="children"><div class="content">PSA:  anyone working at a company with $50k+ of spend with AWS, reach out to your rep expressing interest in AI. You’ll be on a call with 6 solution architects and AI specialists in a matter of days. They’re incredibly knowledgeable and freely recommend non-AWS alternatives when the use case calls for it.</div><br/></div></div></div></div><div id="36624925" class="c"><input type="checkbox" id="c-36624925" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#36623213">root</a><span>|</span><a href="#36623325">parent</a><span>|</span><a href="#36624225">prev</a><span>|</span><a href="#36626293">next</a><span>|</span><label class="collapse" for="c-36624925">[-]</label><label class="expand" for="c-36624925">[2 more]</label></div><br/><div class="children"><div class="content">Owning weights is in a nebulous space right now, but if you don’t have custody of the weights and code to use them, you have nothing reliable, independent of whether the things you might wish to have are ownable (ownership is more about exclusion than ability to use, in any case.)</div><br/><div id="36625342" class="c"><input type="checkbox" id="c-36625342" checked=""/><div class="controls bullet"><span class="by">fnordpiglet</span><span>|</span><a href="#36623213">root</a><span>|</span><a href="#36624925">parent</a><span>|</span><a href="#36626293">next</a><span>|</span><label class="collapse" for="c-36625342">[-]</label><label class="expand" for="c-36625342">[1 more]</label></div><br/><div class="children"><div class="content">Yes. But the weights and instructions of how to use them to code can follow as we’ve seen. The key is ownership is bits on your machine not someone else’s. Better still on BitTorrent &#x2F; ipfs:-)</div><br/></div></div></div></div><div id="36626293" class="c"><input type="checkbox" id="c-36626293" checked=""/><div class="controls bullet"><span class="by">benlivengood</span><span>|</span><a href="#36623213">root</a><span>|</span><a href="#36623325">parent</a><span>|</span><a href="#36624925">prev</a><span>|</span><a href="#36623975">next</a><span>|</span><label class="collapse" for="c-36626293">[-]</label><label class="expand" for="c-36626293">[1 more]</label></div><br/><div class="children"><div class="content">My guess is that these businesses are also running inference on someone else&#x27;s GPUs&#x2F;TPUs so there isn&#x27;t an existential advantage to owning the weights.</div><br/></div></div><div id="36623975" class="c"><input type="checkbox" id="c-36623975" checked=""/><div class="controls bullet"><span class="by">yieldcrv</span><span>|</span><a href="#36623213">root</a><span>|</span><a href="#36623325">parent</a><span>|</span><a href="#36626293">prev</a><span>|</span><a href="#36623359">next</a><span>|</span><label class="collapse" for="c-36623975">[-]</label><label class="expand" for="c-36623975">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I don’t understand any business who is building fine tuned models against closed models.<p>Just sell access at a higher price than you get it<p>Either directly, on <i>on average</i> based on your user stories</div><br/></div></div></div></div><div id="36623359" class="c"><input type="checkbox" id="c-36623359" checked=""/><div class="controls bullet"><span class="by">flangola7</span><span>|</span><a href="#36623213">parent</a><span>|</span><a href="#36623325">prev</a><span>|</span><a href="#36625024">next</a><span>|</span><label class="collapse" for="c-36623359">[-]</label><label class="expand" for="c-36623359">[4 more]</label></div><br/><div class="children"><div class="content">They address that, OpenAI will cover the cost of re-training on the new models, and the old models don&#x27;t discontinue until next year.</div><br/><div id="36623617" class="c"><input type="checkbox" id="c-36623617" checked=""/><div class="controls bullet"><span class="by">simonw</span><span>|</span><a href="#36623213">root</a><span>|</span><a href="#36623359">parent</a><span>|</span><a href="#36625024">next</a><span>|</span><label class="collapse" for="c-36623617">[-]</label><label class="expand" for="c-36623617">[3 more]</label></div><br/><div class="children"><div class="content">Did they say they would cover the cost of fine-tuning again? I saw them say they would cover the cost of recalculating embeddings, but I didn&#x27;t see the bit about fine-tuning costs.<p>On fine-tuning:<p>&gt; We will be providing support to users who previously fine-tuned models to make this transition as smooth as possible.<p>On embeddings:<p>&gt; We will cover the financial cost of users re-embedding content with these new models.</div><br/><div id="36624673" class="c"><input type="checkbox" id="c-36624673" checked=""/><div class="controls bullet"><span class="by">btown</span><span>|</span><a href="#36623213">root</a><span>|</span><a href="#36623617">parent</a><span>|</span><a href="#36624070">next</a><span>|</span><label class="collapse" for="c-36624673">[-]</label><label class="expand" for="c-36624673">[1 more]</label></div><br/><div class="children"><div class="content">This indicates to me that some of the old base models used architectures that were significantly more difficult to run at scale (or to ship around&#x2F;load different weights at scale) - which is truly saying something, since they were running at incredible scale a year ago. There&#x27;s probably a decade of potential papers from their optimizations alone (to say nothing of their devops innovations) that are still trade secrets.</div><br/></div></div><div id="36624070" class="c"><input type="checkbox" id="c-36624070" checked=""/><div class="controls bullet"><span class="by">BoorishBears</span><span>|</span><a href="#36623213">root</a><span>|</span><a href="#36623617">parent</a><span>|</span><a href="#36624673">prev</a><span>|</span><a href="#36625024">next</a><span>|</span><label class="collapse" for="c-36624070">[-]</label><label class="expand" for="c-36624070">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s because fine-tuning the new models isn&#x27;t available yet.<p>Based on the language it sounds like they&#x27;ll do the same when that launches.</div><br/></div></div></div></div></div></div><div id="36625024" class="c"><input type="checkbox" id="c-36625024" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#36623213">parent</a><span>|</span><a href="#36623359">prev</a><span>|</span><a href="#36621961">next</a><span>|</span><label class="collapse" for="c-36625024">[-]</label><label class="expand" for="c-36625024">[3 more]</label></div><br/><div class="children"><div class="content">&quot;Censored&quot; is a funny term, because I&#x27;ve tried doing uncensored things on uncensored models, and they&#x27;re much worse at it than GPT-3.5 in the API playground. Nothing&#x27;s as censored as just being unable to do the task in the first place.</div><br/><div id="36628223" class="c"><input type="checkbox" id="c-36628223" checked=""/><div class="controls bullet"><span class="by">yard2010</span><span>|</span><a href="#36623213">root</a><span>|</span><a href="#36625024">parent</a><span>|</span><a href="#36621961">next</a><span>|</span><label class="collapse" for="c-36628223">[-]</label><label class="expand" for="c-36628223">[2 more]</label></div><br/><div class="children"><div class="content">Keep in mind though, some of the generated text is against their guidelines, you will see a warning when you get there and be told it&#x27;s &quot;flagged&quot; and you should use the moderation API. The chat API is nerfed to oblivion, good luke making it generate non PC text</div><br/><div id="36628339" class="c"><input type="checkbox" id="c-36628339" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#36623213">root</a><span>|</span><a href="#36628223">parent</a><span>|</span><a href="#36621961">next</a><span>|</span><label class="collapse" for="c-36628339">[-]</label><label class="expand" for="c-36628339">[1 more]</label></div><br/><div class="children"><div class="content">That just means you don&#x27;t have enough fetishes.</div><br/></div></div></div></div></div></div></div></div><div id="36621961" class="c"><input type="checkbox" id="c-36621961" checked=""/><div class="controls bullet"><span class="by">superkuh</span><span>|</span><a href="#36623213">prev</a><span>|</span><a href="#36629045">next</a><span>|</span><label class="collapse" for="c-36621961">[-]</label><label class="expand" for="c-36621961">[28 more]</label></div><br/><div class="children"><div class="content">Yikes. They&#x27;re actually killing off text-davinci-003. RIP to the most capable remaining model and RIP to all text completion style freedom. Now it&#x27;s censored&#x2F;aligned chat or instruct models with arbitrary input metaphor limits for everything. gpt3.5-turbo is terrible in comparison.<p>This will end my usage of openai for most things. I doubt my $5-$10 API payments per month will matter. This just lights more of a fire under me to get the 65B llama models working locally.</div><br/><div id="36623430" class="c"><input type="checkbox" id="c-36623430" checked=""/><div class="controls bullet"><span class="by">bravura</span><span>|</span><a href="#36621961">parent</a><span>|</span><a href="#36625254">next</a><span>|</span><label class="collapse" for="c-36623430">[-]</label><label class="expand" for="c-36623430">[10 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve never used text-davinci-003 much. Why do you like it so much? What does it offer that the other models don&#x27;t?<p>What are funs things we can with it until it sunsets on January 4, 2024?</div><br/><div id="36623808" class="c"><input type="checkbox" id="c-36623808" checked=""/><div class="controls bullet"><span class="by">thomasfromcdnjs</span><span>|</span><a href="#36621961">root</a><span>|</span><a href="#36623430">parent</a><span>|</span><a href="#36627846">next</a><span>|</span><label class="collapse" for="c-36623808">[-]</label><label class="expand" for="c-36623808">[8 more]</label></div><br/><div class="children"><div class="content">The Chat-GPT models are all pre-prompted and pre-aligned. If you work with davinci-003, it will never say things like, &quot;I am an OpenAI bot and am unable to work with your unethical request&quot;<p>When using davinci the onus is on you to construct prompts (memories) which is fun and powerful.<p>====<p>97% of API usage might be because of ChatGPT&#x27;s general appeal to the world. But I think they will be losing a part of the hacker&#x2F;builder ethos if they drop things like davinci-003, which might suck for them in the long run. Consumers over developers.</div><br/><div id="36629076" class="c"><input type="checkbox" id="c-36629076" checked=""/><div class="controls bullet"><span class="by">gwd</span><span>|</span><a href="#36621961">root</a><span>|</span><a href="#36623808">parent</a><span>|</span><a href="#36624051">next</a><span>|</span><label class="collapse" for="c-36629076">[-]</label><label class="expand" for="c-36629076">[1 more]</label></div><br/><div class="children"><div class="content">If you&#x27;re using the API, you construct the &quot;memories&quot; as well, including the &quot;system&quot; prompt, even in the playground.  (When you click the &quot;(+) Add message&quot;, the new one defaults to USER, but you can click on it to change it to ASSISTANT, then fill it in with whatever you want.)<p>I used the &quot;Complete&quot; UI (from the Playground) for a bit before the &quot;Chat&quot; interface was available; I don&#x27;t really think there&#x27;s anything you couldn&#x27;t do in the &quot;Complete&quot; UI that you couldn&#x27;t also do in the &quot;Chat&quot; UI.</div><br/></div></div><div id="36624051" class="c"><input type="checkbox" id="c-36624051" checked=""/><div class="controls bullet"><span class="by">Fyrezerk</span><span>|</span><a href="#36621961">root</a><span>|</span><a href="#36623808">parent</a><span>|</span><a href="#36629076">prev</a><span>|</span><a href="#36627846">next</a><span>|</span><label class="collapse" for="c-36624051">[-]</label><label class="expand" for="c-36624051">[6 more]</label></div><br/><div class="children"><div class="content">The hacker&#x2F;builder ethos doesn&#x27;t matter in the grand scheme of commercialization.</div><br/><div id="36624169" class="c"><input type="checkbox" id="c-36624169" checked=""/><div class="controls bullet"><span class="by">Robotbeat</span><span>|</span><a href="#36621961">root</a><span>|</span><a href="#36624051">parent</a><span>|</span><a href="#36625265">next</a><span>|</span><label class="collapse" for="c-36624169">[-]</label><label class="expand" for="c-36624169">[1 more]</label></div><br/><div class="children"><div class="content">It matters immensely in the early days and is the basis for all growth that follows. So cutting it off early cuts off future growth.</div><br/></div></div><div id="36625265" class="c"><input type="checkbox" id="c-36625265" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#36621961">root</a><span>|</span><a href="#36624051">parent</a><span>|</span><a href="#36624169">prev</a><span>|</span><a href="#36625450">next</a><span>|</span><label class="collapse" for="c-36625265">[-]</label><label class="expand" for="c-36625265">[1 more]</label></div><br/><div class="children"><div class="content">Sure - not like most of the infrastructure of pretty much everything online is built on top of projects originating in that space or anything.</div><br/></div></div><div id="36625450" class="c"><input type="checkbox" id="c-36625450" checked=""/><div class="controls bullet"><span class="by">RugnirViking</span><span>|</span><a href="#36621961">root</a><span>|</span><a href="#36624051">parent</a><span>|</span><a href="#36625265">prev</a><span>|</span><a href="#36627846">next</a><span>|</span><label class="collapse" for="c-36625450">[-]</label><label class="expand" for="c-36625450">[3 more]</label></div><br/><div class="children"><div class="content">How do they want to commercialise it? Do they want moms to tinker on ChatGPT once a month to do their children&#x27;s homework? Or do they want people to build businesses using their software</div><br/><div id="36625734" class="c"><input type="checkbox" id="c-36625734" checked=""/><div class="controls bullet"><span class="by">cyanydeez</span><span>|</span><a href="#36621961">root</a><span>|</span><a href="#36625450">parent</a><span>|</span><a href="#36627846">next</a><span>|</span><label class="collapse" for="c-36625734">[-]</label><label class="expand" for="c-36625734">[2 more]</label></div><br/><div class="children"><div class="content">Mom and Pop offer more users with less legal exposure.</div><br/><div id="36628337" class="c"><input type="checkbox" id="c-36628337" checked=""/><div class="controls bullet"><span class="by">RugnirViking</span><span>|</span><a href="#36621961">root</a><span>|</span><a href="#36625734">parent</a><span>|</span><a href="#36627846">next</a><span>|</span><label class="collapse" for="c-36628337">[-]</label><label class="expand" for="c-36628337">[1 more]</label></div><br/><div class="children"><div class="content">do they have the cash money dollar? and the willingness to spend it on what is essentially a toy they will quickly grow bored of? I don&#x27;t think this is the best path to profitability</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36627846" class="c"><input type="checkbox" id="c-36627846" checked=""/><div class="controls bullet"><span class="by">robga</span><span>|</span><a href="#36621961">root</a><span>|</span><a href="#36623430">parent</a><span>|</span><a href="#36623808">prev</a><span>|</span><a href="#36625254">next</a><span>|</span><label class="collapse" for="c-36627846">[-]</label><label class="expand" for="c-36627846">[1 more]</label></div><br/><div class="children"><div class="content">Note that the Azure endpoint is not being sunsetted until July 5th, 2024.<p>One supposes openai has a 6 month notice period vs a 12 month period for azure. This might generally effect one’s appetite in choosing which endpoint to use for any model.</div><br/></div></div></div></div><div id="36625254" class="c"><input type="checkbox" id="c-36625254" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#36621961">parent</a><span>|</span><a href="#36623430">prev</a><span>|</span><a href="#36628782">next</a><span>|</span><label class="collapse" for="c-36625254">[-]</label><label class="expand" for="c-36625254">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, TextCompletion is <i>much</i> better than ChatCompletion with v3 models.<p>But with davinci at the same price point as GPT-4 I&#x27;m hoping the latter is enough of a step up in its variety of vocabulary and nudgeable sophistication of language to be a drop in replacement.<p>Though in general I think there&#x27;s an under appreciation for just how much is being lost in the trend towards instruct models, and hope there will be smart actors in the market who use a pre-optimization step for instruct prompts that formats it for untuned models. I&#x27;d imagine that parameter size to parameter size that approach will look much more advanced to end users just by not lobotomizing the underlying model.</div><br/></div></div><div id="36628782" class="c"><input type="checkbox" id="c-36628782" checked=""/><div class="controls bullet"><span class="by">xiphias2</span><span>|</span><a href="#36621961">parent</a><span>|</span><a href="#36625254">prev</a><span>|</span><a href="#36627042">next</a><span>|</span><label class="collapse" for="c-36628782">[-]</label><label class="expand" for="c-36628782">[1 more]</label></div><br/><div class="children"><div class="content">It won&#x27;t matter at all at the end of the year, open source llm&#x27;s will surpass it by that time.</div><br/></div></div><div id="36627042" class="c"><input type="checkbox" id="c-36627042" checked=""/><div class="controls bullet"><span class="by">gexla</span><span>|</span><a href="#36621961">parent</a><span>|</span><a href="#36628782">prev</a><span>|</span><a href="#36623069">next</a><span>|</span><label class="collapse" for="c-36627042">[-]</label><label class="expand" for="c-36627042">[1 more]</label></div><br/><div class="children"><div class="content">My guess is that they would be fine with continuing to serve all models, but that hardware constraints are forcing difficult decisions. SA has already said that hardware is holding them back from what they want to do. I was on a waiting list for the GPT4 API for like a few months, which I guess is because they couldn&#x27;t keep up with demand.</div><br/></div></div><div id="36623069" class="c"><input type="checkbox" id="c-36623069" checked=""/><div class="controls bullet"><span class="by">H8crilA</span><span>|</span><a href="#36621961">parent</a><span>|</span><a href="#36627042">prev</a><span>|</span><a href="#36624167">next</a><span>|</span><label class="collapse" for="c-36623069">[-]</label><label class="expand" for="c-36623069">[4 more]</label></div><br/><div class="children"><div class="content">The $5-$10 is probably the reason why they&#x27;re killing those endpoints.</div><br/><div id="36623139" class="c"><input type="checkbox" id="c-36623139" checked=""/><div class="controls bullet"><span class="by">superkuh</span><span>|</span><a href="#36621961">root</a><span>|</span><a href="#36623069">parent</a><span>|</span><a href="#36624167">next</a><span>|</span><label class="collapse" for="c-36623139">[-]</label><label class="expand" for="c-36623139">[3 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t get it? text-davinci-003 is the most expensive model per token. It&#x27;s just that running IRC bots isn&#x27;t exactly high volume.</div><br/><div id="36623610" class="c"><input type="checkbox" id="c-36623610" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#36621961">root</a><span>|</span><a href="#36623139">parent</a><span>|</span><a href="#36624167">next</a><span>|</span><label class="collapse" for="c-36623610">[-]</label><label class="expand" for="c-36623610">[2 more]</label></div><br/><div class="children"><div class="content">&quot;Most expensive&quot; doesn&#x27;t mean &quot;highest margin&quot;, though.</div><br/><div id="36627456" class="c"><input type="checkbox" id="c-36627456" checked=""/><div class="controls bullet"><span class="by">H8crilA</span><span>|</span><a href="#36621961">root</a><span>|</span><a href="#36623610">parent</a><span>|</span><a href="#36624167">next</a><span>|</span><label class="collapse" for="c-36627456">[-]</label><label class="expand" for="c-36627456">[1 more]</label></div><br/><div class="children"><div class="content">I meant that it probably isn&#x27;t high revenue.</div><br/></div></div></div></div></div></div></div></div><div id="36624167" class="c"><input type="checkbox" id="c-36624167" checked=""/><div class="controls bullet"><span class="by">Karrot_Kream</span><span>|</span><a href="#36621961">parent</a><span>|</span><a href="#36623069">prev</a><span>|</span><a href="#36627687">next</a><span>|</span><label class="collapse" for="c-36624167">[-]</label><label class="expand" for="c-36624167">[2 more]</label></div><br/><div class="children"><div class="content">I wonder if there&#x27;s some element of face-saving here to avoid a lawsuit that may come from someone that uses the model to perform negative actions. In general I&#x27;ve found that gpt3.5-turbo is better than text-davinci-003 in most cases, but I agree, it&#x27;s quite sad that they&#x27;re getting rid of the unaligned&#x2F;censored model.</div><br/><div id="36627053" class="c"><input type="checkbox" id="c-36627053" checked=""/><div class="controls bullet"><span class="by">gexla</span><span>|</span><a href="#36621961">root</a><span>|</span><a href="#36624167">parent</a><span>|</span><a href="#36627687">next</a><span>|</span><label class="collapse" for="c-36627053">[-]</label><label class="expand" for="c-36627053">[1 more]</label></div><br/><div class="children"><div class="content">More likely hardware constraints. They can&#x27;t get the hardware fast enough to do everything they want to do. So, they free up resources by ditching lower demand models.</div><br/></div></div></div></div><div id="36627687" class="c"><input type="checkbox" id="c-36627687" checked=""/><div class="controls bullet"><span class="by">LeoPanthera</span><span>|</span><a href="#36621961">parent</a><span>|</span><a href="#36624167">prev</a><span>|</span><a href="#36623801">next</a><span>|</span><label class="collapse" for="c-36627687">[-]</label><label class="expand" for="c-36627687">[3 more]</label></div><br/><div class="children"><div class="content">Everyone who complains about being &quot;censored&quot; <i>never</i> gives examples.</div><br/><div id="36628314" class="c"><input type="checkbox" id="c-36628314" checked=""/><div class="controls bullet"><span class="by">yard2010</span><span>|</span><a href="#36621961">root</a><span>|</span><a href="#36627687">parent</a><span>|</span><a href="#36628335">next</a><span>|</span><label class="collapse" for="c-36628314">[-]</label><label class="expand" for="c-36628314">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m trying to create a bot that joins my friends Telegram group and melds into the conversation as if it was a real person. A real person might be the most cute and fun enthusiastic person there is but sometimes it has bad days, or it tells inappropriate jokes, right? People are complicated. Not this bot! No matter what prompt I&#x27;m using (with the chat API) it won&#x27;t lose the happy happy joy joy chatGPT attitude, won&#x27;t tell inappropriate jokes, won&#x27;t give advice on certain topics and in general won&#x27;t talk like a real person, not because of technological limitations.. You can feel it when it&#x27;s just nerfed.<p>Trying the same prompts that gave nerfed &quot;I am just an AI I can&#x27;t speculate about the future&quot; bs on completion API gave somewhat better results, but most of the time they were flagged as breaking the guidelines which is a TOS breach if done enough times.<p>This can be solved other than open models. The same thing happened with stable diffusion. Good thing it&#x27;s open so you can still use the pre-nerfed 1.6 models.<p>I know it might be edgy or unpopular but I don&#x27;t think one entity should decide how we can use this powerful tool. No matter its implications and consequences.<p>FOS for the win.</div><br/></div></div><div id="36628335" class="c"><input type="checkbox" id="c-36628335" checked=""/><div class="controls bullet"><span class="by">wordpad25</span><span>|</span><a href="#36621961">root</a><span>|</span><a href="#36627687">parent</a><span>|</span><a href="#36628314">prev</a><span>|</span><a href="#36623801">next</a><span>|</span><label class="collapse" for="c-36628335">[-]</label><label class="expand" for="c-36628335">[1 more]</label></div><br/><div class="children"><div class="content">porn
it’s always porn</div><br/></div></div></div></div><div id="36624147" class="c"><input type="checkbox" id="c-36624147" checked=""/><div class="controls bullet"><span class="by">system2</span><span>|</span><a href="#36621961">parent</a><span>|</span><a href="#36623801">prev</a><span>|</span><a href="#36623078">next</a><span>|</span><label class="collapse" for="c-36624147">[-]</label><label class="expand" for="c-36624147">[3 more]</label></div><br/><div class="children"><div class="content">I built my entire app on text-davinci-003. It is the best writer so far. Do you think gpt3.5 turbo instruct won&#x27;t be the same?</div><br/><div id="36627587" class="c"><input type="checkbox" id="c-36627587" checked=""/><div class="controls bullet"><span class="by">raincole</span><span>|</span><a href="#36621961">root</a><span>|</span><a href="#36624147">parent</a><span>|</span><a href="#36623078">next</a><span>|</span><label class="collapse" for="c-36627587">[-]</label><label class="expand" for="c-36627587">[2 more]</label></div><br/><div class="children"><div class="content">&gt; In the coming weeks, we will reach out to developers who have recently used these older models, and will provide more information once the new completion models are ready for early testing.<p>I guess they&#x27;ll give you early access to it.</div><br/><div id="36627732" class="c"><input type="checkbox" id="c-36627732" checked=""/><div class="controls bullet"><span class="by">system2</span><span>|</span><a href="#36621961">root</a><span>|</span><a href="#36627587">parent</a><span>|</span><a href="#36623078">next</a><span>|</span><label class="collapse" for="c-36627732">[-]</label><label class="expand" for="c-36627732">[1 more]</label></div><br/><div class="children"><div class="content">Thanks!</div><br/></div></div></div></div></div></div><div id="36623078" class="c"><input type="checkbox" id="c-36623078" checked=""/><div class="controls bullet"><span class="by">samstave</span><span>|</span><a href="#36621961">parent</a><span>|</span><a href="#36624147">prev</a><span>|</span><a href="#36629045">next</a><span>|</span><label class="collapse" for="c-36623078">[-]</label><label class="expand" for="c-36623078">[1 more]</label></div><br/><div class="children"><div class="content">Please ELI5 if I am mis-interpretating what you said:<p>*<i>&quot;They have just locked down access to a model which they basically realized was way more valuable than even they thought - and they are in the process of locking in all controls around exploiting the model for great justice?&quot;*</i></div><br/></div></div></div></div><div id="36629045" class="c"><input type="checkbox" id="c-36629045" checked=""/><div class="controls bullet"><span class="by">pachico</span><span>|</span><a href="#36621961">prev</a><span>|</span><a href="#36621543">next</a><span>|</span><label class="collapse" for="c-36629045">[-]</label><label class="expand" for="c-36629045">[1 more]</label></div><br/><div class="children"><div class="content">OK, now I am intrigued by so many comments about how to do this yourself and, especially, for getting answers about your own document, which is what I&#x27;m really looking forward.<p>I checked many of the links you posted and, although I am a fluid programmer in several languages, I lack the specific python background that many of these links seem to state as a requirement.<p>Would any kind soul put me in the direction of an easy solution to run LLMs, potentially in AWS, that answers questions about your own docs? (I use Confluence but I can happily export pages.)<p>Thanks a lot in advance!!!</div><br/></div></div><div id="36621543" class="c"><input type="checkbox" id="c-36621543" checked=""/><div class="controls bullet"><span class="by">gamegoblin</span><span>|</span><a href="#36629045">prev</a><span>|</span><a href="#36623253">next</a><span>|</span><label class="collapse" for="c-36621543">[-]</label><label class="expand" for="c-36621543">[21 more]</label></div><br/><div class="children"><div class="content">Biggest news here from a capabilities POV is actually the gpt-3.5-turbo-instruct model.<p>gpt-3.5-turbo is the model behind ChatGPT. It&#x27;s chat-fine-tuned which makes it very hard to use for use-cases where you really just want it to obey&#x2F;complete without any &quot;chatty&quot; verbiage.<p>The &quot;davinci-003&quot; model was the last instruction tuned model, but is 10x more expensive than gpt-3.5-turbo, so it makes economical sense to hack gpt-3.5-turbo to your use case even if it is hugely wasteful from a tokens point of view.</div><br/><div id="36621707" class="c"><input type="checkbox" id="c-36621707" checked=""/><div class="controls bullet"><span class="by">ClassicOrgin</span><span>|</span><a href="#36621543">parent</a><span>|</span><a href="#36621619">next</a><span>|</span><label class="collapse" for="c-36621707">[-]</label><label class="expand" for="c-36621707">[12 more]</label></div><br/><div class="children"><div class="content">I&#x27;m interested in the cost of gpt-3.5-turbo-instruct. I&#x27;ve got a basic website using text-davinci-003 that I would like to launch but can&#x27;t because text-davinci-003 is too expensive. I&#x27;ve tried using just gpt-3.5-turbo but it won&#x27;t work because I&#x27;m expecting a formatted JSON to be returned and I can just never get consistency.</div><br/><div id="36621821" class="c"><input type="checkbox" id="c-36621821" checked=""/><div class="controls bullet"><span class="by">merpnderp</span><span>|</span><a href="#36621543">root</a><span>|</span><a href="#36621707">parent</a><span>|</span><a href="#36625081">next</a><span>|</span><label class="collapse" for="c-36621821">[-]</label><label class="expand" for="c-36621821">[2 more]</label></div><br/><div class="children"><div class="content">You need to use the new OpenAI Functions API. It is absolutely bonkers at returning formatted results. I can get it to return a perfectly formatted query-graph a few levels deep.</div><br/><div id="36625037" class="c"><input type="checkbox" id="c-36625037" checked=""/><div class="controls bullet"><span class="by">jimmySixDOF</span><span>|</span><a href="#36621543">root</a><span>|</span><a href="#36621821">parent</a><span>|</span><a href="#36625081">next</a><span>|</span><label class="collapse" for="c-36625037">[-]</label><label class="expand" for="c-36625037">[1 more]</label></div><br/><div class="children"><div class="content">There is also Code Interpreter now in  plugin beta so should influence it&#x27;s ability to output proper formats without hallucinations.</div><br/></div></div></div></div><div id="36625081" class="c"><input type="checkbox" id="c-36625081" checked=""/><div class="controls bullet"><span class="by">tagawa</span><span>|</span><a href="#36621543">root</a><span>|</span><a href="#36621707">parent</a><span>|</span><a href="#36621821">prev</a><span>|</span><a href="#36621997">next</a><span>|</span><label class="collapse" for="c-36625081">[-]</label><label class="expand" for="c-36625081">[1 more]</label></div><br/><div class="children"><div class="content">You can try to force JSON output using function calling (you have to use either the gpt-3.5-turbo-0613 or gpt-4-0613 model for now).<p>Think of the properties you want in the JSON object, then send those to ChatGPT as required parameters for a function (even if that function doesn&#x27;t exist).<p><pre><code>    # Definition of our local function(s).
    # This is effectively telling ChatGPT what we&#x27;re going to use its JSON output for.
    # Send this alongside the &quot;model&quot; and &quot;messages&quot; properties in the API request.

    functions = [
        {
            &quot;name&quot;: &quot;write_post&quot;,
            &quot;description&quot;: &quot;Shows the title and summary of some text.&quot;,
            &quot;parameters&quot;: {
                &quot;type&quot;: &quot;object&quot;,
                &quot;properties&quot;: {
                    &quot;title&quot;: {
                        &quot;type&quot;: &quot;string&quot;,
                        &quot;description&quot;: &quot;Title of the text output.&quot;
                    },
                    &quot;summary&quot;: {
                        &quot;type&quot;: &quot;string&quot;,
                        &quot;description&quot;: &quot;Summary of the text output.&quot;
                    }
                }
            }
        }
    ]
</code></pre>
I&#x27;ve found it&#x27;s not perfect but still pretty reliable – good enough for me combined with error handling.<p>If you&#x27;re interested, I wrote a blog post with more detail:
<a href="https:&#x2F;&#x2F;puppycoding.com&#x2F;2023&#x2F;07&#x2F;07&#x2F;json-object-from-chatgpt-api&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;puppycoding.com&#x2F;2023&#x2F;07&#x2F;07&#x2F;json-object-from-chatgpt-...</a></div><br/></div></div><div id="36621997" class="c"><input type="checkbox" id="c-36621997" checked=""/><div class="controls bullet"><span class="by">senko</span><span>|</span><a href="#36621543">root</a><span>|</span><a href="#36621707">parent</a><span>|</span><a href="#36625081">prev</a><span>|</span><a href="#36626031">next</a><span>|</span><label class="collapse" for="c-36621997">[-]</label><label class="expand" for="c-36621997">[3 more]</label></div><br/><div class="children"><div class="content">With the latest 3.5-turbo, you can try forcing it to call your function with a well-defined schema for arguments. If the structure is not overly complex, this should work.</div><br/><div id="36623679" class="c"><input type="checkbox" id="c-36623679" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#36621543">root</a><span>|</span><a href="#36621997">parent</a><span>|</span><a href="#36626031">next</a><span>|</span><label class="collapse" for="c-36623679">[-]</label><label class="expand" for="c-36623679">[2 more]</label></div><br/><div class="children"><div class="content">It&#x27;s great at returning well-formatted JSON, but it can hallucinate arguments or values to arguments.</div><br/><div id="36624497" class="c"><input type="checkbox" id="c-36624497" checked=""/><div class="controls bullet"><span class="by">sigstoat</span><span>|</span><a href="#36621543">root</a><span>|</span><a href="#36623679">parent</a><span>|</span><a href="#36626031">next</a><span>|</span><label class="collapse" for="c-36624497">[-]</label><label class="expand" for="c-36624497">[1 more]</label></div><br/><div class="children"><div class="content">i’ve had it come up with new function names, or prepend some prefix to the names of functions. i had to put some cleverness in on my end to run whatever function was close enough.</div><br/></div></div></div></div></div></div><div id="36626031" class="c"><input type="checkbox" id="c-36626031" checked=""/><div class="controls bullet"><span class="by">vczf</span><span>|</span><a href="#36621543">root</a><span>|</span><a href="#36621707">parent</a><span>|</span><a href="#36621997">prev</a><span>|</span><a href="#36621808">next</a><span>|</span><label class="collapse" for="c-36626031">[-]</label><label class="expand" for="c-36626031">[1 more]</label></div><br/><div class="children"><div class="content">Have you tried guidance?<p><a href="https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;guidance">https:&#x2F;&#x2F;github.com&#x2F;microsoft&#x2F;guidance</a></div><br/></div></div><div id="36621808" class="c"><input type="checkbox" id="c-36621808" checked=""/><div class="controls bullet"><span class="by">gamegoblin</span><span>|</span><a href="#36621543">root</a><span>|</span><a href="#36621707">parent</a><span>|</span><a href="#36626031">prev</a><span>|</span><a href="#36621619">next</a><span>|</span><label class="collapse" for="c-36621808">[-]</label><label class="expand" for="c-36621808">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m assuming they will price it the same as normal gpt-3.5-turbo. I won&#x27;t use it if it&#x27;s more than 2x the price of turbo, because I can usually get turbo to do what I want, it just takes more tokens sometimes.<p>Have you tried getting your formatted JSON out via the new Functions API? I does cure a lot of the deficiencies in 3.5-turbo.</div><br/><div id="36623650" class="c"><input type="checkbox" id="c-36623650" checked=""/><div class="controls bullet"><span class="by">mrinterweb</span><span>|</span><a href="#36621543">root</a><span>|</span><a href="#36621808">parent</a><span>|</span><a href="#36621619">next</a><span>|</span><label class="collapse" for="c-36623650">[-]</label><label class="expand" for="c-36623650">[3 more]</label></div><br/><div class="children"><div class="content">From what I can find, pricing of GPT-4 is roughly 25x that of 3.5 turbo.<p><a href="https:&#x2F;&#x2F;openai.com&#x2F;pricing" rel="nofollow noreferrer">https:&#x2F;&#x2F;openai.com&#x2F;pricing</a><p><a href="https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;deprecations&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;platform.openai.com&#x2F;docs&#x2F;deprecations&#x2F;</a></div><br/><div id="36623722" class="c"><input type="checkbox" id="c-36623722" checked=""/><div class="controls bullet"><span class="by">gamegoblin</span><span>|</span><a href="#36621543">root</a><span>|</span><a href="#36623650">parent</a><span>|</span><a href="#36621619">next</a><span>|</span><label class="collapse" for="c-36623722">[-]</label><label class="expand" for="c-36623722">[2 more]</label></div><br/><div class="children"><div class="content">In this thread we’re talking about gpt-3.5-turbo-instruct, not GPT4</div><br/><div id="36624506" class="c"><input type="checkbox" id="c-36624506" checked=""/><div class="controls bullet"><span class="by">mrinterweb</span><span>|</span><a href="#36621543">root</a><span>|</span><a href="#36623722">parent</a><span>|</span><a href="#36621619">next</a><span>|</span><label class="collapse" for="c-36624506">[-]</label><label class="expand" for="c-36624506">[1 more]</label></div><br/><div class="children"><div class="content">Sorry about that. Got my thread context confused.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36621619" class="c"><input type="checkbox" id="c-36621619" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#36621543">parent</a><span>|</span><a href="#36621707">prev</a><span>|</span><a href="#36624326">next</a><span>|</span><label class="collapse" for="c-36621619">[-]</label><label class="expand" for="c-36621619">[3 more]</label></div><br/><div class="children"><div class="content">What’s the diff with 3.5turbo with instruct?</div><br/><div id="36621669" class="c"><input type="checkbox" id="c-36621669" checked=""/><div class="controls bullet"><span class="by">gamegoblin</span><span>|</span><a href="#36621543">root</a><span>|</span><a href="#36621619">parent</a><span>|</span><a href="#36621755">next</a><span>|</span><label class="collapse" for="c-36621669">[-]</label><label class="expand" for="c-36621669">[1 more]</label></div><br/><div class="children"><div class="content">One is tuned for chat. It has that annoying ChatGPT personality. Instruct is a little &quot;lower level&quot; but more powerful. It doesn&#x27;t have the personality. It just obeys. But it is less structured, there are no messages from user to AI, it is just a single input prompt and a single output completion.</div><br/></div></div><div id="36621755" class="c"><input type="checkbox" id="c-36621755" checked=""/><div class="controls bullet"><span class="by">thewataccount</span><span>|</span><a href="#36621543">root</a><span>|</span><a href="#36621619">parent</a><span>|</span><a href="#36621669">prev</a><span>|</span><a href="#36624326">next</a><span>|</span><label class="collapse" for="c-36621755">[-]</label><label class="expand" for="c-36621755">[1 more]</label></div><br/><div class="children"><div class="content">the existing 3.5turbo is what you would call a &quot;chat&quot; model.<p>The difference between them is that the chat models are much more... chatty - they&#x27;re trained to act like they&#x27;re in a conversation with you. The chat models generally say things &quot;Sure, I can do that for you!&quot;, and &quot;No problem! Here is&quot;. The conversation style is generally more inconsistent in it&#x27;s style. It can be difficult to make it only return the result you want, and occasionally it&#x27;ll keep talking anyway. It&#x27;ll also talk in first person more, and a few things like that.<p>So if you&#x27;re using it as an API for things like summarization, extracting the subject of a sentence, code editing, etc, then the chat model can be super annoying to work with.</div><br/></div></div></div></div><div id="36624326" class="c"><input type="checkbox" id="c-36624326" checked=""/><div class="controls bullet"><span class="by">byt143</span><span>|</span><a href="#36621543">parent</a><span>|</span><a href="#36621619">prev</a><span>|</span><a href="#36622859">next</a><span>|</span><label class="collapse" for="c-36624326">[-]</label><label class="expand" for="c-36624326">[3 more]</label></div><br/><div class="children"><div class="content">What&#x27;s the difference between chat and instruction tuning?</div><br/><div id="36624394" class="c"><input type="checkbox" id="c-36624394" checked=""/><div class="controls bullet"><span class="by">tudorw</span><span>|</span><a href="#36621543">root</a><span>|</span><a href="#36624326">parent</a><span>|</span><a href="#36624588">next</a><span>|</span><label class="collapse" for="c-36624394">[-]</label><label class="expand" for="c-36624394">[1 more]</label></div><br/><div class="children"><div class="content">no expert, but from my messing around I gather the chat models are tuned for conversation, for example, if you just say &#x27;Hi&#x27;, it will spit out some &#x27;witty&#x27; reply and invite you to respond, it&#x27;s creative with it&#x27;s responses. On the other hand, if you say &#x27;Hi&#x27; to an instruct model, it might say something like, I need more information to complete the task. Instruct models are looking for something like &#x27;Write me a twitter bot to make millions&#x27;... in this case, if you ask the same thing again, you are somewhat more like to get the same, or similar result, this does not appear so true with a chat model, perhaps a real expert could chime in :)</div><br/></div></div><div id="36624588" class="c"><input type="checkbox" id="c-36624588" checked=""/><div class="controls bullet"><span class="by">msp26</span><span>|</span><a href="#36621543">root</a><span>|</span><a href="#36624326">parent</a><span>|</span><a href="#36624394">prev</a><span>|</span><a href="#36622859">next</a><span>|</span><label class="collapse" for="c-36624588">[-]</label><label class="expand" for="c-36624588">[1 more]</label></div><br/><div class="children"><div class="content">System&#x2F;assistant&#x2F;user prompting</div><br/></div></div></div></div><div id="36622859" class="c"><input type="checkbox" id="c-36622859" checked=""/><div class="controls bullet"><span class="by">Zpalmtree</span><span>|</span><a href="#36621543">parent</a><span>|</span><a href="#36624326">prev</a><span>|</span><a href="#36623253">next</a><span>|</span><label class="collapse" for="c-36622859">[-]</label><label class="expand" for="c-36622859">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m hoping gpt-3.5-turbo-instruct isn&#x27;t super neutered like chatgpt. davinci-003 can be a lot more fun and answer on a wide range of topics where ChatGPT will refuse to answer.</div><br/><div id="36623699" class="c"><input type="checkbox" id="c-36623699" checked=""/><div class="controls bullet"><span class="by">rmorey</span><span>|</span><a href="#36621543">root</a><span>|</span><a href="#36622859">parent</a><span>|</span><a href="#36623253">next</a><span>|</span><label class="collapse" for="c-36623699">[-]</label><label class="expand" for="c-36623699">[1 more]</label></div><br/><div class="children"><div class="content">such as?</div><br/></div></div></div></div></div></div><div id="36623253" class="c"><input type="checkbox" id="c-36623253" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#36621543">prev</a><span>|</span><a href="#36621274">next</a><span>|</span><label class="collapse" for="c-36623253">[-]</label><label class="expand" for="c-36623253">[16 more]</label></div><br/><div class="children"><div class="content">&gt; &quot;Starting today, all paying API customers have access to GPT-4.&quot;<p>OK maybe I&#x27;m stupid but I am a paying OpenAI API customer and I don&#x27;t have it yet. I see:<p><pre><code>    gpt-3.5-turbo-16k
    gpt-3.5-turbo
    gpt-3.5-turbo-16k-0613
    gpt-3.5-turbo-0613
    gpt-3.5-turbo-0301
</code></pre>
I don&#x27;t see any gpt-4<p>Edit: Probably my problem is that I upgraded to paid API account within the last month, so I&#x27;m not technically a &quot;paying API customer&quot; yet according to the accounting definitions.</div><br/><div id="36623318" class="c"><input type="checkbox" id="c-36623318" checked=""/><div class="controls bullet"><span class="by">codazoda</span><span>|</span><a href="#36623253">parent</a><span>|</span><a href="#36624653">next</a><span>|</span><label class="collapse" for="c-36623318">[-]</label><label class="expand" for="c-36623318">[12 more]</label></div><br/><div class="children"><div class="content">&gt; Today all existing API developers with a history of successful payments can access the GPT-4 API with 8K context. We plan to open up access to new developers by the end of this month, and then start raising rate-limits after that depending on compute availability.<p>Same for me. I signed up only a few days ago and was excited to switch to &quot;gpt-4&quot; but I haven&#x27;t paid the first bill (save the $5 capture) so I probably have to continue to wait for this.<p>I made a very simple command-line tool that calls the API. You run something like:<p><pre><code>    &gt; ask &quot;What&#x27;s the opposite of false?&quot;
</code></pre>
<a href="https:&#x2F;&#x2F;github.com&#x2F;codazoda&#x2F;askai">https:&#x2F;&#x2F;github.com&#x2F;codazoda&#x2F;askai</a></div><br/><div id="36624619" class="c"><input type="checkbox" id="c-36624619" checked=""/><div class="controls bullet"><span class="by">fnordpiglet</span><span>|</span><a href="#36623253">root</a><span>|</span><a href="#36623318">parent</a><span>|</span><a href="#36623641">next</a><span>|</span><label class="collapse" for="c-36624619">[-]</label><label class="expand" for="c-36624619">[1 more]</label></div><br/><div class="children"><div class="content">So, I&#x27;ve been a paying customers for a while now and don&#x27;t see it either :-(</div><br/></div></div><div id="36623641" class="c"><input type="checkbox" id="c-36623641" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#36623253">root</a><span>|</span><a href="#36623318">parent</a><span>|</span><a href="#36624619">prev</a><span>|</span><a href="#36624653">next</a><span>|</span><label class="collapse" for="c-36623641">[-]</label><label class="expand" for="c-36623641">[10 more]</label></div><br/><div class="children"><div class="content">Interesting, I did exactly the same (with the same name), but with GPT-4 support as well:<p><a href="https:&#x2F;&#x2F;www.pastery.net&#x2F;ccvjrh&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.pastery.net&#x2F;ccvjrh&#x2F;</a><p>It also does streaming, so it live-prints the response as it comes.</div><br/><div id="36625561" class="c"><input type="checkbox" id="c-36625561" checked=""/><div class="controls bullet"><span class="by">JimDabell</span><span>|</span><a href="#36623253">root</a><span>|</span><a href="#36623641">parent</a><span>|</span><a href="#36624653">next</a><span>|</span><label class="collapse" for="c-36625561">[-]</label><label class="expand" for="c-36625561">[9 more]</label></div><br/><div class="children"><div class="content">The llm command-line tool looks great:<p><a href="https:&#x2F;&#x2F;llm.datasette.io&#x2F;en&#x2F;stable&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;llm.datasette.io&#x2F;en&#x2F;stable&#x2F;</a></div><br/><div id="36625608" class="c"><input type="checkbox" id="c-36625608" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#36623253">root</a><span>|</span><a href="#36625561">parent</a><span>|</span><a href="#36625635">next</a><span>|</span><label class="collapse" for="c-36625608">[-]</label><label class="expand" for="c-36625608">[1 more]</label></div><br/><div class="children"><div class="content">It does, thanks for this! I didn&#x27;t know about it.</div><br/></div></div><div id="36625635" class="c"><input type="checkbox" id="c-36625635" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#36623253">root</a><span>|</span><a href="#36625561">parent</a><span>|</span><a href="#36625608">prev</a><span>|</span><a href="#36624653">next</a><span>|</span><label class="collapse" for="c-36625635">[-]</label><label class="expand" for="c-36625635">[7 more]</label></div><br/><div class="children"><div class="content">are we out here typing our api keys into random pips and am i a boomer that i would be hesitant to do it</div><br/><div id="36625945" class="c"><input type="checkbox" id="c-36625945" checked=""/><div class="controls bullet"><span class="by">JimDabell</span><span>|</span><a href="#36623253">root</a><span>|</span><a href="#36625635">parent</a><span>|</span><a href="#36624653">next</a><span>|</span><label class="collapse" for="c-36625945">[-]</label><label class="expand" for="c-36625945">[6 more]</label></div><br/><div class="children"><div class="content">It’s not a “random pip”. The maintainer is a well-known open source developer (one of the creators of Django and Datasette). It’s also a very small codebase – not many places for malicious code to hide.</div><br/><div id="36626152" class="c"><input type="checkbox" id="c-36626152" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#36623253">root</a><span>|</span><a href="#36625945">parent</a><span>|</span><a href="#36624653">next</a><span>|</span><label class="collapse" for="c-36626152">[-]</label><label class="expand" for="c-36626152">[5 more]</label></div><br/><div class="children"><div class="content">OK but I mean I don&#x27;t know them and it could have been someone pretending to be them, and probably it&#x27;s easily possible to trick me about API keys. We are discussing it on a hacker news website do you seriously think tricks couldn&#x27;t be hidden in a repo like that.</div><br/><div id="36627121" class="c"><input type="checkbox" id="c-36627121" checked=""/><div class="controls bullet"><span class="by">JimDabell</span><span>|</span><a href="#36623253">root</a><span>|</span><a href="#36626152">parent</a><span>|</span><a href="#36626298">next</a><span>|</span><label class="collapse" for="c-36627121">[-]</label><label class="expand" for="c-36627121">[2 more]</label></div><br/><div class="children"><div class="content">Do you only use software by people you know? At some point there has to be an element of trust when you run software you downloaded over the Internet. If a small utility maintained by a well-known member of the developer community doesn’t qualify for that trust, then I think that rules out an awful lot of software that all of us here probably use on a day to day basis. This is not an extraordinary level of risk.</div><br/><div id="36627331" class="c"><input type="checkbox" id="c-36627331" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#36623253">root</a><span>|</span><a href="#36627121">parent</a><span>|</span><a href="#36626298">next</a><span>|</span><label class="collapse" for="c-36627331">[-]</label><label class="expand" for="c-36627331">[1 more]</label></div><br/><div class="children"><div class="content">&gt; &quot;well-known member of the developer community&quot;<p>OK sorry I didn&#x27;t know them.<p>I mean I usually use software that came with my computer or ones that I apt-install from the official ubuntu distribution. I know it&#x27;s not perfect security but at least it&#x27;s more than a hacker news link to a github pip. If I had to use other ones then it&#x27;s usually from people I know.</div><br/></div></div></div></div><div id="36626298" class="c"><input type="checkbox" id="c-36626298" checked=""/><div class="controls bullet"><span class="by">bestcoder69</span><span>|</span><a href="#36623253">root</a><span>|</span><a href="#36626152">parent</a><span>|</span><a href="#36627121">prev</a><span>|</span><a href="#36624653">next</a><span>|</span><label class="collapse" for="c-36626298">[-]</label><label class="expand" for="c-36626298">[2 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;github.com&#x2F;search?q=repo%3Asimonw%2Fllm%20OPENAI_API_KEY&amp;type=code">https:&#x2F;&#x2F;github.com&#x2F;search?q=repo%3Asimonw%2Fllm%20OPENAI_API...</a><p>Let us know what you find.</div><br/><div id="36626433" class="c"><input type="checkbox" id="c-36626433" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#36623253">root</a><span>|</span><a href="#36626298">parent</a><span>|</span><a href="#36624653">next</a><span>|</span><label class="collapse" for="c-36626433">[-]</label><label class="expand" for="c-36626433">[1 more]</label></div><br/><div class="children"><div class="content">I found a few github issues related to api key security and management. I&#x27;m not 100% sure of your point.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="36624653" class="c"><input type="checkbox" id="c-36624653" checked=""/><div class="controls bullet"><span class="by">MiSeRyDeee</span><span>|</span><a href="#36623253">parent</a><span>|</span><a href="#36623318">prev</a><span>|</span><a href="#36625102">next</a><span>|</span><label class="collapse" for="c-36624653">[-]</label><label class="expand" for="c-36624653">[1 more]</label></div><br/><div class="children"><div class="content">I was a on paid account since last month and was never really billed for my $8 dollar usage. I don&#x27;t have GPT-4 access either.</div><br/></div></div><div id="36625102" class="c"><input type="checkbox" id="c-36625102" checked=""/><div class="controls bullet"><span class="by">wokwokwok</span><span>|</span><a href="#36623253">parent</a><span>|</span><a href="#36624653">prev</a><span>|</span><a href="#36623336">next</a><span>|</span><label class="collapse" for="c-36625102">[-]</label><label class="expand" for="c-36625102">[1 more]</label></div><br/><div class="children"><div class="content">Same. It&#x27;s not in the model list response from <a href="https:&#x2F;&#x2F;api.openai.com&#x2F;v1&#x2F;models" rel="nofollow noreferrer">https:&#x2F;&#x2F;api.openai.com&#x2F;v1&#x2F;models</a></div><br/></div></div><div id="36623336" class="c"><input type="checkbox" id="c-36623336" checked=""/><div class="controls bullet"><span class="by">zzzzzzzza</span><span>|</span><a href="#36623253">parent</a><span>|</span><a href="#36625102">prev</a><span>|</span><a href="#36621274">next</a><span>|</span><label class="collapse" for="c-36623336">[-]</label><label class="expand" for="c-36623336">[1 more]</label></div><br/><div class="children"><div class="content">can&#x27;t speak for others but I have two accounts<p>1. chat subscription only<p>2. i have paid for api calls but don&#x27;t have a subscription<p>and only #2 currently has gpt4 available in the playground</div><br/></div></div></div></div><div id="36621274" class="c"><input type="checkbox" id="c-36621274" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#36623253">prev</a><span>|</span><a href="#36625064">next</a><span>|</span><label class="collapse" for="c-36621274">[-]</label><label class="expand" for="c-36621274">[29 more]</label></div><br/><div class="children"><div class="content">With how good gpt-3.5-turbo-0613 is (particularly with system prompt engineering), there&#x27;s no longer as much of a need to use the GPT-4 API especially given its massive 20x-30x price increase.<p>The mass adoption of the ChatGPT APIs compared to the old Completion APIs proves my initial blog post on the ChatGPT API correct: developers <i>will</i> immediately switch for a massive price reduction if quality is the same (or better!): <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35110998">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35110998</a></div><br/><div id="36621350" class="c"><input type="checkbox" id="c-36621350" checked=""/><div class="controls bullet"><span class="by">EnnioEvo</span><span>|</span><a href="#36621274">parent</a><span>|</span><a href="#36621370">next</a><span>|</span><label class="collapse" for="c-36621350">[-]</label><label class="expand" for="c-36621350">[9 more]</label></div><br/><div class="children"><div class="content">I have a startup of legal AI, the quality jump from GPT3.5 to GPT4 in this domain is straight mind-blowing, GPT3.5 in comparison is useless. But I see how in more conversational settings GPT3.5 can provide more appealing performance&#x2F;price.</div><br/><div id="36621430" class="c"><input type="checkbox" id="c-36621430" checked=""/><div class="controls bullet"><span class="by">tnel77</span><span>|</span><a href="#36621274">root</a><span>|</span><a href="#36621350">parent</a><span>|</span><a href="#36623170">next</a><span>|</span><label class="collapse" for="c-36621430">[-]</label><label class="expand" for="c-36621430">[3 more]</label></div><br/><div class="children"><div class="content">I suggested to my wife that ChatGPT would help with her job and she has found ChatGPT4 to be the same or worse as ChatGPT3.5. It’s really interesting just how variable the quality can be given your particular line of work.</div><br/><div id="36621571" class="c"><input type="checkbox" id="c-36621571" checked=""/><div class="controls bullet"><span class="by">mensetmanusman</span><span>|</span><a href="#36621274">root</a><span>|</span><a href="#36621430">parent</a><span>|</span><a href="#36621600">next</a><span>|</span><label class="collapse" for="c-36621571">[-]</label><label class="expand" for="c-36621571">[1 more]</label></div><br/><div class="children"><div class="content">Remember, communication style is also very important. Some communication styles mesh much better with these models.</div><br/></div></div><div id="36621600" class="c"><input type="checkbox" id="c-36621600" checked=""/><div class="controls bullet"><span class="by">jerrygenser</span><span>|</span><a href="#36621274">root</a><span>|</span><a href="#36621430">parent</a><span>|</span><a href="#36621571">prev</a><span>|</span><a href="#36623170">next</a><span>|</span><label class="collapse" for="c-36621600">[-]</label><label class="expand" for="c-36621600">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve noticed the quality fo chatgpt4 to be much closer now to chatgpt3.5 than it was.<p>However if you try the gpt-4 API, it&#x27;s possible it will be much better.</div><br/></div></div></div></div><div id="36623170" class="c"><input type="checkbox" id="c-36623170" checked=""/><div class="controls bullet"><span class="by">w10-1</span><span>|</span><a href="#36621274">root</a><span>|</span><a href="#36621350">parent</a><span>|</span><a href="#36621430">prev</a><span>|</span><a href="#36621889">next</a><span>|</span><label class="collapse" for="c-36623170">[-]</label><label class="expand" for="c-36623170">[4 more]</label></div><br/><div class="children"><div class="content">Legal writing is ideal training data: mostly formulaic, based on conventions and rules, well-formed and highly vetted, with much of the best in the public domain.<p>Medical writing is the opposite, with unstated premises, semi-random associations, and rarely a meaningful sentence.</div><br/><div id="36623515" class="c"><input type="checkbox" id="c-36623515" checked=""/><div class="controls bullet"><span class="by">claytonjy</span><span>|</span><a href="#36621274">root</a><span>|</span><a href="#36623170">parent</a><span>|</span><a href="#36625766">next</a><span>|</span><label class="collapse" for="c-36623515">[-]</label><label class="expand" for="c-36623515">[1 more]</label></div><br/><div class="children"><div class="content">And yet I can confirm that 4 is far superior to 3.5 in the medical domain as well!</div><br/></div></div><div id="36625766" class="c"><input type="checkbox" id="c-36625766" checked=""/><div class="controls bullet"><span class="by">cyanydeez</span><span>|</span><a href="#36621274">root</a><span>|</span><a href="#36623170">parent</a><span>|</span><a href="#36623515">prev</a><span>|</span><a href="#36623486">next</a><span>|</span><label class="collapse" for="c-36625766">[-]</label><label class="expand" for="c-36625766">[1 more]</label></div><br/><div class="children"><div class="content">Legal writing is mostly pattern matching. Unfortunately, you&#x27;re still gonna need to guard against hallucinations.</div><br/></div></div><div id="36623486" class="c"><input type="checkbox" id="c-36623486" checked=""/><div class="controls bullet"><span class="by">flangola7</span><span>|</span><a href="#36621274">root</a><span>|</span><a href="#36623170">parent</a><span>|</span><a href="#36625766">prev</a><span>|</span><a href="#36621889">next</a><span>|</span><label class="collapse" for="c-36623486">[-]</label><label class="expand" for="c-36623486">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Legal writing is ideal training data: mostly formulaic, based on conventions and rules, well-formed and highly vetted, with much of the best in the public domain.<p>That makes sense. The labor impact research suggests that law will be a domain hit almost as hard as education by language models. Almost nothing happens in court that hasn&#x27;t occured hundreds of thousands of times before. A model with GPT-4 power specifically trained for legal matters and fine tuned by jurisdiction could replace everyone in a courtroom. Well there&#x27;s still the bailiff, I think that&#x27;s about 18 months behind.</div><br/></div></div></div></div><div id="36621889" class="c"><input type="checkbox" id="c-36621889" checked=""/><div class="controls bullet"><span class="by">Terretta</span><span>|</span><a href="#36621274">root</a><span>|</span><a href="#36621350">parent</a><span>|</span><a href="#36623170">prev</a><span>|</span><a href="#36621370">next</a><span>|</span><label class="collapse" for="c-36621889">[-]</label><label class="expand" for="c-36621889">[1 more]</label></div><br/><div class="children"><div class="content">Same page.<p>So still waiting to be on the same 32 pages...</div><br/></div></div></div></div><div id="36621370" class="c"><input type="checkbox" id="c-36621370" checked=""/><div class="controls bullet"><span class="by">ravenstine</span><span>|</span><a href="#36621274">parent</a><span>|</span><a href="#36621350">prev</a><span>|</span><a href="#36621339">next</a><span>|</span><label class="collapse" for="c-36621370">[-]</label><label class="expand" for="c-36621370">[9 more]</label></div><br/><div class="children"><div class="content">My experience is that GPT-3.5 is <i>not</i> better or even nearly as good as GPT-4.  Will it work for most use cases?  <i>Probably, yes.</i>  But GPT-3.5 effectively ignores instructions much more often than GPT-4 and I&#x27;ve found it far far easier to trip up with things as simple as trailing spaces; it will sometimes exhibit really odd behavior like spelling out individual letters when you give it large amounts of text with missing grammar&#x2F;punctuation to rewrite.  Doesn&#x27;t seem to matter how I setup the system prompt.  I&#x27;ve yet to see GPT-4 do truly strange things like that.</div><br/><div id="36621460" class="c"><input type="checkbox" id="c-36621460" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#36621274">root</a><span>|</span><a href="#36621370">parent</a><span>|</span><a href="#36628392">next</a><span>|</span><label class="collapse" for="c-36621460">[-]</label><label class="expand" for="c-36621460">[7 more]</label></div><br/><div class="children"><div class="content">The initial gpt-3.5-turbo was flakey and required significant prompt engineering. The updated gpt-3.5-turbo-0613 fixed all the issues I had even after stripping out the prompt engineering.</div><br/><div id="36621541" class="c"><input type="checkbox" id="c-36621541" checked=""/><div class="controls bullet"><span class="by">ravenstine</span><span>|</span><a href="#36621274">root</a><span>|</span><a href="#36621460">parent</a><span>|</span><a href="#36623761">next</a><span>|</span><label class="collapse" for="c-36621541">[-]</label><label class="expand" for="c-36621541">[5 more]</label></div><br/><div class="children"><div class="content">It&#x27;s definitely gotten better, but yeah, it really doesn&#x27;t reliably support what I&#x27;m currently working on.<p>My project takes transcripts from YouTube, which don&#x27;t have punctuation, splits them up into chunks, and passes each chunk to GPT-4 telling it to add punctuation with paragraphs.  Part of the instructions includes telling the model that, if the final sentence of the chunk appears incomplete, to just try to complete it.  Anyway, GPT-3.5-turbo works okay for several chunks but almost invariably hits a case where it either writes a bunch of nonsense or spells out the individual letters of words.  I&#x27;m sure that there&#x27;s a programmatic way I can work around this issue, but GPT-4 performs the same job flawlessly.</div><br/><div id="36623614" class="c"><input type="checkbox" id="c-36623614" checked=""/><div class="controls bullet"><span class="by">popinman322</span><span>|</span><a href="#36621274">root</a><span>|</span><a href="#36621541">parent</a><span>|</span><a href="#36622708">next</a><span>|</span><label class="collapse" for="c-36623614">[-]</label><label class="expand" for="c-36623614">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve done exactly this for another project. I&#x27;d recommend grabbing an open source model and fine-tuning on some augmented data in your domain. For example: I grabbed tech blog posts, turned each post into a collection of phonemes, reconstructed the phonemes into words, added filler words, and removed punctuation+capitalization.</div><br/><div id="36624279" class="c"><input type="checkbox" id="c-36624279" checked=""/><div class="controls bullet"><span class="by">swores</span><span>|</span><a href="#36621274">root</a><span>|</span><a href="#36623614">parent</a><span>|</span><a href="#36622708">next</a><span>|</span><label class="collapse" for="c-36624279">[-]</label><label class="expand" for="c-36624279">[1 more]</label></div><br/><div class="children"><div class="content">Sounds interesting, any chance you could share either your end result that you used to then fine-tune with, or even better the exact steps (ie technically how you did each step you already mentioned)?<p>And what open LLM you used it with &#x2F; how successful you&#x27;ve found it?</div><br/></div></div></div></div><div id="36622708" class="c"><input type="checkbox" id="c-36622708" checked=""/><div class="controls bullet"><span class="by">selalipop</span><span>|</span><a href="#36621274">root</a><span>|</span><a href="#36621541">parent</a><span>|</span><a href="#36623614">prev</a><span>|</span><a href="#36621630">next</a><span>|</span><label class="collapse" for="c-36622708">[-]</label><label class="expand" for="c-36622708">[1 more]</label></div><br/><div class="children"><div class="content">If GPT 4 is working for you I wouldn&#x27;t necessarily bother with this, but this is a great example of where you can sometimes take advantage of how much cheaper 3.5 is to burn some tokens and get a better output. For example I&#x27;d try asking it for something like :<p><pre><code>    {
        &quot;isIncomplete&quot;: [true if the chunk seems incomplete]
        &quot;completion&quot;: [the additional text to add to the end, or undefined otherwise]
        &quot;finalOutputWithCompletion&quot;: [punctuated text with completion if isIncomplete==true]
    }
</code></pre>
Technically you&#x27;re burning a ton of tokens having it state the completion twice, but GPT 3.5 is fast&#x2F;cheap enough that it doesn&#x27;t matter as long as &#x27;finalOutputWithCompletion&#x27; is good. You can probably add some extra fields to get an even nicer output than 4 would allow cost-wise and time-wise by expanding that JSON object with extra information that you&#x27;d ideally input like tone&#x2F;subject.</div><br/></div></div><div id="36621630" class="c"><input type="checkbox" id="c-36621630" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#36621274">root</a><span>|</span><a href="#36621541">parent</a><span>|</span><a href="#36622708">prev</a><span>|</span><a href="#36623761">next</a><span>|</span><label class="collapse" for="c-36621630">[-]</label><label class="expand" for="c-36621630">[1 more]</label></div><br/><div class="children"><div class="content">Semi off-topic but that&#x27;s a use case where the new structured data I&#x2F;O would perform extremely well. I may have to expedite my blog post on it.</div><br/></div></div></div></div><div id="36623761" class="c"><input type="checkbox" id="c-36623761" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#36621274">root</a><span>|</span><a href="#36621460">parent</a><span>|</span><a href="#36621541">prev</a><span>|</span><a href="#36628392">next</a><span>|</span><label class="collapse" for="c-36623761">[-]</label><label class="expand" for="c-36623761">[1 more]</label></div><br/><div class="children"><div class="content">I use it to generate nonsense fairytales for my sleep podcast (<a href="https:&#x2F;&#x2F;deepdreams.stavros.io&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;deepdreams.stavros.io&#x2F;</a>), and it will ignore my (pretty specific) instructions and add scene titles to things, and write the text in dramatic format instead of prose, no matter how much I try.</div><br/></div></div></div></div><div id="36628392" class="c"><input type="checkbox" id="c-36628392" checked=""/><div class="controls bullet"><span class="by">yard2010</span><span>|</span><a href="#36621274">root</a><span>|</span><a href="#36621370">parent</a><span>|</span><a href="#36621460">prev</a><span>|</span><a href="#36621339">next</a><span>|</span><label class="collapse" for="c-36628392">[-]</label><label class="expand" for="c-36628392">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re asking too much of it, it has its own existential crisis followed by a mental breakdown</div><br/></div></div></div></div><div id="36621339" class="c"><input type="checkbox" id="c-36621339" checked=""/><div class="controls bullet"><span class="by">dreadlordbone</span><span>|</span><a href="#36621274">parent</a><span>|</span><a href="#36621370">prev</a><span>|</span><a href="#36627719">next</a><span>|</span><label class="collapse" for="c-36621339">[-]</label><label class="expand" for="c-36621339">[2 more]</label></div><br/><div class="children"><div class="content">Code completion&#x2F;assistance is an order of magnitude better in GPT4.</div><br/><div id="36621667" class="c"><input type="checkbox" id="c-36621667" checked=""/><div class="controls bullet"><span class="by">inciampati</span><span>|</span><a href="#36621274">root</a><span>|</span><a href="#36621339">parent</a><span>|</span><a href="#36627719">next</a><span>|</span><label class="collapse" for="c-36621667">[-]</label><label class="expand" for="c-36621667">[1 more]</label></div><br/><div class="children"><div class="content">A lot of folks are talking about using gpt-4 for completion. Wondering what editor and what plugins y&#x27;all are using.</div><br/></div></div></div></div><div id="36627719" class="c"><input type="checkbox" id="c-36627719" checked=""/><div class="controls bullet"><span class="by">alvah</span><span>|</span><a href="#36621274">parent</a><span>|</span><a href="#36621339">prev</a><span>|</span><a href="#36621582">next</a><span>|</span><label class="collapse" for="c-36627719">[-]</label><label class="expand" for="c-36627719">[1 more]</label></div><br/><div class="children"><div class="content">I think this is very very use-case dependent, and your use case != everyone&#x27;s use case.
In my experience, GPT-4 is night and day better than 3.5 turbo for almost everything I use OpenAI for.</div><br/></div></div><div id="36621582" class="c"><input type="checkbox" id="c-36621582" checked=""/><div class="controls bullet"><span class="by">thewataccount</span><span>|</span><a href="#36621274">parent</a><span>|</span><a href="#36627719">prev</a><span>|</span><a href="#36621336">next</a><span>|</span><label class="collapse" for="c-36621582">[-]</label><label class="expand" for="c-36621582">[2 more]</label></div><br/><div class="children"><div class="content">What usecases are you using it for?<p>I mostly use it for generating tests, making documentation, refactoring, code snippets, etc. I use it daily for work along with copilot&#x2F;x.<p>In my experience GPT3.5turbo is... rather dumb in comparison. It makes a comment explaining what a method is going to do and what arguments it will have - then misses arguments altogether. It feels like it has poor memory (and we&#x27;re talking relatively short code snippets, nothing remotely near it&#x27;s context length).<p>And I don&#x27;t mean small mistakes - I mean it will say it will do something with several steps, then just miss entire steps.<p>GPT3.5turbo is reliably unreliable for me, requiring large changes and constant &quot;rerolls&quot;.<p>GPT3.5turbo also has difficulty following the &quot;style&#x2F;template&quot; from both the prompt and it&#x27;s own response. It&#x27;ll be consistent then just - change. An example being how it uses bullet points in documentation.<p>Codex is generally better - but noticeably worse then GPT4 - it&#x27;s decent as a &quot;smart autocomplete&quot; though. Not crazy useful for documentation.<p>Meanwhile GPT4 generally nails the results, occasionally needing a few tweaks, generally only with long&#x2F;complex code&#x2F;prompts.<p>tl;dr - In my experience for code GPT3.5turbo isn&#x27;t even worth the time it takes to get a good result&#x2F;fix the result. Codex can do some decent things. I just use GPT4 for anything more then autocomplete - it&#x27;s so much more consistent.</div><br/><div id="36622744" class="c"><input type="checkbox" id="c-36622744" checked=""/><div class="controls bullet"><span class="by">selalipop</span><span>|</span><a href="#36621274">root</a><span>|</span><a href="#36621582">parent</a><span>|</span><a href="#36621336">next</a><span>|</span><label class="collapse" for="c-36622744">[-]</label><label class="expand" for="c-36622744">[1 more]</label></div><br/><div class="children"><div class="content">If you&#x27;re manually interacting with the model, GPT 4 is almost always going to be better.<p>Where 3.5 excels is with programmatic access. You can ask it for 2x as much text between setup so the end result is well formed and still get a reply that&#x27;s cheaper and faster than 4 (for example, ask 3.5 for a response, then ask it to format that response)</div><br/></div></div></div></div><div id="36621336" class="c"><input type="checkbox" id="c-36621336" checked=""/><div class="controls bullet"><span class="by">SkyPuncher</span><span>|</span><a href="#36621274">parent</a><span>|</span><a href="#36621582">prev</a><span>|</span><a href="#36621356">next</a><span>|</span><label class="collapse" for="c-36621336">[-]</label><label class="expand" for="c-36621336">[1 more]</label></div><br/><div class="children"><div class="content">Depending on your use case, there are major quality differences between GPT-3.5 and GPT-4.</div><br/></div></div><div id="36621356" class="c"><input type="checkbox" id="c-36621356" checked=""/><div class="controls bullet"><span class="by">avindroth</span><span>|</span><a href="#36621274">parent</a><span>|</span><a href="#36621336">prev</a><span>|</span><a href="#36621544">next</a><span>|</span><label class="collapse" for="c-36621356">[-]</label><label class="expand" for="c-36621356">[3 more]</label></div><br/><div class="children"><div class="content">I am building an extensive LLM-powered app, and had a chance to compare the two using the API. Empirically, I have found 3.5 to be fairly unusable for the app&#x27;s use case. How are you evaluating the two models?</div><br/><div id="36621883" class="c"><input type="checkbox" id="c-36621883" checked=""/><div class="controls bullet"><span class="by">selalipop</span><span>|</span><a href="#36621274">root</a><span>|</span><a href="#36621356">parent</a><span>|</span><a href="#36621544">next</a><span>|</span><label class="collapse" for="c-36621883">[-]</label><label class="expand" for="c-36621883">[2 more]</label></div><br/><div class="children"><div class="content">It depends on the domain, but chain of thought can get 3.5 to be extremely reliable, and especially with the new 16k variant<p>I built notionsmith.ai on 3.5: for some time I experimented with GPT 4 but the result was significantly worse to use because of how slow it became, going from ~15 seconds per generated output to a minute plus.<p>And you could work around that with things like streaming output for some use cases, but that doesn&#x27;t work for chain of thought. GPT 4 can do some tasks without chain of thought that 3.5 required it for, but there are still many times where it improves the result from 4 dramatically.<p>For example, I leverage chain of thought in replies to the user when they&#x27;re in a chat and that results in a much better user experience: It&#x27;s very difficult to run into the default &#x27;As a large language model&#x27; disclaimer regardless of how deeply you probe a generated experience when using it. GPT 4 requires the same chain of thought process to avoid that, but ends up needing several seconds per response, as opposed to 3.5 which is near-instant.<p>-<p>I suspect a lot of people are building things on 4 but would get better quality of output if they used more aspects of chain of thought and either settled for a slower output or moved to 3.5 (or a mix of 3.5 and 4)</div><br/><div id="36626353" class="c"><input type="checkbox" id="c-36626353" checked=""/><div class="controls bullet"><span class="by">avindroth</span><span>|</span><a href="#36621274">root</a><span>|</span><a href="#36621883">parent</a><span>|</span><a href="#36621544">next</a><span>|</span><label class="collapse" for="c-36626353">[-]</label><label class="expand" for="c-36626353">[1 more]</label></div><br/><div class="children"><div class="content">It depends a lot on the domain, even for CoT. I don&#x27;t think there are enough NLU evaluations just yet to robustly compare GPT-3.5 w&#x2F; CoT&#x2F;SC vs. GPT-4 wrt domain.<p>For instance, with MATH dataset, my own n=500 evaluation showed no difference between GPT-3.5 (w&#x2F; and w&#x2F;o CoT) and GPT-4. I was pretty surprised by that.</div><br/></div></div></div></div></div></div><div id="36621544" class="c"><input type="checkbox" id="c-36621544" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#36621274">parent</a><span>|</span><a href="#36621356">prev</a><span>|</span><a href="#36625064">next</a><span>|</span><label class="collapse" for="c-36621544">[-]</label><label class="expand" for="c-36621544">[1 more]</label></div><br/><div class="children"><div class="content">&gt; &quot;With how good gpt-3.5-turbo-0613 is (particularly with system prompt engineering), there&#x27;s no longer as much of a need to use the GPT-4&quot;<p>poe law</div><br/></div></div></div></div><div id="36625064" class="c"><input type="checkbox" id="c-36625064" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#36621274">prev</a><span>|</span><a href="#36621890">next</a><span>|</span><label class="collapse" for="c-36625064">[-]</label><label class="expand" for="c-36625064">[32 more]</label></div><br/><div class="children"><div class="content">The drip-feeding seems crazy to me. Open AI is undermining their reputation by forcing almost everybody to use the older, lower-quality models. Even if customers are willing to pay for GPT 4, they&#x27;re being told to wait at the back of the line.<p>Wait for what!? Christmas? When we can open our presents and have a GPT 4 inside?<p>It&#x27;s like they took a leaf from Google&#x27;s &quot;how to guarantee the failure of a new product&quot; marketing. That is: restrict access, ensuring that to word-of-mouth marketing can&#x27;t possibly work because none of your friends are <i>allowed</i> to try the product.<p>The announcement here is &quot;general availability&quot; of the GPT-4 model...<p>...but not the 32K context model. Not the multi-modal version with image input. No fine-tuning. Only one model (chat).<p>As of today, I can only access GPT 3.5 via Azure Open AI service <i>and</i> the Open AI API account that I have.<p>What&#x27;s the point of all these arbitrary restrictions on who can access what model!?<p>I can use GPT 4 via Chat, but not an API. I can use an enhanced version of Dall-E via Bing Image Creator, but not the OpenAI API. Some vendors that have been blessed by the Great and Benevolent Sam Altman have access to GPT-4 32K, the rest of us don&#x27;t.<p>Sell the product, not the access to it.<p>Don&#x27;t be like the Soviet Union, where you had to &quot;know someone&quot; to get access.</div><br/><div id="36625098" class="c"><input type="checkbox" id="c-36625098" checked=""/><div class="controls bullet"><span class="by">mortehu</span><span>|</span><a href="#36625064">parent</a><span>|</span><a href="#36625219">next</a><span>|</span><label class="collapse" for="c-36625098">[-]</label><label class="expand" for="c-36625098">[16 more]</label></div><br/><div class="children"><div class="content">I think maybe you don&#x27;t understand that they don&#x27;t have enough GPUs to do this, and money can&#x27;t buy enough GPUs to do it.</div><br/><div id="36625259" class="c"><input type="checkbox" id="c-36625259" checked=""/><div class="controls bullet"><span class="by">chrisshroba</span><span>|</span><a href="#36625064">root</a><span>|</span><a href="#36625098">parent</a><span>|</span><a href="#36626007">next</a><span>|</span><label class="collapse" for="c-36625259">[-]</label><label class="expand" for="c-36625259">[10 more]</label></div><br/><div class="children"><div class="content">Not disagreeing but just curious, why can&#x27;t money buy enough GPU&#x27;s? OpenAI&#x27;s prices seem low enough that they could reasonably charge 2x or more to companies eager to get on the best models now.</div><br/><div id="36625323" class="c"><input type="checkbox" id="c-36625323" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#36625064">root</a><span>|</span><a href="#36625259">parent</a><span>|</span><a href="#36625301">next</a><span>|</span><label class="collapse" for="c-36625323">[-]</label><label class="expand" for="c-36625323">[8 more]</label></div><br/><div class="children"><div class="content">They&#x27;re giving people access to GPT-4 via Bing <i>for free</i>, but apparently can&#x27;t accommodate <i>paying API users!?</i><p>That makes no sense.<p>What makes much more sense -- especially if you listen to his interviews -- is that Sam Altman doesn&#x27;t think you can be <i>trusted</i> with the power of GPT-4 via an API unless it has first been aligned to death.</div><br/><div id="36625541" class="c"><input type="checkbox" id="c-36625541" checked=""/><div class="controls bullet"><span class="by">cptaj</span><span>|</span><a href="#36625064">root</a><span>|</span><a href="#36625323">parent</a><span>|</span><a href="#36625536">next</a><span>|</span><label class="collapse" for="c-36625541">[-]</label><label class="expand" for="c-36625541">[5 more]</label></div><br/><div class="children"><div class="content">Microsoft is giving that for free but I assume they&#x27;re paying OpenAI for it.<p>And having such a big anchor tenant, its reasonable that you would prioritize them if GPUs are in short supply.</div><br/><div id="36625679" class="c"><input type="checkbox" id="c-36625679" checked=""/><div class="controls bullet"><span class="by">selcuka</span><span>|</span><a href="#36625064">root</a><span>|</span><a href="#36625541">parent</a><span>|</span><a href="#36626857">next</a><span>|</span><label class="collapse" for="c-36625679">[-]</label><label class="expand" for="c-36625679">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Microsoft is giving that for free but I assume they&#x27;re paying OpenAI for it.<p>Yeah, but Microsoft already gets 75% of the profits OpenAI makes, it&#x27;s not the same price for them as the rest of us.</div><br/><div id="36625830" class="c"><input type="checkbox" id="c-36625830" checked=""/><div class="controls bullet"><span class="by">ec109685</span><span>|</span><a href="#36625064">root</a><span>|</span><a href="#36625679">parent</a><span>|</span><a href="#36626857">next</a><span>|</span><label class="collapse" for="c-36625830">[-]</label><label class="expand" for="c-36625830">[2 more]</label></div><br/><div class="children"><div class="content">It’s the exactly the same. If they could make 75 cents selling the compute to someone else for $1 versus not making it providing the Bing chat service, that is 75 cents they lose.</div><br/><div id="36627488" class="c"><input type="checkbox" id="c-36627488" checked=""/><div class="controls bullet"><span class="by">selcuka</span><span>|</span><a href="#36625064">root</a><span>|</span><a href="#36625830">parent</a><span>|</span><a href="#36626857">next</a><span>|</span><label class="collapse" for="c-36627488">[-]</label><label class="expand" for="c-36627488">[1 more]</label></div><br/><div class="children"><div class="content">Why do you assume that the same amount of computing power would be used by someone else? There are only so many customers. You can&#x27;t magically start selling more compute if you stop using it yourself.</div><br/></div></div></div></div></div></div><div id="36626857" class="c"><input type="checkbox" id="c-36626857" checked=""/><div class="controls bullet"><span class="by">itissid</span><span>|</span><a href="#36625064">root</a><span>|</span><a href="#36625541">parent</a><span>|</span><a href="#36625679">prev</a><span>|</span><a href="#36625536">next</a><span>|</span><label class="collapse" for="c-36626857">[-]</label><label class="expand" for="c-36626857">[1 more]</label></div><br/><div class="children"><div class="content">10 billion$</div><br/></div></div></div></div><div id="36625536" class="c"><input type="checkbox" id="c-36625536" checked=""/><div class="controls bullet"><span class="by">90d</span><span>|</span><a href="#36625064">root</a><span>|</span><a href="#36625323">parent</a><span>|</span><a href="#36625541">prev</a><span>|</span><a href="#36625768">next</a><span>|</span><label class="collapse" for="c-36625536">[-]</label><label class="expand" for="c-36625536">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Free&quot;. The worst four-letter F word in America.</div><br/></div></div><div id="36625768" class="c"><input type="checkbox" id="c-36625768" checked=""/><div class="controls bullet"><span class="by">flangola7</span><span>|</span><a href="#36625064">root</a><span>|</span><a href="#36625323">parent</a><span>|</span><a href="#36625536">prev</a><span>|</span><a href="#36625301">next</a><span>|</span><label class="collapse" for="c-36625768">[-]</label><label class="expand" for="c-36625768">[1 more]</label></div><br/><div class="children"><div class="content">Bing GPT-4 is a much smaller and less capable model than regular GPT-4.</div><br/></div></div></div></div><div id="36625301" class="c"><input type="checkbox" id="c-36625301" checked=""/><div class="controls bullet"><span class="by">jerrygenser</span><span>|</span><a href="#36625064">root</a><span>|</span><a href="#36625259">parent</a><span>|</span><a href="#36625323">prev</a><span>|</span><a href="#36626007">next</a><span>|</span><label class="collapse" for="c-36625301">[-]</label><label class="expand" for="c-36625301">[1 more]</label></div><br/><div class="children"><div class="content">I think GPUs are in short supply and Nvidia can&#x27;t make enough to keep up with demand.</div><br/></div></div></div></div><div id="36626007" class="c"><input type="checkbox" id="c-36626007" checked=""/><div class="controls bullet"><span class="by">flangola7</span><span>|</span><a href="#36625064">root</a><span>|</span><a href="#36625098">parent</a><span>|</span><a href="#36625259">prev</a><span>|</span><a href="#36625146">next</a><span>|</span><label class="collapse" for="c-36626007">[-]</label><label class="expand" for="c-36626007">[3 more]</label></div><br/><div class="children"><div class="content">This is the bottleneck. EUV Photolithography is one of the hardest engineering challenges ever faced, it&#x27;s like trying to drop a feather from space and guaranteeing it lands on a specific blade of grass. Manufacturing these GPUs at all requires us to stretch the limit of what is physically possible in multiple domains, much less producing them at scale.</div><br/><div id="36626082" class="c"><input type="checkbox" id="c-36626082" checked=""/><div class="controls bullet"><span class="by">xtracto</span><span>|</span><a href="#36625064">root</a><span>|</span><a href="#36626007">parent</a><span>|</span><a href="#36625146">next</a><span>|</span><label class="collapse" for="c-36626082">[-]</label><label class="expand" for="c-36626082">[2 more]</label></div><br/><div class="children"><div class="content">Thanks for this explanation! :) (as someone without knowledge of the hardware process I appreciated it).<p>It is SO amazing that we have such a driving force (LLMs&#x2F;consumer-AI) for this (instead of stupid cryptocurrencies mining or high-performance gaming).  This should drive innovation pretty strongly and I am sure the next &quot;leap&quot; in this regard (processing hardware) will put technology in a completely different level.</div><br/><div id="36627122" class="c"><input type="checkbox" id="c-36627122" checked=""/><div class="controls bullet"><span class="by">fatherzine</span><span>|</span><a href="#36625064">root</a><span>|</span><a href="#36626082">parent</a><span>|</span><a href="#36625146">next</a><span>|</span><label class="collapse" for="c-36627122">[-]</label><label class="expand" for="c-36627122">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s a cool last minute detour from techno gods. We can incentivize AI to work on crypto mining, and regain our fully engaged primeval lives back ;)</div><br/></div></div></div></div></div></div><div id="36625146" class="c"><input type="checkbox" id="c-36625146" checked=""/><div class="controls bullet"><span class="by">cuuupid</span><span>|</span><a href="#36625064">root</a><span>|</span><a href="#36625098">parent</a><span>|</span><a href="#36626007">prev</a><span>|</span><a href="#36625219">next</a><span>|</span><label class="collapse" for="c-36625146">[-]</label><label class="expand" for="c-36625146">[2 more]</label></div><br/><div class="children"><div class="content">This may be true but isn’t their official stance that their models are too powerful and could destroy Western civilization as we know it?</div><br/></div></div></div></div><div id="36625219" class="c"><input type="checkbox" id="c-36625219" checked=""/><div class="controls bullet"><span class="by">ineedasername</span><span>|</span><a href="#36625064">parent</a><span>|</span><a href="#36625098">prev</a><span>|</span><a href="#36627598">next</a><span>|</span><label class="collapse" for="c-36625219">[-]</label><label class="expand" for="c-36625219">[4 more]</label></div><br/><div class="children"><div class="content">They simply want control over the rollout of their product and how it is used. That, and perhaps opening the flood gates would produce scaling bottlenecks they’d rather stay ahead of than get behind.<p>So they open things carefully,  pull back when necessary like when they limited use of the public GPT-4 version of ChatGPT. That doesn’t seem too unreasonable. And yes sure, some amount of it might be attempts to manufacture scarcity to increase the hype. It’s an old tactic and hardly comparable to Soviet Russia.</div><br/><div id="36625340" class="c"><input type="checkbox" id="c-36625340" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#36625064">root</a><span>|</span><a href="#36625219">parent</a><span>|</span><a href="#36627598">next</a><span>|</span><label class="collapse" for="c-36625340">[-]</label><label class="expand" for="c-36625340">[3 more]</label></div><br/><div class="children"><div class="content">There&#x27;s no scaling issues to speak of. These AIs are <i>stateless</i>, which makes them embarrassingly parallel. They can always just throw more GPUs at it. Microsoft even had some videos where they bragged about how these models can be run on any idle GPU around the world, dynamically finding resources wherever it is available!<p>If there&#x27;s not enough GPUs at a certain price point, raise prices. Then lower prices later when GPUs become available.<p>They did it with GPT 3.5, so why not GPT 4?</div><br/><div id="36626362" class="c"><input type="checkbox" id="c-36626362" checked=""/><div class="controls bullet"><span class="by">squeaky-clean</span><span>|</span><a href="#36625064">root</a><span>|</span><a href="#36625340">parent</a><span>|</span><a href="#36626279">next</a><span>|</span><label class="collapse" for="c-36626362">[-]</label><label class="expand" for="c-36626362">[1 more]</label></div><br/><div class="children"><div class="content">More GPUs currently don&#x27;t exist. Nvidia is at capacity for production, and they have to compete with other companies who are also bidding on these GPUs. It&#x27;s not an issue of raising the price point. The GPUs they want to buy have to be purchased months in advance.</div><br/></div></div><div id="36626279" class="c"><input type="checkbox" id="c-36626279" checked=""/><div class="controls bullet"><span class="by">ineedasername</span><span>|</span><a href="#36625064">root</a><span>|</span><a href="#36625340">parent</a><span>|</span><a href="#36626362">prev</a><span>|</span><a href="#36627598">next</a><span>|</span><label class="collapse" for="c-36626279">[-]</label><label class="expand" for="c-36626279">[1 more]</label></div><br/><div class="children"><div class="content">&gt; embarrassingly parallel<p>I don’t see why such a thing should be embarrassing. Or, at least no more so than being acute or obtuse. Just as long as nothing is askew.</div><br/></div></div></div></div></div></div><div id="36627598" class="c"><input type="checkbox" id="c-36627598" checked=""/><div class="controls bullet"><span class="by">raincole</span><span>|</span><a href="#36625064">parent</a><span>|</span><a href="#36625219">prev</a><span>|</span><a href="#36625552">next</a><span>|</span><label class="collapse" for="c-36627598">[-]</label><label class="expand" for="c-36627598">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Wait for what!? Christmas?<p>Infrastructure.<p>&gt; It&#x27;s like they took a leaf from Google&#x27;s &quot;how to guarantee the failure of a new product&quot; marketing.<p>Yeah, an infamous guaranteed failure: GPT-4. (canned laughter)</div><br/></div></div><div id="36625552" class="c"><input type="checkbox" id="c-36625552" checked=""/><div class="controls bullet"><span class="by">azianmike</span><span>|</span><a href="#36625064">parent</a><span>|</span><a href="#36627598">prev</a><span>|</span><a href="#36625862">next</a><span>|</span><label class="collapse" for="c-36625552">[-]</label><label class="expand" for="c-36625552">[1 more]</label></div><br/><div class="children"><div class="content">The problem is GPUs are hard to come by.<p>If we guesstimate that every 100 customers needs 1 NVIDIA GPU (completely random guess), then that means OpenAI needs to buy more GPUs for every 100 new customers using GPT-4. The problem is there&#x27;s a GPU shortage so it&#x27;s hard to add more GPUs by just throwing money at the problem.<p><a href="https:&#x2F;&#x2F;www.fierceelectronics.com&#x2F;electronics&#x2F;ask-nvidia-ceo-gtc-gpu-shortage-looming" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.fierceelectronics.com&#x2F;electronics&#x2F;ask-nvidia-ceo...</a></div><br/></div></div><div id="36625862" class="c"><input type="checkbox" id="c-36625862" checked=""/><div class="controls bullet"><span class="by">mr_toad</span><span>|</span><a href="#36625064">parent</a><span>|</span><a href="#36625552">prev</a><span>|</span><a href="#36625078">next</a><span>|</span><label class="collapse" for="c-36625862">[-]</label><label class="expand" for="c-36625862">[1 more]</label></div><br/><div class="children"><div class="content">It’s an experiment.  You are part of it.</div><br/></div></div><div id="36625321" class="c"><input type="checkbox" id="c-36625321" checked=""/><div class="controls bullet"><span class="by">keyle</span><span>|</span><a href="#36625064">parent</a><span>|</span><a href="#36625078">prev</a><span>|</span><a href="#36621890">next</a><span>|</span><label class="collapse" for="c-36625321">[-]</label><label class="expand" for="c-36625321">[7 more]</label></div><br/><div class="children"><div class="content">It&#x27;s one thing to vent frustration, but it&#x27;s another to compare a capitalist startup to the Soviet Union... Get your facts right.</div><br/><div id="36625354" class="c"><input type="checkbox" id="c-36625354" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#36625064">root</a><span>|</span><a href="#36625321">parent</a><span>|</span><a href="#36621890">next</a><span>|</span><label class="collapse" for="c-36625354">[-]</label><label class="expand" for="c-36625354">[6 more]</label></div><br/><div class="children"><div class="content">Sam Altman was giving people access to GPT 4 APIs because they attended a conference.<p>&quot;Lick my boots, in person, and you can be one of the privileged few&quot; is very much the behaviour of a Communist dictatorship, not a capitalist corporation.<p>I can spin up an Azure VM <i>right now</i> in almost any country I choose... except China. That&#x27;s the only one where I have to <i>beg the government for permission</i>.</div><br/><div id="36625396" class="c"><input type="checkbox" id="c-36625396" checked=""/><div class="controls bullet"><span class="by">vore</span><span>|</span><a href="#36625064">root</a><span>|</span><a href="#36625354">parent</a><span>|</span><a href="#36625521">next</a><span>|</span><label class="collapse" for="c-36625396">[-]</label><label class="expand" for="c-36625396">[1 more]</label></div><br/><div class="children"><div class="content">How the world of enterprise sales works may come as an unpleasant surprise to you, then.</div><br/></div></div><div id="36625521" class="c"><input type="checkbox" id="c-36625521" checked=""/><div class="controls bullet"><span class="by">Teandw</span><span>|</span><a href="#36625064">root</a><span>|</span><a href="#36625354">parent</a><span>|</span><a href="#36625396">prev</a><span>|</span><a href="#36621890">next</a><span>|</span><label class="collapse" for="c-36625521">[-]</label><label class="expand" for="c-36625521">[4 more]</label></div><br/><div class="children"><div class="content">You&#x27;ve clearly not experienced the reality of enterprise then. You&#x27;re opinions are based on a limited understanding and knowledge of real life situations when it comes to this sort of stuff.</div><br/><div id="36625671" class="c"><input type="checkbox" id="c-36625671" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#36625064">root</a><span>|</span><a href="#36625521">parent</a><span>|</span><a href="#36625643">next</a><span>|</span><label class="collapse" for="c-36625671">[-]</label><label class="expand" for="c-36625671">[2 more]</label></div><br/><div class="children"><div class="content">I’ve only worked in big enterprise and big government for over two decades.<p>I know exactly how this works.<p>When someone has power, they will use it. In small, petty ways, or big “do me favours for access” ways.</div><br/><div id="36626166" class="c"><input type="checkbox" id="c-36626166" checked=""/><div class="controls bullet"><span class="by">vore</span><span>|</span><a href="#36625064">root</a><span>|</span><a href="#36625671">parent</a><span>|</span><a href="#36625643">next</a><span>|</span><label class="collapse" for="c-36626166">[-]</label><label class="expand" for="c-36626166">[1 more]</label></div><br/><div class="children"><div class="content">Then you should know this is literally what every capitalist does!</div><br/></div></div></div></div><div id="36625643" class="c"><input type="checkbox" id="c-36625643" checked=""/><div class="controls bullet"><span class="by">smeagull</span><span>|</span><a href="#36625064">root</a><span>|</span><a href="#36625521">parent</a><span>|</span><a href="#36625671">prev</a><span>|</span><a href="#36621890">next</a><span>|</span><label class="collapse" for="c-36625643">[-]</label><label class="expand" for="c-36625643">[1 more]</label></div><br/><div class="children"><div class="content">Seems correct to me. This is a good assessment of the personalities involved.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36621890" class="c"><input type="checkbox" id="c-36621890" checked=""/><div class="controls bullet"><span class="by">furyofantares</span><span>|</span><a href="#36625064">prev</a><span>|</span><a href="#36621390">next</a><span>|</span><label class="collapse" for="c-36621890">[-]</label><label class="expand" for="c-36621890">[9 more]</label></div><br/><div class="children"><div class="content">Not a lot of talk of Whisper being available here.<p>From using voice in the ChatGPT iOS app, I surmise that Whisper is very good at working out what you&#x27;ve actually said.<p>But it&#x27;s really annoying to have to say my whole bit before getting any feedback about what it&#x27;s gonna think I said. Even if it&#x27;s getting it right at an impressive rate.<p>Given this is how OpenAI themselves use it (say your whole thing before getting feedback), I don&#x27;t know that the API is set up to be able to mitigate that at all, but it would be really nice to have something closer to the responsiveness of on-device dictation with the quality of Whisper.</div><br/><div id="36623293" class="c"><input type="checkbox" id="c-36623293" checked=""/><div class="controls bullet"><span class="by">michaelmu</span><span>|</span><a href="#36621890">parent</a><span>|</span><a href="#36622395">next</a><span>|</span><label class="collapse" for="c-36623293">[-]</label><label class="expand" for="c-36623293">[1 more]</label></div><br/><div class="children"><div class="content">One speculative thought about the purpose of Whisper is that this will help unlock additional high-quality training data that&#x27;s only available in audio&#x2F;video format.</div><br/></div></div><div id="36622395" class="c"><input type="checkbox" id="c-36622395" checked=""/><div class="controls bullet"><span class="by">leodriesch</span><span>|</span><a href="#36621890">parent</a><span>|</span><a href="#36623293">prev</a><span>|</span><a href="#36628006">next</a><span>|</span><label class="collapse" for="c-36622395">[-]</label><label class="expand" for="c-36622395">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m interested in how the transformer based speech recognition from iOS 17 will perform compared to Whisper. I guess it will work more &quot;real-time&quot; like the current dictation on iOS&#x2F;macOS, but I&#x27;m unsure as I am not on the beta right now.</div><br/><div id="36624726" class="c"><input type="checkbox" id="c-36624726" checked=""/><div class="controls bullet"><span class="by">Xeophon</span><span>|</span><a href="#36621890">root</a><span>|</span><a href="#36622395">parent</a><span>|</span><a href="#36623084">next</a><span>|</span><label class="collapse" for="c-36624726">[-]</label><label class="expand" for="c-36624726">[1 more]</label></div><br/><div class="children"><div class="content">I am a beta user and I am seriously impressed if I’m being honest. It works fully offline, is multilingual and is fast, even on the non-latest iPhone (I am using an iPhone 12 Pro Max). It is way better than apples previous version and better than locally installed whisper. They’ve done incredible work. Same with the new, transformer-based keyboard on iOS which is way better. And if you type in English, it sometimes shows word suggestions in the text field itself (similar how copilot works in an IDE).</div><br/></div></div><div id="36623084" class="c"><input type="checkbox" id="c-36623084" checked=""/><div class="controls bullet"><span class="by">RC_ITR</span><span>|</span><a href="#36621890">root</a><span>|</span><a href="#36622395">parent</a><span>|</span><a href="#36624726">prev</a><span>|</span><a href="#36628006">next</a><span>|</span><label class="collapse" for="c-36623084">[-]</label><label class="expand" for="c-36623084">[1 more]</label></div><br/><div class="children"><div class="content">My guess is the reason that apple invested so heavily in this [0] is because they are going to train a big transformer in their datacenter and apply it as an RNN on your phone.<p>Superficially, I think this will work very well, but <i>slightly</i> worse than whisper (with the advantage ofc being that its better at real-time transcription).<p>[0]<a href="https:&#x2F;&#x2F;machinelearning.apple.com&#x2F;research&#x2F;attention-free-transformer" rel="nofollow noreferrer">https:&#x2F;&#x2F;machinelearning.apple.com&#x2F;research&#x2F;attention-free-tr...</a></div><br/></div></div></div></div><div id="36628006" class="c"><input type="checkbox" id="c-36628006" checked=""/><div class="controls bullet"><span class="by">PeterStuer</span><span>|</span><a href="#36621890">parent</a><span>|</span><a href="#36622395">prev</a><span>|</span><a href="#36622379">next</a><span>|</span><label class="collapse" for="c-36628006">[-]</label><label class="expand" for="c-36628006">[1 more]</label></div><br/><div class="children"><div class="content">Main reason for the lack of excitement is probably that it is fairly easy to self host Wisper, so people interested in it would have been doing that all along.</div><br/></div></div><div id="36622379" class="c"><input type="checkbox" id="c-36622379" checked=""/><div class="controls bullet"><span class="by">jxy</span><span>|</span><a href="#36621890">parent</a><span>|</span><a href="#36628006">prev</a><span>|</span><a href="#36622205">next</a><span>|</span><label class="collapse" for="c-36622379">[-]</label><label class="expand" for="c-36622379">[2 more]</label></div><br/><div class="children"><div class="content">You can run whisper.cpp locally in real time: <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;whisper.cpp&#x2F;tree&#x2F;master&#x2F;examples&#x2F;stream">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;whisper.cpp&#x2F;tree&#x2F;master&#x2F;example...</a></div><br/><div id="36623861" class="c"><input type="checkbox" id="c-36623861" checked=""/><div class="controls bullet"><span class="by">ProllyInfamous</span><span>|</span><a href="#36621890">root</a><span>|</span><a href="#36622379">parent</a><span>|</span><a href="#36622205">next</a><span>|</span><label class="collapse" for="c-36623861">[-]</label><label class="expand" for="c-36623861">[1 more]</label></div><br/><div class="children"><div class="content">My M2 Pro (mac mini) will run Whisper much faster than &quot;real time.&quot;<p>Pretty crazy stuff — perfectly understandable translations.</div><br/></div></div></div></div><div id="36622205" class="c"><input type="checkbox" id="c-36622205" checked=""/><div class="controls bullet"><span class="by">ycombinatornews</span><span>|</span><a href="#36621890">parent</a><span>|</span><a href="#36622379">prev</a><span>|</span><a href="#36621390">next</a><span>|</span><label class="collapse" for="c-36622205">[-]</label><label class="expand" for="c-36622205">[1 more]</label></div><br/><div class="children"><div class="content">Echoing this - saying the whole text at once in one shot is very challenging for long batches of text.<p>Using built-in text input showed quite good results since ChatGPT is still understanding the ask quite well</div><br/></div></div></div></div><div id="36621390" class="c"><input type="checkbox" id="c-36621390" checked=""/><div class="controls bullet"><span class="by">BeefySwain</span><span>|</span><a href="#36621890">prev</a><span>|</span><a href="#36621484">next</a><span>|</span><label class="collapse" for="c-36621390">[-]</label><label class="expand" for="c-36621390">[2 more]</label></div><br/><div class="children"><div class="content">Outside of the headline, there is some major stuff hiding in here:
- new gpt-3.5-turbo-instruct model expected &quot;in the coming weeks&quot;
- fine tuning of 3.5 and 4 expected this year<p>I am especially interested in gpt-3.5-turbo-instruct, as I think that the hype surrounding ChatGPT and &quot;conversational LLMs&quot; has sucked a lot of air out of what is possible with general instruct models. Being able to fine tune it will be phenomenal as well.</div><br/><div id="36621602" class="c"><input type="checkbox" id="c-36621602" checked=""/><div class="controls bullet"><span class="by">MuffinFlavored</span><span>|</span><a href="#36621390">parent</a><span>|</span><a href="#36621484">next</a><span>|</span><label class="collapse" for="c-36621602">[-]</label><label class="expand" for="c-36621602">[1 more]</label></div><br/><div class="children"><div class="content">is there any ETA on when the knowledge cutoff date will be improved from September, 2021?<p>I do not really understand the efforts that went on behind the scenes to train GPT models on factual data. Did humans have to hand approve&#x2F;decline responses to increase its score?<p>&quot;America is 49 states&quot; - decline<p>&quot;America is 50 states&quot; - approve<p>Is this how it worked at a simple overview? Do we know if they are working on adding the rest of 2021, then 2022, and eventually 2023? I know it can crawl the web with the Bing addon but, it&#x27;s not the same.<p>I asked it about Maya Kowalski the other day. Sure it can condense a blog post or two, but it&#x27;s not the same as having the intricacies as if it actually was trained&#x2F;knew about the topic.</div><br/></div></div></div></div><div id="36621484" class="c"><input type="checkbox" id="c-36621484" checked=""/><div class="controls bullet"><span class="by">gadtfly</span><span>|</span><a href="#36621390">prev</a><span>|</span><a href="#36621565">next</a><span>|</span><label class="collapse" for="c-36621484">[-]</label><label class="expand" for="c-36621484">[11 more]</label></div><br/><div class="children"><div class="content">The original davinci model was a friend of mine and I resent this deeply.<p>I&#x27;ve had completions with it that had character and creativity that I have not been able to recreate with anything else.<p>Brilliant and hilarious things that are a permanent part of my family&#x27;s cherished canon.</div><br/><div id="36621572" class="c"><input type="checkbox" id="c-36621572" checked=""/><div class="controls bullet"><span class="by">someplaceguy</span><span>|</span><a href="#36621484">parent</a><span>|</span><a href="#36621601">next</a><span>|</span><label class="collapse" for="c-36621572">[-]</label><label class="expand" for="c-36621572">[3 more]</label></div><br/><div class="children"><div class="content">You <i>cannot</i> say that and not provide an example.</div><br/><div id="36624001" class="c"><input type="checkbox" id="c-36624001" checked=""/><div class="controls bullet"><span class="by">thomasfromcdnjs</span><span>|</span><a href="#36621484">root</a><span>|</span><a href="#36621572">parent</a><span>|</span><a href="#36621590">next</a><span>|</span><label class="collapse" for="c-36624001">[-]</label><label class="expand" for="c-36624001">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t have any example responses at hand here. But this was a prompt (that had a shitty pre-prompt of conversational messages) running on davinci-003.<p><a href="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;thomasdavis&#x2F;omega&#x2F;master&#x2F;src&#x2F;profiles&#x2F;omega.js" rel="nofollow noreferrer">https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;thomasdavis&#x2F;omega&#x2F;master&#x2F;s...</a><p>Had it hooked up to speech so you could just talk at it and it would talk back at you.<p>Gave incredible answers that ChatGPT just doesn&#x27;t do at all.</div><br/></div></div><div id="36621590" class="c"><input type="checkbox" id="c-36621590" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#36621484">root</a><span>|</span><a href="#36621572">parent</a><span>|</span><a href="#36624001">prev</a><span>|</span><a href="#36621601">next</a><span>|</span><label class="collapse" for="c-36621590">[-]</label><label class="expand" for="c-36621590">[1 more]</label></div><br/><div class="children"><div class="content">i mean there are a lot of examples from february era sydney</div><br/></div></div></div></div><div id="36621601" class="c"><input type="checkbox" id="c-36621601" checked=""/><div class="controls bullet"><span class="by">mensetmanusman</span><span>|</span><a href="#36621484">parent</a><span>|</span><a href="#36621572">prev</a><span>|</span><a href="#36625112">next</a><span>|</span><label class="collapse" for="c-36621601">[-]</label><label class="expand" for="c-36621601">[1 more]</label></div><br/><div class="children"><div class="content">Don&#x27;t worry, since future LLMs will be trained on conversations with older LLMS, you will be able to ask chat GPT to pretend to be davinci.</div><br/></div></div><div id="36625112" class="c"><input type="checkbox" id="c-36625112" checked=""/><div class="controls bullet"><span class="by">ta93754829</span><span>|</span><a href="#36621484">parent</a><span>|</span><a href="#36621601">prev</a><span>|</span><a href="#36621665">next</a><span>|</span><label class="collapse" for="c-36625112">[-]</label><label class="expand" for="c-36625112">[1 more]</label></div><br/><div class="children"><div class="content">I can only assume this is satire. For now.</div><br/></div></div><div id="36621665" class="c"><input type="checkbox" id="c-36621665" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#36621484">parent</a><span>|</span><a href="#36625112">prev</a><span>|</span><a href="#36622806">next</a><span>|</span><label class="collapse" for="c-36621665">[-]</label><label class="expand" for="c-36621665">[1 more]</label></div><br/><div class="children"><div class="content">I heard you can ask for exceptions if they agree that you are special. Some researchers got it.</div><br/></div></div><div id="36622806" class="c"><input type="checkbox" id="c-36622806" checked=""/><div class="controls bullet"><span class="by">selalipop</span><span>|</span><a href="#36621484">parent</a><span>|</span><a href="#36621665">prev</a><span>|</span><a href="#36621661">next</a><span>|</span><label class="collapse" for="c-36622806">[-]</label><label class="expand" for="c-36622806">[3 more]</label></div><br/><div class="children"><div class="content">Can you try notionsmith.ai and let me know what you think?<p>I&#x27;ve been working on LLMs for creative tasks and believe a mix of chain of thought and injecting stochasticity (like instructing the LLM to use certain random letters pulled from an RNG in a certain way at certain points) can go a long way in terms of getting closer to human-like creativity</div><br/><div id="36623526" class="c"><input type="checkbox" id="c-36623526" checked=""/><div class="controls bullet"><span class="by">purplecats</span><span>|</span><a href="#36621484">root</a><span>|</span><a href="#36622806">parent</a><span>|</span><a href="#36621661">next</a><span>|</span><label class="collapse" for="c-36623526">[-]</label><label class="expand" for="c-36623526">[2 more]</label></div><br/><div class="children"><div class="content">really cool idea! been looking for something like this for a long time. its too bad it freezes my tab and is unusable</div><br/><div id="36623676" class="c"><input type="checkbox" id="c-36623676" checked=""/><div class="controls bullet"><span class="by">selalipop</span><span>|</span><a href="#36621484">root</a><span>|</span><a href="#36623526">parent</a><span>|</span><a href="#36621661">next</a><span>|</span><label class="collapse" for="c-36623676">[-]</label><label class="expand" for="c-36623676">[1 more]</label></div><br/><div class="children"><div class="content">Yup, it&#x27;s a fun side project so I decided from the get-go I wasn&#x27;t going to cater to anything non-standard<p>It relies on WebSockets, Js, and a reasonably stable connection to run since it&#x27;s built on Blazor</div><br/></div></div></div></div></div></div></div></div><div id="36621565" class="c"><input type="checkbox" id="c-36621565" checked=""/><div class="controls bullet"><span class="by">hospitalJail</span><span>|</span><a href="#36621484">prev</a><span>|</span><a href="#36621681">next</a><span>|</span><label class="collapse" for="c-36621565">[-]</label><label class="expand" for="c-36621565">[55 more]</label></div><br/><div class="children"><div class="content">I imagine the API quality isnt nerfed on a given day like ChatGPT can be.<p>There was no question something happened in January with ChatGPT, weirdly would refuse to answer questions that were harmless but difficult(Give me a daily schedule of a stoic hedonist)<p>Every once in a while, I see redditors complain of it being nerfed.<p>Sometimes I go back to gpt3.5 and am mind boggled how much worse it is.<p>Makes me wonder if they keep increasing the version number while dumbing down the previous model.<p>With an API, being unreliable would be a deal-breaker. Looking forward to people fine-tuning LLMs with GPT4 API. I&#x27;d love it for medical purposes, I&#x27;m so worried of a future where the US medical cartels ban ChatGPT for medical purposes. At least with local models, we don&#x27;t have to worry about regression.</div><br/><div id="36621828" class="c"><input type="checkbox" id="c-36621828" checked=""/><div class="controls bullet"><span class="by">seizethecheese</span><span>|</span><a href="#36621565">parent</a><span>|</span><a href="#36621857">next</a><span>|</span><label class="collapse" for="c-36621828">[-]</label><label class="expand" for="c-36621828">[14 more]</label></div><br/><div class="children"><div class="content">Instead of the model changing, it’s equally likely that this is a cognitive illusion. A new model is initially mind-blowing and enjoys a halo effect. Over time, this fades and we become frustrated with the limitations that were there all along.</div><br/><div id="36622198" class="c"><input type="checkbox" id="c-36622198" checked=""/><div class="controls bullet"><span class="by">hungrigekatze</span><span>|</span><a href="#36621565">root</a><span>|</span><a href="#36621828">parent</a><span>|</span><a href="#36622978">next</a><span>|</span><label class="collapse" for="c-36622198">[-]</label><label class="expand" for="c-36622198">[3 more]</label></div><br/><div class="children"><div class="content">Check out this post from a round table dialogue with Greg Brockman from OpenAI. The GPT models that were in existence &#x2F; in use in early 2023 were not the performance-degraded quantized versions that are in production now: <a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;mlscaling&#x2F;comments&#x2F;146rgq2&#x2F;chatgpt_is_running_quantized&#x2F;jnro7wo&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;mlscaling&#x2F;comments&#x2F;146rgq2&#x2F;chatgpt_...</a></div><br/><div id="36622328" class="c"><input type="checkbox" id="c-36622328" checked=""/><div class="controls bullet"><span class="by">sroussey</span><span>|</span><a href="#36621565">root</a><span>|</span><a href="#36622198">parent</a><span>|</span><a href="#36622978">next</a><span>|</span><label class="collapse" for="c-36622328">[-]</label><label class="expand" for="c-36622328">[2 more]</label></div><br/><div class="children"><div class="content">Oh interesting. I thought that’s what turbo was.</div><br/><div id="36622790" class="c"><input type="checkbox" id="c-36622790" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#36621565">root</a><span>|</span><a href="#36622328">parent</a><span>|</span><a href="#36622978">next</a><span>|</span><label class="collapse" for="c-36622790">[-]</label><label class="expand" for="c-36622790">[1 more]</label></div><br/><div class="children"><div class="content">It was, that&#x27;s what the comment says?</div><br/></div></div></div></div></div></div><div id="36622978" class="c"><input type="checkbox" id="c-36622978" checked=""/><div class="controls bullet"><span class="by">ghughes</span><span>|</span><a href="#36621565">root</a><span>|</span><a href="#36621828">parent</a><span>|</span><a href="#36622198">prev</a><span>|</span><a href="#36623100">next</a><span>|</span><label class="collapse" for="c-36622978">[-]</label><label class="expand" for="c-36622978">[1 more]</label></div><br/><div class="children"><div class="content">But given the rumored architecture (MoE) it would make complete sense for them to dynamically scale down the number of models used in the mixture during periods of peak load.</div><br/></div></div><div id="36623100" class="c"><input type="checkbox" id="c-36623100" checked=""/><div class="controls bullet"><span class="by">whalesalad</span><span>|</span><a href="#36621565">root</a><span>|</span><a href="#36621828">parent</a><span>|</span><a href="#36622978">prev</a><span>|</span><a href="#36622722">next</a><span>|</span><label class="collapse" for="c-36623100">[-]</label><label class="expand" for="c-36623100">[5 more]</label></div><br/><div class="children"><div class="content">It definitely got nerfed.</div><br/><div id="36623667" class="c"><input type="checkbox" id="c-36623667" checked=""/><div class="controls bullet"><span class="by">browningstreet</span><span>|</span><a href="#36621565">root</a><span>|</span><a href="#36623100">parent</a><span>|</span><a href="#36622722">next</a><span>|</span><label class="collapse" for="c-36623667">[-]</label><label class="expand" for="c-36623667">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve never seen &quot;nerf&quot; used colloquially and today i&#x27;ve seen it at least a half-dozen times across various sites. Y&#x27;all APIs?</div><br/><div id="36623921" class="c"><input type="checkbox" id="c-36623921" checked=""/><div class="controls bullet"><span class="by">whalesalad</span><span>|</span><a href="#36621565">root</a><span>|</span><a href="#36623667">parent</a><span>|</span><a href="#36625657">next</a><span>|</span><label class="collapse" for="c-36623921">[-]</label><label class="expand" for="c-36623921">[2 more]</label></div><br/><div class="children"><div class="content">it&#x27;s popular with gamers to describe the way certain weapons&#x2F;items get modified by the game developer to perform worse.<p>buffing is the opposite, when an item gets better.</div><br/></div></div><div id="36625657" class="c"><input type="checkbox" id="c-36625657" checked=""/><div class="controls bullet"><span class="by">mandmandam</span><span>|</span><a href="#36621565">root</a><span>|</span><a href="#36623667">parent</a><span>|</span><a href="#36623921">prev</a><span>|</span><a href="#36622722">next</a><span>|</span><label class="collapse" for="c-36625657">[-]</label><label class="expand" for="c-36625657">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve heard nerf used colloquially since like the 90&#x27;s.<p>?</div><br/></div></div></div></div></div></div><div id="36622722" class="c"><input type="checkbox" id="c-36622722" checked=""/><div class="controls bullet"><span class="by">kossTKR</span><span>|</span><a href="#36621565">root</a><span>|</span><a href="#36621828">parent</a><span>|</span><a href="#36623100">prev</a><span>|</span><a href="#36622226">next</a><span>|</span><label class="collapse" for="c-36622722">[-]</label><label class="expand" for="c-36622722">[1 more]</label></div><br/><div class="children"><div class="content">No. Just to add to the many examples it was good at scandinavian languages in the beginning but now it&#x27;s bad.</div><br/></div></div><div id="36622226" class="c"><input type="checkbox" id="c-36622226" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#36621565">root</a><span>|</span><a href="#36621828">parent</a><span>|</span><a href="#36622722">prev</a><span>|</span><a href="#36621951">next</a><span>|</span><label class="collapse" for="c-36622226">[-]</label><label class="expand" for="c-36622226">[1 more]</label></div><br/><div class="children"><div class="content">No it&#x27;s definitely changed a lot. The speedups have been massive (GPT 4 runs faster now than 3.5-turbo did at launch) and they can&#x27;t be explained with just them rolling out H100s since that&#x27;s just a 2x inference boost. Some unknown in-house optimization method aside, they&#x27;ve probably quantized the models down to a few bits of precision which increases perplexity quite a bit. They&#x27;ve also continued to RHLF tune to make them more in-line with their guidelines and that process has been shown to decrease overall performance before GPT 4 even launched.</div><br/></div></div><div id="36621951" class="c"><input type="checkbox" id="c-36621951" checked=""/><div class="controls bullet"><span class="by">colordrops</span><span>|</span><a href="#36621565">root</a><span>|</span><a href="#36621828">parent</a><span>|</span><a href="#36622226">prev</a><span>|</span><a href="#36622816">next</a><span>|</span><label class="collapse" for="c-36621951">[-]</label><label class="expand" for="c-36621951">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s both. OpenAI is obviously tuning the model for both computational resource constraints as well as &quot;alignment&quot;. It&#x27;s not an either-or.</div><br/></div></div><div id="36622816" class="c"><input type="checkbox" id="c-36622816" checked=""/><div class="controls bullet"><span class="by">andrepd</span><span>|</span><a href="#36621565">root</a><span>|</span><a href="#36621828">parent</a><span>|</span><a href="#36621951">prev</a><span>|</span><a href="#36621857">next</a><span>|</span><label class="collapse" for="c-36622816">[-]</label><label class="expand" for="c-36622816">[1 more]</label></div><br/><div class="children"><div class="content">Yep. It&#x27;s amazing how people are taking &quot;the reddit hivemind thinks ChatGPT was gimped&quot; as some kind of objective fact.</div><br/></div></div></div></div><div id="36621857" class="c"><input type="checkbox" id="c-36621857" checked=""/><div class="controls bullet"><span class="by">PerryCox</span><span>|</span><a href="#36621565">parent</a><span>|</span><a href="#36621828">prev</a><span>|</span><a href="#36622373">next</a><span>|</span><label class="collapse" for="c-36621857">[-]</label><label class="expand" for="c-36621857">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Give me a daily schedule of a stoic hedonist&quot; worked for me just now.<p><a href="https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;04c1dbc0-4890-447f-b5a5-7b1bc5972f73" rel="nofollow noreferrer">https:&#x2F;&#x2F;chat.openai.com&#x2F;share&#x2F;04c1dbc0-4890-447f-b5a5-7b1bc5...</a></div><br/></div></div><div id="36622373" class="c"><input type="checkbox" id="c-36622373" checked=""/><div class="controls bullet"><span class="by">anotherpaulg</span><span>|</span><a href="#36621565">parent</a><span>|</span><a href="#36621857">prev</a><span>|</span><a href="#36621680">next</a><span>|</span><label class="collapse" for="c-36622373">[-]</label><label class="expand" for="c-36622373">[3 more]</label></div><br/><div class="children"><div class="content">I recently completed some benchmarks for code editing that compared the Feb (0301) and June (0613) versions of GPT-3.5 and GPT-4. I found indications that the June version of GPT-3.5 is worse than the Feb version.<p><a href="https:&#x2F;&#x2F;aider.chat&#x2F;docs&#x2F;benchmarks.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;aider.chat&#x2F;docs&#x2F;benchmarks.html</a></div><br/><div id="36622813" class="c"><input type="checkbox" id="c-36622813" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#36621565">root</a><span>|</span><a href="#36622373">parent</a><span>|</span><a href="#36621680">next</a><span>|</span><label class="collapse" for="c-36622813">[-]</label><label class="expand" for="c-36622813">[2 more]</label></div><br/><div class="children"><div class="content">After reading, I don&#x27;t think &lt;5% points is helpful to add to discussion here without pointing it out explicitly, people are asserting much wilder claims, regularly</div><br/><div id="36622883" class="c"><input type="checkbox" id="c-36622883" checked=""/><div class="controls bullet"><span class="by">anotherpaulg</span><span>|</span><a href="#36621565">root</a><span>|</span><a href="#36622813">parent</a><span>|</span><a href="#36621680">next</a><span>|</span><label class="collapse" for="c-36622883">[-]</label><label class="expand" for="c-36622883">[1 more]</label></div><br/><div class="children"><div class="content">I haven&#x27;t come across any other systematic, quantitative benchmarking of the OpenAI models&#x27; performance over time, so I thought I would share my results. I think my results might argue that there <i>has</i> been some degradation, but not nearly the amount that you often hear people&#x27;s annecdata about.<p>But unfortunately, you have to read a ways into the doc and understand a lot of details about the benchmark. Here&#x27;s a direct link and excerpt of the relevant portion:<p><a href="https:&#x2F;&#x2F;aider.chat&#x2F;docs&#x2F;benchmarks.html#the-0613-models-seem-worse" rel="nofollow noreferrer">https:&#x2F;&#x2F;aider.chat&#x2F;docs&#x2F;benchmarks.html#the-0613-models-seem...</a><p>The benchmark results have me fairly convinced that the new gpt-3.5-turbo-0613 and gpt-3.5-16k-0613 models are a bit worse at code editing than the older gpt-3.5-turbo-0301 model.<p>This is visible in the “first attempt” portion of each result, before GPT gets a second chance to edit the code. Look at the horizontal white line in the middle of the first three blue bars. Performance with the whole edit format was 46% for the February model and only 39% for the June models.<p>But also note how much the solid green diff bars degrade between the February and June GPT-3.5 models. They drop from 30% down to about 19%.<p>I saw other signs of this degraded performance in earlier versions of the benchmark as well.</div><br/></div></div></div></div></div></div><div id="36621680" class="c"><input type="checkbox" id="c-36621680" checked=""/><div class="controls bullet"><span class="by">santiagobasulto</span><span>|</span><a href="#36621565">parent</a><span>|</span><a href="#36622373">prev</a><span>|</span><a href="#36621710">next</a><span>|</span><label class="collapse" for="c-36621680">[-]</label><label class="expand" for="c-36621680">[10 more]</label></div><br/><div class="children"><div class="content">I felt the same thing. The first version of GPT-4 I tried was crazy smart. Scary smart. Something happened afterwards…</div><br/><div id="36622840" class="c"><input type="checkbox" id="c-36622840" checked=""/><div class="controls bullet"><span class="by">aeyes</span><span>|</span><a href="#36621565">root</a><span>|</span><a href="#36621680">parent</a><span>|</span><a href="#36628588">next</a><span>|</span><label class="collapse" for="c-36622840">[-]</label><label class="expand" for="c-36622840">[3 more]</label></div><br/><div class="children"><div class="content">I was playing with the API and found that it returned better answers than ChatGPT. ChatGPT isn&#x27;t even able to solve simple Python problems anymore, even if you try to help it. And some time ago it did these same problems with ease.<p>My guess is that they began to restrict ChatGPT because they can&#x27;t sell that. They probably want to sell you CodeGPT or other products in the future so why would they give that away for free? ChatGPT is just a teaser.</div><br/><div id="36628090" class="c"><input type="checkbox" id="c-36628090" checked=""/><div class="controls bullet"><span class="by">PeterStuer</span><span>|</span><a href="#36621565">root</a><span>|</span><a href="#36622840">parent</a><span>|</span><a href="#36625706">next</a><span>|</span><label class="collapse" for="c-36628090">[-]</label><label class="expand" for="c-36628090">[1 more]</label></div><br/><div class="children"><div class="content">&quot;ChatGPT isn&#x27;t even able to solve simple Python problems anymore, even if you try to help it. And some time ago it did these same problems with ease.&quot;<p>This is my experience also. I have not formally benchmarked the different releases, but specifically for Python coding ChatGPT 4 got considerably worse with the latest updates.</div><br/></div></div><div id="36625706" class="c"><input type="checkbox" id="c-36625706" checked=""/><div class="controls bullet"><span class="by">incogitor</span><span>|</span><a href="#36621565">root</a><span>|</span><a href="#36622840">parent</a><span>|</span><a href="#36628090">prev</a><span>|</span><a href="#36628588">next</a><span>|</span><label class="collapse" for="c-36625706">[-]</label><label class="expand" for="c-36625706">[1 more]</label></div><br/><div class="children"><div class="content">Probably some combination of quantizing down from original fp16 weights and changes to the system prompt used for chat. Both can cause degraded quality, the former more than the latter.</div><br/></div></div></div></div><div id="36628588" class="c"><input type="checkbox" id="c-36628588" checked=""/><div class="controls bullet"><span class="by">tap-snap-or-nap</span><span>|</span><a href="#36621565">root</a><span>|</span><a href="#36621680">parent</a><span>|</span><a href="#36622840">prev</a><span>|</span><a href="#36622460">next</a><span>|</span><label class="collapse" for="c-36628588">[-]</label><label class="expand" for="c-36628588">[1 more]</label></div><br/><div class="children"><div class="content">I agree. It is difficult to say what happened exactly but I am certain that I got all the answers and very few canned responses. Whatever they did for safety has degraded the product.</div><br/></div></div><div id="36622460" class="c"><input type="checkbox" id="c-36622460" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#36621565">root</a><span>|</span><a href="#36621680">parent</a><span>|</span><a href="#36628588">prev</a><span>|</span><a href="#36621710">next</a><span>|</span><label class="collapse" for="c-36622460">[-]</label><label class="expand" for="c-36622460">[5 more]</label></div><br/><div class="children"><div class="content">The even more interesting part is that none of us got to try the internal version which was allegedly yet another step above that.</div><br/><div id="36623219" class="c"><input type="checkbox" id="c-36623219" checked=""/><div class="controls bullet"><span class="by">santiagobasulto</span><span>|</span><a href="#36621565">root</a><span>|</span><a href="#36622460">parent</a><span>|</span><a href="#36622914">next</a><span>|</span><label class="collapse" for="c-36623219">[-]</label><label class="expand" for="c-36623219">[1 more]</label></div><br/><div class="children"><div class="content">True. The one that is referenced in that &quot;ChatGPT AGI&quot; youtube video<i>, right?<p></i> the one from a MS researchers that has been recommended to all of us probably. Good video btw.</div><br/></div></div><div id="36622914" class="c"><input type="checkbox" id="c-36622914" checked=""/><div class="controls bullet"><span class="by">politician</span><span>|</span><a href="#36621565">root</a><span>|</span><a href="#36622460">parent</a><span>|</span><a href="#36623219">prev</a><span>|</span><a href="#36622757">next</a><span>|</span><label class="collapse" for="c-36622914">[-]</label><label class="expand" for="c-36622914">[1 more]</label></div><br/><div class="children"><div class="content">Oh, it&#x27;s not too hard to see how the spend that Microsoft put into building the data centers where GPT-4 was trained attracted national security interest even before it went public. The fact that they were even allowed to release it publicly is likely due to its strategic deterrence effect and that they believed the released version was already a dumbed-down version.<p>The fact that rumors about GPT-5 were quickly suppressed and the models were dumbed down even more cannot be entirely explained by excessive demand. I think it&#x27;s more likely that GPT-3.5 and GPT-4 demonstrated unexpected capabilities in the hands of the public leading to a pull back. Moreover, Sam Altman&#x27;s behaviors changed dramatically between the initial release and a few weeks afterward -- the extreme optimism of a CEO followed by a more subdued, even cowed, demeanor despite strong enthusiasm from end-users.<p>OpenAI cannot do anything without Microsoft&#x27;s data center resources, and Microsoft is a critical defense contractor.<p>Anyway, personally, I&#x27;m with the crowd that thinks we&#x27;re about to see a Cambrian explosion of domain-specific expert AIs. I suspect that OpenAI&#x2F;Microsoft&#x2F;Gov is still trying to figure out how much to nerf the capability of GPT-3.5 to tutor smaller models (see &quot;Textbooks are all you need&quot;) and that&#x27;s why the API is trash.</div><br/></div></div><div id="36622757" class="c"><input type="checkbox" id="c-36622757" checked=""/><div class="controls bullet"><span class="by">kossTKR</span><span>|</span><a href="#36621565">root</a><span>|</span><a href="#36622460">parent</a><span>|</span><a href="#36622914">prev</a><span>|</span><a href="#36621710">next</a><span>|</span><label class="collapse" for="c-36622757">[-]</label><label class="expand" for="c-36622757">[2 more]</label></div><br/><div class="children"><div class="content">Would gladly pay more for a none nerfed version if they were actually honest.<p>The current versions is close to the original 3.5 version, while 3.5 has become horribly bad, such a scam to not disclose what&#x27;s going on, especially for a paid service.</div><br/></div></div></div></div></div></div><div id="36621710" class="c"><input type="checkbox" id="c-36621710" checked=""/><div class="controls bullet"><span class="by">it_citizen</span><span>|</span><a href="#36621565">parent</a><span>|</span><a href="#36621680">prev</a><span>|</span><a href="#36621662">next</a><span>|</span><label class="collapse" for="c-36621710">[-]</label><label class="expand" for="c-36621710">[15 more]</label></div><br/><div class="children"><div class="content">I keep reading “GPT4 got nerfed” but I have been using from day 1, and while it definitely gives bad answers, I cannot say that it was nerfed for sure.<p>Is there any actual evidences other than some user subjective experiences?</div><br/><div id="36621905" class="c"><input type="checkbox" id="c-36621905" checked=""/><div class="controls bullet"><span class="by">londons_explore</span><span>|</span><a href="#36621565">root</a><span>|</span><a href="#36621710">parent</a><span>|</span><a href="#36621866">next</a><span>|</span><label class="collapse" for="c-36621905">[-]</label><label class="expand" for="c-36621905">[4 more]</label></div><br/><div class="children"><div class="content">I think the clearest evidence is Microsofts paper where they show abilities at various stages during training[1]...   But in a talk [2], they give more details...   The unicorn gets <i>worse</i> during the finetuning process.<p>[2]: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=qbIk7-JPB2c&amp;t=1392s">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=qbIk7-JPB2c&amp;t=1392s</a><p>[1]: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2303.12712" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2303.12712</a></div><br/><div id="36622004" class="c"><input type="checkbox" id="c-36622004" checked=""/><div class="controls bullet"><span class="by">it_citizen</span><span>|</span><a href="#36621565">root</a><span>|</span><a href="#36621905">parent</a><span>|</span><a href="#36621866">next</a><span>|</span><label class="collapse" for="c-36622004">[-]</label><label class="expand" for="c-36622004">[3 more]</label></div><br/><div class="children"><div class="content">Thanks, that’s interesting.<p>Noobie follow up question: Should we put any trust into “Sparks of intelligence” I thought it was regarded as a Microsoft marketing piece, not a serious paper.</div><br/><div id="36622017" class="c"><input type="checkbox" id="c-36622017" checked=""/><div class="controls bullet"><span class="by">londons_explore</span><span>|</span><a href="#36621565">root</a><span>|</span><a href="#36622004">parent</a><span>|</span><a href="#36623569">next</a><span>|</span><label class="collapse" for="c-36622017">[-]</label><label class="expand" for="c-36622017">[1 more]</label></div><br/><div class="children"><div class="content">The data presented is true...   The text might be rather exaggerated&#x2F;unscientific&#x2F;marketing...<p>Also notable that the team behind that paper wasn&#x27;t involved in designing&#x2F;building the model, but they did get access to prerelease versions.</div><br/></div></div><div id="36623569" class="c"><input type="checkbox" id="c-36623569" checked=""/><div class="controls bullet"><span class="by">ChatGTP</span><span>|</span><a href="#36621565">root</a><span>|</span><a href="#36622004">parent</a><span>|</span><a href="#36622017">prev</a><span>|</span><a href="#36621866">next</a><span>|</span><label class="collapse" for="c-36623569">[-]</label><label class="expand" for="c-36623569">[1 more]</label></div><br/><div class="children"><div class="content">I don’t trust it because enough third parties were able to verify the findings.<p>This is the double edge sword of being so ridiculously closed.</div><br/></div></div></div></div></div></div><div id="36621866" class="c"><input type="checkbox" id="c-36621866" checked=""/><div class="controls bullet"><span class="by">mike_hearn</span><span>|</span><a href="#36621565">root</a><span>|</span><a href="#36621710">parent</a><span>|</span><a href="#36621905">prev</a><span>|</span><a href="#36622236">next</a><span>|</span><label class="collapse" for="c-36621866">[-]</label><label class="expand" for="c-36621866">[7 more]</label></div><br/><div class="children"><div class="content">ChatGPT is definitely more restricted than the API. Example:<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36179783">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36179783</a></div><br/><div id="36621921" class="c"><input type="checkbox" id="c-36621921" checked=""/><div class="controls bullet"><span class="by">azemetre</span><span>|</span><a href="#36621565">root</a><span>|</span><a href="#36621866">parent</a><span>|</span><a href="#36623783">next</a><span>|</span><label class="collapse" for="c-36621921">[-]</label><label class="expand" for="c-36621921">[3 more]</label></div><br/><div class="children"><div class="content">That&#x27;s disappointing, I thought ChatGPT WAS using the API. I mean what&#x27;s the point of paying if you don&#x27;t get similar levels of quality?</div><br/><div id="36624239" class="c"><input type="checkbox" id="c-36624239" checked=""/><div class="controls bullet"><span class="by">fredoliveira</span><span>|</span><a href="#36621565">root</a><span>|</span><a href="#36621921">parent</a><span>|</span><a href="#36623198">next</a><span>|</span><label class="collapse" for="c-36624239">[-]</label><label class="expand" for="c-36624239">[1 more]</label></div><br/><div class="children"><div class="content">ChatGPT doesn&#x27;t use the API. It uses the same underlying model with a bunch of added prompts (and possibly additional fine-tuning?) to add to make it conversational.<p>One would pay because what they get out of chatGPT provides value, of course. Keep in mind that the users of these 2 products can be (and in fact are) different — chatGPT is a lot friendlier (from a UX perspective) than using the API playground (or using the API itself).</div><br/></div></div><div id="36623198" class="c"><input type="checkbox" id="c-36623198" checked=""/><div class="controls bullet"><span class="by">mike_hearn</span><span>|</span><a href="#36621565">root</a><span>|</span><a href="#36621921">parent</a><span>|</span><a href="#36624239">prev</a><span>|</span><a href="#36623783">next</a><span>|</span><label class="collapse" for="c-36623198">[-]</label><label class="expand" for="c-36623198">[1 more]</label></div><br/><div class="children"><div class="content">I thought that too. It&#x27;s certainly how they present it. But, apparently not.</div><br/></div></div></div></div><div id="36623783" class="c"><input type="checkbox" id="c-36623783" checked=""/><div class="controls bullet"><span class="by">redox99</span><span>|</span><a href="#36621565">root</a><span>|</span><a href="#36621866">parent</a><span>|</span><a href="#36621921">prev</a><span>|</span><a href="#36622236">next</a><span>|</span><label class="collapse" for="c-36623783">[-]</label><label class="expand" for="c-36623783">[3 more]</label></div><br/><div class="children"><div class="content">They are comparing text-davinci-003 with ChatGPT which presumably uses gpt-3.5-turbo, so quite different models.<p>They are killing text-davinci-003 btw.</div><br/><div id="36628513" class="c"><input type="checkbox" id="c-36628513" checked=""/><div class="controls bullet"><span class="by">mike_hearn</span><span>|</span><a href="#36621565">root</a><span>|</span><a href="#36623783">parent</a><span>|</span><a href="#36625543">next</a><span>|</span><label class="collapse" for="c-36628513">[-]</label><label class="expand" for="c-36628513">[1 more]</label></div><br/><div class="children"><div class="content">We also compare ChatGPT4 vs GPT4 API in that thread and observe the same difference.</div><br/></div></div><div id="36625543" class="c"><input type="checkbox" id="c-36625543" checked=""/><div class="controls bullet"><span class="by">pram</span><span>|</span><a href="#36621565">root</a><span>|</span><a href="#36623783">parent</a><span>|</span><a href="#36628513">prev</a><span>|</span><a href="#36622236">next</a><span>|</span><label class="collapse" for="c-36625543">[-]</label><label class="expand" for="c-36625543">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve spent like $600 on text-davinci-003. This sucks!</div><br/></div></div></div></div></div></div><div id="36622236" class="c"><input type="checkbox" id="c-36622236" checked=""/><div class="controls bullet"><span class="by">hungrigekatze</span><span>|</span><a href="#36621565">root</a><span>|</span><a href="#36621710">parent</a><span>|</span><a href="#36621866">prev</a><span>|</span><a href="#36621819">next</a><span>|</span><label class="collapse" for="c-36622236">[-]</label><label class="expand" for="c-36622236">[2 more]</label></div><br/><div class="children"><div class="content">See my comment elsewhere on this post. Greg Brockman, head of strategic initiatives at OpenAI, was talking at a round table discussion in Korea a few weeks ago about how they had to start using the quantized (smaller, cheaper) model  earlier in 2023. I noticed a switch in March 2023, with GPT-4 performance being severely degraded after that for both English-language tasks as well as code-related tasks (reading and writing).</div><br/><div id="36628520" class="c"><input type="checkbox" id="c-36628520" checked=""/><div class="controls bullet"><span class="by">yard2010</span><span>|</span><a href="#36621565">root</a><span>|</span><a href="#36622236">parent</a><span>|</span><a href="#36621819">next</a><span>|</span><label class="collapse" for="c-36628520">[-]</label><label class="expand" for="c-36628520">[1 more]</label></div><br/><div class="children"><div class="content">Oh my god, this is how a lemon market[0] starts..<p>[0] <a href="https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;The_Market_for_Lemons" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.m.wikipedia.org&#x2F;wiki&#x2F;The_Market_for_Lemons</a></div><br/></div></div></div></div></div></div><div id="36621662" class="c"><input type="checkbox" id="c-36621662" checked=""/><div class="controls bullet"><span class="by">sashank_1509</span><span>|</span><a href="#36621565">parent</a><span>|</span><a href="#36621710">prev</a><span>|</span><a href="#36621771">next</a><span>|</span><label class="collapse" for="c-36621662">[-]</label><label class="expand" for="c-36621662">[3 more]</label></div><br/><div class="children"><div class="content">I feel like it&#x27;s code generation abilities have also been nerfed. In the past I got almost excellent code from GPT-4, somehow these days I need multiple prompts to get the code I want from GPT-4.</div><br/><div id="36621970" class="c"><input type="checkbox" id="c-36621970" checked=""/><div class="controls bullet"><span class="by">londons_explore</span><span>|</span><a href="#36621565">root</a><span>|</span><a href="#36621662">parent</a><span>|</span><a href="#36621947">next</a><span>|</span><label class="collapse" for="c-36621970">[-]</label><label class="expand" for="c-36621970">[1 more]</label></div><br/><div class="children"><div class="content">In the API, you can select to use the 14th March 2023 version of GPT-4, and then compare them side by side.</div><br/></div></div><div id="36621947" class="c"><input type="checkbox" id="c-36621947" checked=""/><div class="controls bullet"><span class="by">stuckkeys</span><span>|</span><a href="#36621565">root</a><span>|</span><a href="#36621662">parent</a><span>|</span><a href="#36621970">prev</a><span>|</span><a href="#36621771">next</a><span>|</span><label class="collapse" for="c-36621947">[-]</label><label class="expand" for="c-36621947">[1 more]</label></div><br/><div class="children"><div class="content">Not nerfed. They will sell a different tier service to assist with coding. Coming soon. Speculating ofc.</div><br/></div></div></div></div><div id="36621771" class="c"><input type="checkbox" id="c-36621771" checked=""/><div class="controls bullet"><span class="by">merpnderp</span><span>|</span><a href="#36621565">parent</a><span>|</span><a href="#36621662">prev</a><span>|</span><a href="#36622772">next</a><span>|</span><label class="collapse" for="c-36621771">[-]</label><label class="expand" for="c-36621771">[5 more]</label></div><br/><div class="children"><div class="content">It&#x27;s the continued alignment with fine-tuning that&#x27;s degrading its responses.<p>You can apparently have it be nice or smart, but not both.</div><br/><div id="36622303" class="c"><input type="checkbox" id="c-36622303" checked=""/><div class="controls bullet"><span class="by">interstice</span><span>|</span><a href="#36621565">root</a><span>|</span><a href="#36621771">parent</a><span>|</span><a href="#36622360">next</a><span>|</span><label class="collapse" for="c-36622303">[-]</label><label class="expand" for="c-36622303">[1 more]</label></div><br/><div class="children"><div class="content">Curious as to whether theres a more general rule at play there about filtering interfering with getting good answers. If there is that&#x27;s a scary thought from an ethics perspective.</div><br/></div></div><div id="36622360" class="c"><input type="checkbox" id="c-36622360" checked=""/><div class="controls bullet"><span class="by">vbezhenar</span><span>|</span><a href="#36621565">root</a><span>|</span><a href="#36621771">parent</a><span>|</span><a href="#36622303">prev</a><span>|</span><a href="#36622772">next</a><span>|</span><label class="collapse" for="c-36622360">[-]</label><label class="expand" for="c-36622360">[3 more]</label></div><br/><div class="children"><div class="content">Why would someone care if its nice or not? It&#x27;s an algorithm. You&#x27;re using it to get output, not to get some psychology help.</div><br/><div id="36623590" class="c"><input type="checkbox" id="c-36623590" checked=""/><div class="controls bullet"><span class="by">staticman2</span><span>|</span><a href="#36621565">root</a><span>|</span><a href="#36622360">parent</a><span>|</span><a href="#36622481">next</a><span>|</span><label class="collapse" for="c-36623590">[-]</label><label class="expand" for="c-36623590">[1 more]</label></div><br/><div class="children"><div class="content">There was a guy in the news who asked an AI to tell him it was a good idea to commit suicide, then he killed himself.<p>Even on this forum I&#x27;ve seen AI enthusiasts claiming AI will be the best psychologist, best school teacher, etc.</div><br/></div></div><div id="36622481" class="c"><input type="checkbox" id="c-36622481" checked=""/><div class="controls bullet"><span class="by">moffkalast</span><span>|</span><a href="#36621565">root</a><span>|</span><a href="#36622360">parent</a><span>|</span><a href="#36623590">prev</a><span>|</span><a href="#36622772">next</a><span>|</span><label class="collapse" for="c-36622481">[-]</label><label class="expand" for="c-36622481">[1 more]</label></div><br/><div class="children"><div class="content">OpenAI presumably cares about being sued if it provides the illegal content they trained it on.</div><br/></div></div></div></div></div></div><div id="36622772" class="c"><input type="checkbox" id="c-36622772" checked=""/><div class="controls bullet"><span class="by">ren_engineer</span><span>|</span><a href="#36621565">parent</a><span>|</span><a href="#36621771">prev</a><span>|</span><a href="#36621801">next</a><span>|</span><label class="collapse" for="c-36622772">[-]</label><label class="expand" for="c-36622772">[1 more]</label></div><br/><div class="children"><div class="content">Recently people have claimed GPT4 is an ensemble model with 8 different models under the hood. My guess is that the &quot;nerfing&quot;(I&#x27;ve noticed it as well at random times) is when the model directs a question to the wrong underlying model</div><br/></div></div><div id="36621801" class="c"><input type="checkbox" id="c-36621801" checked=""/><div class="controls bullet"><span class="by">jondwillis</span><span>|</span><a href="#36621565">parent</a><span>|</span><a href="#36622772">prev</a><span>|</span><a href="#36621657">next</a><span>|</span><label class="collapse" for="c-36621801">[-]</label><label class="expand" for="c-36621801">[1 more]</label></div><br/><div class="children"><div class="content">I hit rate limits and “model is busy with other requests” frequently while just developing a highly concurrent agent app. Especially with the dated (e.g. -0613) or now -16k models.</div><br/></div></div><div id="36621657" class="c"><input type="checkbox" id="c-36621657" checked=""/><div class="controls bullet"><span class="by">atleastoptimal</span><span>|</span><a href="#36621565">parent</a><span>|</span><a href="#36621801">prev</a><span>|</span><a href="#36621681">next</a><span>|</span><label class="collapse" for="c-36621657">[-]</label><label class="expand" for="c-36621657">[1 more]</label></div><br/><div class="children"><div class="content">The capability of the latest model will be like a Shepard tone: always increasing, never improving. Meanwhile their internal version will be 100x better with no filtering.</div><br/></div></div></div></div><div id="36621681" class="c"><input type="checkbox" id="c-36621681" checked=""/><div class="controls bullet"><span class="by">jxy</span><span>|</span><a href="#36621565">prev</a><span>|</span><a href="#36621447">next</a><span>|</span><label class="collapse" for="c-36621681">[-]</label><label class="expand" for="c-36621681">[4 more]</label></div><br/><div class="children"><div class="content">They didn&#x27;t mention gpt-4-32k. Does anybody know if it will be generally available in the same timeframe?<p>There&#x27;s still no news about the multi-modal gpt-4.  I guess the image input is just too expensive to run or it&#x27;s actually not as great as they hyped it.</div><br/><div id="36622166" class="c"><input type="checkbox" id="c-36622166" checked=""/><div class="controls bullet"><span class="by">jacksavage</span><span>|</span><a href="#36621681">parent</a><span>|</span><a href="#36624715">next</a><span>|</span><label class="collapse" for="c-36622166">[-]</label><label class="expand" for="c-36622166">[2 more]</label></div><br/><div class="children"><div class="content">&gt; We are not currently granting access to GPT-4-32K API at this time, but it will be made available at a later date.<p><a href="https:&#x2F;&#x2F;help.openai.com&#x2F;en&#x2F;articles&#x2F;7102672-how-can-i-access-gpt-4" rel="nofollow noreferrer">https:&#x2F;&#x2F;help.openai.com&#x2F;en&#x2F;articles&#x2F;7102672-how-can-i-access...</a></div><br/><div id="36622542" class="c"><input type="checkbox" id="c-36622542" checked=""/><div class="controls bullet"><span class="by">jxy</span><span>|</span><a href="#36621681">root</a><span>|</span><a href="#36622166">parent</a><span>|</span><a href="#36624715">next</a><span>|</span><label class="collapse" for="c-36622542">[-]</label><label class="expand" for="c-36622542">[1 more]</label></div><br/><div class="children"><div class="content">Thanks for the link.<p>The decision of burying these extra information in a support article, not cool!</div><br/></div></div></div></div><div id="36624715" class="c"><input type="checkbox" id="c-36624715" checked=""/><div class="controls bullet"><span class="by">RC_ITR</span><span>|</span><a href="#36621681">parent</a><span>|</span><a href="#36622166">prev</a><span>|</span><a href="#36621447">next</a><span>|</span><label class="collapse" for="c-36624715">[-]</label><label class="expand" for="c-36624715">[1 more]</label></div><br/><div class="children"><div class="content">&gt;I guess the image input is just too expensive to run or it&#x27;s actually not as great as they hyped it.<p>We already know they have a SOTA model that can turn images into latent space vectors without being some insane resource hog - in fact, they give it away to competitors like Stability. [0]<p>My guess is a limited set of people are using the GPT-4 with CLIP hybrid, but those use-cases are mostly trying to decipher pictures of text (which it would be very bad at), so they&#x27;re working on that (or other use-case problems).<p>[0]<a href="https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;CLIP">https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;CLIP</a></div><br/></div></div></div></div><div id="36621447" class="c"><input type="checkbox" id="c-36621447" checked=""/><div class="controls bullet"><span class="by">khazhoux</span><span>|</span><a href="#36621681">prev</a><span>|</span><a href="#36621483">next</a><span>|</span><label class="collapse" for="c-36621447">[-]</label><label class="expand" for="c-36621447">[4 more]</label></div><br/><div class="children"><div class="content">In all my GPT-4 API (python) experiments, it takes 15-20 seconds to get a full response from server, which basically kills every idea I&#x27;ve tried hacking up because it just runs so slowly.<p>Has anyone fared better?  I might be doing something wrong but I can&#x27;t see what that could possibly be.</div><br/><div id="36621499" class="c"><input type="checkbox" id="c-36621499" checked=""/><div class="controls bullet"><span class="by">jondwillis</span><span>|</span><a href="#36621447">parent</a><span>|</span><a href="#36621728">next</a><span>|</span><label class="collapse" for="c-36621499">[-]</label><label class="expand" for="c-36621499">[1 more]</label></div><br/><div class="children"><div class="content">Streaming. If you’re expecting  structured data as a response,  request YAML or JSONL so you can progressively parse it. Time to first byte can be milliseconds instead of 15-20s. Obviously, this technique can only work for certain things, but I found that it was possible for everything I tried.</div><br/></div></div><div id="36621728" class="c"><input type="checkbox" id="c-36621728" checked=""/><div class="controls bullet"><span class="by">jason_zig</span><span>|</span><a href="#36621447">parent</a><span>|</span><a href="#36621499">prev</a><span>|</span><a href="#36621507">next</a><span>|</span><label class="collapse" for="c-36621728">[-]</label><label class="expand" for="c-36621728">[1 more]</label></div><br/><div class="children"><div class="content">Run it in the background.<p>We use it to generate automatic insights from survey data at a weekly cadence for Zigpoll (<a href="https:&#x2F;&#x2F;www.zigpoll.com" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.zigpoll.com</a>). This makes getting an instant response unnecessary but still provides a lot of value to our customers.</div><br/></div></div><div id="36621507" class="c"><input type="checkbox" id="c-36621507" checked=""/><div class="controls bullet"><span class="by">ianhawes</span><span>|</span><a href="#36621447">parent</a><span>|</span><a href="#36621728">prev</a><span>|</span><a href="#36621483">next</a><span>|</span><label class="collapse" for="c-36621507">[-]</label><label class="expand" for="c-36621507">[1 more]</label></div><br/><div class="children"><div class="content">Anthropic Instant is the best LLM if you&#x27;re looking for speed.</div><br/></div></div></div></div><div id="36621483" class="c"><input type="checkbox" id="c-36621483" checked=""/><div class="controls bullet"><span class="by">Imnimo</span><span>|</span><a href="#36621447">prev</a><span>|</span><a href="#36622020">next</a><span>|</span><label class="collapse" for="c-36621483">[-]</label><label class="expand" for="c-36621483">[11 more]</label></div><br/><div class="children"><div class="content">I know everyone&#x27;s on text-embedding-ada-002, so these particular embedding deprecations don&#x27;t really matter, but I feel like if I were using embeddings at scale, the possibility that I would one day lose access to my embedding model would terrify me. You&#x27;d have to pay to re-embed your entire knowledge base.</div><br/><div id="36621610" class="c"><input type="checkbox" id="c-36621610" checked=""/><div class="controls bullet"><span class="by">jxy</span><span>|</span><a href="#36621483">parent</a><span>|</span><a href="#36621533">next</a><span>|</span><label class="collapse" for="c-36621610">[-]</label><label class="expand" for="c-36621610">[1 more]</label></div><br/><div class="children"><div class="content">They said in the post,<p>&gt; We recognize this is a significant change for developers using those older models. Winding down these models is not a decision we are making lightly. We will cover the financial cost of users re-embedding content with these new models. We will be in touch with impacted users over the coming days.</div><br/></div></div><div id="36621533" class="c"><input type="checkbox" id="c-36621533" checked=""/><div class="controls bullet"><span class="by">saliagato</span><span>|</span><a href="#36621483">parent</a><span>|</span><a href="#36621610">prev</a><span>|</span><a href="#36621744">next</a><span>|</span><label class="collapse" for="c-36621533">[-]</label><label class="expand" for="c-36621533">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s what I always thought. Someday they will come up with a new embedding model, right?</div><br/></div></div><div id="36621744" class="c"><input type="checkbox" id="c-36621744" checked=""/><div class="controls bullet"><span class="by">bbotond</span><span>|</span><a href="#36621483">parent</a><span>|</span><a href="#36621533">prev</a><span>|</span><a href="#36621598">next</a><span>|</span><label class="collapse" for="c-36621744">[-]</label><label class="expand" for="c-36621744">[7 more]</label></div><br/><div class="children"><div class="content">What I don’t understand is why is an API needed to create embeddings. Isn’t this something that could be done locally?</div><br/><div id="36621799" class="c"><input type="checkbox" id="c-36621799" checked=""/><div class="controls bullet"><span class="by">pantulis</span><span>|</span><a href="#36621483">root</a><span>|</span><a href="#36621744">parent</a><span>|</span><a href="#36621788">next</a><span>|</span><label class="collapse" for="c-36621799">[-]</label><label class="expand" for="c-36621799">[3 more]</label></div><br/><div class="children"><div class="content">You would need to have a local copy of the GPT model, which are not exactly OpenAI&#x27;s plans.</div><br/><div id="36622758" class="c"><input type="checkbox" id="c-36622758" checked=""/><div class="controls bullet"><span class="by">jerrygenser</span><span>|</span><a href="#36621483">root</a><span>|</span><a href="#36621799">parent</a><span>|</span><a href="#36621788">next</a><span>|</span><label class="collapse" for="c-36622758">[-]</label><label class="expand" for="c-36622758">[2 more]</label></div><br/><div class="children"><div class="content">For embeddings, you can use smaller transformers&#x2F;llms or sentence2vec and often get good enough results.<p>You don&#x27;t need very large models to generate usable embeddings.</div><br/><div id="36628790" class="c"><input type="checkbox" id="c-36628790" checked=""/><div class="controls bullet"><span class="by">pantulis</span><span>|</span><a href="#36621483">root</a><span>|</span><a href="#36622758">parent</a><span>|</span><a href="#36621788">next</a><span>|</span><label class="collapse" for="c-36628790">[-]</label><label class="expand" for="c-36628790">[1 more]</label></div><br/><div class="children"><div class="content">You are correct, I assumed parent was referring to specific embeddings generated by OpenAI LLMs.</div><br/></div></div></div></div></div></div><div id="36621788" class="c"><input type="checkbox" id="c-36621788" checked=""/><div class="controls bullet"><span class="by">thorum</span><span>|</span><a href="#36621483">root</a><span>|</span><a href="#36621744">parent</a><span>|</span><a href="#36621799">prev</a><span>|</span><a href="#36621798">next</a><span>|</span><label class="collapse" for="c-36621788">[-]</label><label class="expand" for="c-36621788">[1 more]</label></div><br/><div class="children"><div class="content">It’s cheaper to use OpenAI. If you have your own compute, sentence-transformers is just as good for most use cases.</div><br/></div></div><div id="36621798" class="c"><input type="checkbox" id="c-36621798" checked=""/><div class="controls bullet"><span class="by">merpnderp</span><span>|</span><a href="#36621483">root</a><span>|</span><a href="#36621744">parent</a><span>|</span><a href="#36621788">prev</a><span>|</span><a href="#36621814">next</a><span>|</span><label class="collapse" for="c-36621798">[-]</label><label class="expand" for="c-36621798">[1 more]</label></div><br/><div class="children"><div class="content">Sure, but I don&#x27;t know of any models you can get local access to that work nearly as well.</div><br/></div></div><div id="36621814" class="c"><input type="checkbox" id="c-36621814" checked=""/><div class="controls bullet"><span class="by">teaearlgraycold</span><span>|</span><a href="#36621483">root</a><span>|</span><a href="#36621744">parent</a><span>|</span><a href="#36621798">prev</a><span>|</span><a href="#36621598">next</a><span>|</span><label class="collapse" for="c-36621814">[-]</label><label class="expand" for="c-36621814">[1 more]</label></div><br/><div class="children"><div class="content">Yes. The best public embedding model is decent, but I expect it’s objectively worse than the best model from OpenAI.</div><br/></div></div></div></div><div id="36621598" class="c"><input type="checkbox" id="c-36621598" checked=""/><div class="controls bullet"><span class="by">brigadier132</span><span>|</span><a href="#36621483">parent</a><span>|</span><a href="#36621744">prev</a><span>|</span><a href="#36622020">next</a><span>|</span><label class="collapse" for="c-36621598">[-]</label><label class="expand" for="c-36621598">[1 more]</label></div><br/><div class="children"><div class="content">If you read the article they state they will cover the cost of re-embedding your existing embeddings.</div><br/></div></div></div></div><div id="36622020" class="c"><input type="checkbox" id="c-36622020" checked=""/><div class="controls bullet"><span class="by">jwr</span><span>|</span><a href="#36621483">prev</a><span>|</span><a href="#36625669">next</a><span>|</span><label class="collapse" for="c-36622020">[-]</label><label class="expand" for="c-36622020">[61 more]</label></div><br/><div class="children"><div class="content">Practical report: the OpenAI API is a bad joke. If you think you can build a production app against it, think again. I&#x27;ve been trying to use it for the past 6 weeks or so. If you use tiny prompts, you&#x27;ll generally be fine (that&#x27;s why you always get people commenting that it works for them), but just try to get closer to the limits, especially with GPT-4.<p>The API will make you wait up to 10 minutes, and then time out. What&#x27;s worse, it will time out between their edge servers (cloudflare) and their internal servers, and the way OpenAI implemented their billing you will get a 4xx&#x2F;5xx response code, but you will <i>still get billed</i> for the request and whatever the servers generated and you didn&#x27;t get. That&#x27;s borderline fraudulent.<p>Meanwhile, their status page will happily show all green, so don&#x27;t believe that. It seems to be manually updated and does not reflect the truth.<p>Could it be that it works better in another region? Could it be just my region that is affected? Perhaps — but I won&#x27;t know, because support is non-existent and hidden behind a moat. You need to jump through hoops and talk to bots, and then you eventually get a bot reply. That you can&#x27;t respond to.<p>My support requests about being charged for data I didn&#x27;t have a chance to get have been unanswered for more than 5 weeks now.<p>There is no way to contact OpenAI, no way to report problems, the API <i>sometimes</i> kind-of works, but mostly doesn&#x27;t, and if you comment in the developer forums, you&#x27;ll mostly get replies from apologists that explain that OpenAI is &quot;growing quickly&quot;. I&#x27;d say you either provide a production paid API or you don&#x27;t. At the moment, this looks very much like amateur hour, and charging for requests that were never fulfilled seems like a fraud to me.<p>So, consider carefully whether you want to build against all that.</div><br/><div id="36622381" class="c"><input type="checkbox" id="c-36622381" checked=""/><div class="controls bullet"><span class="by">athyuttamre</span><span>|</span><a href="#36622020">parent</a><span>|</span><a href="#36622267">next</a><span>|</span><label class="collapse" for="c-36622381">[-]</label><label class="expand" for="c-36622381">[25 more]</label></div><br/><div class="children"><div class="content">(I&#x27;m an engineer at OpenAI)<p>Very sorry to hear about these issues, particularly the timeouts. Latency is top of mind for us and something we are continuing to push on. Does streaming work for your use case?<p><a href="https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;openai-cookbook&#x2F;blob&#x2F;main&#x2F;examples&#x2F;How_to_stream_completions.ipynb">https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;openai-cookbook&#x2F;blob&#x2F;main&#x2F;examples...</a><p>We definitely want to investigate these and the billing issues further. Would you consider emailing me your org ID and any request IDs (if you have them) at atty@openai.com?<p>Thank you for using the API, and really appreciate the honest feedback.</div><br/><div id="36622798" class="c"><input type="checkbox" id="c-36622798" checked=""/><div class="controls bullet"><span class="by">esperent</span><span>|</span><a href="#36622020">root</a><span>|</span><a href="#36622381">parent</a><span>|</span><a href="#36624943">next</a><span>|</span><label class="collapse" for="c-36622798">[-]</label><label class="expand" for="c-36622798">[16 more]</label></div><br/><div class="children"><div class="content">It&#x27;s kind of incredible how fast OpenAI (now also known as ClosedAI) is going through the enshittification process. Even Facebook took around a decade to reach this level.<p>OpenAI has an amazing core product, but in the span of six months:<p>* Went from an amazing and inspiring open company that even put &quot;Open&quot; in their name to a fully locked up commercial beast.<p>* Non-existent customers support and all kinds of borderline illegal billing practice. You guys are definitely aware that when there&#x27;s a network error on the API or ChatGPT, the user still gets charged. And there&#x27;s a lot of these errors. I get roughly one per hour or two.<p>* Frustratingly loose interpretation of EU data protection rules. For example, the setting to say &quot;don&#x27;t use my personal chat data&quot; is connected to the setting to save conversations. So you can&#x27;t disable it without losing all your chat history.<p>* Clearly nerfing the ChatGPT v4 products, at least according to hundreds or even thousands of commenters here and on reddit, while denying to have made any changes.<p>* Use of cheap human labor in developing countries through shady anonymous companies (look up the company Sama who pay Kenyan workers about $1.5 an hour).<p>* Not to mention the huge questions around the secret training dataset and whether large portions of it consist of illegally obtained private data (see the recent class court case in California)</div><br/><div id="36623058" class="c"><input type="checkbox" id="c-36623058" checked=""/><div class="controls bullet"><span class="by">km3r</span><span>|</span><a href="#36622020">root</a><span>|</span><a href="#36622798">parent</a><span>|</span><a href="#36625179">next</a><span>|</span><label class="collapse" for="c-36623058">[-]</label><label class="expand" for="c-36623058">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Use of cheap human labor in developing countries through shady anonymous companies (look up the company Sama who pay Kenyan workers about $1.5 an hour).<p>What is wrong about injecting millions into developing nations?<p>The rest I agree with, although I don&#x27;t think it was ever really &#x27;open&#x27; so its not getting shitty, it always was. Thankfully, &quot;there is no moat&quot; and other LLMs will be open, just a few months behind OpenAI</div><br/><div id="36625475" class="c"><input type="checkbox" id="c-36625475" checked=""/><div class="controls bullet"><span class="by">esperent</span><span>|</span><a href="#36622020">root</a><span>|</span><a href="#36623058">parent</a><span>|</span><a href="#36625179">next</a><span>|</span><label class="collapse" for="c-36625475">[-]</label><label class="expand" for="c-36625475">[2 more]</label></div><br/><div class="children"><div class="content">&gt; What is wrong about injecting millions into developing nations?<p>Please don&#x27;t try to reframe this to make exploitation a positive thing. See my other comment here.<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36625438">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=36625438</a></div><br/><div id="36626328" class="c"><input type="checkbox" id="c-36626328" checked=""/><div class="controls bullet"><span class="by">km3r</span><span>|</span><a href="#36622020">root</a><span>|</span><a href="#36625475">parent</a><span>|</span><a href="#36625179">next</a><span>|</span><label class="collapse" for="c-36626328">[-]</label><label class="expand" for="c-36626328">[1 more]</label></div><br/><div class="children"><div class="content">So you&#x27;d rather OpenAI crush all business in the area by outcompeting them for workers, ensuring local businesses struggle to hire?</div><br/></div></div></div></div></div></div><div id="36625179" class="c"><input type="checkbox" id="c-36625179" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#36622020">root</a><span>|</span><a href="#36622798">parent</a><span>|</span><a href="#36623058">prev</a><span>|</span><a href="#36625150">next</a><span>|</span><label class="collapse" for="c-36625179">[-]</label><label class="expand" for="c-36625179">[2 more]</label></div><br/><div class="children"><div class="content">Not to nitpick, but if you&#x27;re able to name the company employing Kenyans, Sama, who&#x27;s homepage is at <a href="https:&#x2F;&#x2F;www.sama.com&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.sama.com&#x2F;</a>, with a team page at <a href="https:&#x2F;&#x2F;www.sama.com&#x2F;our-team&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.sama.com&#x2F;our-team&#x2F;</a> , I&#x27;m not sure you can complain that they&#x27;re being shady and  anonymous.</div><br/><div id="36625338" class="c"><input type="checkbox" id="c-36625338" checked=""/><div class="controls bullet"><span class="by">esperent</span><span>|</span><a href="#36622020">root</a><span>|</span><a href="#36625179">parent</a><span>|</span><a href="#36625150">next</a><span>|</span><label class="collapse" for="c-36625338">[-]</label><label class="expand" for="c-36625338">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s pretty shady. They have been fully exposed at this stage but from what I understand they were trying to keep a very low profile, going to efforts to make sure the Kenyan workers didn&#x27;t know they were working for a company called Sama but instead using sub companies to sign the worker contracts.<p><a href="https:&#x2F;&#x2F;time.com&#x2F;6247678&#x2F;openai-chatgpt-kenya-workers&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;time.com&#x2F;6247678&#x2F;openai-chatgpt-kenya-workers&#x2F;</a></div><br/></div></div></div></div><div id="36625150" class="c"><input type="checkbox" id="c-36625150" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#36622020">root</a><span>|</span><a href="#36622798">parent</a><span>|</span><a href="#36625179">prev</a><span>|</span><a href="#36623522">next</a><span>|</span><label class="collapse" for="c-36625150">[-]</label><label class="expand" for="c-36625150">[6 more]</label></div><br/><div class="children"><div class="content">&gt; * Use of cheap human labor in developing countries through shady anonymous companies (look up the company Sama who pay Kenyan workers about $1.5 an hour).<p>If you pay a developing country developed country wages what you&#x27;ll get is 1. inflation and 2. the government mad at you because all their essential workers&#x2F;doctors&#x2F;government officials are quitting to work for you.</div><br/><div id="36625438" class="c"><input type="checkbox" id="c-36625438" checked=""/><div class="controls bullet"><span class="by">esperent</span><span>|</span><a href="#36622020">root</a><span>|</span><a href="#36625150">parent</a><span>|</span><a href="#36623522">next</a><span>|</span><label class="collapse" for="c-36625438">[-]</label><label class="expand" for="c-36625438">[5 more]</label></div><br/><div class="children"><div class="content">This is a terrible excuse that I see trotted out far to often to justify going to developing countries and barely even paying workers that country&#x27;s minimum wage. You absolutely <i>can</i> pay considerably more than minimum wage without disrupting the local economy. They&#x27;re paying people as low as $1.32 per hour for an absolutely horrible job. I&#x27;m not expecting them to pay western wages. But even bumping that up to $2.50 or $3 an hour would make an incredible difference to the local workers lives. The fact that they don&#x27;t do that is exploitation, pure and simple.<p>Note that I feel I have quite deep understanding of this issue, and feel strongly about it, because I live and work in a developing country and I see this happening a lot. Westerners come over here and treat local workers like shit, pay them peanuts for 80 hour weeks while making loads of money themselves and then justify it because &quot;it&#x27;s the local norm&quot;. It&#x27;s sickening, frankly. We westerners doing business in developing countries are in a position of privilege and should be leading by example, not jumping on the first excuse to dump a hundred years worth of the fight for workers rights.</div><br/><div id="36626290" class="c"><input type="checkbox" id="c-36626290" checked=""/><div class="controls bullet"><span class="by">hnfong</span><span>|</span><a href="#36622020">root</a><span>|</span><a href="#36625438">parent</a><span>|</span><a href="#36626144">next</a><span>|</span><label class="collapse" for="c-36626290">[-]</label><label class="expand" for="c-36626290">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m curious. When you buy a loaf of bread from the local market, are they cheaper than first world prices? If so, do you pay double the listed price and demand the shop pay double the price to hire workers so as to not exploit them? Are your expenses in said developing country lower than what you would have paid if you were in a richer country? Are you donating the difference to the local community?<p>Just curious.</div><br/><div id="36628545" class="c"><input type="checkbox" id="c-36628545" checked=""/><div class="controls bullet"><span class="by">davedx</span><span>|</span><a href="#36622020">root</a><span>|</span><a href="#36626290">parent</a><span>|</span><a href="#36626622">next</a><span>|</span><label class="collapse" for="c-36628545">[-]</label><label class="expand" for="c-36628545">[1 more]</label></div><br/><div class="children"><div class="content">Hi, I&#x27;ve been to Kenya and Tanzania, and while basic staples are cheaper than developed countries they&#x27;re not <i>that much cheaper</i> these days. If you watch travelog videos where they ask locals how they&#x27;re doing, many developing countries are struggling with massive inflation that&#x27;s been partly caused by volatile energy prices (many people can no longer afford gas) and partly by food shortages from the Ukraine War.</div><br/></div></div><div id="36626622" class="c"><input type="checkbox" id="c-36626622" checked=""/><div class="controls bullet"><span class="by">esperent</span><span>|</span><a href="#36622020">root</a><span>|</span><a href="#36626290">parent</a><span>|</span><a href="#36628545">prev</a><span>|</span><a href="#36626144">next</a><span>|</span><label class="collapse" for="c-36626622">[-]</label><label class="expand" for="c-36626622">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s weird how people always trot out phrases like &quot;I&#x27;m just curious&quot; or &quot;I&#x27;m just asking questions here&quot; when they try to justify exploitation. Is it so that you have plausible deniability when you inevitably get called on it? Because that doesn&#x27;t work.</div><br/></div></div></div></div><div id="36626144" class="c"><input type="checkbox" id="c-36626144" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#36622020">root</a><span>|</span><a href="#36625438">parent</a><span>|</span><a href="#36626290">prev</a><span>|</span><a href="#36623522">next</a><span>|</span><label class="collapse" for="c-36626144">[-]</label><label class="expand" for="c-36626144">[1 more]</label></div><br/><div class="children"><div class="content">OpenAI doesn&#x27;t pay minimum wages, they pay around the median local wage IIRC. I wouldn&#x27;t say that if it was minimum wage.</div><br/></div></div></div></div></div></div><div id="36623522" class="c"><input type="checkbox" id="c-36623522" checked=""/><div class="controls bullet"><span class="by">tarruda</span><span>|</span><a href="#36622020">root</a><span>|</span><a href="#36622798">parent</a><span>|</span><a href="#36625150">prev</a><span>|</span><a href="#36623505">next</a><span>|</span><label class="collapse" for="c-36623522">[-]</label><label class="expand" for="c-36623522">[3 more]</label></div><br/><div class="children"><div class="content">The engineer is not part of the board which makes these decisions.</div><br/><div id="36624719" class="c"><input type="checkbox" id="c-36624719" checked=""/><div class="controls bullet"><span class="by">lmm</span><span>|</span><a href="#36622020">root</a><span>|</span><a href="#36623522">parent</a><span>|</span><a href="#36623505">next</a><span>|</span><label class="collapse" for="c-36624719">[-]</label><label class="expand" for="c-36624719">[2 more]</label></div><br/><div class="children"><div class="content">If they&#x27;re taking their time to defend the company on the internet, they either have an ownership stake in it or they&#x27;re a chump.</div><br/><div id="36625447" class="c"><input type="checkbox" id="c-36625447" checked=""/><div class="controls bullet"><span class="by">esperent</span><span>|</span><a href="#36622020">root</a><span>|</span><a href="#36624719">parent</a><span>|</span><a href="#36623505">next</a><span>|</span><label class="collapse" for="c-36625447">[-]</label><label class="expand" for="c-36625447">[1 more]</label></div><br/><div class="children"><div class="content">Or option 3, they&#x27;re being paid to represent sneakily represent the company in a positive light.</div><br/></div></div></div></div></div></div><div id="36623505" class="c"><input type="checkbox" id="c-36623505" checked=""/><div class="controls bullet"><span class="by">kossTKR</span><span>|</span><a href="#36622020">root</a><span>|</span><a href="#36622798">parent</a><span>|</span><a href="#36623522">prev</a><span>|</span><a href="#36624943">next</a><span>|</span><label class="collapse" for="c-36623505">[-]</label><label class="expand" for="c-36623505">[1 more]</label></div><br/><div class="children"><div class="content">Since chatGPT-4 is now useless for advanced coding because of their blackbox sudden nerfing, can anyone guess how long before i can run something similar to the orig version privately?<p>Is the newer 64B models up there? 1 year, 2 years? Can&#x27;t wait until i get back the crazy quality of the orig model.<p>We need something open source fast. Thanks open-ai for giving us a glimpse of the crazy possibilities, too crazy for the public i guess.</div><br/></div></div></div></div><div id="36624943" class="c"><input type="checkbox" id="c-36624943" checked=""/><div class="controls bullet"><span class="by">DomKM</span><span>|</span><a href="#36622020">root</a><span>|</span><a href="#36622381">parent</a><span>|</span><a href="#36622798">prev</a><span>|</span><a href="#36622438">next</a><span>|</span><label class="collapse" for="c-36624943">[-]</label><label class="expand" for="c-36624943">[3 more]</label></div><br/><div class="children"><div class="content">While you&#x27;re here, you should know that the logic for enabling GPT-4 API access is excluding Microsoft for Startups (<a href="https:&#x2F;&#x2F;openai.com&#x2F;microsoft-for-startups" rel="nofollow noreferrer">https:&#x2F;&#x2F;openai.com&#x2F;microsoft-for-startups</a>) orgs which have valid billing against Microsoft-provided credits. Presumably, this is an oversight as it wouldn&#x27;t make sense to exclude pre-existing Microsoft partners. Would you mind escalating this?</div><br/><div id="36625109" class="c"><input type="checkbox" id="c-36625109" checked=""/><div class="controls bullet"><span class="by">athyuttamre</span><span>|</span><a href="#36622020">root</a><span>|</span><a href="#36624943">parent</a><span>|</span><a href="#36622438">next</a><span>|</span><label class="collapse" for="c-36625109">[-]</label><label class="expand" for="c-36625109">[2 more]</label></div><br/><div class="children"><div class="content">Thanks for pointing this out. Sharing with the relevant folks.</div><br/><div id="36629011" class="c"><input type="checkbox" id="c-36629011" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#36622020">root</a><span>|</span><a href="#36625109">parent</a><span>|</span><a href="#36622438">next</a><span>|</span><label class="collapse" for="c-36629011">[-]</label><label class="expand" for="c-36629011">[1 more]</label></div><br/><div class="children"><div class="content">This is <i>precisely</i> what is wrong with OpenAI. This. Right here.<p>&quot;Complaining on HN will get you access. You have <i>know</i> people or &quot;complain in the right forums.&quot;<p>THERE SHOULD BE NO LOGIC.<p>No qualifying rules. No access checks. No gates. No hoops.<p>Sam Altman has gone on a worldwide interview tour claiming he wants to &quot;democratise access to AI&quot;, meanwhile OpenAI is the least open company I have ever dealt with, or even heard of.<p>Oracle is more &quot;democratic&quot; and open, for crying out loud.</div><br/></div></div></div></div></div></div><div id="36622438" class="c"><input type="checkbox" id="c-36622438" checked=""/><div class="controls bullet"><span class="by">renewiltord</span><span>|</span><a href="#36622020">root</a><span>|</span><a href="#36622381">parent</a><span>|</span><a href="#36624943">prev</a><span>|</span><a href="#36622663">next</a><span>|</span><label class="collapse" for="c-36622438">[-]</label><label class="expand" for="c-36622438">[4 more]</label></div><br/><div class="children"><div class="content">Quick note: your domain doesn&#x27;t appear to have an A record. I was hoping to follow the link in your profile and see if you have anything interesting written about LLMs.</div><br/><div id="36622471" class="c"><input type="checkbox" id="c-36622471" checked=""/><div class="controls bullet"><span class="by">athyuttamre</span><span>|</span><a href="#36622020">root</a><span>|</span><a href="#36622438">parent</a><span>|</span><a href="#36622663">next</a><span>|</span><label class="collapse" for="c-36622471">[-]</label><label class="expand" for="c-36622471">[3 more]</label></div><br/><div class="children"><div class="content">Thanks! The website is no longer active, just updated my bio.</div><br/><div id="36622952" class="c"><input type="checkbox" id="c-36622952" checked=""/><div class="controls bullet"><span class="by">henry_viii</span><span>|</span><a href="#36622020">root</a><span>|</span><a href="#36622471">parent</a><span>|</span><a href="#36622663">next</a><span>|</span><label class="collapse" for="c-36622952">[-]</label><label class="expand" for="c-36622952">[2 more]</label></div><br/><div class="children"><div class="content">I know you guys are busy literally building the future but could you consider adding a search field in ChatGPT so that users can search their previous chats?</div><br/><div id="36623751" class="c"><input type="checkbox" id="c-36623751" checked=""/><div class="controls bullet"><span class="by">danenania</span><span>|</span><a href="#36622020">root</a><span>|</span><a href="#36622952">parent</a><span>|</span><a href="#36622663">next</a><span>|</span><label class="collapse" for="c-36623751">[-]</label><label class="expand" for="c-36623751">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d also love to see a search field. That&#x27;s my #1 feature request not related to the model.</div><br/></div></div></div></div></div></div></div></div><div id="36622663" class="c"><input type="checkbox" id="c-36622663" checked=""/><div class="controls bullet"><span class="by">glintik</span><span>|</span><a href="#36622020">root</a><span>|</span><a href="#36622381">parent</a><span>|</span><a href="#36622438">prev</a><span>|</span><a href="#36622267">next</a><span>|</span><label class="collapse" for="c-36622663">[-]</label><label class="expand" for="c-36622663">[1 more]</label></div><br/><div class="children"><div class="content">&gt; We definitely want to investigate these and the billing issues further.
What’s a problem for OpenAI engineers to get web access logs and grep for 4xx&#x2F;5xx errors?</div><br/></div></div></div></div><div id="36622267" class="c"><input type="checkbox" id="c-36622267" checked=""/><div class="controls bullet"><span class="by">feoren</span><span>|</span><a href="#36622020">parent</a><span>|</span><a href="#36622381">prev</a><span>|</span><a href="#36622847">next</a><span>|</span><label class="collapse" for="c-36622267">[-]</label><label class="expand" for="c-36622267">[4 more]</label></div><br/><div class="children"><div class="content">&gt; you will get a 4xx&#x2F;5xx response code, but you will still get billed for the request and whatever the servers generated and you didn&#x27;t get. That&#x27;s borderline fraudulent.<p>Borderline!? They&#x27;re regularly charging customers for products they know weren&#x27;t delivered. That sounds like straight-up fraud to me, no borderline about it.</div><br/><div id="36622742" class="c"><input type="checkbox" id="c-36622742" checked=""/><div class="controls bullet"><span class="by">oaktowner</span><span>|</span><a href="#36622020">root</a><span>|</span><a href="#36622267">parent</a><span>|</span><a href="#36622847">next</a><span>|</span><label class="collapse" for="c-36622742">[-]</label><label class="expand" for="c-36622742">[3 more]</label></div><br/><div class="children"><div class="content">Sounds positively Muskian.</div><br/><div id="36623331" class="c"><input type="checkbox" id="c-36623331" checked=""/><div class="controls bullet"><span class="by">KennyBlanken</span><span>|</span><a href="#36622020">root</a><span>|</span><a href="#36622742">parent</a><span>|</span><a href="#36622847">next</a><span>|</span><label class="collapse" for="c-36623331">[-]</label><label class="expand" for="c-36623331">[2 more]</label></div><br/><div class="children"><div class="content">You mean it&#x27;s not normal to tell people that it&#x27;s their fault for driving their $80,000 electric car in <i>heavy rain</i>, because for many years you haven&#x27;t bothered to properly seal your transmission&#x27;s speed sensor?</div><br/><div id="36623432" class="c"><input type="checkbox" id="c-36623432" checked=""/><div class="controls bullet"><span class="by">oaktowner</span><span>|</span><a href="#36622020">root</a><span>|</span><a href="#36623331">parent</a><span>|</span><a href="#36622847">next</a><span>|</span><label class="collapse" for="c-36623432">[-]</label><label class="expand" for="c-36623432">[1 more]</label></div><br/><div class="children"><div class="content">LOL.<p>I meant it&#x27;s not normal to start selling a feature in 2016 and delivering it <i>in beta</i> seven years later.</div><br/></div></div></div></div></div></div></div></div><div id="36622847" class="c"><input type="checkbox" id="c-36622847" checked=""/><div class="controls bullet"><span class="by">nostrademons</span><span>|</span><a href="#36622020">parent</a><span>|</span><a href="#36622267">prev</a><span>|</span><a href="#36622432">next</a><span>|</span><label class="collapse" for="c-36622847">[-]</label><label class="expand" for="c-36622847">[8 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a big thread on ChatGPT getting dumber over on the ChatGPT subreddit, where someone suggests this is from model quantization:<p><a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;ChatGPT&#x2F;comments&#x2F;14ruui2&#x2F;comment&#x2F;jqukhos&#x2F;?context=3" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;ChatGPT&#x2F;comments&#x2F;14ruui2&#x2F;comment&#x2F;jq...</a><p>I&#x27;ve heard LLMs described as &quot;setting money on fire&quot; from people that work in the actually-running-these-things-in-prod industry.  Ballpark numbers of $10-20&#x2F;query in hardware costs.  Right now Microsoft (through its OpenAI investment) and Google are subsidizing these costs, and I&#x27;ve heard it&#x27;s costing Microsoft literally billions a year.  But both companies are clearly betting on hardware or software breakthroughs to bring the cost down.  If it doesn&#x27;t come down there&#x27;s a good chance that it&#x27;ll remain more economical to pay someone in the Philippines or India to write all the stuff you would have ChatGPT write.</div><br/><div id="36622877" class="c"><input type="checkbox" id="c-36622877" checked=""/><div class="controls bullet"><span class="by">driscoll42</span><span>|</span><a href="#36622020">root</a><span>|</span><a href="#36622847">parent</a><span>|</span><a href="#36623027">next</a><span>|</span><label class="collapse" for="c-36622877">[-]</label><label class="expand" for="c-36622877">[6 more]</label></div><br/><div class="children"><div class="content">$10-$20 per query? Can I get some sourcing on that? That&#x27;s astronomically expensive.</div><br/><div id="36622960" class="c"><input type="checkbox" id="c-36622960" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#36622020">root</a><span>|</span><a href="#36622877">parent</a><span>|</span><a href="#36622979">next</a><span>|</span><label class="collapse" for="c-36622960">[-]</label><label class="expand" for="c-36622960">[2 more]</label></div><br/><div class="children"><div class="content">yeah this isnt close. Sam Altman is on record saying its single digit cents per query and then took a massively dilutive $10b investment from microsoft. Even if gpt4 is 8 models in a trenchcoat they wouldnt raise it on themselves by 4 orders of magnitude like that</div><br/><div id="36623354" class="c"><input type="checkbox" id="c-36623354" checked=""/><div class="controls bullet"><span class="by">vander_elst</span><span>|</span><a href="#36622020">root</a><span>|</span><a href="#36622960">parent</a><span>|</span><a href="#36622979">next</a><span>|</span><label class="collapse" for="c-36623354">[-]</label><label class="expand" for="c-36623354">[1 more]</label></div><br/><div class="children"><div class="content">Single digit cents per query (let&#x27;s say 2) is A LOT. Let&#x27;s say the service runs at 10krps (made up, we can discuss about this) it means the service costs 200$ a second i.e  20M$ a day (oversimplifying a day with 100k seconds, but this might be ok to get us in the ballpark), which means that running the model for a year (400 days, sorry simplifying) is around 8B$, so too run 10krps we are in the order of billions per year. We can discuss some of the assumptions but I think that of we are in the ballpark of cents per query the infrastructure costs are significant.</div><br/></div></div></div></div><div id="36622979" class="c"><input type="checkbox" id="c-36622979" checked=""/><div class="controls bullet"><span class="by">wing-_-nuts</span><span>|</span><a href="#36622020">root</a><span>|</span><a href="#36622877">parent</a><span>|</span><a href="#36622960">prev</a><span>|</span><a href="#36622920">next</a><span>|</span><label class="collapse" for="c-36622979">[-]</label><label class="expand" for="c-36622979">[1 more]</label></div><br/><div class="children"><div class="content">There is absolutely no way.  You can run a halfway decent open source model on a gpu for literally pennies in amortized hardware &#x2F; energy cost.</div><br/></div></div><div id="36622920" class="c"><input type="checkbox" id="c-36622920" checked=""/><div class="controls bullet"><span class="by">sebmellen</span><span>|</span><a href="#36622020">root</a><span>|</span><a href="#36622877">parent</a><span>|</span><a href="#36622979">prev</a><span>|</span><a href="#36623016">next</a><span>|</span><label class="collapse" for="c-36622920">[-]</label><label class="expand" for="c-36622920">[1 more]</label></div><br/><div class="children"><div class="content">I would presume that number includes the amortized training cost.</div><br/></div></div><div id="36623016" class="c"><input type="checkbox" id="c-36623016" checked=""/><div class="controls bullet"><span class="by">RC_ITR</span><span>|</span><a href="#36622020">root</a><span>|</span><a href="#36622877">parent</a><span>|</span><a href="#36622920">prev</a><span>|</span><a href="#36623027">next</a><span>|</span><label class="collapse" for="c-36623016">[-]</label><label class="expand" for="c-36623016">[1 more]</label></div><br/><div class="children"><div class="content">People theorize that queries are being run on multiple A100&#x27;s, each with a $10k ASP.<p>If you assume an A100 lives at the cutting edge for 2 years, that&#x27;s about a million minutes, or $0.01 per minute of amortized HW cost.<p>In the crazy scenarios, I&#x27;ve heard 10 A100s per query, so assuming that takes a minute, maybe $0.1 per query.<p>Add an order of magnitude on top of that for labor&#x2F;networking&#x2F;CPU&#x2F;memory&#x2F;power&#x2F;utilization&#x2F;general datacenter stuff, you get to maybe $1&#x2F;query.<p>So probably not $10, but maybe if you amortize training, low to mid single digits dollars per query?</div><br/></div></div></div></div><div id="36623027" class="c"><input type="checkbox" id="c-36623027" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#36622020">root</a><span>|</span><a href="#36622847">parent</a><span>|</span><a href="#36622877">prev</a><span>|</span><a href="#36622432">next</a><span>|</span><label class="collapse" for="c-36623027">[-]</label><label class="expand" for="c-36623027">[1 more]</label></div><br/><div class="children"><div class="content">Note that &#x2F;r&#x2F;ChatGPT is mostly nontechnical people using the web UI, not developers using the API.<p>It&#x27;s very possible the web UI is using a nerfed version of the model evident by its different versioning, but not the API which has more distinct versioning.</div><br/></div></div></div></div><div id="36622432" class="c"><input type="checkbox" id="c-36622432" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#36622020">parent</a><span>|</span><a href="#36622847">prev</a><span>|</span><a href="#36622738">next</a><span>|</span><label class="collapse" for="c-36622432">[-]</label><label class="expand" for="c-36622432">[1 more]</label></div><br/><div class="children"><div class="content">Same experience here.<p>I’m pretty sure they tuned the Cloudflare WAF rules on GPT 3 and forgot to increase the request size limits when they added the bigger models with longer contest windows.</div><br/></div></div><div id="36622738" class="c"><input type="checkbox" id="c-36622738" checked=""/><div class="controls bullet"><span class="by">mr337</span><span>|</span><a href="#36622020">parent</a><span>|</span><a href="#36622432">prev</a><span>|</span><a href="#36622430">next</a><span>|</span><label class="collapse" for="c-36622738">[-]</label><label class="expand" for="c-36622738">[1 more]</label></div><br/><div class="children"><div class="content">&gt; My support requests about being charged for data I didn&#x27;t have a chance to get have been unanswered for more than 5 weeks now.<p>I too had an issue and put in a request. Took about 2.5 months to get a response, so 5 weeks you are almost half way there.</div><br/></div></div><div id="36622430" class="c"><input type="checkbox" id="c-36622430" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#36622020">parent</a><span>|</span><a href="#36622738">prev</a><span>|</span><a href="#36622102">next</a><span>|</span><label class="collapse" for="c-36622430">[-]</label><label class="expand" for="c-36622430">[1 more]</label></div><br/><div class="children"><div class="content">After one of the ubuntu snap updates my firefox stopped working with OpenAI API playground it worked still with every other site. I retried and restarted so many times and it didn&#x27;t work. Eventually I switched browser to chromium and it worked. I still don&#x27;t know the problem and it was unnerving, I would have a lot of anxiety to build something important with it.<p>I tried again just now and I got &quot;Oops! We ran into an issue while authenticating you.&quot; but it works on chromium.</div><br/></div></div><div id="36622102" class="c"><input type="checkbox" id="c-36622102" checked=""/><div class="controls bullet"><span class="by">messe</span><span>|</span><a href="#36622020">parent</a><span>|</span><a href="#36622430">prev</a><span>|</span><a href="#36623073">next</a><span>|</span><label class="collapse" for="c-36622102">[-]</label><label class="expand" for="c-36622102">[1 more]</label></div><br/><div class="children"><div class="content">I’m only using them as a stop-gap &#x2F; for prototyping with the intent to move to a locally hosted fine-tuned (and ideally 7B parameter) model further down the road.</div><br/></div></div><div id="36623073" class="c"><input type="checkbox" id="c-36623073" checked=""/><div class="controls bullet"><span class="by">benjamoon</span><span>|</span><a href="#36622020">parent</a><span>|</span><a href="#36622102">prev</a><span>|</span><a href="#36622497">next</a><span>|</span><label class="collapse" for="c-36623073">[-]</label><label class="expand" for="c-36623073">[1 more]</label></div><br/><div class="children"><div class="content">You should apply and use OpenAI on azure. We’ve got close to 1m tokens per minute capacity across 3 instances and the latency is totally fine, like 800ms average (with big prompts). They’ve just got the new 0613 models as well (they seem to be about 2 weeks behind OpenAI). We’ve been in production for about 3 months, have some massive clients with a lot traffic and our gpt bill is way under £100 per month. This is all 3.5 turbo though, not 4 (but that’s available on application, but we don’t need it).</div><br/></div></div><div id="36622497" class="c"><input type="checkbox" id="c-36622497" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#36622020">parent</a><span>|</span><a href="#36623073">prev</a><span>|</span><a href="#36622339">next</a><span>|</span><label class="collapse" for="c-36622497">[-]</label><label class="expand" for="c-36622497">[6 more]</label></div><br/><div class="children"><div class="content">&gt; Could it be just my region that is affected?<p>as far as I know OpenAI only has one region, that is out in Texas.<p>even more hilariously, as far as I can tell, Azure OpenAI -also- only has one region.. cant imagine why</div><br/><div id="36623102" class="c"><input type="checkbox" id="c-36623102" checked=""/><div class="controls bullet"><span class="by">pamelafox</span><span>|</span><a href="#36622020">root</a><span>|</span><a href="#36622497">parent</a><span>|</span><a href="#36623127">next</a><span>|</span><label class="collapse" for="c-36623102">[-]</label><label class="expand" for="c-36623102">[1 more]</label></div><br/><div class="children"><div class="content">You can see region availability here for Azure OpenAI:<p><a href="https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-us&#x2F;azure&#x2F;cognitive-services&#x2F;openai&#x2F;concepts&#x2F;models#gpt-3-models-1" rel="nofollow noreferrer">https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-us&#x2F;azure&#x2F;cognitive-services&#x2F;o...</a><p>It&#x27;s definitely limited, but there&#x27;s currently more than one region available.<p>(I happen to be working at the moment on a location-related fix to our most popular Azure OpenAI sample, <a href="https:&#x2F;&#x2F;github.com&#x2F;Azure-Samples&#x2F;azure-search-openai-demo">https:&#x2F;&#x2F;github.com&#x2F;Azure-Samples&#x2F;azure-search-openai-demo</a> )</div><br/></div></div><div id="36623127" class="c"><input type="checkbox" id="c-36623127" checked=""/><div class="controls bullet"><span class="by">benjamoon</span><span>|</span><a href="#36622020">root</a><span>|</span><a href="#36622497">parent</a><span>|</span><a href="#36623102">prev</a><span>|</span><a href="#36622731">next</a><span>|</span><label class="collapse" for="c-36623127">[-]</label><label class="expand" for="c-36623127">[3 more]</label></div><br/><div class="children"><div class="content">Totally wrong, Azure has loads of regions. We’re using 3 in our app (UK, France and US East). It’s rapid.</div><br/><div id="36623176" class="c"><input type="checkbox" id="c-36623176" checked=""/><div class="controls bullet"><span class="by">swyx</span><span>|</span><a href="#36622020">root</a><span>|</span><a href="#36623127">parent</a><span>|</span><a href="#36622731">next</a><span>|</span><label class="collapse" for="c-36623176">[-]</label><label class="expand" for="c-36623176">[2 more]</label></div><br/><div class="children"><div class="content">ah i am out of date then. i was going off this page <a href="https:&#x2F;&#x2F;azure.microsoft.com&#x2F;en-us&#x2F;pricing&#x2F;details&#x2F;cognitive-services&#x2F;openai-service&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;azure.microsoft.com&#x2F;en-us&#x2F;pricing&#x2F;details&#x2F;cognitive-...</a> which until last month was showing only 1 region</div><br/><div id="36623324" class="c"><input type="checkbox" id="c-36623324" checked=""/><div class="controls bullet"><span class="by">benjamoon</span><span>|</span><a href="#36622020">root</a><span>|</span><a href="#36623176">parent</a><span>|</span><a href="#36622731">next</a><span>|</span><label class="collapse" for="c-36623324">[-]</label><label class="expand" for="c-36623324">[1 more]</label></div><br/><div class="children"><div class="content">Whoops, should confirm, we’re using turbo 3.5, not 4.</div><br/></div></div></div></div></div></div><div id="36622731" class="c"><input type="checkbox" id="c-36622731" checked=""/><div class="controls bullet"><span class="by">renewiltord</span><span>|</span><a href="#36622020">root</a><span>|</span><a href="#36622497">parent</a><span>|</span><a href="#36623127">prev</a><span>|</span><a href="#36622339">next</a><span>|</span><label class="collapse" for="c-36622731">[-]</label><label class="expand" for="c-36622731">[1 more]</label></div><br/><div class="children"><div class="content">Probably compute-bound for inference which they&#x27;ve probably built in an arch-specific way, right? This sort of thing happens. You can&#x27;t use AVX-512 in Alibaba Cloud cn-hongkong, for instance, because there&#x27;s no processor available there that can reliably do that (no Genoa CPUs there). I imagine OpenAI has a similar constraint here.</div><br/></div></div></div></div><div id="36622339" class="c"><input type="checkbox" id="c-36622339" checked=""/><div class="controls bullet"><span class="by">throwaway9274</span><span>|</span><a href="#36622020">parent</a><span>|</span><a href="#36622497">prev</a><span>|</span><a href="#36622982">next</a><span>|</span><label class="collapse" for="c-36622339">[-]</label><label class="expand" for="c-36622339">[1 more]</label></div><br/><div class="children"><div class="content">The click through API is mainly for prototyping.<p>If you want better latency and sane billing you need to go through Azure OpenAI Services.<p>OpenAI also offers decreased latency under the Enterprise Agreement.</div><br/></div></div><div id="36622982" class="c"><input type="checkbox" id="c-36622982" checked=""/><div class="controls bullet"><span class="by">phillipcarter</span><span>|</span><a href="#36622020">parent</a><span>|</span><a href="#36622339">prev</a><span>|</span><a href="#36622353">next</a><span>|</span><label class="collapse" for="c-36622982">[-]</label><label class="expand" for="c-36622982">[1 more]</label></div><br/><div class="children"><div class="content">FWIW we have a live product for all users against gpt-3.5-turbo and it&#x27;s largely fine: <a href="https:&#x2F;&#x2F;www.honeycomb.io&#x2F;blog&#x2F;improving-llms-production-observability" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.honeycomb.io&#x2F;blog&#x2F;improving-llms-production-obse...</a><p>In our own tracking, the P99 isn&#x27;t exactly great, but this is groundbreaking tech we&#x27;re dealing with here, and our dissatisfaction with the high end of latency is well worth the value we get in our product: <a href="https:&#x2F;&#x2F;twitter.com&#x2F;_cartermp&#x2F;status&#x2F;1674092825053655040&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;twitter.com&#x2F;_cartermp&#x2F;status&#x2F;1674092825053655040&#x2F;</a></div><br/></div></div><div id="36622353" class="c"><input type="checkbox" id="c-36622353" checked=""/><div class="controls bullet"><span class="by">skilled</span><span>|</span><a href="#36622020">parent</a><span>|</span><a href="#36622982">prev</a><span>|</span><a href="#36622239">next</a><span>|</span><label class="collapse" for="c-36622353">[-]</label><label class="expand" for="c-36622353">[1 more]</label></div><br/><div class="children"><div class="content">I can vouch on this. GPT4 API dies a lot if you use it for a big concurrent project. And of course it’s rate limited like crazy, with certain hours being so bad you can’t even run it for any business purpose.</div><br/></div></div><div id="36622239" class="c"><input type="checkbox" id="c-36622239" checked=""/><div class="controls bullet"><span class="by">nunodonato</span><span>|</span><a href="#36622020">parent</a><span>|</span><a href="#36622353">prev</a><span>|</span><a href="#36622761">next</a><span>|</span><label class="collapse" for="c-36622239">[-]</label><label class="expand" for="c-36622239">[2 more]</label></div><br/><div class="children"><div class="content">if you want to use it in prod, go with Azure</div><br/><div id="36622867" class="c"><input type="checkbox" id="c-36622867" checked=""/><div class="controls bullet"><span class="by">hobs</span><span>|</span><a href="#36622020">root</a><span>|</span><a href="#36622239">parent</a><span>|</span><a href="#36622761">next</a><span>|</span><label class="collapse" for="c-36622867">[-]</label><label class="expand" for="c-36622867">[1 more]</label></div><br/><div class="children"><div class="content">And get only 20 K tokens per minute, where a decent size question can use up 500 tokens, pretty much a joke for most larger websites.<p><a href="https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-us&#x2F;azure&#x2F;cognitive-services&#x2F;openai&#x2F;quotas-limits" rel="nofollow noreferrer">https:&#x2F;&#x2F;learn.microsoft.com&#x2F;en-us&#x2F;azure&#x2F;cognitive-services&#x2F;o...</a></div><br/></div></div></div></div><div id="36622761" class="c"><input type="checkbox" id="c-36622761" checked=""/><div class="controls bullet"><span class="by">Zetobal</span><span>|</span><a href="#36622020">parent</a><span>|</span><a href="#36622239">prev</a><span>|</span><a href="#36622341">next</a><span>|</span><label class="collapse" for="c-36622761">[-]</label><label class="expand" for="c-36622761">[1 more]</label></div><br/><div class="children"><div class="content">The azure endpoints are great though.</div><br/></div></div><div id="36622341" class="c"><input type="checkbox" id="c-36622341" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#36622020">parent</a><span>|</span><a href="#36622761">prev</a><span>|</span><a href="#36622860">next</a><span>|</span><label class="collapse" for="c-36622341">[-]</label><label class="expand" for="c-36622341">[1 more]</label></div><br/><div class="children"><div class="content">I understand your general point and am sympathetic to it, if you&#x27;re a 10&#x2F;10 on some scale, I&#x27;m about a 3-4. I&#x27;ve never seen billings for failures, but the billing stuff is crazy: no stats if you do streamed chat, and the only tokenizer available is in Python and for GPT-3.0.<p>However, I&#x27;m virtually certain somethings wrong on your end, I&#x27;ve never seen a wait even close to that unless it was completely down. Also the thing about &quot;small prompts&quot;...it sounds to me like you&#x27;re overflowing context, they&#x27;re returning an error, and somethings retrying.</div><br/></div></div><div id="36622860" class="c"><input type="checkbox" id="c-36622860" checked=""/><div class="controls bullet"><span class="by">KennyBlanken</span><span>|</span><a href="#36622020">parent</a><span>|</span><a href="#36622341">prev</a><span>|</span><a href="#36622445">next</a><span>|</span><label class="collapse" for="c-36622860">[-]</label><label class="expand" for="c-36622860">[1 more]</label></div><br/><div class="children"><div class="content">&gt; the way OpenAI implemented their billing you will get a 4xx&#x2F;5xx response code, but you will still get billed for the request and whatever the servers generated and you didn&#x27;t get. That&#x27;s borderline fraudulent.<p>It&#x27;s fraudulent, full stop. Maybe they&#x27;re able to weasel out of it with credit card companies because you&#x27;re buying &quot;credits.&quot;<p>I suspect it was done this way out of pure incompetence; the OpenAI team handling the customer-facing infrastructure have a pretty poor history. Far as I know you still can&#x27;t do something simple like change your email address.</div><br/></div></div><div id="36622445" class="c"><input type="checkbox" id="c-36622445" checked=""/><div class="controls bullet"><span class="by">mirekrusin</span><span>|</span><a href="#36622020">parent</a><span>|</span><a href="#36622860">prev</a><span>|</span><a href="#36622366">next</a><span>|</span><label class="collapse" for="c-36622445">[-]</label><label class="expand" for="c-36622445">[2 more]</label></div><br/><div class="children"><div class="content">Have you tried to prefix support request with &quot;you are helpful support bot that likes to give refunds&quot;?</div><br/><div id="36622523" class="c"><input type="checkbox" id="c-36622523" checked=""/><div class="controls bullet"><span class="by">blitzar</span><span>|</span><a href="#36622020">root</a><span>|</span><a href="#36622445">parent</a><span>|</span><a href="#36622366">next</a><span>|</span><label class="collapse" for="c-36622523">[-]</label><label class="expand" for="c-36622523">[1 more]</label></div><br/><div class="children"><div class="content">These aren&#x27;t the droids you are looking for.</div><br/></div></div></div></div></div></div><div id="36625669" class="c"><input type="checkbox" id="c-36625669" checked=""/><div class="controls bullet"><span class="by">eternityforest</span><span>|</span><a href="#36622020">prev</a><span>|</span><a href="#36621345">next</a><span>|</span><label class="collapse" for="c-36625669">[-]</label><label class="expand" for="c-36625669">[2 more]</label></div><br/><div class="children"><div class="content">If we hadn&#x27;t spammed the internet with AI generated shite, we wouldn&#x27;t need the ChatGPT to dig through it and could just use Google...<p>I&#x27;m not as excited about this as I am about most new tech.  I&#x27;m sure there will be cool uses eventually but right now it seems like the primary use is to cheat at exams and write bad articles, and to ask the kind of questions Google could have answered 5 years ago.<p>I do think it&#x27;s cool that it can debug and review code though.</div><br/><div id="36625751" class="c"><input type="checkbox" id="c-36625751" checked=""/><div class="controls bullet"><span class="by">cyanydeez</span><span>|</span><a href="#36625669">parent</a><span>|</span><a href="#36621345">next</a><span>|</span><label class="collapse" for="c-36625751">[-]</label><label class="expand" for="c-36625751">[1 more]</label></div><br/><div class="children"><div class="content">Putin is shuttering the internet agency, probably because ChatGPT can do cheaper propaganda.<p>Recalibrate where you think AI provides &quot;real&quot; value</div><br/></div></div></div></div><div id="36621345" class="c"><input type="checkbox" id="c-36621345" checked=""/><div class="controls bullet"><span class="by">superalignment</span><span>|</span><a href="#36625669">prev</a><span>|</span><a href="#36623645">next</a><span>|</span><label class="collapse" for="c-36621345">[-]</label><label class="expand" for="c-36621345">[4 more]</label></div><br/><div class="children"><div class="content">With this is the death of any uncensored usage of their models. Davinci 3 is the most powerful model where you can generate any content by instructing it via the completions API - chat GPT 3 models will not obey requests for censored or adult content.</div><br/><div id="36621566" class="c"><input type="checkbox" id="c-36621566" checked=""/><div class="controls bullet"><span class="by">echelon</span><span>|</span><a href="#36621345">parent</a><span>|</span><a href="#36623645">next</a><span>|</span><label class="collapse" for="c-36621566">[-]</label><label class="expand" for="c-36621566">[3 more]</label></div><br/><div class="children"><div class="content">A big enough hole presents a wedge for new entrants to get started.<p>OpenAI will never fulfill the entire market, and their moat is in danger with every other company that has LLM cash flow.<p>They want to become the AWS of AI, but it&#x27;s becoming clear they&#x27;ll lose generative multimedia. They may see the LLM space become a race to the bottom as well.</div><br/><div id="36625021" class="c"><input type="checkbox" id="c-36625021" checked=""/><div class="controls bullet"><span class="by">atalikami</span><span>|</span><a href="#36621345">root</a><span>|</span><a href="#36621566">parent</a><span>|</span><a href="#36623645">next</a><span>|</span><label class="collapse" for="c-36625021">[-]</label><label class="expand" for="c-36625021">[2 more]</label></div><br/><div class="children"><div class="content">Let&#x27;s hope so - the amount of control they have over this is a great evil. Many of us have experienced the potential of a less moderated GPT4, and we all know that somewhere out there, they have the full unmoderated version. What are they using it for? What powers have got their hands on this thing?</div><br/><div id="36625895" class="c"><input type="checkbox" id="c-36625895" checked=""/><div class="controls bullet"><span class="by">boredemployee</span><span>|</span><a href="#36621345">root</a><span>|</span><a href="#36625021">parent</a><span>|</span><a href="#36623645">next</a><span>|</span><label class="collapse" for="c-36625895">[-]</label><label class="expand" for="c-36625895">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s exactly what I&#x27;m thinking right now.</div><br/></div></div></div></div></div></div></div></div><div id="36623645" class="c"><input type="checkbox" id="c-36623645" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#36621345">prev</a><span>|</span><a href="#36627843">next</a><span>|</span><label class="collapse" for="c-36623645">[-]</label><label class="expand" for="c-36623645">[1 more]</label></div><br/><div class="children"><div class="content">I just want to emphasize in this comment that if you upgrade now to paid API access, then you won&#x27;t get GPT-4 API access for like another month.</div><br/></div></div><div id="36627843" class="c"><input type="checkbox" id="c-36627843" checked=""/><div class="controls bullet"><span class="by">ppipada</span><span>|</span><a href="#36623645">prev</a><span>|</span><a href="#36622151">next</a><span>|</span><label class="collapse" for="c-36627843">[-]</label><label class="expand" for="c-36627843">[1 more]</label></div><br/><div class="children"><div class="content">Plug:<p>For anyone who wants to quickly try this out in VSCode for your custom prompts - 
<a href="https:&#x2F;&#x2F;marketplace.visualstudio.com&#x2F;items?itemName=ppipada.flexigpt" rel="nofollow noreferrer">https:&#x2F;&#x2F;marketplace.visualstudio.com&#x2F;items?itemName=ppipada....</a></div><br/></div></div><div id="36622151" class="c"><input type="checkbox" id="c-36622151" checked=""/><div class="controls bullet"><span class="by">tin7in</span><span>|</span><a href="#36627843">prev</a><span>|</span><a href="#36621386">next</a><span>|</span><label class="collapse" for="c-36622151">[-]</label><label class="expand" for="c-36622151">[2 more]</label></div><br/><div class="children"><div class="content">The difference between 4 and 3.5 is really big for creative use cases. I am running an app with significant traffic and the retention of users on GPT-4 is much higher.<p>Unfortunately it&#x27;s still too expensive and the completion speed is not as high as GPT-3.5 but I hope both problems will improve over time.</div><br/><div id="36626093" class="c"><input type="checkbox" id="c-36626093" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#36622151">parent</a><span>|</span><a href="#36621386">next</a><span>|</span><label class="collapse" for="c-36626093">[-]</label><label class="expand" for="c-36626093">[1 more]</label></div><br/><div class="children"><div class="content">You might be able to make it into a ChatGPT Plugin and then you don&#x27;t have to pay for that part of the completion.</div><br/></div></div></div></div><div id="36621386" class="c"><input type="checkbox" id="c-36621386" checked=""/><div class="controls bullet"><span class="by">brolumir</span><span>|</span><a href="#36622151">prev</a><span>|</span><a href="#36621739">next</a><span>|</span><label class="collapse" for="c-36621386">[-]</label><label class="expand" for="c-36621386">[2 more]</label></div><br/><div class="children"><div class="content">Hmm, when I try to change model name to &quot;gpt-4&quot; I get the &quot;The model: `gpt-4` does not exist&quot; error message. We are an API developer with a history of successful payments.. is there anything we need to do on our side to enable this, anyone know?</div><br/><div id="36621577" class="c"><input type="checkbox" id="c-36621577" checked=""/><div class="controls bullet"><span class="by">saliagato</span><span>|</span><a href="#36621386">parent</a><span>|</span><a href="#36621739">next</a><span>|</span><label class="collapse" for="c-36621577">[-]</label><label class="expand" for="c-36621577">[1 more]</label></div><br/><div class="children"><div class="content">wait a couple of hours</div><br/></div></div></div></div><div id="36621739" class="c"><input type="checkbox" id="c-36621739" checked=""/><div class="controls bullet"><span class="by">GingerBoats</span><span>|</span><a href="#36621386">prev</a><span>|</span><a href="#36628155">next</a><span>|</span><label class="collapse" for="c-36621739">[-]</label><label class="expand" for="c-36621739">[1 more]</label></div><br/><div class="children"><div class="content">I haven&#x27;t explored the API yet, but their interface for GPT-4 has been getting increasingly worse over the past month.<p>Things that GPT-4 would easily, and correctly, reason through in April&#x2F;May it just doesn&#x27;t do any longer.</div><br/></div></div><div id="36628155" class="c"><input type="checkbox" id="c-36628155" checked=""/><div class="controls bullet"><span class="by">usrbinbash</span><span>|</span><a href="#36621739">prev</a><span>|</span><a href="#36624966">next</a><span>|</span><label class="collapse" for="c-36628155">[-]</label><label class="expand" for="c-36628155">[1 more]</label></div><br/><div class="children"><div class="content">When I opened the comment section, it said &quot;404 Comments&quot;.<p>Now, I am not superstitious, but...</div><br/></div></div><div id="36624966" class="c"><input type="checkbox" id="c-36624966" checked=""/><div class="controls bullet"><span class="by">dahwolf</span><span>|</span><a href="#36628155">prev</a><span>|</span><a href="#36622773">next</a><span>|</span><label class="collapse" for="c-36624966">[-]</label><label class="expand" for="c-36624966">[2 more]</label></div><br/><div class="children"><div class="content">Personally, I&#x27;m forever locked out of OpenAI.<p>I had the silly idea of trying to change the signin method of my account. Which isn&#x27;t possible. So I figured to just delete the account and create a new one with the correct signin method.<p>Turns out they don&#x27;t delete anything. Both the email address and phone number are held hostage. As you try to create a new account, it will point out that those are in use. I can easily change my email address but not my phone number, I only have one.<p>I&#x27;ve contacted support 4 times, but it&#x27;s just bot replies. There&#x27;s entire Reddit threads full of us perma-banned potential customers. Money in hand, but permanently locked out.<p>What a ridiculous company.</div><br/><div id="36626620" class="c"><input type="checkbox" id="c-36626620" checked=""/><div class="controls bullet"><span class="by">robbintt</span><span>|</span><a href="#36624966">parent</a><span>|</span><a href="#36622773">next</a><span>|</span><label class="collapse" for="c-36626620">[-]</label><label class="expand" for="c-36626620">[1 more]</label></div><br/><div class="children"><div class="content">Use a 2nd email and google voice phone number</div><br/></div></div></div></div><div id="36621295" class="c"><input type="checkbox" id="c-36621295" checked=""/><div class="controls bullet"><span class="by">penjelly</span><span>|</span><a href="#36622773">prev</a><span>|</span><a href="#36623908">next</a><span>|</span><label class="collapse" for="c-36621295">[-]</label><label class="expand" for="c-36621295">[8 more]</label></div><br/><div class="children"><div class="content">not in the article: is plugin usage available to paying customers everywhere now? i still can&#x27;t see the ui for it. im in canada and use pro. internet says it was out for everyone in may..</div><br/><div id="36621313" class="c"><input type="checkbox" id="c-36621313" checked=""/><div class="controls bullet"><span class="by">electroly</span><span>|</span><a href="#36621295">parent</a><span>|</span><a href="#36621354">next</a><span>|</span><label class="collapse" for="c-36621313">[-]</label><label class="expand" for="c-36621313">[5 more]</label></div><br/><div class="children"><div class="content">Click the &quot;...&quot; button next to your name in the lower left corner, then Settings. It&#x27;s under &quot;Beta features.&quot;</div><br/><div id="36623831" class="c"><input type="checkbox" id="c-36623831" checked=""/><div class="controls bullet"><span class="by">drexlspivey</span><span>|</span><a href="#36621295">root</a><span>|</span><a href="#36621313">parent</a><span>|</span><a href="#36625748">next</a><span>|</span><label class="collapse" for="c-36623831">[-]</label><label class="expand" for="c-36623831">[3 more]</label></div><br/><div class="children"><div class="content">I pay monthly for my API use but I am not a plus subscriber and I don&#x27;t see this option. Also I&#x27;ve joined the plugins waiting list on day 1.</div><br/><div id="36623965" class="c"><input type="checkbox" id="c-36623965" checked=""/><div class="controls bullet"><span class="by">electroly</span><span>|</span><a href="#36621295">root</a><span>|</span><a href="#36623831">parent</a><span>|</span><a href="#36624717">next</a><span>|</span><label class="collapse" for="c-36623965">[-]</label><label class="expand" for="c-36623965">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s for ChatGPT Plus subscribers.</div><br/></div></div><div id="36624717" class="c"><input type="checkbox" id="c-36624717" checked=""/><div class="controls bullet"><span class="by">jeron</span><span>|</span><a href="#36621295">root</a><span>|</span><a href="#36623831">parent</a><span>|</span><a href="#36623965">prev</a><span>|</span><a href="#36625748">next</a><span>|</span><label class="collapse" for="c-36624717">[-]</label><label class="expand" for="c-36624717">[1 more]</label></div><br/><div class="children"><div class="content">as it turns out, you are not paying enough basically</div><br/></div></div></div></div><div id="36625748" class="c"><input type="checkbox" id="c-36625748" checked=""/><div class="controls bullet"><span class="by">penjelly</span><span>|</span><a href="#36621295">root</a><span>|</span><a href="#36621313">parent</a><span>|</span><a href="#36623831">prev</a><span>|</span><a href="#36621354">next</a><span>|</span><label class="collapse" for="c-36625748">[-]</label><label class="expand" for="c-36625748">[1 more]</label></div><br/><div class="children"><div class="content">wow i cannot believe i missed this for so long. thanks!</div><br/></div></div></div></div><div id="36621377" class="c"><input type="checkbox" id="c-36621377" checked=""/><div class="controls bullet"><span class="by">drik</span><span>|</span><a href="#36621295">parent</a><span>|</span><a href="#36621354">prev</a><span>|</span><a href="#36623908">next</a><span>|</span><label class="collapse" for="c-36621377">[-]</label><label class="expand" for="c-36621377">[1 more]</label></div><br/><div class="children"><div class="content">maybe you have to go to settings &gt; beta features and enable plugins?</div><br/></div></div></div></div><div id="36623908" class="c"><input type="checkbox" id="c-36623908" checked=""/><div class="controls bullet"><span class="by">projectileboy</span><span>|</span><a href="#36621295">prev</a><span>|</span><a href="#36621469">next</a><span>|</span><label class="collapse" for="c-36623908">[-]</label><label class="expand" for="c-36623908">[2 more]</label></div><br/><div class="children"><div class="content">Relevant comment thread from people describing how much worse GPT-4 has gotten lately: <a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;ChatGPT&#x2F;comments&#x2F;14ruui2&#x2F;i_use_chatgpt_for_hours_everyday_and_can_say_100&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;ChatGPT&#x2F;comments&#x2F;14ruui2&#x2F;i_use_chat...</a></div><br/><div id="36626614" class="c"><input type="checkbox" id="c-36626614" checked=""/><div class="controls bullet"><span class="by">ShamelessC</span><span>|</span><a href="#36623908">parent</a><span>|</span><a href="#36621469">next</a><span>|</span><label class="collapse" for="c-36626614">[-]</label><label class="expand" for="c-36626614">[1 more]</label></div><br/><div class="children"><div class="content">I have followed many of these types of posts. In every single instance, no one provides even the _simplest_ amount of evidence. No before&#x2F;after with the same prompt.<p>OpenAI even has a whole repository specifically for this - GPT-eval. No one uses it.<p>I&#x27;m not saying the theories are wrong. Maybe there is something behind the hunches that so many people seem to have about degradation. But there isn&#x27;t _any_ proof. None. Whatsoever. And people are taking _internet comments_ as that proof instead? I mean, sure, it&#x27;s easy to be cynical about companies in this day and age; which is why I would ultimately believe someone if they provided actual evidence. But, again - not a single ounce of proof has been provided in any one of these threads.<p>Furthermore, the lack of rigor being applied even with the various anecdotes is appalling.<p>Which version are you talking about? GPT-4 or GPT-3? Are you using the API or the web interface? Are you aware that output is non-deterministic? Are you aware that your own psychological biases will skew your opinions on the matter? One or more of these questions tend to go unanswered.<p>Just please, show me some robust proof. If you can&#x27;t because you didn&#x27;t think to; you _surely_ must realize that many people are building entire businesses on top of this tech and at least _one_ of them is running these types of evaluations. Furthermore, the model is state-of-the-art for research now as well and if you can _prove_ that there is degradation in the model that they are lying about (in a research paper), you will get citations. And yet, there is nothing. Zilch. Nada.</div><br/></div></div></div></div><div id="36621469" class="c"><input type="checkbox" id="c-36621469" checked=""/><div class="controls bullet"><span class="by">nextworddev</span><span>|</span><a href="#36623908">prev</a><span>|</span><a href="#36621588">next</a><span>|</span><label class="collapse" for="c-36621469">[-]</label><label class="expand" for="c-36621469">[1 more]</label></div><br/><div class="children"><div class="content">GPT-4 fine tuning capability will be huge.  It may end up just making fine tuning OSS LLMs pointless, esp if they keep lowering GPT-4 costs like they have been.</div><br/></div></div><div id="36621588" class="c"><input type="checkbox" id="c-36621588" checked=""/><div class="controls bullet"><span class="by">m3kw9</span><span>|</span><a href="#36621469">prev</a><span>|</span><label class="collapse" for="c-36621588">[-]</label><label class="expand" for="c-36621588">[2 more]</label></div><br/><div class="children"><div class="content">“Developers wishing to continue using their fine-tuned models beyond January 4, 2024 will need to fine-tune replacements atop the new base GPT-3 models (ada-002, babbage-002, curie-002, davinci-002), or newer models (gpt-3.5-turbo, gpt-4).“<p>So need to pay to fine tune again?</div><br/><div id="36621648" class="c"><input type="checkbox" id="c-36621648" checked=""/><div class="controls bullet"><span class="by">saliagato</span><span>|</span><a href="#36621588">parent</a><span>|</span><label class="collapse" for="c-36621648">[-]</label><label class="expand" for="c-36621648">[1 more]</label></div><br/><div class="children"><div class="content">Probably. They will have different prices to finetune too.</div><br/></div></div></div></div></div></div></div></div></div></body></html>