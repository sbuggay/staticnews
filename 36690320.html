<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1689152467644" as="style"/><link rel="stylesheet" href="styles.css?v=1689152467644"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2307.02666">Chiplet ASIC supercomputers for LLMs like GPT-4</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>kraken12</span> | <span>16 comments</span></div><br/><div><div id="36692006" class="c"><input type="checkbox" id="c-36692006" checked=""/><div class="controls bullet"><span class="by">SomeRndName11</span><span>|</span><a href="#36691689">next</a><span>|</span><label class="collapse" for="c-36692006">[-]</label><label class="expand" for="c-36692006">[1 more]</label></div><br/><div class="children"><div class="content">The problem with hardware solution, is lack of flexibility. LLM is not (yet) as established technology to warrant fixed in-silico solutions, compared to say GPUs.</div><br/></div></div><div id="36691689" class="c"><input type="checkbox" id="c-36691689" checked=""/><div class="controls bullet"><span class="by">why_only_15</span><span>|</span><a href="#36692006">prev</a><span>|</span><a href="#36691680">next</a><span>|</span><label class="collapse" for="c-36691689">[-]</label><label class="expand" for="c-36691689">[2 more]</label></div><br/><div class="children"><div class="content">This seems like a pretty bad paper. Their headline claim that they are 300x faster than an A100 at serving GPT-3 uses obviously wrong numbers for how fast A100s can run GPT-3. They seem to have misread the DeepSpeed Inference paper and claim that the best throughput on GPT-3 sized models was 18 tok&#x2F;s, but if you look at figure 8 on page 11 of the paper [1], it shows that they are able to achieve ~74 teraflops on serving LM-175B (GPT-3), which is about 211 tok&#x2F;s. To calculate TCO, we can say 211 tok&#x2F;s = 760,000 tok&#x2F;hour and A100s are about $1&#x2F;hr, so TCO per 1k tokens using that paper&#x27;s method is about $0.0013, much lower than the $0.02 that they claim, reducing the claimed TCO advantage from 94x to 6.2x. Combine this with the fact that the paper they used is from a year ago and there are more efficient inference methods than there were then and the speedup probably goes even lower, maybe to 3x. This is without even looking at the chip design itself, whose costs are probably far underestimated.<p>[1]: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2207.00032.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2207.00032.pdf</a></div><br/><div id="36691896" class="c"><input type="checkbox" id="c-36691896" checked=""/><div class="controls bullet"><span class="by">jacquesm</span><span>|</span><a href="#36691689">parent</a><span>|</span><a href="#36691680">next</a><span>|</span><label class="collapse" for="c-36691896">[-]</label><label class="expand" for="c-36691896">[1 more]</label></div><br/><div class="children"><div class="content">The whole thing is imaginary: &quot;In this paper, we propose Chiplet Cloud, a chiplet-based ASIC AI-supercomputer architecture that optimizes total cost of ownership
(TCO) per generated token for serving large generative language
models to reduce the overall cost to deploy and run these applica-
tions in the real world.&quot;<p>So they are comparing actual implementations with a theoretical implementation. Never mind that they got the A100 figures wrong, they are still in the &#x27;wouldn&#x27;t it be nice if we had &#x27;x&#x27;&#x27; stage. This looks like a paper whose sole purpose is to raise funds for a research project that will probably ultimately go nowhere and they needed a reason that looks good on paper to increase their chances of getting funded. A100 can already be had for $0.87&#x2F;hour so even their theoretical advantage is under significant pressure and assuming they got everything else right by the time the project has run the market will have overtaken them. This is what usually happens to CPUs that are application specific.</div><br/></div></div></div></div><div id="36691680" class="c"><input type="checkbox" id="c-36691680" checked=""/><div class="controls bullet"><span class="by">alain94040</span><span>|</span><a href="#36691689">prev</a><span>|</span><a href="#36691392">next</a><span>|</span><label class="collapse" for="c-36691680">[-]</label><label class="expand" for="c-36691680">[1 more]</label></div><br/><div class="children"><div class="content">The key point:<p><i>A key architectural feature to achieve this is the ability to fit all model parameters inside the on-chip SRAMs of the chiplets to eliminate bandwidth limitations. Doing so is non-trivial as the amount of memory required is very large and growing for modern LLMs</i><p>...<p><i>On-chip memories such as SRAM have better read latency and read&#x2F;write energy than external memories such as DDR or HBM but require more silicon per bit. We show this design choice wins in the competition of TCO per performance for serving large generative language models but requires careful consideration with respect to the chiplet die size, chiplet memory capacity and total number of chiplets to balance the fabrication cost and model performance (Sec.3.2.2) We observe that the inter-chiplet communication issues can be effectively mitigated through proper software-hardware co- design leveraging mapping strategies such as tensor and pipeline model parallelism</i></div><br/></div></div><div id="36691392" class="c"><input type="checkbox" id="c-36691392" checked=""/><div class="controls bullet"><span class="by">SCUSKU</span><span>|</span><a href="#36691680">prev</a><span>|</span><a href="#36691841">next</a><span>|</span><label class="collapse" for="c-36691392">[-]</label><label class="expand" for="c-36691392">[2 more]</label></div><br/><div class="children"><div class="content">94x cost improvement over GPU and 15x TPU is insane, but fits right in there with performance gains seen in Moore&#x27;s Law.<p>This development presents a more compelling case that we are in fact on the precipice of larger LLMs being able to serve everyone for cheap. Still not really convinced by the AGI argument, but this does spook me. Overall though very cool.</div><br/><div id="36691839" class="c"><input type="checkbox" id="c-36691839" checked=""/><div class="controls bullet"><span class="by">jacquesm</span><span>|</span><a href="#36691392">parent</a><span>|</span><a href="#36691841">next</a><span>|</span><label class="collapse" for="c-36691839">[-]</label><label class="expand" for="c-36691839">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s insane because it is theoretical. They haven&#x27;t shown that it works, think of this paper as a prelude to a funding round or research grant so they have to show some kind of advantage. Which I&#x27;m <i>highly</i> skeptical of, usually when papers show this kind of improvement over SOTA it tends to be either a mistake or purposeful nonsense.</div><br/></div></div></div></div><div id="36691841" class="c"><input type="checkbox" id="c-36691841" checked=""/><div class="controls bullet"><span class="by">fzliu</span><span>|</span><a href="#36691392">prev</a><span>|</span><a href="#36691400">next</a><span>|</span><label class="collapse" for="c-36691841">[-]</label><label class="expand" for="c-36691841">[1 more]</label></div><br/><div class="children"><div class="content">Just skimmed the paper. Seems to me like this paper wants to optimize transformer inference e2e, i.e. from ASIC level all the way to cloud.<p>I&#x27;m not exactly convinced though, since all the results seem to be purely theoretical or simulated. I would&#x27;ve liked to see a prototype built across several FPGAs with clock speeds extrapolated for ASICs.</div><br/></div></div><div id="36691400" class="c"><input type="checkbox" id="c-36691400" checked=""/><div class="controls bullet"><span class="by">msoad</span><span>|</span><a href="#36691841">prev</a><span>|</span><a href="#36691488">next</a><span>|</span><label class="collapse" for="c-36691400">[-]</label><label class="expand" for="c-36691400">[6 more]</label></div><br/><div class="children"><div class="content">They claim the investment will be justified for a 1.5 year life span of the system. But LLMs are changing and improving at a much faster speed that 1.5 years feels like centuries!</div><br/><div id="36691634" class="c"><input type="checkbox" id="c-36691634" checked=""/><div class="controls bullet"><span class="by">thelittleone</span><span>|</span><a href="#36691400">parent</a><span>|</span><a href="#36691488">next</a><span>|</span><label class="collapse" for="c-36691634">[-]</label><label class="expand" for="c-36691634">[5 more]</label></div><br/><div class="children"><div class="content">&quot;Moving fast&quot; may take on a whole new meaning and I&#x27;d put money on the rate of iteration soon being beyond the vast majority&#x27;s comprehension (myself included).</div><br/><div id="36691694" class="c"><input type="checkbox" id="c-36691694" checked=""/><div class="controls bullet"><span class="by">128bytes</span><span>|</span><a href="#36691400">root</a><span>|</span><a href="#36691634">parent</a><span>|</span><a href="#36691488">next</a><span>|</span><label class="collapse" for="c-36691694">[-]</label><label class="expand" for="c-36691694">[4 more]</label></div><br/><div class="children"><div class="content">it&#x27;s already beyond my comprehension, i&#x27;ve not lived very long but in the time i have i&#x27;ve never seen any technology develop so rapidly and at such a rapidly increasing pace. I assume this is what it must have felt like during the dawn of the age of computing.</div><br/><div id="36691908" class="c"><input type="checkbox" id="c-36691908" checked=""/><div class="controls bullet"><span class="by">jacquesm</span><span>|</span><a href="#36691400">root</a><span>|</span><a href="#36691694">parent</a><span>|</span><a href="#36691940">next</a><span>|</span><label class="collapse" for="c-36691908">[-]</label><label class="expand" for="c-36691908">[2 more]</label></div><br/><div class="children"><div class="content">It makes you wonder if those singularity proponents don&#x27;t have a point, and it all depends on whether it keeps accelerating or whether it will slow down again. I hope for the latter and I fear for the former. Even if it does slow down eventually a long enough period of such change is going to make the industrial revolution (whose negative effects we are still coming to terms with today!) like a walk in the park.</div><br/><div id="36691954" class="c"><input type="checkbox" id="c-36691954" checked=""/><div class="controls bullet"><span class="by">ChatGTP</span><span>|</span><a href="#36691400">root</a><span>|</span><a href="#36691908">parent</a><span>|</span><a href="#36691940">next</a><span>|</span><label class="collapse" for="c-36691954">[-]</label><label class="expand" for="c-36691954">[1 more]</label></div><br/><div class="children"><div class="content">Unloading Ray Kurzweil to the cloud in 5, 4, 3…</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36691488" class="c"><input type="checkbox" id="c-36691488" checked=""/><div class="controls bullet"><span class="by">seydor</span><span>|</span><a href="#36691400">prev</a><span>|</span><a href="#36691169">next</a><span>|</span><label class="collapse" for="c-36691488">[-]</label><label class="expand" for="c-36691488">[1 more]</label></div><br/><div class="children"><div class="content">nice, soon i will have my Pocket best friend</div><br/></div></div></div></div></div></div></div></body></html>