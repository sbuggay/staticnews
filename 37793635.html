<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1696669263483" as="style"/><link rel="stylesheet" href="styles.css?v=1696669263483"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.hpcwire.com/2023/10/05/how-amd-may-get-across-the-cuda-moat/">AMD may get across the CUDA moat</a> <span class="domain">(<a href="https://www.hpcwire.com">www.hpcwire.com</a>)</span></div><div class="subtext"><span>danzheng</span> | <span>221 comments</span></div><br/><div><div id="37795443" class="c"><input type="checkbox" id="c-37795443" checked=""/><div class="controls bullet"><span class="by">omneity</span><span>|</span><a href="#37794916">next</a><span>|</span><label class="collapse" for="c-37795443">[-]</label><label class="expand" for="c-37795443">[41 more]</label></div><br/><div class="children"><div class="content">I was able to use ROCm recently with Pytorch and after pulling some hair it worked quite well. The Radeon GPU I had on hand was a bit old and underpowered (RDNA2) and it only supported matmul on fp64, but for the job I needed done I saw a 200x increase in it&#x2F;s over CPU despite the need to cast everywhere, and that made me super happy.<p>Best of all is that I simply set the device to `torch.device(&#x27;cuda&#x27;)` rather than openCL, which does wonders for compatibility and to keep code simple.<p>Protip: Use the official ROCM Pytorch base docker image [0]. The AMD setup is so finicky and dependent on specific versions of sdk&#x2F;drivers&#x2F;libraries and it will be much harder to make work if you try to install them separately.<p>[0]: <a href="https:&#x2F;&#x2F;rocm.docs.amd.com&#x2F;en&#x2F;latest&#x2F;how_to&#x2F;pytorch_install&#x2F;pytorch_install.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;rocm.docs.amd.com&#x2F;en&#x2F;latest&#x2F;how_to&#x2F;pytorch_install&#x2F;p...</a></div><br/><div id="37795767" class="c"><input type="checkbox" id="c-37795767" checked=""/><div class="controls bullet"><span class="by">mikepurvis</span><span>|</span><a href="#37795443">parent</a><span>|</span><a href="#37796495">next</a><span>|</span><label class="collapse" for="c-37795767">[-]</label><label class="expand" for="c-37795767">[27 more]</label></div><br/><div class="children"><div class="content">Sigh. It&#x27;s great that these container images exist to give people an easy on-ramp, but they definitely don&#x27;t work for every use case (especially once you&#x27;re in embedded where space matters and you might not be online to pull multi-gb updates from some registry).<p>So it&#x27;s important that vendors don&#x27;t feel let off the hook to provide sane packaging just because there&#x27;s an option to use a kitchen-sink container image they rebuild every day from source.</div><br/><div id="37796502" class="c"><input type="checkbox" id="c-37796502" checked=""/><div class="controls bullet"><span class="by">xahrepap</span><span>|</span><a href="#37795443">root</a><span>|</span><a href="#37795767">parent</a><span>|</span><a href="#37796668">next</a><span>|</span><label class="collapse" for="c-37796502">[-]</label><label class="expand" for="c-37796502">[10 more]</label></div><br/><div class="children"><div class="content">I know it&#x27;s still different than what you&#x27;re looking for, so you probably already know this, but many projects like this have the Dockerfile on github which shows exactly how they set up the image. For example:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;RadeonOpenCompute&#x2F;ROCm-docker&#x2F;blob&#x2F;master&#x2F;dev&#x2F;Dockerfile-ubuntu-22.04">https:&#x2F;&#x2F;github.com&#x2F;RadeonOpenCompute&#x2F;ROCm-docker&#x2F;blob&#x2F;master...</a><p>They also have some for Fedora. Looks like for this you need to install their repo:<p><pre><code>    curl -sL https:&#x2F;&#x2F;repo.radeon.com&#x2F;rocm&#x2F;rocm.gpg.key | apt-key add - \
    &amp;&amp; printf &quot;deb [arch=amd64] https:&#x2F;&#x2F;repo.radeon.com&#x2F;rocm&#x2F;apt&#x2F;$ROCM_VERSION&#x2F; jammy main&quot; | tee &#x2F;etc&#x2F;apt&#x2F;sources.list.d&#x2F;rocm.list \
    &amp;&amp; printf &quot;deb [arch=amd64] https:&#x2F;&#x2F;repo.radeon.com&#x2F;amdgpu&#x2F;$AMDGPU_VERSION&#x2F;ubuntu jammy main&quot; | tee &#x2F;etc&#x2F;apt&#x2F;sources.list.d&#x2F;amdgpu.list \
</code></pre>
then install Python, a couple other dependencies (build-essential, etc) and then the package in question: rocm-dev<p>So they are doing the packaging. There might even be documentation elsewhere for that type of setup.</div><br/><div id="37796776" class="c"><input type="checkbox" id="c-37796776" checked=""/><div class="controls bullet"><span class="by">mikepurvis</span><span>|</span><a href="#37795443">root</a><span>|</span><a href="#37796502">parent</a><span>|</span><a href="#37796668">next</a><span>|</span><label class="collapse" for="c-37796776">[-]</label><label class="expand" for="c-37796776">[9 more]</label></div><br/><div class="children"><div class="content">Oh yeah, I mean... having the source for the container build is kind of table stakes at this point. No one would accept a 10gb mystery meat blob as the basis of their production system. It&#x27;s bad enough that we still accept binary-only drivers and  proprietary libraries like TensorRT.<p>I think my issue is more just with the <i>mindset</i> that it&#x27;s okay to have one narrow slice of supported versions of everything that are &quot;known to work together&quot; and those are what&#x27;s in the container and anything outside of those and you&#x27;re immediately pooched.<p>This is not hypothetical btw, I&#x27;ve run into real problems around it with libraries like gproto, where tensorflow&#x27;s bazel build pulls in an exact version that&#x27;s different from the default one in nixpkgs, and now you get symbol conflicts when something tries to link to the tensorflow c++ API while linking to another component already using the default gproto. I know these problems are solveable with symbol visibility control and whatever, but that stuff is far from universal and hard to get right, especially if the person setting up the build rules for the library doesn&#x27;t themselves use it in that type of heterogeneous environment (like, everyone at Google just  links the same global proto version from the monorepo so it doesn&#x27;t matter).</div><br/><div id="37798903" class="c"><input type="checkbox" id="c-37798903" checked=""/><div class="controls bullet"><span class="by">mgaunard</span><span>|</span><a href="#37795443">root</a><span>|</span><a href="#37796776">parent</a><span>|</span><a href="#37798182">next</a><span>|</span><label class="collapse" for="c-37798903">[-]</label><label class="expand" for="c-37798903">[7 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know what world you live in, but this is a problem for any software development.<p>You need to ensure that there is only one version of any library used globally throughout the code and that the set of versions is compatible with each other, and preferably you also want everything to be built against the same toolchain with the same flags.<p>That usually means onboarding third-party libraries into your own build system.</div><br/><div id="37799734" class="c"><input type="checkbox" id="c-37799734" checked=""/><div class="controls bullet"><span class="by">anuraaga</span><span>|</span><a href="#37795443">root</a><span>|</span><a href="#37798903">parent</a><span>|</span><a href="#37799978">next</a><span>|</span><label class="collapse" for="c-37799734">[-]</label><label class="expand" for="c-37799734">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;d say with semver becoming far better known, this is not a problem for &quot;any&quot; software development. The developer gets the choice to pick libraries that are stable, often also influencing language choice. Mistakes happen, Guava broke the Java ecosystem for about two years, but it&#x27;s never something that is accepted as just a fact of software development, it is a mistake.<p>Wanting to hold Python+C ecosystem more accountable is fair I think, at least from my own experience around half a year ago, Anaconda doesn&#x27;t work and you need a Dockerfile for any sort of reproducibility, which can have issues since GPU with docker isn&#x27;t that easy. And this means developers from the vendors working with Anaconda, for example, on solving the issue rather than just hoping for contributors to do it. If AMD were to make easy, reproducible builds without root or VM a reality, that would be reason enough to try their hardware. If not, hopefully Nvidia does and then there really would be no way across the moat for me at least.</div><br/><div id="37800085" class="c"><input type="checkbox" id="c-37800085" checked=""/><div class="controls bullet"><span class="by">anthk</span><span>|</span><a href="#37795443">root</a><span>|</span><a href="#37799734">parent</a><span>|</span><a href="#37799978">next</a><span>|</span><label class="collapse" for="c-37800085">[-]</label><label class="expand" for="c-37800085">[1 more]</label></div><br/><div class="children"><div class="content">This would be the work for Guix. Much better than docker, and exportable to a lot of formats. Or just build a vm from the CLI, an ad-hoc environment, a Docker export or a direct rootfs to deploy and run in any compatible machine.</div><br/></div></div></div></div><div id="37799978" class="c"><input type="checkbox" id="c-37799978" checked=""/><div class="controls bullet"><span class="by">iopq</span><span>|</span><a href="#37795443">root</a><span>|</span><a href="#37798903">parent</a><span>|</span><a href="#37799734">prev</a><span>|</span><a href="#37799598">next</a><span>|</span><label class="collapse" for="c-37799978">[-]</label><label class="expand" for="c-37799978">[1 more]</label></div><br/><div class="children"><div class="content">In NixOS, I can install multiple versions of libraries<p>Or rather, I install no versions of libraries because NixOS will put them all in the store in different folders, and will compile the executable to use the correct path (or patch the elf when needed)<p>it has an issue with pip because it&#x27;s allergic to just randomly executing things as part of package management, but pip in general is wtf</div><br/></div></div><div id="37799598" class="c"><input type="checkbox" id="c-37799598" checked=""/><div class="controls bullet"><span class="by">josephg</span><span>|</span><a href="#37795443">root</a><span>|</span><a href="#37798903">parent</a><span>|</span><a href="#37799978">prev</a><span>|</span><a href="#37800080">next</a><span>|</span><label class="collapse" for="c-37799598">[-]</label><label class="expand" for="c-37799598">[2 more]</label></div><br/><div class="children"><div class="content">It’s not a universal problem. A lot of modern languages allow multiple versions of a library to be pulled in to the same code base, through different dependency paths. (Eg nodejs, rust). It’s not a perfect answer by any means, but it’s nice not needing to worry about some package pulling in an inconvenient version of one of its dependencies.<p>Also, just to name it, it’s ridiculous that a specific graphics card manages to restrict the version of gproto that you’re using. You don’t have this problem with nvidia drivers, since cuda stuff is much less fiddly. AMD needs to pull a finger out and fix the bugs in their stack that make it so fragile like this.</div><br/></div></div><div id="37800080" class="c"><input type="checkbox" id="c-37800080" checked=""/><div class="controls bullet"><span class="by">anthk</span><span>|</span><a href="#37795443">root</a><span>|</span><a href="#37798903">parent</a><span>|</span><a href="#37799598">prev</a><span>|</span><a href="#37798182">next</a><span>|</span><label class="collapse" for="c-37800080">[-]</label><label class="expand" for="c-37800080">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s trivial with Guix.</div><br/></div></div></div></div><div id="37798182" class="c"><input type="checkbox" id="c-37798182" checked=""/><div class="controls bullet"><span class="by">hotstickyballs</span><span>|</span><a href="#37795443">root</a><span>|</span><a href="#37796776">parent</a><span>|</span><a href="#37798903">prev</a><span>|</span><a href="#37796668">next</a><span>|</span><label class="collapse" for="c-37798182">[-]</label><label class="expand" for="c-37798182">[1 more]</label></div><br/><div class="children"><div class="content">If anything, the situation with tensor rt shows that companies are absolutely willing to accept a multi gig meat blob</div><br/></div></div></div></div></div></div><div id="37796668" class="c"><input type="checkbox" id="c-37796668" checked=""/><div class="controls bullet"><span class="by">amelius</span><span>|</span><a href="#37795443">root</a><span>|</span><a href="#37795767">parent</a><span>|</span><a href="#37796502">prev</a><span>|</span><a href="#37795868">next</a><span>|</span><label class="collapse" for="c-37796668">[-]</label><label class="expand" for="c-37796668">[1 more]</label></div><br/><div class="children"><div class="content">&gt; So it&#x27;s important that vendors don&#x27;t feel let off the hook to provide sane packaging just because there&#x27;s an option to use a kitchen-sink container image they rebuild every day.<p>Sadly if e.g. 95% of their users can use the container, then it could make economical sense to do it that way.</div><br/></div></div><div id="37795868" class="c"><input type="checkbox" id="c-37795868" checked=""/><div class="controls bullet"><span class="by">fwsgonzo</span><span>|</span><a href="#37795443">root</a><span>|</span><a href="#37795767">parent</a><span>|</span><a href="#37796668">prev</a><span>|</span><a href="#37796033">next</a><span>|</span><label class="collapse" for="c-37795868">[-]</label><label class="expand" for="c-37795868">[2 more]</label></div><br/><div class="children"><div class="content">I feel the same way, especially about build systems. OpenSSL and v8 are among a large list of things that have horrid build systems. Only way to build them sanely is to use some randos CMake fork, then it Just Works. Literally a two-liner in your build system to add them to your project with a sane CMake script.</div><br/><div id="37795916" class="c"><input type="checkbox" id="c-37795916" checked=""/><div class="controls bullet"><span class="by">mikepurvis</span><span>|</span><a href="#37795443">root</a><span>|</span><a href="#37795868">parent</a><span>|</span><a href="#37796033">next</a><span>|</span><label class="collapse" for="c-37795916">[-]</label><label class="expand" for="c-37795916">[1 more]</label></div><br/><div class="children"><div class="content">I was part of a Nix migration over the past two years, and literally one of the first things we checked is that there was already a community-maintained tensorflow+gpu package in nixpkgs because without that the whole thing would have been a complete non-starter, and we sure as heck didn&#x27;t have the resources or know-how to figure it out for ourselves as a small DevOps team just trying to do basic packaging.</div><br/></div></div></div></div><div id="37796033" class="c"><input type="checkbox" id="c-37796033" checked=""/><div class="controls bullet"><span class="by">mathisfun123</span><span>|</span><a href="#37795443">root</a><span>|</span><a href="#37795767">parent</a><span>|</span><a href="#37795868">prev</a><span>|</span><a href="#37798006">next</a><span>|</span><label class="collapse" for="c-37796033">[-]</label><label class="expand" for="c-37796033">[12 more]</label></div><br/><div class="children"><div class="content">&gt; especially once you&#x27;re in embedded<p>is this a real problem? exactly which embedded platform has a device that ROCm supports?</div><br/><div id="37796141" class="c"><input type="checkbox" id="c-37796141" checked=""/><div class="controls bullet"><span class="by">mikepurvis</span><span>|</span><a href="#37795443">root</a><span>|</span><a href="#37796033">parent</a><span>|</span><a href="#37798006">next</a><span>|</span><label class="collapse" for="c-37796141">[-]</label><label class="expand" for="c-37796141">[11 more]</label></div><br/><div class="children"><div class="content">Robotic perception is the one relevant to me. You want to do object recognition on an industrial x86 or Jetson-type machine, without having to use Ubuntu or whatever the one &quot;blessed&quot; underlay system is (either natively or implicitly because you pulled a container based on it).</div><br/><div id="37796935" class="c"><input type="checkbox" id="c-37796935" checked=""/><div class="controls bullet"><span class="by">mathisfun123</span><span>|</span><a href="#37795443">root</a><span>|</span><a href="#37796141">parent</a><span>|</span><a href="#37798006">next</a><span>|</span><label class="collapse" for="c-37796935">[-]</label><label class="expand" for="c-37796935">[10 more]</label></div><br/><div class="children"><div class="content">&gt;industrial x86 or Jetson-type machine<p>that&#x27;s not embedded dev. if you<p>1. use underpowered devices to perform sophisticated tasks<p>2. using code&#x2F;tools that operate at extremely high levels of &quot;abstraction&quot;<p>don&#x27;t be surprised when all the inherent complexity is tamed using just more layers of &quot;abstraction&quot;. if that becomes a problem for your cost&#x2F;power&#x2F;space budget then reconsider choice 1 or choice 2.</div><br/><div id="37797000" class="c"><input type="checkbox" id="c-37797000" checked=""/><div class="controls bullet"><span class="by">mikepurvis</span><span>|</span><a href="#37795443">root</a><span>|</span><a href="#37796935">parent</a><span>|</span><a href="#37798006">next</a><span>|</span><label class="collapse" for="c-37797000">[-]</label><label class="expand" for="c-37797000">[9 more]</label></div><br/><div class="children"><div class="content">Not sure this is worth an argument over semantics, but modern &quot;embedded&quot; development is a lot bigger than just microcontrollers and wearables. IMO as soon as you&#x27;re deploying a computer into any kind of &quot;appliance&quot;, or you&#x27;re offline for periods of time, or you&#x27;re running on batteries or your primary network connection is wireless... then yeah, you&#x27;re starting to hit the requirements associated with embedded and need to seek established solutions for them, including using distros which account for those requirements.</div><br/><div id="37797771" class="c"><input type="checkbox" id="c-37797771" checked=""/><div class="controls bullet"><span class="by">serf</span><span>|</span><a href="#37795443">root</a><span>|</span><a href="#37797000">parent</a><span>|</span><a href="#37797332">next</a><span>|</span><label class="collapse" for="c-37797771">[-]</label><label class="expand" for="c-37797771">[1 more]</label></div><br/><div class="children"><div class="content">fwiw CompTIA classifies an embedded engineer&#x2F;developer as &quot; those who develop an optimized code for specific hardware platforms.&quot;</div><br/></div></div><div id="37797332" class="c"><input type="checkbox" id="c-37797332" checked=""/><div class="controls bullet"><span class="by">mathisfun123</span><span>|</span><a href="#37795443">root</a><span>|</span><a href="#37797000">parent</a><span>|</span><a href="#37797771">prev</a><span>|</span><a href="#37798006">next</a><span>|</span><label class="collapse" for="c-37797332">[-]</label><label class="expand" for="c-37797332">[7 more]</label></div><br/><div class="children"><div class="content">&gt; IMO as soon as you&#x27;re deploying a computer into any kind of &quot;appliance&quot;, or you&#x27;re offline for periods of time, or you&#x27;re running on batteries or your primary network connection is wireless<p>yes and in those instances you do not reach for pytorch&#x2F;tensorflow on top of ubuntu on top of x86 with a discrete gpu and 32gb of ram. instead you reach for C and micro or some arm soc that supports baremetal or at most rtos. that&#x27;s embedded dev.<p>so i&#x27;ll repeat myself: if you want to run extremely high-level code then don&#x27;t be &quot;surprised pikachu&quot; when your underpowered platform, that you chose due to concrete, tight budgets doesn&#x27;t work out.</div><br/><div id="37797827" class="c"><input type="checkbox" id="c-37797827" checked=""/><div class="controls bullet"><span class="by">Const-me</span><span>|</span><a href="#37795443">root</a><span>|</span><a href="#37797332">parent</a><span>|</span><a href="#37798127">next</a><span>|</span><label class="collapse" for="c-37797827">[-]</label><label class="expand" for="c-37797827">[4 more]</label></div><br/><div class="children"><div class="content">The hardware can be fast, actually. Here’s an example of relatively modern industrial x86: <a href="https:&#x2F;&#x2F;www.onlogic.com&#x2F;ml100g-41&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.onlogic.com&#x2F;ml100g-41&#x2F;</a> That thing is probably faster than half of currently sold laptops.<p>However, containers or Ubuntu Linux don’t perform great in that environment. Ubuntu is for desktops, containers are for cloud data centers. An offline stand-alone device is different. BTW, end users don’t typically aware that thing is a computer at all.<p>Personally, I usually pick Alpine or Debian Linux for similar use cases, bare metal i.e. without any containers.</div><br/><div id="37798739" class="c"><input type="checkbox" id="c-37798739" checked=""/><div class="controls bullet"><span class="by">cannonpalms</span><span>|</span><a href="#37795443">root</a><span>|</span><a href="#37797827">parent</a><span>|</span><a href="#37798098">next</a><span>|</span><label class="collapse" for="c-37798739">[-]</label><label class="expand" for="c-37798739">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Ubuntu is for desktops<p>Tell that to their (much larger, more profitable, and better-funded) server org. This is far from true.</div><br/><div id="37799996" class="c"><input type="checkbox" id="c-37799996" checked=""/><div class="controls bullet"><span class="by">iopq</span><span>|</span><a href="#37795443">root</a><span>|</span><a href="#37798739">parent</a><span>|</span><a href="#37798098">next</a><span>|</span><label class="collapse" for="c-37799996">[-]</label><label class="expand" for="c-37799996">[1 more]</label></div><br/><div class="children"><div class="content">It also works much better as a server. Snaps work really well for things like certbot<p>On Desktop you have to worry about things like... UIs, sound, Wine, etc.</div><br/></div></div></div></div><div id="37798098" class="c"><input type="checkbox" id="c-37798098" checked=""/><div class="controls bullet"><span class="by">ngcc_hk</span><span>|</span><a href="#37795443">root</a><span>|</span><a href="#37797827">parent</a><span>|</span><a href="#37798739">prev</a><span>|</span><a href="#37798127">next</a><span>|</span><label class="collapse" for="c-37798098">[-]</label><label class="expand" for="c-37798098">[1 more]</label></div><br/><div class="children"><div class="content">That is the moat they tried to cross.  Imagine you have a PyTorch app and run on iOS, arm based, amd based and intel … cloud, or embedded.   just imagine.  You scale and embed as your business case, not as any one firm current strategy is.<p>Or at least you have some case as heaven never come.  Or come just we do not aware now like internet.  Can you need to use ibm to rub sna to provide a token ring based network.  In 1980 …<p>Imagine and let us or they competite …</div><br/></div></div></div></div><div id="37798127" class="c"><input type="checkbox" id="c-37798127" checked=""/><div class="controls bullet"><span class="by">rcxdude</span><span>|</span><a href="#37795443">root</a><span>|</span><a href="#37797332">parent</a><span>|</span><a href="#37797827">prev</a><span>|</span><a href="#37797805">next</a><span>|</span><label class="collapse" for="c-37798127">[-]</label><label class="expand" for="c-37798127">[1 more]</label></div><br/><div class="children"><div class="content">Not that I want to encourage gatekeeping in the first place, but you&#x27;ll have more success if you have a clue what the other person is talking about in the first place (and some idea of what embedded looks like outside of tiny micros, and how the concerns about abstractions extend beyond matters of how much computational power is available).</div><br/></div></div><div id="37797805" class="c"><input type="checkbox" id="c-37797805" checked=""/><div class="controls bullet"><span class="by">nightski</span><span>|</span><a href="#37795443">root</a><span>|</span><a href="#37797332">parent</a><span>|</span><a href="#37798127">prev</a><span>|</span><a href="#37798006">next</a><span>|</span><label class="collapse" for="c-37797805">[-]</label><label class="expand" for="c-37797805">[1 more]</label></div><br/><div class="children"><div class="content">Clearly you&#x27;ve never used a Nvidia Jetson and have no idea what it is.  You don&#x27;t need a discrete GPU, it has a quite sophisticated GPU in the SoC.  It&#x27;s Nvidia&#x27;s embedded platform for ML&#x2F;AI.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="37798006" class="c"><input type="checkbox" id="c-37798006" checked=""/><div class="controls bullet"><span class="by">ngcc_hk</span><span>|</span><a href="#37795443">root</a><span>|</span><a href="#37795767">parent</a><span>|</span><a href="#37796033">prev</a><span>|</span><a href="#37796495">next</a><span>|</span><label class="collapse" for="c-37798006">[-]</label><label class="expand" for="c-37798006">[1 more]</label></div><br/><div class="children"><div class="content">Better to come if the tide shift so we can have compatible layer.  The key is the tide.  Obviously would n try to sue … it would be a sign that finally we have real competition.  Gar is where innovation do.<p>X86 cannot do 64 bit let us do this and that so the market can use only our cpu.   Repeat with me x86-64 is impossible.<p>Not sure Apple is in this otherwise the real great competition come.</div><br/></div></div></div></div><div id="37796495" class="c"><input type="checkbox" id="c-37796495" checked=""/><div class="controls bullet"><span class="by">wyldfire</span><span>|</span><a href="#37795443">parent</a><span>|</span><a href="#37795767">prev</a><span>|</span><a href="#37797037">next</a><span>|</span><label class="collapse" for="c-37796495">[-]</label><label class="expand" for="c-37796495">[7 more]</label></div><br/><div class="children"><div class="content">&gt; Best of all is that I simply set the device to `torch.device(&#x27;cuda&#x27;)` rather than openCL, which does wonders for compatibility<p>Man oh man where did we go wrong that cuda is the more compatible option over OpenCL?</div><br/><div id="37796596" class="c"><input type="checkbox" id="c-37796596" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#37795443">root</a><span>|</span><a href="#37796495">parent</a><span>|</span><a href="#37799394">next</a><span>|</span><label class="collapse" for="c-37796596">[-]</label><label class="expand" for="c-37796596">[5 more]</label></div><br/><div class="children"><div class="content">It must be a misnomer on PyTorch&#x27;s side. Clearly it&#x27;s neither CUDA nor OpenCL.<p>AMD should just get it&#x27;s shit together. This is ridiculous. Not the name, but the fact that you can only do FP64 on a GPU. Everybody is moving to FP16 and AMD is stuck on doubles?</div><br/><div id="37797003" class="c"><input type="checkbox" id="c-37797003" checked=""/><div class="controls bullet"><span class="by">omneity</span><span>|</span><a href="#37795443">root</a><span>|</span><a href="#37796596">parent</a><span>|</span><a href="#37799394">next</a><span>|</span><label class="collapse" for="c-37797003">[-]</label><label class="expand" for="c-37797003">[4 more]</label></div><br/><div class="children"><div class="content">I believe the fp64 limitation came from the laptop-grade GPU I had rather than inherent to AMD or ROCm.<p>The API level I could target was at least two or three versions behind the latest they have to offer.</div><br/><div id="37797209" class="c"><input type="checkbox" id="c-37797209" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#37795443">root</a><span>|</span><a href="#37797003">parent</a><span>|</span><a href="#37799394">next</a><span>|</span><label class="collapse" for="c-37797209">[-]</label><label class="expand" for="c-37797209">[3 more]</label></div><br/><div class="children"><div class="content">Might very well be true. I don&#x27;t blame anyone for not diving deeper into figuring out why this stuff doesn&#x27;t work.<p>But this is one of the great strengths of CUDA: I can develop a kernel on my workstation, my boss can demo it on his laptop and we can deploy it on Jetsons or the multi-gpu cluster with minimal changes and i can be sure that everything runs everywhere.</div><br/><div id="37800005" class="c"><input type="checkbox" id="c-37800005" checked=""/><div class="controls bullet"><span class="by">iopq</span><span>|</span><a href="#37795443">root</a><span>|</span><a href="#37797209">parent</a><span>|</span><a href="#37798287">next</a><span>|</span><label class="collapse" for="c-37800005">[-]</label><label class="expand" for="c-37800005">[1 more]</label></div><br/><div class="children"><div class="content">Sorry, still trying to install some dependencies for DNN and CUDA, not sure why it says my Clang version is too new (!)</div><br/></div></div><div id="37798287" class="c"><input type="checkbox" id="c-37798287" checked=""/><div class="controls bullet"><span class="by">brutus1213</span><span>|</span><a href="#37795443">root</a><span>|</span><a href="#37797209">parent</a><span>|</span><a href="#37800005">prev</a><span>|</span><a href="#37799394">next</a><span>|</span><label class="collapse" for="c-37798287">[-]</label><label class="expand" for="c-37798287">[1 more]</label></div><br/><div class="children"><div class="content">There is indeed something excellent about CUDA from a user perspective that is hard to beat. I do high-level DNN and it is not clear to me what it is or why that is. Anytime I have worked on optimizing to mobile hardware (not Jetson, but actual phones or accelerators), it is just a world of hurt and incompatibilities. This notion that operators or subgraphs can be accelerated by lower level closed blobs .. I wonder if that is part of the issue. But then why doesn&#x27;t OpenCL not just work? I thought it gave a CUDA kernel like general purpose abstraction.<p>I just don&#x27;t understand the details enough to understand why things are problematic without CUDA :(</div><br/></div></div></div></div></div></div></div></div><div id="37799394" class="c"><input type="checkbox" id="c-37799394" checked=""/><div class="controls bullet"><span class="by">NavinF</span><span>|</span><a href="#37795443">root</a><span>|</span><a href="#37796495">parent</a><span>|</span><a href="#37796596">prev</a><span>|</span><a href="#37797037">next</a><span>|</span><label class="collapse" for="c-37799394">[-]</label><label class="expand" for="c-37799394">[1 more]</label></div><br/><div class="children"><div class="content">This has always been the case. OpenCL is a shit show</div><br/></div></div></div></div><div id="37797037" class="c"><input type="checkbox" id="c-37797037" checked=""/><div class="controls bullet"><span class="by">RockRobotRock</span><span>|</span><a href="#37795443">parent</a><span>|</span><a href="#37796495">prev</a><span>|</span><a href="#37799169">next</a><span>|</span><label class="collapse" for="c-37797037">[-]</label><label class="expand" for="c-37797037">[4 more]</label></div><br/><div class="children"><div class="content">Have you gotten it to work with Whisper by any chance?</div><br/><div id="37798752" class="c"><input type="checkbox" id="c-37798752" checked=""/><div class="controls bullet"><span class="by">kkielhofner</span><span>|</span><a href="#37795443">root</a><span>|</span><a href="#37797037">parent</a><span>|</span><a href="#37799129">next</a><span>|</span><label class="collapse" for="c-37798752">[-]</label><label class="expand" for="c-37798752">[2 more]</label></div><br/><div class="children"><div class="content">Whisper is actually a great example of why Nvidia has such a stronghold on ML&#x2F;AI and why it’s so difficult to compete.<p>There’s getting something to “work”, which is often enough of a challenge with ROCm. Then there’s getting it to work well (next challenge).<p>Then there’s getting it to work as well as Nvidia&#x2F;CUDA.<p>With Whisper, as one example, you should be running it with ctranslate2[0]. Of all the platforms on their supported list you won’t find ROCm.<p>When you really start to look around you’ll find that ROCm is (at best) still very much in the “get it to work (sometimes)” stage. In most cases it’s still a long way away from getting it to work well, and even further away from making it actually competitive with Nvidia for serious use cases and applications.<p>People get excited about the progress ROCm has made getting basic things to work with PyTorch and this is good - progress is progress. But saving 20% on the hardware when the equivalent Nvidia product is often somewhere between 5-10x as performant (at a fraction of the development time) because of vastly superior software support you realize pretty quickly Nvidia is actually a bargain compared to AMD.<p>I’m desperately rooting for Nvidia to have some actual competition but after six years of ROCm and my own repeated failed attempts to have it make any sense overall I’m only more and more skeptical that real competition in the space will come from AMD.<p>[0] - <a href="https:&#x2F;&#x2F;github.com&#x2F;OpenNMT&#x2F;CTranslate2">https:&#x2F;&#x2F;github.com&#x2F;OpenNMT&#x2F;CTranslate2</a></div><br/><div id="37800102" class="c"><input type="checkbox" id="c-37800102" checked=""/><div class="controls bullet"><span class="by">errnoh</span><span>|</span><a href="#37795443">root</a><span>|</span><a href="#37798752">parent</a><span>|</span><a href="#37799129">next</a><span>|</span><label class="collapse" for="c-37800102">[-]</label><label class="expand" for="c-37800102">[1 more]</label></div><br/><div class="children"><div class="content">While I agree that it&#x27;s much more effort to get things working on AMD cards than it is with Nvidia, I was a bit surprised to see this comment mention Whisper being an example of &quot;5-10x as performant&quot;.<p><a href="https:&#x2F;&#x2F;www.tomshardware.com&#x2F;news&#x2F;whisper-audio-transcription-gpus-benchmarked" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.tomshardware.com&#x2F;news&#x2F;whisper-audio-transcriptio...</a> is a good example of Nvidia having no excuses being double the price when it comes to Whisper inference, with 7900XTX being directly comparable with 4080, albeit with higher power draw. To be fair it&#x27;s not using ROCm but Direct3D 11, but for performance&#x2F;price arguments sake that detail is not relevant.<p>EDIT:
Also using CTranslate2 as an example is not great as it&#x27;s actually a good showcase why ROCm is so far behind CUDA: It&#x27;s all about adapting the tech and getting the popular libraries to support it. Things usually get implemented in CUDA first and then would need additional effort to add ROCm support that projects with low amount of (possibly hobbyist) maintainers might not have available. There&#x27;s even an issue in CTranslate2 where they clearly state no-one is working to get ROCm supported in the library. ( <a href="https:&#x2F;&#x2F;github.com&#x2F;OpenNMT&#x2F;CTranslate2&#x2F;issues&#x2F;1072#issuecomment-1696983435">https:&#x2F;&#x2F;github.com&#x2F;OpenNMT&#x2F;CTranslate2&#x2F;issues&#x2F;1072#issuecomm...</a> )</div><br/></div></div></div></div><div id="37799129" class="c"><input type="checkbox" id="c-37799129" checked=""/><div class="controls bullet"><span class="by">pedrovhb</span><span>|</span><a href="#37795443">root</a><span>|</span><a href="#37797037">parent</a><span>|</span><a href="#37798752">prev</a><span>|</span><a href="#37799169">next</a><span>|</span><label class="collapse" for="c-37799129">[-]</label><label class="expand" for="c-37799129">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve had luck with an RX5700XT and whisper.cpp built with clblast. Works like a charm, not entirely a scarring experience getting it to work (easier than most other stuff which was surprising to me).<p>One arcane detail is that whereas for PyTorch I have to set the env var HSA_OVERRIDE_GFX_VERSION to 10.3.0, getting it to run with whisper.cpp and llama.cpp requires setting it to 10.1.0. Good luck and may it cost you less hair than it did me.</div><br/></div></div></div></div><div id="37799169" class="c"><input type="checkbox" id="c-37799169" checked=""/><div class="controls bullet"><span class="by">incognition</span><span>|</span><a href="#37795443">parent</a><span>|</span><a href="#37797037">prev</a><span>|</span><a href="#37794916">next</a><span>|</span><label class="collapse" for="c-37799169">[-]</label><label class="expand" for="c-37799169">[2 more]</label></div><br/><div class="children"><div class="content">Fp64??</div><br/><div id="37799487" class="c"><input type="checkbox" id="c-37799487" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#37795443">root</a><span>|</span><a href="#37799169">parent</a><span>|</span><a href="#37794916">next</a><span>|</span><label class="collapse" for="c-37799487">[-]</label><label class="expand" for="c-37799487">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Double-precision_floating-point_format" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Double-precision_floating-poin...</a><p>NVIDIA fp32 (H100) has 2x more TFLOPS than AMD&#x27;s fp32 (MI250) and AI doesn&#x27;t need fp64 precision.</div><br/></div></div></div></div></div></div><div id="37794916" class="c"><input type="checkbox" id="c-37794916" checked=""/><div class="controls bullet"><span class="by">javchz</span><span>|</span><a href="#37795443">prev</a><span>|</span><a href="#37799659">next</a><span>|</span><label class="collapse" for="c-37794916">[-]</label><label class="expand" for="c-37794916">[49 more]</label></div><br/><div class="children"><div class="content">CUDA is the only reason I have an Nvidia card, but if more projects start migrating to a more agnostic environment, I&#x27;ll be really grateful.<p>Running Nvidia in Linux isn&#x27;t as much fun. Fedora and Debian can be incredibly reliable systems, but when you add an Nvidia card, I feel like I am back in Windows Vista with kernel crashes from time to time.</div><br/><div id="37798875" class="c"><input type="checkbox" id="c-37798875" checked=""/><div class="controls bullet"><span class="by">nextaccountic</span><span>|</span><a href="#37794916">parent</a><span>|</span><a href="#37797072">next</a><span>|</span><label class="collapse" for="c-37798875">[-]</label><label class="expand" for="c-37798875">[4 more]</label></div><br/><div class="children"><div class="content">&gt; CUDA is the only reason I have an Nvidia card, but if more projects start migrating to a more agnostic environment, I&#x27;ll be really grateful.<p>What AMD really needs is to have 100% feature parity with CUDA without changing a single line of code. Maybe for this to happen it needs to add hardware features or something (I see people saying that CUDA as an API is very tailored to the capabilities of nvidia GPUs), I don&#x27;t know.<p>If AMD relies on people changing their code to make it portable, it already lost.</div><br/><div id="37799486" class="c"><input type="checkbox" id="c-37799486" checked=""/><div class="controls bullet"><span class="by">mrweasel</span><span>|</span><a href="#37794916">root</a><span>|</span><a href="#37798875">parent</a><span>|</span><a href="#37800002">next</a><span>|</span><label class="collapse" for="c-37799486">[-]</label><label class="expand" for="c-37799486">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I see people saying that CUDA as an API is very tailored to the capabilities of nvidia GPUs<p>I&#x27;m wondering how true that is, because that could give NVidia issues in the future if they need to redesign their GPU should they hit some limit with the current designs. Dependence on certain instruction makes sense, but there&#x27;s not technical preventing AMD from implementing those instructions, only legal mumbo jumbo.</div><br/></div></div><div id="37800002" class="c"><input type="checkbox" id="c-37800002" checked=""/><div class="controls bullet"><span class="by">mschuetz</span><span>|</span><a href="#37794916">root</a><span>|</span><a href="#37798875">parent</a><span>|</span><a href="#37799486">prev</a><span>|</span><a href="#37799378">next</a><span>|</span><label class="collapse" for="c-37800002">[-]</label><label class="expand" for="c-37800002">[1 more]</label></div><br/><div class="children"><div class="content">Not just feature parity, but proper UX. Things need to just work, without spending hours or days to make them work.</div><br/></div></div><div id="37799378" class="c"><input type="checkbox" id="c-37799378" checked=""/><div class="controls bullet"><span class="by">javchz</span><span>|</span><a href="#37794916">root</a><span>|</span><a href="#37798875">parent</a><span>|</span><a href="#37800002">prev</a><span>|</span><a href="#37797072">next</a><span>|</span><label class="collapse" for="c-37799378">[-]</label><label class="expand" for="c-37799378">[1 more]</label></div><br/><div class="children"><div class="content">I think that could work too. I wonder if they could do a translation layer, something like Apple with the M1 chips that translates JIT x86 to ARM.</div><br/></div></div></div></div><div id="37797072" class="c"><input type="checkbox" id="c-37797072" checked=""/><div class="controls bullet"><span class="by">distract8901</span><span>|</span><a href="#37794916">parent</a><span>|</span><a href="#37798875">prev</a><span>|</span><a href="#37795662">next</a><span>|</span><label class="collapse" for="c-37797072">[-]</label><label class="expand" for="c-37797072">[4 more]</label></div><br/><div class="children"><div class="content">My Arch system would occasionally boot to a black screen. When this happened, no amount of tinkering could get it back. I had to reinstall the whole OS.<p>Turns out it was a conflict between nvidia drivers and my (10 year old) Intel integrated GPU. But once I switched to an AMD card, everything works flawlessly.<p>Ubuntu based systems barely worked at all. Incredibly unstable and would occasionally corrupt the output and barf colors and fragments of the desktop all over my screens.<p>AMD on arch has been an absolute delight. It just. Works. It&#x27;s more stable than nvidia on windows.<p>For a lot of reasons-- but mainly Linux drivers-- I&#x27;ve totally sworn off nvidia cards. AMD just works better for me.</div><br/><div id="37797542" class="c"><input type="checkbox" id="c-37797542" checked=""/><div class="controls bullet"><span class="by">aftbit</span><span>|</span><a href="#37794916">root</a><span>|</span><a href="#37797072">parent</a><span>|</span><a href="#37795662">next</a><span>|</span><label class="collapse" for="c-37797542">[-]</label><label class="expand" for="c-37797542">[3 more]</label></div><br/><div class="children"><div class="content">As a counter-argument, I ran Arch Linux + nvidia GPUs + Intel CPUs between 2012 and 2020, and still run Arch + nvidia (now with AMD CPU) to this day. I won&#x27;t say it has been bug free at all, but it generally works pretty well. If you find a problem in Arch that you cannot fix without reinstalling, you do not sufficiently understand the problem or Arch itself. &quot;Installing&quot; Arch is refreshingly manual and &quot;simple&quot; compared to the magic that is other Linux distros or the closed source OSes.</div><br/><div id="37800029" class="c"><input type="checkbox" id="c-37800029" checked=""/><div class="controls bullet"><span class="by">iopq</span><span>|</span><a href="#37794916">root</a><span>|</span><a href="#37797542">parent</a><span>|</span><a href="#37798042">next</a><span>|</span><label class="collapse" for="c-37800029">[-]</label><label class="expand" for="c-37800029">[1 more]</label></div><br/><div class="children"><div class="content">I tried using an Nvidia card with OBS to record my screen and it kind of freezes in Wine. I switched from x11 to Wayland and now Wine shows horizontal lines (!) and performs like crap.<p>Even my 4GB RX 570 from years ago gives a better experience doing this. You just install OBS from flathub, Wayland works, everything works without any setup or tinkering. You click record and you can record your gameplay footage.</div><br/></div></div><div id="37798042" class="c"><input type="checkbox" id="c-37798042" checked=""/><div class="controls bullet"><span class="by">wildzzz</span><span>|</span><a href="#37794916">root</a><span>|</span><a href="#37797542">parent</a><span>|</span><a href="#37800029">prev</a><span>|</span><a href="#37795662">next</a><span>|</span><label class="collapse" for="c-37798042">[-]</label><label class="expand" for="c-37798042">[1 more]</label></div><br/><div class="children"><div class="content">I ran a laptop with the swappable dedicated Nvidia and integrated Intel GPU for a decade with no issues. Used to use something called Bumblebee to swap between them depending on workload, actually worked surprisingly well given the circumstances. Eventually I just dropped back to integrated only when I stopped doing anything intensive with the machine.</div><br/></div></div></div></div></div></div><div id="37795662" class="c"><input type="checkbox" id="c-37795662" checked=""/><div class="controls bullet"><span class="by">PH95VuimJjqBqy</span><span>|</span><a href="#37794916">parent</a><span>|</span><a href="#37797072">prev</a><span>|</span><a href="#37795841">next</a><span>|</span><label class="collapse" for="c-37795662">[-]</label><label class="expand" for="c-37795662">[16 more]</label></div><br/><div class="children"><div class="content">I see these complains from time to time and I never understand them.<p>I&#x27;ve literally been running nvidia on linux since the TNT2 days and have _never_ had this sort of issue.  That&#x27;s across many drivers and many cards over the many many years.</div><br/><div id="37797419" class="c"><input type="checkbox" id="c-37797419" checked=""/><div class="controls bullet"><span class="by">LtWorf</span><span>|</span><a href="#37794916">root</a><span>|</span><a href="#37795662">parent</a><span>|</span><a href="#37796601">next</a><span>|</span><label class="collapse" for="c-37797419">[-]</label><label class="expand" for="c-37797419">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve had kernel panics that disappeared when I started using the on board intel graphics instead of the nvidia.<p>Your statement makes no sense. It&#x27;s like a smoker claiming that since he didn&#x27;t die of lung cancer, smoke is 100% safe.</div><br/><div id="37798666" class="c"><input type="checkbox" id="c-37798666" checked=""/><div class="controls bullet"><span class="by">kkielhofner</span><span>|</span><a href="#37794916">root</a><span>|</span><a href="#37797419">parent</a><span>|</span><a href="#37796601">next</a><span>|</span><label class="collapse" for="c-37798666">[-]</label><label class="expand" for="c-37798666">[3 more]</label></div><br/><div class="children"><div class="content">Describing kernel panics and general nightmare scenarios as the general course with Nvidia doesn’t make sense either.<p>Nvidia has 80% market share of the discrete GPU desktop market and at least 90% market share of cloud&#x2F;datacenter.<p>Nvidia GPUs are used almost exclusively for every cloud powered AI service and to train virtually every ML model in existence. Almost always on Linux.<p>Do you really think any of this would be possible if what you are describing was anything approaching the typical experience starting at the &#x2F;driver&#x2F; level?<p>Nvidia would have never achieved their market dominance nor held on to it this long if the issues you’ve experienced impacted anything approaching a statistically significant number of users or applications.<p>Nvidia gets a lot of hate on HN and elsewhere (much of it fair) but I will never understand the people who claim it doesn’t work and get the job done (often very well).</div><br/><div id="37800042" class="c"><input type="checkbox" id="c-37800042" checked=""/><div class="controls bullet"><span class="by">iopq</span><span>|</span><a href="#37794916">root</a><span>|</span><a href="#37798666">parent</a><span>|</span><a href="#37799194">next</a><span>|</span><label class="collapse" for="c-37800042">[-]</label><label class="expand" for="c-37800042">[1 more]</label></div><br/><div class="children"><div class="content">Nvidia is bad when combined with Wine&#x2F;Firefox&#x2F;Chrome on Wayland<p>Which is literally only 1% of users anyway</div><br/></div></div><div id="37799194" class="c"><input type="checkbox" id="c-37799194" checked=""/><div class="controls bullet"><span class="by">mr_toad</span><span>|</span><a href="#37794916">root</a><span>|</span><a href="#37798666">parent</a><span>|</span><a href="#37800042">prev</a><span>|</span><a href="#37796601">next</a><span>|</span><label class="collapse" for="c-37799194">[-]</label><label class="expand" for="c-37799194">[1 more]</label></div><br/><div class="children"><div class="content">People use flakey software all their time.  As long as it mostly works most of the time most people put up with it.  Examples: Windows in the 90’s and 00’s, or any AAA game on first release in the last 10 years.</div><br/></div></div></div></div></div></div><div id="37796601" class="c"><input type="checkbox" id="c-37796601" checked=""/><div class="controls bullet"><span class="by">jjoonathan</span><span>|</span><a href="#37794916">root</a><span>|</span><a href="#37795662">parent</a><span>|</span><a href="#37797419">prev</a><span>|</span><a href="#37796278">next</a><span>|</span><label class="collapse" for="c-37796601">[-]</label><label class="expand" for="c-37796601">[5 more]</label></div><br/><div class="children"><div class="content">Same but linux experience is a steep and bumpy function of hardware.<p>My guess: something like laptop GPU switching failed badly in the nvidia binary, earning it a reputation.</div><br/><div id="37796654" class="c"><input type="checkbox" id="c-37796654" checked=""/><div class="controls bullet"><span class="by">HideousKojima</span><span>|</span><a href="#37794916">root</a><span>|</span><a href="#37796601">parent</a><span>|</span><a href="#37796278">next</a><span>|</span><label class="collapse" for="c-37796654">[-]</label><label class="expand" for="c-37796654">[4 more]</label></div><br/><div class="children"><div class="content">That was my experience, Nvidia Optimus (which is what allows dynamic switching between the integrated and dedicated GPU in laptops) was completely broken (as in a black screen, not just crashes or other issues) for several years, and Nvidia didn&#x27;t care to do anything about it.</div><br/><div id="37796699" class="c"><input type="checkbox" id="c-37796699" checked=""/><div class="controls bullet"><span class="by">lhl</span><span>|</span><a href="#37794916">root</a><span>|</span><a href="#37796654">parent</a><span>|</span><a href="#37796678">next</a><span>|</span><label class="collapse" for="c-37796699">[-]</label><label class="expand" for="c-37796699">[2 more]</label></div><br/><div class="children"><div class="content">Yeah, Optimus was a huge PITA. I remember fighting with workarounds like bumblebee and prime for years. Also Nvidia dragged their feet on Wayland support for a few years too (and simultaneously was seemingly intent on sabotaging Nouveau).</div><br/><div id="37797136" class="c"><input type="checkbox" id="c-37797136" checked=""/><div class="controls bullet"><span class="by">distract8901</span><span>|</span><a href="#37794916">root</a><span>|</span><a href="#37796699">parent</a><span>|</span><a href="#37796678">next</a><span>|</span><label class="collapse" for="c-37797136">[-]</label><label class="expand" for="c-37797136">[1 more]</label></div><br/><div class="children"><div class="content">I tried bumblebee again recently, and it works shockingly well now. I have a thinkpad T530 from 2013 with an NVS5400m.<p>There is some strange issue with some games where they don&#x27;t get full performance from the dGPU, but more than the iGPU. I have to use optirun to get full performance.<p>It also has problems when the computer wakes from sleep. For whatever reason, hardware video decoding doesn&#x27;t work after entering standby. Makes steam in home streaming crash on the client, but flipping to software decoding usually works fine.<p>The important part is that battery life is almost as good with bumblebee as it is with the dGPU turned off. No more fucking with Prime or rebooting into BIOS to turn the GPU back on.</div><br/></div></div></div></div><div id="37796678" class="c"><input type="checkbox" id="c-37796678" checked=""/><div class="controls bullet"><span class="by">PH95VuimJjqBqy</span><span>|</span><a href="#37794916">root</a><span>|</span><a href="#37796654">parent</a><span>|</span><a href="#37796699">prev</a><span>|</span><a href="#37796278">next</a><span>|</span><label class="collapse" for="c-37796678">[-]</label><label class="expand" for="c-37796678">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t run laptops except when work requires it and that tends to be windows so that may explain the difference in experience.</div><br/></div></div></div></div></div></div><div id="37796278" class="c"><input type="checkbox" id="c-37796278" checked=""/><div class="controls bullet"><span class="by">temp0826</span><span>|</span><a href="#37794916">root</a><span>|</span><a href="#37795662">parent</a><span>|</span><a href="#37796601">prev</a><span>|</span><a href="#37795676">next</a><span>|</span><label class="collapse" for="c-37796278">[-]</label><label class="expand" for="c-37796278">[3 more]</label></div><br/><div class="children"><div class="content">I understand it, but I also haven&#x27;t had any trouble since I figured out the right procedure for me on fedora (which probably took some time, but it&#x27;s been so long that I can&#x27;t remember). Whenever I read people having issues it sounds like they are using a package installed via dnf for the driver&#x2F;etc. I&#x27;ve always had issues with dkms and the like and just install the latest .run from nvidia&#x27;s website whenever I have a kernel update (I made a one-line script to call it with the silent option and flags for signing for secure boot so I don&#x27;t really think about it). No issues in a very long time even with the whackiness of prime&#x2F;optimus offloading on my old laptop.</div><br/><div id="37797772" class="c"><input type="checkbox" id="c-37797772" checked=""/><div class="controls bullet"><span class="by">bootsmann</span><span>|</span><a href="#37794916">root</a><span>|</span><a href="#37796278">parent</a><span>|</span><a href="#37796685">next</a><span>|</span><label class="collapse" for="c-37797772">[-]</label><label class="expand" for="c-37797772">[1 more]</label></div><br/><div class="children"><div class="content">So you don‘t recommend going the rpm-fusion route?</div><br/></div></div><div id="37796685" class="c"><input type="checkbox" id="c-37796685" checked=""/><div class="controls bullet"><span class="by">PH95VuimJjqBqy</span><span>|</span><a href="#37794916">root</a><span>|</span><a href="#37796278">parent</a><span>|</span><a href="#37797772">prev</a><span>|</span><a href="#37795676">next</a><span>|</span><label class="collapse" for="c-37796685">[-]</label><label class="expand" for="c-37796685">[1 more]</label></div><br/><div class="children"><div class="content">actually, it&#x27;s a good point because that&#x27;s how I always install nvidia drivers as well.  Never from the local package manager.</div><br/></div></div></div></div><div id="37795676" class="c"><input type="checkbox" id="c-37795676" checked=""/><div class="controls bullet"><span class="by">ant6n</span><span>|</span><a href="#37794916">root</a><span>|</span><a href="#37795662">parent</a><span>|</span><a href="#37796278">prev</a><span>|</span><a href="#37797320">next</a><span>|</span><label class="collapse" for="c-37795676">[-]</label><label class="expand" for="c-37795676">[2 more]</label></div><br/><div class="children"><div class="content">Well tnt2 should be pretty well supported by now ;-)</div><br/><div id="37796686" class="c"><input type="checkbox" id="c-37796686" checked=""/><div class="controls bullet"><span class="by">PH95VuimJjqBqy</span><span>|</span><a href="#37794916">root</a><span>|</span><a href="#37795676">parent</a><span>|</span><a href="#37797320">next</a><span>|</span><label class="collapse" for="c-37796686">[-]</label><label class="expand" for="c-37796686">[1 more]</label></div><br/><div class="children"><div class="content">lmao, touche :)</div><br/></div></div></div></div><div id="37797320" class="c"><input type="checkbox" id="c-37797320" checked=""/><div class="controls bullet"><span class="by">einpoklum</span><span>|</span><a href="#37794916">root</a><span>|</span><a href="#37795662">parent</a><span>|</span><a href="#37795676">prev</a><span>|</span><a href="#37795841">next</a><span>|</span><label class="collapse" for="c-37797320">[-]</label><label class="expand" for="c-37797320">[1 more]</label></div><br/><div class="children"><div class="content">I have been NVIDIA cards for compute capabilities only, both personally and at work, for nearly a decade. I&#x27;ve had dozens and dozens of different issues involving the hardware, the drivers, integration with the rest of the OS, version compatibilities, ensuring my desktop environment doesn&#x27;t try to use the NVIDIA cards, etc. etc.<p>Having said that - I (or rarely, other people) have almost always managed to work out those issues and get my systems to work. Not in all cases though.</div><br/></div></div></div></div><div id="37795841" class="c"><input type="checkbox" id="c-37795841" checked=""/><div class="controls bullet"><span class="by">kombine</span><span>|</span><a href="#37794916">parent</a><span>|</span><a href="#37795662">prev</a><span>|</span><a href="#37797784">next</a><span>|</span><label class="collapse" for="c-37795841">[-]</label><label class="expand" for="c-37795841">[3 more]</label></div><br/><div class="children"><div class="content">I use a rolling distro (OpenSUSE Tumbleweed) and have had zero issues with my NVIDIA card despite it pulling the kernel and driver updates as they get released. The driver repo is maintained by NVIDIA itself, which is amazing.</div><br/><div id="37796023" class="c"><input type="checkbox" id="c-37796023" checked=""/><div class="controls bullet"><span class="by">filterfiber</span><span>|</span><a href="#37794916">root</a><span>|</span><a href="#37795841">parent</a><span>|</span><a href="#37797784">next</a><span>|</span><label class="collapse" for="c-37796023">[-]</label><label class="expand" for="c-37796023">[2 more]</label></div><br/><div class="children"><div class="content">Do you use wayland, multiple monitors, and&#x2F;or play games or is it just for ML&#x2F;AI?</div><br/><div id="37796572" class="c"><input type="checkbox" id="c-37796572" checked=""/><div class="controls bullet"><span class="by">smoldesu</span><span>|</span><a href="#37794916">root</a><span>|</span><a href="#37796023">parent</a><span>|</span><a href="#37797784">next</a><span>|</span><label class="collapse" for="c-37796572">[-]</label><label class="expand" for="c-37796572">[1 more]</label></div><br/><div class="children"><div class="content">I do all of those things with my 3070 and it works just fine. Most of them will depend on your DE&#x27;s Wayland implementation.<p>I&#x27;m not here to desparage anyone experiencing issues, but my experience on the NixOS rolling-release channel has also been pretty boring. There was a time when my old 1050 Ti struggled, but the modern upstream drivers feel just as smooth as my Intel system does.</div><br/></div></div></div></div></div></div><div id="37797784" class="c"><input type="checkbox" id="c-37797784" checked=""/><div class="controls bullet"><span class="by">gymbeaux</span><span>|</span><a href="#37794916">parent</a><span>|</span><a href="#37795841">prev</a><span>|</span><a href="#37798225">next</a><span>|</span><label class="collapse" for="c-37797784">[-]</label><label class="expand" for="c-37797784">[3 more]</label></div><br/><div class="children"><div class="content">I often have issues booting to the installer or first boot after install with an NVidia GPU.<p>Pop_OS, Fedora and OpenSUSE work out of the box. Those are all Wayland I believe. Debian&#x2F;Ubuntu distros are a bad time. I think they’re still X11. It’s ironic because X11 is supposed to be the more stable window manager.</div><br/><div id="37800022" class="c"><input type="checkbox" id="c-37800022" checked=""/><div class="controls bullet"><span class="by">Flameancer</span><span>|</span><a href="#37794916">root</a><span>|</span><a href="#37797784">parent</a><span>|</span><a href="#37800106">next</a><span>|</span><label class="collapse" for="c-37800022">[-]</label><label class="expand" for="c-37800022">[1 more]</label></div><br/><div class="children"><div class="content">I think they moved to Wayland on 23.04 or 23.10. I just recently installed both to try and get a 7800xt working with PyTorch and the default was Wayland.</div><br/></div></div><div id="37800106" class="c"><input type="checkbox" id="c-37800106" checked=""/><div class="controls bullet"><span class="by">anthk</span><span>|</span><a href="#37794916">root</a><span>|</span><a href="#37797784">parent</a><span>|</span><a href="#37800022">prev</a><span>|</span><a href="#37798225">next</a><span>|</span><label class="collapse" for="c-37800106">[-]</label><label class="expand" for="c-37800106">[1 more]</label></div><br/><div class="children"><div class="content">X11 is not a window manager.</div><br/></div></div></div></div><div id="37798225" class="c"><input type="checkbox" id="c-37798225" checked=""/><div class="controls bullet"><span class="by">chaostheory</span><span>|</span><a href="#37794916">parent</a><span>|</span><a href="#37797784">prev</a><span>|</span><a href="#37795130">next</a><span>|</span><label class="collapse" for="c-37798225">[-]</label><label class="expand" for="c-37798225">[1 more]</label></div><br/><div class="children"><div class="content">Yeah with my CUDA setup, it feels like I just ducktaped my deployment. I am very hesitant to make changes and it’s not easy to replicate</div><br/></div></div><div id="37795130" class="c"><input type="checkbox" id="c-37795130" checked=""/><div class="controls bullet"><span class="by">smoldesu</span><span>|</span><a href="#37794916">parent</a><span>|</span><a href="#37798225">prev</a><span>|</span><a href="#37795201">next</a><span>|</span><label class="collapse" for="c-37795130">[-]</label><label class="expand" for="c-37795130">[4 more]</label></div><br/><div class="children"><div class="content">Those problems might just be GNOME-related at this point. I&#x27;ve been daily-driving two different Nvidia cards for ~3 years now (1050 Ti then 3070 Ti) and Wayland has felt pretty stable for the past 12 months. The worst problem I had experienced in that time was Electron and Java apps drawing incorrectly in xWayland, but both of those are fixed upstream.<p>I&#x27;m definitely not against better hardware support for AI, but I think your problems are more GNOME&#x27;s fault than Nvidia&#x27;s. KDE&#x27;s Wayland session is almost flawless on Nvidia nowadays.</div><br/><div id="37795144" class="c"><input type="checkbox" id="c-37795144" checked=""/><div class="controls bullet"><span class="by">arsome</span><span>|</span><a href="#37794916">root</a><span>|</span><a href="#37795130">parent</a><span>|</span><a href="#37795959">next</a><span>|</span><label class="collapse" for="c-37795144">[-]</label><label class="expand" for="c-37795144">[1 more]</label></div><br/><div class="children"><div class="content">If GNOME can tank the kernel, it ain&#x27;t GNOME&#x27;s fault.</div><br/></div></div><div id="37795959" class="c"><input type="checkbox" id="c-37795959" checked=""/><div class="controls bullet"><span class="by">kombine</span><span>|</span><a href="#37794916">root</a><span>|</span><a href="#37795130">parent</a><span>|</span><a href="#37795144">prev</a><span>|</span><a href="#37795201">next</a><span>|</span><label class="collapse" for="c-37795959">[-]</label><label class="expand" for="c-37795959">[2 more]</label></div><br/><div class="children"><div class="content">I really hope that with KDE 6 I can finally switch to Wayland!</div><br/><div id="37799275" class="c"><input type="checkbox" id="c-37799275" checked=""/><div class="controls bullet"><span class="by">Zardoz84</span><span>|</span><a href="#37794916">root</a><span>|</span><a href="#37795959">parent</a><span>|</span><a href="#37795201">next</a><span>|</span><label class="collapse" for="c-37799275">[-]</label><label class="expand" for="c-37799275">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m using KDE on Debian 12 with AMD GPU with Wayland, and works. it keeps being a bit annoying compared with X11 with a few programs (Eclipse, Dbeaver... I need to launch both with flags to not use Wayland backend). But even I can play AAA games without problems</div><br/></div></div></div></div></div></div><div id="37795201" class="c"><input type="checkbox" id="c-37795201" checked=""/><div class="controls bullet"><span class="by">wubrr</span><span>|</span><a href="#37794916">parent</a><span>|</span><a href="#37795130">prev</a><span>|</span><a href="#37799659">next</a><span>|</span><label class="collapse" for="c-37795201">[-]</label><label class="expand" for="c-37795201">[13 more]</label></div><br/><div class="children"><div class="content">Yeah, nvidia linux support is meh, but still much better than amd.</div><br/><div id="37796946" class="c"><input type="checkbox" id="c-37796946" checked=""/><div class="controls bullet"><span class="by">phkahler</span><span>|</span><a href="#37794916">root</a><span>|</span><a href="#37795201">parent</a><span>|</span><a href="#37795306">next</a><span>|</span><label class="collapse" for="c-37796946">[-]</label><label class="expand" for="c-37796946">[1 more]</label></div><br/><div class="children"><div class="content">&gt;&gt; Yeah, nvidia linux support is meh, but still much better than amd.<p>Can not confirm. I used nvidia for years when it was the only option. Then used the nouveau driver on a well supported card because it worked well and eliminated hassle. Now I&#x27;m on AMD APU and it just works out of the box. YMMV of course. We do get reports of issues with AMD on specific driver versions, but I can&#x27;t reproduce.</div><br/></div></div><div id="37795306" class="c"><input type="checkbox" id="c-37795306" checked=""/><div class="controls bullet"><span class="by">Zambyte</span><span>|</span><a href="#37794916">root</a><span>|</span><a href="#37795201">parent</a><span>|</span><a href="#37796946">prev</a><span>|</span><a href="#37796258">next</a><span>|</span><label class="collapse" for="c-37795306">[-]</label><label class="expand" for="c-37795306">[8 more]</label></div><br/><div class="children"><div class="content">Is it better than AMD? I have had literally no graphics issues on my 6650 XT with swaywm using the built in kernel drivers.</div><br/><div id="37796281" class="c"><input type="checkbox" id="c-37796281" checked=""/><div class="controls bullet"><span class="by">aseipp</span><span>|</span><a href="#37794916">root</a><span>|</span><a href="#37795306">parent</a><span>|</span><a href="#37795529">next</a><span>|</span><label class="collapse" for="c-37796281">[-]</label><label class="expand" for="c-37796281">[1 more]</label></div><br/><div class="children"><div class="content">This week I upgraded my kernel on a 2017 workstation to 6.5.5 and when I rebooted and looked at &#x27;dmesg&#x27; there were no less than 7 kernel faults with stack traces in my &#x27;dmesg&#x27; from amdgpu. Just from booting up. This is a no-graphical-desktop system using a Radeon Pro W5500, which is 3.5 years old (I just had the card and needed something to plug in for it to POST.)<p>I have come to accept that graphics card drivers and hardware stability ultimately comes down to whether or not ghosts have decided to haunt you.</div><br/></div></div><div id="37795529" class="c"><input type="checkbox" id="c-37795529" checked=""/><div class="controls bullet"><span class="by">HansHamster</span><span>|</span><a href="#37794916">root</a><span>|</span><a href="#37795306">parent</a><span>|</span><a href="#37796281">prev</a><span>|</span><a href="#37795506">next</a><span>|</span><label class="collapse" for="c-37795529">[-]</label><label class="expand" for="c-37795529">[1 more]</label></div><br/><div class="children"><div class="content">Guess I&#x27;m also doing something wrong. Never had any serious issues with either Nvidia or AMD on Linux (and only a few annoyances on RNDA2 shortly after release)...</div><br/></div></div><div id="37795506" class="c"><input type="checkbox" id="c-37795506" checked=""/><div class="controls bullet"><span class="by">treprinum</span><span>|</span><a href="#37794916">root</a><span>|</span><a href="#37795306">parent</a><span>|</span><a href="#37795529">prev</a><span>|</span><a href="#37795418">next</a><span>|</span><label class="collapse" for="c-37795506">[-]</label><label class="expand" for="c-37795506">[4 more]</label></div><br/><div class="children"><div class="content">I never had an issue with nVidia drivers on Linux in the past 5 years, but recently bought a laptop with a 4090 and AMD CPU. Now I get random freezes, often right after I login into Cinnamon but can&#x27;t really tell if it&#x27;s the nVidia driver for 4090, AMDGPU driver for integrated RDNA, kernel 6.2 or Cinnamon issue. The laptop just hangs and stops responding to keyboard so I can&#x27;t login to console and dmesg it.</div><br/><div id="37795537" class="c"><input type="checkbox" id="c-37795537" checked=""/><div class="controls bullet"><span class="by">SoftTalker</span><span>|</span><a href="#37794916">root</a><span>|</span><a href="#37795506">parent</a><span>|</span><a href="#37795418">next</a><span>|</span><label class="collapse" for="c-37795537">[-]</label><label class="expand" for="c-37795537">[3 more]</label></div><br/><div class="children"><div class="content">The main issue with Nvidia on Linux AIUI is that they don&#x27;t release the source code for their drivers.</div><br/><div id="37796155" class="c"><input type="checkbox" id="c-37796155" checked=""/><div class="controls bullet"><span class="by">treprinum</span><span>|</span><a href="#37794916">root</a><span>|</span><a href="#37795537">parent</a><span>|</span><a href="#37795418">next</a><span>|</span><label class="collapse" for="c-37796155">[-]</label><label class="expand" for="c-37796155">[2 more]</label></div><br/><div class="children"><div class="content">That might be a philosophical problem that never prevented me from training models on Linux. The half-baked half-crashing AMD solutions just lead to wasting time I can spend on ML research instead.</div><br/><div id="37799162" class="c"><input type="checkbox" id="c-37799162" checked=""/><div class="controls bullet"><span class="by">65a</span><span>|</span><a href="#37794916">root</a><span>|</span><a href="#37796155">parent</a><span>|</span><a href="#37795418">next</a><span>|</span><label class="collapse" for="c-37799162">[-]</label><label class="expand" for="c-37799162">[1 more]</label></div><br/><div class="children"><div class="content">I literally gave away my last laptop with a discrete nVidia card because it wasted so much of my time.</div><br/></div></div></div></div></div></div></div></div><div id="37795418" class="c"><input type="checkbox" id="c-37795418" checked=""/><div class="controls bullet"><span class="by">christkv</span><span>|</span><a href="#37794916">root</a><span>|</span><a href="#37795306">parent</a><span>|</span><a href="#37795506">prev</a><span>|</span><a href="#37796258">next</a><span>|</span><label class="collapse" for="c-37795418">[-]</label><label class="expand" for="c-37795418">[1 more]</label></div><br/><div class="children"><div class="content">I think the problems are pro drivers and the issues with ROCm being buggy not the open source graphics drivers.</div><br/></div></div></div></div><div id="37796258" class="c"><input type="checkbox" id="c-37796258" checked=""/><div class="controls bullet"><span class="by">bryanlarsen</span><span>|</span><a href="#37794916">root</a><span>|</span><a href="#37795201">parent</a><span>|</span><a href="#37795306">prev</a><span>|</span><a href="#37796941">next</a><span>|</span><label class="collapse" for="c-37796258">[-]</label><label class="expand" for="c-37796258">[1 more]</label></div><br/><div class="children"><div class="content">Not my experience.   The open source AMD drivers are much more pleasant to deal with than the closed source Nvidia ones.</div><br/></div></div><div id="37796941" class="c"><input type="checkbox" id="c-37796941" checked=""/><div class="controls bullet"><span class="by">silisili</span><span>|</span><a href="#37794916">root</a><span>|</span><a href="#37795201">parent</a><span>|</span><a href="#37796258">prev</a><span>|</span><a href="#37795513">next</a><span>|</span><label class="collapse" for="c-37796941">[-]</label><label class="expand" for="c-37796941">[1 more]</label></div><br/><div class="children"><div class="content">In the closed source days of fglrx or whatever it&#x27;s called I&#x27;d agree.  Since they went open source, hard disagree.  AMD graphics work in Linux about as well as Intel always has.</div><br/></div></div><div id="37795513" class="c"><input type="checkbox" id="c-37795513" checked=""/><div class="controls bullet"><span class="by">acomjean</span><span>|</span><a href="#37794916">root</a><span>|</span><a href="#37795201">parent</a><span>|</span><a href="#37796941">prev</a><span>|</span><a href="#37799659">next</a><span>|</span><label class="collapse" for="c-37795513">[-]</label><label class="expand" for="c-37795513">[1 more]</label></div><br/><div class="children"><div class="content">As someone who was tasked with trying to get nvidia working on Ubuntu,  it’s a pretty terrible experience.<p>I have a nvidia laptop with popos.  That works well.</div><br/></div></div></div></div></div></div><div id="37799659" class="c"><input type="checkbox" id="c-37799659" checked=""/><div class="controls bullet"><span class="by">Roark66</span><span>|</span><a href="#37794916">prev</a><span>|</span><a href="#37794165">next</a><span>|</span><label class="collapse" for="c-37799659">[-]</label><label class="expand" for="c-37799659">[1 more]</label></div><br/><div class="children"><div class="content">I think the article claiming &quot;PyTorch has dropped the drawbridge on the CUDA moat&quot; is way over optimistic. Jest pytorch is widely used by researchers and by users to quickly iterate various over various ways to use the models, but when it comes to inference there are huge gains to be had by going a different route. Llama.cpp has showed 10x speedups on my hardware for example (32gb of gpu ram + 32gb of cpu ram)for models like falcon-40b-instruct, for much smaller models on the cpu (under 10b) I saw up to 3x speedup just by switching to onnc and openvino.<p>Apple has showed us in practice the benefits of CPU&#x2F;GPU memory sharing, will AMD be able to follow in their footsteps? The article claims AMD has a design with up to 192gb of shared ram. Apple is already shipping a design with the same amount of RAM(if you can afford it). I wish them-and) success, but I believe they need to aim higher than just matching apple in some unspecified future.</div><br/></div></div><div id="37794165" class="c"><input type="checkbox" id="c-37794165" checked=""/><div class="controls bullet"><span class="by">IronWolve</span><span>|</span><a href="#37799659">prev</a><span>|</span><a href="#37799713">next</a><span>|</span><label class="collapse" for="c-37794165">[-]</label><label class="expand" for="c-37794165">[31 more]</label></div><br/><div class="children"><div class="content">Yup, thank the hobbyists. Pytorch is allowing other hardware. Stable diffusion working on m chips, intel arc, and Amd.<p>Now what I&#x27;d like to see is real benchmarks for compute power. Might even get a few startups to compete in this new area.</div><br/><div id="37795509" class="c"><input type="checkbox" id="c-37795509" checked=""/><div class="controls bullet"><span class="by">mandevil</span><span>|</span><a href="#37794165">parent</a><span>|</span><a href="#37794471">next</a><span>|</span><label class="collapse" for="c-37795509">[-]</label><label class="expand" for="c-37795509">[10 more]</label></div><br/><div class="children"><div class="content">It isn&#x27;t the hobbyists who are making sure that PyTorch and other frameworks runs well on these chips, but teams of engineers who work for NVIDIA, AMD, Intel, etc. who are doing this as their primary assigned jobs, in exchange for money from their employer, who are paying those salaries because they want to sell chips into the enormous demand for running PyTorch faster.<p>Hobbyist and open-source are definitely not synonyms.</div><br/><div id="37798130" class="c"><input type="checkbox" id="c-37798130" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#37794165">root</a><span>|</span><a href="#37795509">parent</a><span>|</span><a href="#37796037">next</a><span>|</span><label class="collapse" for="c-37798130">[-]</label><label class="expand" for="c-37798130">[1 more]</label></div><br/><div class="children"><div class="content">Special mention to Facebook and Google AI research teams that maintain PyTorch and Tensorflow respectively. And also to ptrblck on the PyTorch forums [1] who has the answer to basically every question it seems. He alone is probably responsible for hundreds of millions of dollars of productivity gain.<p>[1] <a href="https:&#x2F;&#x2F;discuss.pytorch.org&#x2F;u&#x2F;ptrblck&#x2F;summary" rel="nofollow noreferrer">https:&#x2F;&#x2F;discuss.pytorch.org&#x2F;u&#x2F;ptrblck&#x2F;summary</a></div><br/></div></div><div id="37796037" class="c"><input type="checkbox" id="c-37796037" checked=""/><div class="controls bullet"><span class="by">Eisenstein</span><span>|</span><a href="#37794165">root</a><span>|</span><a href="#37795509">parent</a><span>|</span><a href="#37798130">prev</a><span>|</span><a href="#37794471">next</a><span>|</span><label class="collapse" for="c-37796037">[-]</label><label class="expand" for="c-37796037">[8 more]</label></div><br/><div class="children"><div class="content">People don&#x27;t usually get employed to make things with no demand, and people who work for companies with a budget line don&#x27;t really care how much the nVidia tax is. You can thank hobbyists for creating a lot of demand for compatability with other cards.</div><br/><div id="37797244" class="c"><input type="checkbox" id="c-37797244" checked=""/><div class="controls bullet"><span class="by">kiratp</span><span>|</span><a href="#37794165">root</a><span>|</span><a href="#37796037">parent</a><span>|</span><a href="#37798246">next</a><span>|</span><label class="collapse" for="c-37797244">[-]</label><label class="expand" for="c-37797244">[1 more]</label></div><br/><div class="children"><div class="content">There are so many billions of dollar being spent on this hardware that everyone other than Nvidia is doing everything they can to make competition happen.<p>Eg: 
<a href="https:&#x2F;&#x2F;www.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;developer&#x2F;videos&#x2F;optimize-dl-workloads-intel-optimized-pytorch.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;developer&#x2F;videos&#x2F;opt...</a><p><a href="https:&#x2F;&#x2F;www.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;developer&#x2F;tools&#x2F;oneapi&#x2F;optimization-for-tensorflow.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;developer&#x2F;tools&#x2F;onea...</a><p><a href="https:&#x2F;&#x2F;developer.apple.com&#x2F;metal&#x2F;tensorflow-plugin&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;developer.apple.com&#x2F;metal&#x2F;tensorflow-plugin&#x2F;</a><p>Large scale opensource is, outside of a few exceptions, built by engineers paid to build it.</div><br/></div></div><div id="37798246" class="c"><input type="checkbox" id="c-37798246" checked=""/><div class="controls bullet"><span class="by">johngossman</span><span>|</span><a href="#37794165">root</a><span>|</span><a href="#37796037">parent</a><span>|</span><a href="#37797244">prev</a><span>|</span><a href="#37797444">next</a><span>|</span><label class="collapse" for="c-37798246">[-]</label><label class="expand" for="c-37798246">[1 more]</label></div><br/><div class="children"><div class="content">I can only point you to cloud financial results and the huge cost of the AI race. Note also the story recently about OpenAI looking at building their own chips. Companies absolutely care immensely about the cost of GPUs. It&#x27;s billions of dollars.</div><br/></div></div><div id="37797444" class="c"><input type="checkbox" id="c-37797444" checked=""/><div class="controls bullet"><span class="by">roenxi</span><span>|</span><a href="#37794165">root</a><span>|</span><a href="#37796037">parent</a><span>|</span><a href="#37798246">prev</a><span>|</span><a href="#37794471">next</a><span>|</span><label class="collapse" for="c-37797444">[-]</label><label class="expand" for="c-37797444">[5 more]</label></div><br/><div class="children"><div class="content">There is huge demand for AMD cards that can efficiently multiply matrices together. The issue is that while there are currently isolated cases where people can make them do that, it doesn&#x27;t seem to be possible at the scale that it needs to happen at.<p>AMD are being dragged along by the market. Willingly, they aren&#x27;t fighting it, but their focus has been on other areas.</div><br/><div id="37800072" class="c"><input type="checkbox" id="c-37800072" checked=""/><div class="controls bullet"><span class="by">iopq</span><span>|</span><a href="#37794165">root</a><span>|</span><a href="#37797444">parent</a><span>|</span><a href="#37797735">next</a><span>|</span><label class="collapse" for="c-37800072">[-]</label><label class="expand" for="c-37800072">[1 more]</label></div><br/><div class="children"><div class="content">Look at the earnings call:<p><a href="https:&#x2F;&#x2F;www.fool.com&#x2F;earnings&#x2F;call-transcripts&#x2F;2023&#x2F;08&#x2F;01&#x2F;advanced-micro-devices-amd-q2-2023-earnings-call-t&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.fool.com&#x2F;earnings&#x2F;call-transcripts&#x2F;2023&#x2F;08&#x2F;01&#x2F;ad...</a><p>it&#x27;s literally ALL AI, server, enterprise talk - AI is mentioned 64 times<p>AMD literally doesn&#x27;t care about gaming anymore, server is their primary focus</div><br/></div></div><div id="37797735" class="c"><input type="checkbox" id="c-37797735" checked=""/><div class="controls bullet"><span class="by">viewtransform</span><span>|</span><a href="#37794165">root</a><span>|</span><a href="#37797444">parent</a><span>|</span><a href="#37800072">prev</a><span>|</span><a href="#37798175">next</a><span>|</span><label class="collapse" for="c-37797735">[-]</label><label class="expand" for="c-37797735">[2 more]</label></div><br/><div class="children"><div class="content">&lt;but their focus has been on other areas.&gt;<p>They&#x27;ve shifted a large pool of experienced engineers from legacy software projects to AI and moved the team under a veteran Xilinx AI director.  Fingers crossed we should see significant changes in 2024.</div><br/><div id="37800081" class="c"><input type="checkbox" id="c-37800081" checked=""/><div class="controls bullet"><span class="by">Flameancer</span><span>|</span><a href="#37794165">root</a><span>|</span><a href="#37797735">parent</a><span>|</span><a href="#37798175">next</a><span>|</span><label class="collapse" for="c-37800081">[-]</label><label class="expand" for="c-37800081">[1 more]</label></div><br/><div class="children"><div class="content">As a new owner of a 7800XT I’m excited.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="37794471" class="c"><input type="checkbox" id="c-37794471" checked=""/><div class="controls bullet"><span class="by">mattnewton</span><span>|</span><a href="#37794165">parent</a><span>|</span><a href="#37795509">prev</a><span>|</span><a href="#37795318">next</a><span>|</span><label class="collapse" for="c-37794471">[-]</label><label class="expand" for="c-37794471">[15 more]</label></div><br/><div class="children"><div class="content">Re: startups, Geohotz raised a few million for this already. <a href="https:&#x2F;&#x2F;tinygrad.org&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;tinygrad.org&#x2F;</a></div><br/><div id="37794922" class="c"><input type="checkbox" id="c-37794922" checked=""/><div class="controls bullet"><span class="by">IntelMiner</span><span>|</span><a href="#37794165">root</a><span>|</span><a href="#37794471">parent</a><span>|</span><a href="#37796401">next</a><span>|</span><label class="collapse" for="c-37794922">[-]</label><label class="expand" for="c-37794922">[13 more]</label></div><br/><div class="children"><div class="content">Didn&#x27;t he do what he always does. Rake in a ton of money, fart around and then cash out exclaiming it&#x27;s everyone else&#x27;s fault?<p>The way he stole Fail0verflow&#x27;s work with the PS3 security leak after failing to find a hypervisor exploit for months absolutely soured any respect I had for him at the time</div><br/><div id="37795046" class="c"><input type="checkbox" id="c-37795046" checked=""/><div class="controls bullet"><span class="by">throwitawayfam</span><span>|</span><a href="#37794165">root</a><span>|</span><a href="#37794922">parent</a><span>|</span><a href="#37795316">next</a><span>|</span><label class="collapse" for="c-37795046">[-]</label><label class="expand" for="c-37795046">[4 more]</label></div><br/><div class="children"><div class="content">Yep, did exactly that. IMO he threw a fit, even though AMD was working with him squashing bugs. <a href="https:&#x2F;&#x2F;github.com&#x2F;RadeonOpenCompute&#x2F;ROCm&#x2F;issues&#x2F;2198#issuecomment-1574383483">https:&#x2F;&#x2F;github.com&#x2F;RadeonOpenCompute&#x2F;ROCm&#x2F;issues&#x2F;2198#issuec...</a></div><br/><div id="37796331" class="c"><input type="checkbox" id="c-37796331" checked=""/><div class="controls bullet"><span class="by">aeyes</span><span>|</span><a href="#37794165">root</a><span>|</span><a href="#37795046">parent</a><span>|</span><a href="#37796478">next</a><span>|</span><label class="collapse" for="c-37796331">[-]</label><label class="expand" for="c-37796331">[1 more]</label></div><br/><div class="children"><div class="content">He&#x27;s back on it after getting AMD&#x27;s CEO to commit resources to this:<p><a href="https:&#x2F;&#x2F;twitter.com&#x2F;realGeorgeHotz&#x2F;status&#x2F;1669803464082489347" rel="nofollow noreferrer">https:&#x2F;&#x2F;twitter.com&#x2F;realGeorgeHotz&#x2F;status&#x2F;166980346408248934...</a><p><a href="https:&#x2F;&#x2F;twitter.com&#x2F;LisaSu&#x2F;status&#x2F;1669848494637735936" rel="nofollow noreferrer">https:&#x2F;&#x2F;twitter.com&#x2F;LisaSu&#x2F;status&#x2F;1669848494637735936</a></div><br/></div></div><div id="37796478" class="c"><input type="checkbox" id="c-37796478" checked=""/><div class="controls bullet"><span class="by">nomel</span><span>|</span><a href="#37794165">root</a><span>|</span><a href="#37795046">parent</a><span>|</span><a href="#37796331">prev</a><span>|</span><a href="#37796436">next</a><span>|</span><label class="collapse" for="c-37796478">[-]</label><label class="expand" for="c-37796478">[1 more]</label></div><br/><div class="children"><div class="content">To be fair, kernel crashes from running an AMD provided demo loop isn’t something he should have to work with them on. That’s borderline incompetence. His perspective was around integration into his product, where every AMD bug is a bug in his product. They deserve criticism, and responded accordingly (actual resources to get their shit together). It’s not like GPU accelerated ML is some new thing.</div><br/></div></div></div></div><div id="37795316" class="c"><input type="checkbox" id="c-37795316" checked=""/><div class="controls bullet"><span class="by">kinematikk</span><span>|</span><a href="#37794165">root</a><span>|</span><a href="#37794922">parent</a><span>|</span><a href="#37795046">prev</a><span>|</span><a href="#37795412">next</a><span>|</span><label class="collapse" for="c-37795316">[-]</label><label class="expand" for="c-37795316">[5 more]</label></div><br/><div class="children"><div class="content">Do you have a source on the stealing part? A quick Google search didn&#x27;t result in anything</div><br/><div id="37795519" class="c"><input type="checkbox" id="c-37795519" checked=""/><div class="controls bullet"><span class="by">IntelMiner</span><span>|</span><a href="#37794165">root</a><span>|</span><a href="#37795316">parent</a><span>|</span><a href="#37795412">next</a><span>|</span><label class="collapse" for="c-37795519">[-]</label><label class="expand" for="c-37795519">[4 more]</label></div><br/><div class="children"><div class="content">Marcan (of Asahi Linux fame) has talked about it <i>many</i> times before. But an abridged version<p>Fail0verflow demoed how they were able to derive the private signing keys for the Sony Playstation 3 console at I believe CCC<p>Geohot after watching the livestream raced into action to demo a &quot;hello world!&quot; jailbreak application and absolutely stole their thunder without giving any credit</div><br/><div id="37797814" class="c"><input type="checkbox" id="c-37797814" checked=""/><div class="controls bullet"><span class="by">ryanjshaw</span><span>|</span><a href="#37794165">root</a><span>|</span><a href="#37795519">parent</a><span>|</span><a href="#37797567">next</a><span>|</span><label class="collapse" for="c-37797814">[-]</label><label class="expand" for="c-37797814">[1 more]</label></div><br/><div class="children"><div class="content">If they demod something then  they released it publically and it was fair game?<p>In any case he absolutely did credit them, it&#x27;s easily verifiable: <a href="https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20110104040706&#x2F;http:&#x2F;&#x2F;geohot.com&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20110104040706&#x2F;http:&#x2F;&#x2F;geohot.com...</a><p>Sony sued them both, afterall!</div><br/></div></div><div id="37797567" class="c"><input type="checkbox" id="c-37797567" checked=""/><div class="controls bullet"><span class="by">aftbit</span><span>|</span><a href="#37794165">root</a><span>|</span><a href="#37795519">parent</a><span>|</span><a href="#37797814">prev</a><span>|</span><a href="#37795412">next</a><span>|</span><label class="collapse" for="c-37797567">[-]</label><label class="expand" for="c-37797567">[2 more]</label></div><br/><div class="children"><div class="content">This apparently worked pretty well for him, as I still remember him primarily as &quot;that guy who hacked PS3&quot;. Some people let someone else do the hard technical core, then do all the other easy but boring stuff and claim 100% credit.</div><br/><div id="37797815" class="c"><input type="checkbox" id="c-37797815" checked=""/><div class="controls bullet"><span class="by">22c</span><span>|</span><a href="#37794165">root</a><span>|</span><a href="#37797567">parent</a><span>|</span><a href="#37795412">next</a><span>|</span><label class="collapse" for="c-37797815">[-]</label><label class="expand" for="c-37797815">[1 more]</label></div><br/><div class="children"><div class="content">I remember geohot as being one of the people who developed a fairly successful jailbreak for iPhone. I understand that iPhone jailbreaking is often standing on the shoulders of predecessors, but I believe he does deserve significant credit for at least one popular iPhone jailbreak.</div><br/></div></div></div></div></div></div></div></div><div id="37795412" class="c"><input type="checkbox" id="c-37795412" checked=""/><div class="controls bullet"><span class="by">adastra22</span><span>|</span><a href="#37794165">root</a><span>|</span><a href="#37794922">parent</a><span>|</span><a href="#37795316">prev</a><span>|</span><a href="#37797927">next</a><span>|</span><label class="collapse" for="c-37795412">[-]</label><label class="expand" for="c-37795412">[1 more]</label></div><br/><div class="children"><div class="content">Wow, TIL</div><br/></div></div><div id="37797927" class="c"><input type="checkbox" id="c-37797927" checked=""/><div class="controls bullet"><span class="by">ShamelessC</span><span>|</span><a href="#37794165">root</a><span>|</span><a href="#37794922">parent</a><span>|</span><a href="#37795412">prev</a><span>|</span><a href="#37796401">next</a><span>|</span><label class="collapse" for="c-37797927">[-]</label><label class="expand" for="c-37797927">[2 more]</label></div><br/><div class="children"><div class="content">&gt; The way he stole Fail0verflow&#x27;s work with the PS3 security leak after failing to find a hypervisor exploit for months absolutely soured any respect I had for him at the time<p>That sounds interesting. I tried googling about it but can&#x27;t really find much other than that failoverflow found a key and didn&#x27;t release it, and then geohot released his own subsequently. I&#x27;d love to hear more about how directly he &quot;stole&quot; the work from the Fail0verflow team.<p>edit: Reading some sibling comments here, it seems you are either mistaken and&#x2F;or were exaggerating your claim about the &quot;theft&quot; here. As far as I can tell, he simply took their findings and made his own version of an exploit that they had detailed publicly. That may be in poor taste in this particular community but it&#x27;s certainly not theft. I do agree that his behavior there was lacking in decency, but not to the degree implied here where I was thinking he _literally_ stole their exploit by hacking them, or something similar to that.</div><br/><div id="37798518" class="c"><input type="checkbox" id="c-37798518" checked=""/><div class="controls bullet"><span class="by">cyrux004</span><span>|</span><a href="#37794165">root</a><span>|</span><a href="#37797927">parent</a><span>|</span><a href="#37796401">next</a><span>|</span><label class="collapse" for="c-37798518">[-]</label><label class="expand" for="c-37798518">[1 more]</label></div><br/><div class="children"><div class="content">People here generally try to bash people who are much smarter than them, throwing shade at their background. They will say that he abandoned his first company, gave up on tiny grad but both of them are very much alive projects</div><br/></div></div></div></div></div></div><div id="37796401" class="c"><input type="checkbox" id="c-37796401" checked=""/><div class="controls bullet"><span class="by">nomel</span><span>|</span><a href="#37794165">root</a><span>|</span><a href="#37794471">parent</a><span>|</span><a href="#37794922">prev</a><span>|</span><a href="#37795318">next</a><span>|</span><label class="collapse" for="c-37796401">[-]</label><label class="expand" for="c-37796401">[1 more]</label></div><br/><div class="children"><div class="content">Obligatory Lex Fridman podcast, where he discusses it: <a href="https:&#x2F;&#x2F;youtu.be&#x2F;dNrTrx42DGQ?t=2408" rel="nofollow noreferrer">https:&#x2F;&#x2F;youtu.be&#x2F;dNrTrx42DGQ?t=2408</a></div><br/></div></div></div></div><div id="37795318" class="c"><input type="checkbox" id="c-37795318" checked=""/><div class="controls bullet"><span class="by">jauntywundrkind</span><span>|</span><a href="#37794165">parent</a><span>|</span><a href="#37794471">prev</a><span>|</span><a href="#37799713">next</a><span>|</span><label class="collapse" for="c-37795318">[-]</label><label class="expand" for="c-37795318">[5 more]</label></div><br/><div class="children"><div class="content">Pytorch is just using Google&#x27;s OpenXLA now, &amp; OpenXLA is the actual cross platform thing, no? I&#x27;m not very well versed in this area, so pardon if mistaken. <a href="https:&#x2F;&#x2F;pytorch.org&#x2F;blog&#x2F;pytorch-2.0-xla-path-forward&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;pytorch.org&#x2F;blog&#x2F;pytorch-2.0-xla-path-forward&#x2F;</a></div><br/><div id="37796616" class="c"><input type="checkbox" id="c-37796616" checked=""/><div class="controls bullet"><span class="by">fotcorn</span><span>|</span><a href="#37794165">root</a><span>|</span><a href="#37795318">parent</a><span>|</span><a href="#37797405">next</a><span>|</span><label class="collapse" for="c-37796616">[-]</label><label class="expand" for="c-37796616">[2 more]</label></div><br/><div class="children"><div class="content">You can use OpenXLA, but it&#x27;s not the default. The main use-case for OpenXLA is running  PyTorch on Google TPUs. OpenXLA also supports GPUs, but I am not sure how many people use that. Afaik JAX uses OpenXLA as backend to run on GPUs.<p>If you use model.compile() in PyTorch, you use TorchInductor and OpenAIs Triton by default.</div><br/><div id="37798711" class="c"><input type="checkbox" id="c-37798711" checked=""/><div class="controls bullet"><span class="by">jauntywundrkind</span><span>|</span><a href="#37794165">root</a><span>|</span><a href="#37796616">parent</a><span>|</span><a href="#37797405">next</a><span>|</span><label class="collapse" for="c-37798711">[-]</label><label class="expand" for="c-37798711">[1 more]</label></div><br/><div class="children"><div class="content">Thank you for saying something useful here. I was vaguely under the impression that pytorch 2.0 had fully flipped to defaulting to openxla. That seems to not be the case.<p>Good to hear more than a cheap snub. OpenAI Triton as the reason other GPUs work is a real non-shit answer, it seems. And interesting to hear JAX too. Thank you for being robustly useful &amp; informative.</div><br/></div></div></div></div><div id="37797405" class="c"><input type="checkbox" id="c-37797405" checked=""/><div class="controls bullet"><span class="by">mathisfun123</span><span>|</span><a href="#37794165">root</a><span>|</span><a href="#37795318">parent</a><span>|</span><a href="#37796616">prev</a><span>|</span><a href="#37798536">next</a><span>|</span><label class="collapse" for="c-37797405">[-]</label><label class="expand" for="c-37797405">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Pytorch is just using Google&#x27;s OpenXLA now<p>this is so far from accurate it should be considered libelous; from the link<p>&gt; PyTorch&#x2F;XLA is set to migrate to the open source OpenXLA<p>so PyTorch on the XLA backend is set to migrate to use OpenXLA instead of XLA. but basically everyone moved from XLA to OpenXLA because there is no more OSS XLA. so that&#x27;s it. in general, PyTorch has several backends, including plenty of homegrown CUDA and CPU kernels. in fact the majority of your PyTorch code runs through PyTorch&#x27;s own kernels.</div><br/></div></div><div id="37798536" class="c"><input type="checkbox" id="c-37798536" checked=""/><div class="controls bullet"><span class="by">voz_</span><span>|</span><a href="#37794165">root</a><span>|</span><a href="#37795318">parent</a><span>|</span><a href="#37797405">prev</a><span>|</span><a href="#37799713">next</a><span>|</span><label class="collapse" for="c-37798536">[-]</label><label class="expand" for="c-37798536">[1 more]</label></div><br/><div class="children"><div class="content">Wrong.</div><br/></div></div></div></div></div></div><div id="37799713" class="c"><input type="checkbox" id="c-37799713" checked=""/><div class="controls bullet"><span class="by">physicsguy</span><span>|</span><a href="#37794165">prev</a><span>|</span><a href="#37795847">next</a><span>|</span><label class="collapse" for="c-37799713">[-]</label><label class="expand" for="c-37799713">[1 more]</label></div><br/><div class="children"><div class="content">Don’t agree at all. PyTorch is one library - yes, it’s important that it supports AMD GPUs but it’s not enough.<p>The ROCm libraries just aren’t good enough currently. The documentation is poor. AMD need to heavily invest in their software ecosystem around it, because library authors need decent support to adopt it. If you need to be a Facebook sized organisation to write an AMD and CUDA compatible library then the barrier to entry is too high.</div><br/></div></div><div id="37795847" class="c"><input type="checkbox" id="c-37795847" checked=""/><div class="controls bullet"><span class="by">pama</span><span>|</span><a href="#37799713">prev</a><span>|</span><a href="#37797478">next</a><span>|</span><label class="collapse" for="c-37795847">[-]</label><label class="expand" for="c-37795847">[4 more]</label></div><br/><div class="children"><div class="content">There is only limited empirical evidence of AMD closing the gap that NVidia has created in the science or ML software. Even when considering pytorch only, the engineering effort to maintain specialized ROCm along with CUDA solutions is not trivial (think flashattention, or any customization that optimizes your own model). If your GPUs only need a simple ML workflow all times for a few years nonstop, maybe there exist corner cases where the finances make sense.  It is hard for AMD now to close the gap across the scientific&#x2F;industrial software base of CUDA.  NVidia feels like a software company for the hardware they produce; luckily they make the money from hardware thus cannot lock the software libraries.<p>(Edited “no” to limited empirical evidence after a fellow user mentioned El Capitan.)</div><br/><div id="37796700" class="c"><input type="checkbox" id="c-37796700" checked=""/><div class="controls bullet"><span class="by">fotcorn</span><span>|</span><a href="#37795847">parent</a><span>|</span><a href="#37796128">next</a><span>|</span><label class="collapse" for="c-37796700">[-]</label><label class="expand" for="c-37796700">[1 more]</label></div><br/><div class="children"><div class="content">ROCm has HIP (1) which is a compatibility layer to run CUDA code on AMD GPUs. In theory, you only have to adjust #includes, and everything should just work, but as usual, reality is different.<p>Newer backends for AI frameworks like OpenXLA and OpenAI Triton directly generate GPU native code using MLIR and LLVM, they do not use CUDA apart from some glue code to actually load the code onto the GPU and get the data there. Both already support ROCm, but from what I&#x27;ve read the support is not as mature yet compared to NVIDIA.<p>1: <a href="https:&#x2F;&#x2F;github.com&#x2F;ROCm-Developer-Tools&#x2F;HIP">https:&#x2F;&#x2F;github.com&#x2F;ROCm-Developer-Tools&#x2F;HIP</a></div><br/></div></div><div id="37796128" class="c"><input type="checkbox" id="c-37796128" checked=""/><div class="controls bullet"><span class="by">Certhas</span><span>|</span><a href="#37795847">parent</a><span>|</span><a href="#37796700">prev</a><span>|</span><a href="#37797478">next</a><span>|</span><label class="collapse" for="c-37796128">[-]</label><label class="expand" for="c-37796128">[2 more]</label></div><br/><div class="children"><div class="content">The fact that El Capitan is AMD says that at least for Science&#x2F;HPC there definitely is evidence of a closing gap.</div><br/><div id="37796321" class="c"><input type="checkbox" id="c-37796321" checked=""/><div class="controls bullet"><span class="by">pama</span><span>|</span><a href="#37795847">root</a><span>|</span><a href="#37796128">parent</a><span>|</span><a href="#37797478">next</a><span>|</span><label class="collapse" for="c-37796321">[-]</label><label class="expand" for="c-37796321">[1 more]</label></div><br/><div class="children"><div class="content">Thanks. You are actually right that this new supercomputer might move the needle once it is in production mode. I will wait and see how it goes.</div><br/></div></div></div></div></div></div><div id="37797478" class="c"><input type="checkbox" id="c-37797478" checked=""/><div class="controls bullet"><span class="by">withwarmup</span><span>|</span><a href="#37795847">prev</a><span>|</span><a href="#37794750">next</a><span>|</span><label class="collapse" for="c-37797478">[-]</label><label class="expand" for="c-37797478">[6 more]</label></div><br/><div class="children"><div class="content">CUDA is the result of years of NVIDIA supporting the ecosystem, some people likes to complain because they bought hardware that was cheaper but can&#x27;t use it for what they want to use it, when you buy NVIDIA, you aren&#x27;t buying only the hardware, but the insane amount of work they have put into the ecosystem, the same goes for Intel, mkl and scikit-learn intelex aren&#x27;t free to develop.<p>AMD has the hardware but the support for HPC is non-existent outside of the joke that is bliss and AOCL.<p>I really wish for more competitors to enter the market in HPC, but AMD has a shitload of work to do.</div><br/><div id="37798252" class="c"><input type="checkbox" id="c-37798252" checked=""/><div class="controls bullet"><span class="by">arcanus</span><span>|</span><a href="#37797478">parent</a><span>|</span><a href="#37798989">next</a><span>|</span><label class="collapse" for="c-37798252">[-]</label><label class="expand" for="c-37798252">[2 more]</label></div><br/><div class="children"><div class="content">&gt; AMD has the hardware but the support for HPC is non-existent outside of the joke that is bliss and AOCL.<p>You are probably two years behind the state of the art. The world&#x27;s largest supercomputer, OLCF&#x27;s Frontier, runs AMD CPUs and GPUs. It&#x27;s emphatically using ROCm, not just BLIS and AOCL. See for example: <a href="https:&#x2F;&#x2F;docs.olcf.ornl.gov&#x2F;systems&#x2F;frontier_user_guide.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;docs.olcf.ornl.gov&#x2F;systems&#x2F;frontier_user_guide.html</a><p>That&#x27;s hardly non-existent support for HPC.</div><br/><div id="37799173" class="c"><input type="checkbox" id="c-37799173" checked=""/><div class="controls bullet"><span class="by">65a</span><span>|</span><a href="#37797478">root</a><span>|</span><a href="#37798252">parent</a><span>|</span><a href="#37798989">next</a><span>|</span><label class="collapse" for="c-37799173">[-]</label><label class="expand" for="c-37799173">[1 more]</label></div><br/><div class="children"><div class="content">Agreed...the main gap is support on consumer and workstation cards, which is where nVidia made headway, but that is starting erode super recently. ROCm works pretty well for me, I have had a lot more problems with specific packagers than the ROCm layer.</div><br/></div></div></div></div><div id="37798989" class="c"><input type="checkbox" id="c-37798989" checked=""/><div class="controls bullet"><span class="by">aiunboxed</span><span>|</span><a href="#37797478">parent</a><span>|</span><a href="#37798252">prev</a><span>|</span><a href="#37797850">next</a><span>|</span><label class="collapse" for="c-37798989">[-]</label><label class="expand" for="c-37798989">[1 more]</label></div><br/><div class="children"><div class="content">Exactly, with NVIDIAs core focus on AI way before it was cool has lead to them being in this advantageous position. For AMD just being a price friendly competitor to Intel and Nvidia was the motto.</div><br/></div></div><div id="37797850" class="c"><input type="checkbox" id="c-37797850" checked=""/><div class="controls bullet"><span class="by">runiq</span><span>|</span><a href="#37797478">parent</a><span>|</span><a href="#37798989">prev</a><span>|</span><a href="#37794750">next</a><span>|</span><label class="collapse" for="c-37797850">[-]</label><label class="expand" for="c-37797850">[2 more]</label></div><br/><div class="children"><div class="content">Yeah, that&#x27;s a pretty shortsighted take of things. Do you really believe that Nvidia hasn&#x27;t taken steps do make sure their moat is as wide as possible?</div><br/><div id="37798206" class="c"><input type="checkbox" id="c-37798206" checked=""/><div class="controls bullet"><span class="by">Blammar</span><span>|</span><a href="#37797478">root</a><span>|</span><a href="#37797850">parent</a><span>|</span><a href="#37794750">next</a><span>|</span><label class="collapse" for="c-37798206">[-]</label><label class="expand" for="c-37798206">[1 more]</label></div><br/><div class="children"><div class="content">The thing about owning the CUDA spec is that Nvidia can add new features quickly without having to argue with other hardware vendors. I find that a positive thing overall.<p>Also, I choose to pay the ~$120 Windows tax once (per box), everything works very well, and I don&#x27;t have the driver issues that some fraction of other users seem to have with Linux and Nvidia cards. Seems like a good use of my time.</div><br/></div></div></div></div></div></div><div id="37794750" class="c"><input type="checkbox" id="c-37794750" checked=""/><div class="controls bullet"><span class="by">pixelesque</span><span>|</span><a href="#37797478">prev</a><span>|</span><a href="#37798344">next</a><span>|</span><label class="collapse" for="c-37794750">[-]</label><label class="expand" for="c-37794750">[3 more]</label></div><br/><div class="children"><div class="content">Does AMD have a solution to forward device combatibility (like PTX for NVidia)?<p>Last time I looked into ROCm (two years ago?), you seemed to have to compile stuff explicitly for the architecture you were using, so if a new card came out, you couldn&#x27;t use it without a recompile.</div><br/><div id="37795173" class="c"><input type="checkbox" id="c-37795173" checked=""/><div class="controls bullet"><span class="by">mnau</span><span>|</span><a href="#37794750">parent</a><span>|</span><a href="#37797362">next</a><span>|</span><label class="collapse" for="c-37795173">[-]</label><label class="expand" for="c-37795173">[1 more]</label></div><br/><div class="children"><div class="content">Not natively, but AdaptiveCpp (previously hiSycl, then OpenSycl) has a single source single compiler pass, where they basically store LLVM IR as an intermediate representation.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;AdaptiveCpp&#x2F;AdaptiveCpp&#x2F;blob&#x2F;develop&#x2F;doc&#x2F;compilation.md">https:&#x2F;&#x2F;github.com&#x2F;AdaptiveCpp&#x2F;AdaptiveCpp&#x2F;blob&#x2F;develop&#x2F;doc&#x2F;...</a><p>Performance penalty was within ew precents, at least according to the paper (figure 9 and 10)
<a href="https:&#x2F;&#x2F;cdrdv2-public.intel.com&#x2F;786536&#x2F;Heidelberg_IWOCL__SYCLCon_2023_paper_2566-1.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;cdrdv2-public.intel.com&#x2F;786536&#x2F;Heidelberg_IWOCL__SYC...</a></div><br/></div></div><div id="37797362" class="c"><input type="checkbox" id="c-37797362" checked=""/><div class="controls bullet"><span class="by">einpoklum</span><span>|</span><a href="#37794750">parent</a><span>|</span><a href="#37795173">prev</a><span>|</span><a href="#37798344">next</a><span>|</span><label class="collapse" for="c-37797362">[-]</label><label class="expand" for="c-37797362">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t know what they do with ROCm, but with OpenCL, the answer is: Certainly. It&#x27;s called SPIR:<p><a href="https:&#x2F;&#x2F;www.khronos.org&#x2F;spir&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.khronos.org&#x2F;spir&#x2F;</a></div><br/></div></div></div></div><div id="37798344" class="c"><input type="checkbox" id="c-37798344" checked=""/><div class="controls bullet"><span class="by">mark_l_watson</span><span>|</span><a href="#37794750">prev</a><span>|</span><a href="#37795259">next</a><span>|</span><label class="collapse" for="c-37798344">[-]</label><label class="expand" for="c-37798344">[1 more]</label></div><br/><div class="children"><div class="content">NVidia hardware&#x2F;CUDA stack is great, but I also love to see competition from AMD, George Hotz’s Tiny Corp, etc.<p>Off topic, but I am also looking with great interest at Apple Silicon SOCs with large internal RAM. The internal bandwidth also keeps getting better which is important for running trained LLMs.<p>Back on topic: I don’t own any current Intel computers but using Colab and services like Lambda Labs GPU VPSs is simple and flexible. A few people here mentioned if AMD can’t handle 100% of their workload they will stick with Intel and NVidia - understandable position, but there are workarounds.</div><br/></div></div><div id="37795259" class="c"><input type="checkbox" id="c-37795259" checked=""/><div class="controls bullet"><span class="by">bigcat12345678</span><span>|</span><a href="#37798344">prev</a><span>|</span><a href="#37794933">next</a><span>|</span><label class="collapse" for="c-37795259">[-]</label><label class="expand" for="c-37795259">[3 more]</label></div><br/><div class="children"><div class="content">Cuda is the foundation<p>NVIDIA moat is the years of work built by oss community, big corporations, research insistute<p>They spend all time building for cuda, a lot of implicit designs are derived from cuda&#x27;s characteristic<p>That will be the main challenge</div><br/><div id="37795837" class="c"><input type="checkbox" id="c-37795837" checked=""/><div class="controls bullet"><span class="by">mikepurvis</span><span>|</span><a href="#37795259">parent</a><span>|</span><a href="#37794933">next</a><span>|</span><label class="collapse" for="c-37795837">[-]</label><label class="expand" for="c-37795837">[2 more]</label></div><br/><div class="children"><div class="content">It depends on the domain. Increasingly people&#x27;s interfaces to this stuff are the higher level libraries like tensorflow, pytorch, numpy&#x2F;cupy, and to a lesser degree accelerated processing libraries such as opencv, PCL, suitesparse, ceres-solver, and friends.<p>If you can add hardware support to a major library <i>and</i> improve on the packaging and deployment front while also undercutting on price, that&#x27;s the moat gone overnight. CUDA itself only matters in terms of lock-in if you&#x27;re calling CUDA&#x27;s own functions.</div><br/><div id="37795929" class="c"><input type="checkbox" id="c-37795929" checked=""/><div class="controls bullet"><span class="by">bigcat12345678</span><span>|</span><a href="#37795259">root</a><span>|</span><a href="#37795837">parent</a><span>|</span><a href="#37794933">next</a><span>|</span><label class="collapse" for="c-37795929">[-]</label><label class="expand" for="c-37795929">[1 more]</label></div><br/><div class="children"><div class="content">what I meant is that all these stuff have 15 years of implicit accumulation of knowledge and tips and even hacks builtin in the software<p>No matter what you depends on, you&#x27;ll have a slew of larger or minor obstacles or annoyance<p>That collectively is the most itself<p>As you said, already it&#x27;s clear that replacing cuda itself is not that daunting</div><br/></div></div></div></div></div></div><div id="37794933" class="c"><input type="checkbox" id="c-37794933" checked=""/><div class="controls bullet"><span class="by">nabla9</span><span>|</span><a href="#37795259">prev</a><span>|</span><a href="#37799696">next</a><span>|</span><label class="collapse" for="c-37794933">[-]</label><label class="expand" for="c-37794933">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Crossing the CUDA moat for AMD GPUs may be as easy as using PyTorch.<p>Nvidia has spent huge amount of work to make code run smoothly and fast. AMD has to work hard to catch up.  ROCm code is slower , has more bugs, don&#x27;t  have  enough features and they have compatibility issues between cards.</div><br/><div id="37795780" class="c"><input type="checkbox" id="c-37795780" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#37794933">parent</a><span>|</span><a href="#37797353">next</a><span>|</span><label class="collapse" for="c-37795780">[-]</label><label class="expand" for="c-37795780">[1 more]</label></div><br/><div class="children"><div class="content">Lisa has said that they are committed to improving ROCm, especially for AI workloads. Recent releases (5.6&#x2F;5.7) prove that.</div><br/></div></div><div id="37797353" class="c"><input type="checkbox" id="c-37797353" checked=""/><div class="controls bullet"><span class="by">einpoklum</span><span>|</span><a href="#37794933">parent</a><span>|</span><a href="#37795780">prev</a><span>|</span><a href="#37799696">next</a><span>|</span><label class="collapse" for="c-37797353">[-]</label><label class="expand" for="c-37797353">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Nvidia has spent huge amount of work to make code run smoothly and fast.<p>Well, let&#x27;s say &quot;smoother&quot; rather than &quot;smoothly&quot;.<p>&gt; ROCm code is slower<p>On physically-comparable hardware? Possible, but that&#x27;s not an easy claim to make, certainly not as expansively as you have. References?<p>&gt; has more bugs<p>Possible, but - NVIDIA keeps their bug database secret. I&#x27;m guessing you&#x27;re concluding this from anecdotal experience? That&#x27;s fair enough, but then - say so.<p>&gt; ROCm ... don&#x27;t have enough features and<p>Likely. while AMD has both spent less in that department (and had less to spend I guess); plus, and no less importantly - it tried to go along with the OpenCL initiative, as specified by the Khronos consortium, while NVIDIA has sort of &quot;betrayed&quot; the initiative by investing in it&#x27;s vendor-locked, incompatible ecosystem and letting their OpenCL support decay in some respects.<p>&gt; they have compatibility issues between cards.<p>such as?</div><br/><div id="37798902" class="c"><input type="checkbox" id="c-37798902" checked=""/><div class="controls bullet"><span class="by">kkielhofner</span><span>|</span><a href="#37794933">root</a><span>|</span><a href="#37797353">parent</a><span>|</span><a href="#37799696">next</a><span>|</span><label class="collapse" for="c-37798902">[-]</label><label class="expand" for="c-37798902">[1 more]</label></div><br/><div class="children"><div class="content">I wouldn’t say ROCm code is “slower”, per se, but in practice that’s how it presents. References:<p><a href="https:&#x2F;&#x2F;github.com&#x2F;InternLM&#x2F;lmdeploy">https:&#x2F;&#x2F;github.com&#x2F;InternLM&#x2F;lmdeploy</a><p><a href="https:&#x2F;&#x2F;github.com&#x2F;vllm-project&#x2F;vllm">https:&#x2F;&#x2F;github.com&#x2F;vllm-project&#x2F;vllm</a><p><a href="https:&#x2F;&#x2F;github.com&#x2F;OpenNMT&#x2F;CTranslate2">https:&#x2F;&#x2F;github.com&#x2F;OpenNMT&#x2F;CTranslate2</a><p>You know what’s missing from all of these and many more like them? Support for ROCm. This is all before you get to the really wildly performant stuff like Triton Inference Server, FasterTransformer, TensorRT-LLM, etc.<p>ROCm is at the “get it to work stage” (see top comment, blog posts everywhere celebrating minor successes, etc). CUDA is at the “wring every last penny of performance out of this thing” stage.<p>In terms of hardware support, I think that one is obvious. The U in CUDA originally stood for unified. Look at the list of chips supported by Nvidia drivers and CUDA releases. Literally anything from at least the past 10 years that has Nvidia printed on the box will just run CUDA code.<p>One of my projects specifically targets Pascal up - when I thought even Pascal was a stretch. Cue my surprise when I got a report of someone casually firing it up on Maxwell when I was pretty certain there was no way it could work.<p>A Maxwell <i>laptop</i> chip. It also runs just as well on an H100.<p>THAT is hardware support.</div><br/></div></div></div></div></div></div><div id="37799696" class="c"><input type="checkbox" id="c-37799696" checked=""/><div class="controls bullet"><span class="by">benreesman</span><span>|</span><a href="#37794933">prev</a><span>|</span><a href="#37794569">next</a><span>|</span><label class="collapse" for="c-37799696">[-]</label><label class="expand" for="c-37799696">[1 more]</label></div><br/><div class="children"><div class="content">I know a lot of people don’t like George, I dislike plenty of people who are doing the right thing thing (including by some measures sama and siebel while they were pushing YC forward).<p>But not admitting the tinygrad project is the best Rebel Alliance on this is just a matter of letting vibe overcome results.</div><br/></div></div><div id="37794569" class="c"><input type="checkbox" id="c-37794569" checked=""/><div class="controls bullet"><span class="by">binarymax</span><span>|</span><a href="#37799696">prev</a><span>|</span><a href="#37796631">next</a><span>|</span><label class="collapse" for="c-37794569">[-]</label><label class="expand" for="c-37794569">[27 more]</label></div><br/><div class="children"><div class="content">And the question for most that remains once AMD catches up: will the duopoly result in lower prices to a reasonable level for hobbyists or bootstrapped startups, or will AMD just gouge like NVidia?</div><br/><div id="37795175" class="c"><input type="checkbox" id="c-37795175" checked=""/><div class="controls bullet"><span class="by">quitit</span><span>|</span><a href="#37794569">parent</a><span>|</span><a href="#37794608">next</a><span>|</span><label class="collapse" for="c-37795175">[-]</label><label class="expand" for="c-37795175">[3 more]</label></div><br/><div class="children"><div class="content">I think in this case the changes needed to make AMD useful will open the market to other players as well (e.g. Intel).<p>PyTorch is already walking down this path and while CUDA-based performance is significantly better, that is changing and of course an area of continued focus.<p>It&#x27;s not that people don&#x27;t like Nvidia, rather it&#x27;s just that there is a lot of hardware out there that can technically perform competitively, but the work needs to be done to bring it into the circle.</div><br/><div id="37795297" class="c"><input type="checkbox" id="c-37795297" checked=""/><div class="controls bullet"><span class="by">binarymax</span><span>|</span><a href="#37794569">root</a><span>|</span><a href="#37795175">parent</a><span>|</span><a href="#37794608">next</a><span>|</span><label class="collapse" for="c-37795297">[-]</label><label class="expand" for="c-37795297">[2 more]</label></div><br/><div class="children"><div class="content">Last I checked I saw the H100 was about two gens more advanced for certain components (tensor cores, bfloats, cache, mem bandwidth) - but my research may have been wrong as admittedly I&#x27;m not as familiar with AMDs offerings for GPU.</div><br/><div id="37796357" class="c"><input type="checkbox" id="c-37796357" checked=""/><div class="controls bullet"><span class="by">FuriouslyAdrift</span><span>|</span><a href="#37794569">root</a><span>|</span><a href="#37795297">parent</a><span>|</span><a href="#37794608">next</a><span>|</span><label class="collapse" for="c-37796357">[-]</label><label class="expand" for="c-37796357">[1 more]</label></div><br/><div class="children"><div class="content">They are not behind... <a href="https:&#x2F;&#x2F;www.tomshardware.com&#x2F;news&#x2F;amd-expands-mi300-with-gpu-only-model-eight-gpu-platform-with-15tb-of-hbm3" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.tomshardware.com&#x2F;news&#x2F;amd-expands-mi300-with-gpu...</a><p>You can also actually buy them as opposed to the nVidia offerings which you are going to have to fight for.</div><br/></div></div></div></div></div></div><div id="37794608" class="c"><input type="checkbox" id="c-37794608" checked=""/><div class="controls bullet"><span class="by">klysm</span><span>|</span><a href="#37794569">parent</a><span>|</span><a href="#37795175">prev</a><span>|</span><a href="#37797761">next</a><span>|</span><label class="collapse" for="c-37794608">[-]</label><label class="expand" for="c-37794608">[12 more]</label></div><br/><div class="children"><div class="content">A simplistic economic take would suggest that the competition would result in lower prices, but given two players in the market who knows.</div><br/><div id="37794681" class="c"><input type="checkbox" id="c-37794681" checked=""/><div class="controls bullet"><span class="by">binarymax</span><span>|</span><a href="#37794569">root</a><span>|</span><a href="#37794608">parent</a><span>|</span><a href="#37794850">next</a><span>|</span><label class="collapse" for="c-37794681">[-]</label><label class="expand" for="c-37794681">[3 more]</label></div><br/><div class="children"><div class="content">My intuition is along the lines that if AMD had a competing product earlier, then it would have kept prices down.  But since Nvidia has shown what the market will pay, AMD won&#x27;t be able to resist overcharging.  It will probably come down a little, but nowhere near to the point of affordability.<p>I sure hope I&#x27;m wrong.</div><br/><div id="37795973" class="c"><input type="checkbox" id="c-37795973" checked=""/><div class="controls bullet"><span class="by">tyre</span><span>|</span><a href="#37794569">root</a><span>|</span><a href="#37794681">parent</a><span>|</span><a href="#37794850">next</a><span>|</span><label class="collapse" for="c-37795973">[-]</label><label class="expand" for="c-37795973">[2 more]</label></div><br/><div class="children"><div class="content">AMD might have to charge less to break into customers that are already bought into Nvidia. There has to be a discount to cover the switching costs + still provide savings (or access).</div><br/><div id="37796626" class="c"><input type="checkbox" id="c-37796626" checked=""/><div class="controls bullet"><span class="by">zirgs</span><span>|</span><a href="#37794569">root</a><span>|</span><a href="#37795973">parent</a><span>|</span><a href="#37794850">next</a><span>|</span><label class="collapse" for="c-37796626">[-]</label><label class="expand" for="c-37796626">[1 more]</label></div><br/><div class="children"><div class="content">AMD will have to provide a REALLY steep discount to convince me to come back.</div><br/></div></div></div></div></div></div><div id="37794850" class="c"><input type="checkbox" id="c-37794850" checked=""/><div class="controls bullet"><span class="by">sumtechguy</span><span>|</span><a href="#37794569">root</a><span>|</span><a href="#37794608">parent</a><span>|</span><a href="#37794681">prev</a><span>|</span><a href="#37797467">next</a><span>|</span><label class="collapse" for="c-37794850">[-]</label><label class="expand" for="c-37794850">[7 more]</label></div><br/><div class="children"><div class="content">It is oligopoly pricing.<p><a href="https:&#x2F;&#x2F;www.investopedia.com&#x2F;terms&#x2F;o&#x2F;oligopoly.asp" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.investopedia.com&#x2F;terms&#x2F;o&#x2F;oligopoly.asp</a><p>With that few competitors pricing would not change much.</div><br/><div id="37795049" class="c"><input type="checkbox" id="c-37795049" checked=""/><div class="controls bullet"><span class="by">AnthonyMouse</span><span>|</span><a href="#37794569">root</a><span>|</span><a href="#37794850">parent</a><span>|</span><a href="#37795089">next</a><span>|</span><label class="collapse" for="c-37795049">[-]</label><label class="expand" for="c-37795049">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s mostly when there isn&#x27;t a lot of price elasticity of demand. If you&#x27;re Comcast and Verizon, each customer wants one internet connection and you&#x27;re not going to change the size of the market much by offering better prices.<p>If you&#x27;re AMD and NVIDIA and lowering the price would double the number of customers, you might very well want to do that, unless you&#x27;re supply constrained -- which has been the issue because they&#x27;re both bidding against everyone else for limited fab capacity. But that should be temporary.<p>This is also a market with a network effect. If all your GPUs are $1000 and nobody can afford them then nobody is going to write code for them, and then who wants them? So the winning strategy is actually to make sure that there are kind of okay GPUs available for less than $300 and make sure lots of people have them, then sell very expensive ones that use the same architecture but are faster.<p>That has been the traditional model, but the lack of production capacity meant that they&#x27;ve only been making the overpriced ones recently. Which isn&#x27;t actually in their interests once the supply of fab capacity loosens up.</div><br/><div id="37798193" class="c"><input type="checkbox" id="c-37798193" checked=""/><div class="controls bullet"><span class="by">ngcc_hk</span><span>|</span><a href="#37794569">root</a><span>|</span><a href="#37795049">parent</a><span>|</span><a href="#37795089">next</a><span>|</span><label class="collapse" for="c-37798193">[-]</label><label class="expand" for="c-37798193">[1 more]</label></div><br/><div class="children"><div class="content">Actually there is already a market like this they are in - game.  Most Gpu used are low to mid-range see steam.  The AI has to and will go down to that level for using or gaming.  You cannot just have game for intel … you did.  Then steam work hard and realize the steam deck.  You can have total different software like j and a did.  Hence you really can’t have 1 N to rule for long.  Do thank for it and all the fish, without it we might be still doing Gpu for numerical computing research.</div><br/></div></div></div></div><div id="37795089" class="c"><input type="checkbox" id="c-37795089" checked=""/><div class="controls bullet"><span class="by">ad404b8a372f2b9</span><span>|</span><a href="#37794569">root</a><span>|</span><a href="#37794850">parent</a><span>|</span><a href="#37795049">prev</a><span>|</span><a href="#37795252">next</a><span>|</span><label class="collapse" for="c-37795089">[-]</label><label class="expand" for="c-37795089">[1 more]</label></div><br/><div class="children"><div class="content">Prices seemed to have lowered when AMD came out with CPUs competitive with Intel&#x27;s.</div><br/></div></div><div id="37795252" class="c"><input type="checkbox" id="c-37795252" checked=""/><div class="controls bullet"><span class="by">tibbydudeza</span><span>|</span><a href="#37794569">root</a><span>|</span><a href="#37794850">parent</a><span>|</span><a href="#37795089">prev</a><span>|</span><a href="#37797467">next</a><span>|</span><label class="collapse" for="c-37795252">[-]</label><label class="expand" for="c-37795252">[3 more]</label></div><br/><div class="children"><div class="content">Price difference between 13900K and AMD Ryzen 9 7950x is not big - the latest  7950X3D is about on par with the higher clocked 13900KS as well.</div><br/><div id="37795874" class="c"><input type="checkbox" id="c-37795874" checked=""/><div class="controls bullet"><span class="by">redeeman</span><span>|</span><a href="#37794569">root</a><span>|</span><a href="#37795252">parent</a><span>|</span><a href="#37797467">next</a><span>|</span><label class="collapse" for="c-37795874">[-]</label><label class="expand" for="c-37795874">[2 more]</label></div><br/><div class="children"><div class="content">because intel lowered their prices</div><br/><div id="37800015" class="c"><input type="checkbox" id="c-37800015" checked=""/><div class="controls bullet"><span class="by">tibbydudeza</span><span>|</span><a href="#37794569">root</a><span>|</span><a href="#37795874">parent</a><span>|</span><a href="#37797467">next</a><span>|</span><label class="collapse" for="c-37800015">[-]</label><label class="expand" for="c-37800015">[1 more]</label></div><br/><div class="children"><div class="content">I was on the market last month - Intel was the better choice because AM5 boards and DDR5 was too expensive.<p>Ryzen 9 7950X — $799 on release
Intel 13900K - $589.</div><br/></div></div></div></div></div></div></div></div><div id="37797467" class="c"><input type="checkbox" id="c-37797467" checked=""/><div class="controls bullet"><span class="by">hackerlight</span><span>|</span><a href="#37794569">root</a><span>|</span><a href="#37794608">parent</a><span>|</span><a href="#37794850">prev</a><span>|</span><a href="#37797761">next</a><span>|</span><label class="collapse" for="c-37797467">[-]</label><label class="expand" for="c-37797467">[1 more]</label></div><br/><div class="children"><div class="content">Looking at the CPU market, competition did lead to lower prices: AMD are the best, but their CPUs are very reasonably priced because Intel is close behind.<p>In the gaming market for GPUs, Nvidia has no competition except in some niche areas. Overall, their lead in upscaling software is too commanding so they can price how they want. Customers are paying 15-20% premiums for the same raw hardware performance, all to access Nvidia&#x27;s DLSS, because there&#x27;s no good competition.</div><br/></div></div></div></div><div id="37797761" class="c"><input type="checkbox" id="c-37797761" checked=""/><div class="controls bullet"><span class="by">adamsvystun</span><span>|</span><a href="#37794569">parent</a><span>|</span><a href="#37794608">prev</a><span>|</span><a href="#37795365">next</a><span>|</span><label class="collapse" for="c-37797761">[-]</label><label class="expand" for="c-37797761">[1 more]</label></div><br/><div class="children"><div class="content">This is not a binary question. Two players, while not ideal, are better then just one.</div><br/></div></div><div id="37795365" class="c"><input type="checkbox" id="c-37795365" checked=""/><div class="controls bullet"><span class="by">evanjrowley</span><span>|</span><a href="#37794569">parent</a><span>|</span><a href="#37797761">prev</a><span>|</span><a href="#37798272">next</a><span>|</span><label class="collapse" for="c-37795365">[-]</label><label class="expand" for="c-37795365">[1 more]</label></div><br/><div class="children"><div class="content">AMD prices will go up because of the newfound ability to gouge for AI&#x2F;ML&#x2F;GPGPU workloads. Nvidia&#x27;s will likely go down, but I don&#x27;t expect it will be by much. The market demand is high, so the equilibrium price will also be high. Supply isn&#x27;t at pandemic &#x2F; crypto-rush lows, but the supply of  cards useful for CUDA&#x2F;ROCm still is.</div><br/></div></div><div id="37798272" class="c"><input type="checkbox" id="c-37798272" checked=""/><div class="controls bullet"><span class="by">johngossman</span><span>|</span><a href="#37794569">parent</a><span>|</span><a href="#37795365">prev</a><span>|</span><a href="#37794781">next</a><span>|</span><label class="collapse" for="c-37798272">[-]</label><label class="expand" for="c-37798272">[1 more]</label></div><br/><div class="children"><div class="content">When AMD caught up to Intel in CPUs, prices went down (at least compared to when Intel had a complete monopoly). The same was true when AMD gaming cards were more competitive. Chip manufacturers have shown themselves willing to both raise prices when they can and lower them when they must.</div><br/></div></div><div id="37794781" class="c"><input type="checkbox" id="c-37794781" checked=""/><div class="controls bullet"><span class="by">rafaelmn</span><span>|</span><a href="#37794569">parent</a><span>|</span><a href="#37798272">prev</a><span>|</span><a href="#37795470">next</a><span>|</span><label class="collapse" for="c-37794781">[-]</label><label class="expand" for="c-37794781">[5 more]</label></div><br/><div class="children"><div class="content">If the margins and demand is there Intel will eventually show up</div><br/><div id="37794896" class="c"><input type="checkbox" id="c-37794896" checked=""/><div class="controls bullet"><span class="by">wmf</span><span>|</span><a href="#37794569">root</a><span>|</span><a href="#37794781">parent</a><span>|</span><a href="#37794826">next</a><span>|</span><label class="collapse" for="c-37794896">[-]</label><label class="expand" for="c-37794896">[2 more]</label></div><br/><div class="children"><div class="content">Intel already showed up three or four times but their software is as bad as AMD&#x27;s used to be.</div><br/><div id="37795112" class="c"><input type="checkbox" id="c-37795112" checked=""/><div class="controls bullet"><span class="by">ilc</span><span>|</span><a href="#37794569">root</a><span>|</span><a href="#37794896">parent</a><span>|</span><a href="#37794826">next</a><span>|</span><label class="collapse" for="c-37795112">[-]</label><label class="expand" for="c-37795112">[1 more]</label></div><br/><div class="children"><div class="content">Thankfully, software can be fixed over time as AMD has shown.  Lack of another competitor can&#x27;t be fixed as easily.</div><br/></div></div></div></div><div id="37794826" class="c"><input type="checkbox" id="c-37794826" checked=""/><div class="controls bullet"><span class="by">Havoc</span><span>|</span><a href="#37794569">root</a><span>|</span><a href="#37794781">parent</a><span>|</span><a href="#37794896">prev</a><span>|</span><a href="#37795470">next</a><span>|</span><label class="collapse" for="c-37794826">[-]</label><label class="expand" for="c-37794826">[2 more]</label></div><br/><div class="children"><div class="content">Is either in doubt?</div><br/><div id="37794858" class="c"><input type="checkbox" id="c-37794858" checked=""/><div class="controls bullet"><span class="by">rafaelmn</span><span>|</span><a href="#37794569">root</a><span>|</span><a href="#37794826">parent</a><span>|</span><a href="#37795470">next</a><span>|</span><label class="collapse" for="c-37794858">[-]</label><label class="expand" for="c-37794858">[1 more]</label></div><br/><div class="children"><div class="content">Wouldn&#x27;t be surprised if a bunch of investment is hype bubble and demand correction forces price correction. Maybe not immediately but at Intel&#x27;s pace - they managed to miss out on mining bubble, wouldn&#x27;t be surprised for them to release in a correction.</div><br/></div></div></div></div></div></div><div id="37795470" class="c"><input type="checkbox" id="c-37795470" checked=""/><div class="controls bullet"><span class="by">rdsubhas</span><span>|</span><a href="#37794569">parent</a><span>|</span><a href="#37794781">prev</a><span>|</span><a href="#37794886">next</a><span>|</span><label class="collapse" for="c-37795470">[-]</label><label class="expand" for="c-37795470">[1 more]</label></div><br/><div class="children"><div class="content">Demand will push AMD prices up by couple hundred bucks and Nvidia cards down by couple hundred bucks. A hobbyist customer will be neither better or worse.</div><br/></div></div><div id="37794886" class="c"><input type="checkbox" id="c-37794886" checked=""/><div class="controls bullet"><span class="by">wil421</span><span>|</span><a href="#37794569">parent</a><span>|</span><a href="#37795470">prev</a><span>|</span><a href="#37797930">next</a><span>|</span><label class="collapse" for="c-37794886">[-]</label><label class="expand" for="c-37794886">[1 more]</label></div><br/><div class="children"><div class="content">Why would their investors allow anything else? I’m sure they see it as a huge loss like intel and mobile.</div><br/></div></div><div id="37797930" class="c"><input type="checkbox" id="c-37797930" checked=""/><div class="controls bullet"><span class="by">stjohnswarts</span><span>|</span><a href="#37794569">parent</a><span>|</span><a href="#37794886">prev</a><span>|</span><a href="#37796631">next</a><span>|</span><label class="collapse" for="c-37797930">[-]</label><label class="expand" for="c-37797930">[1 more]</label></div><br/><div class="children"><div class="content">In general I think it will lower prices, certainly not as much as if there were 4+ on the market where it&#x27;s hard to anticipate your rivals. a 2 body system is pretty straight forward, 3 body can be stable for a while with some restrictions, a 4 body problem is really damn hard...</div><br/></div></div></div></div><div id="37796631" class="c"><input type="checkbox" id="c-37796631" checked=""/><div class="controls bullet"><span class="by">ris</span><span>|</span><a href="#37794569">prev</a><span>|</span><a href="#37799135">next</a><span>|</span><label class="collapse" for="c-37796631">[-]</label><label class="expand" for="c-37796631">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t understand the author&#x27;s argument (if there is one) - pytorch has existed for ages. AMD&#x27;s Instinct MI* range has existed for years now. If these are the key ingredients why has it not already happened?</div><br/></div></div><div id="37799135" class="c"><input type="checkbox" id="c-37799135" checked=""/><div class="controls bullet"><span class="by">upbeat_general</span><span>|</span><a href="#37796631">prev</a><span>|</span><a href="#37794735">next</a><span>|</span><label class="collapse" for="c-37799135">[-]</label><label class="expand" for="c-37799135">[3 more]</label></div><br/><div class="children"><div class="content">This article doesn’t address the real challenge [in my mind].<p>Framework support is one thing, but what about the million standalone CUDA kernels that have been written, especially common in research. Nobody wants to spend time re-writing&#x2F;porting those, especially when they probably don’t understand the low-level details in the first place.<p>Not to mention, what is the plan for comprehensive framework support? I’ve experienced the pain of porting models to different hardware architectures where various ops are unsupported. Is it realistic to get full coverage of e.g., PyTorch?</div><br/><div id="37799276" class="c"><input type="checkbox" id="c-37799276" checked=""/><div class="controls bullet"><span class="by">bdowling</span><span>|</span><a href="#37799135">parent</a><span>|</span><a href="#37799740">next</a><span>|</span><label class="collapse" for="c-37799276">[-]</label><label class="expand" for="c-37799276">[1 more]</label></div><br/><div class="children"><div class="content">Someone could reimplement CUDA for AMD hardware. That would be legal because copying APIs for compatibility purposes is not copyright infringement. (See <i>Google LLC v. Oracle America</i>, Inc., 593 U.S. ___ (2021)).<p>AMD is unlikely to do this, however, because it would commodify their own products under their competitor’s API.<p>A third party could do it though. It may make sense as an open source project.</div><br/></div></div><div id="37799740" class="c"><input type="checkbox" id="c-37799740" checked=""/><div class="controls bullet"><span class="by">blueboo</span><span>|</span><a href="#37799135">parent</a><span>|</span><a href="#37799276">prev</a><span>|</span><a href="#37794735">next</a><span>|</span><label class="collapse" for="c-37799740">[-]</label><label class="expand" for="c-37799740">[1 more]</label></div><br/><div class="children"><div class="content">Research kernels mostly turn to ash upon publication anyway. The wheel turns and the next post-doc gives ROCm a try and we move on</div><br/></div></div></div></div><div id="37794735" class="c"><input type="checkbox" id="c-37794735" checked=""/><div class="controls bullet"><span class="by">alecco</span><span>|</span><a href="#37799135">prev</a><span>|</span><a href="#37795689">next</a><span>|</span><label class="collapse" for="c-37794735">[-]</label><label class="expand" for="c-37794735">[1 more]</label></div><br/><div class="children"><div class="content">Regurgitated months-old content. blogspam</div><br/></div></div><div id="37795689" class="c"><input type="checkbox" id="c-37795689" checked=""/><div class="controls bullet"><span class="by">fluxem</span><span>|</span><a href="#37794735">prev</a><span>|</span><a href="#37794749">next</a><span>|</span><label class="collapse" for="c-37795689">[-]</label><label class="expand" for="c-37795689">[5 more]</label></div><br/><div class="children"><div class="content">I call it the 90% problem. If AMD works for 90% of my projects, I would still buy NVIDIA, which works for 100%, even though I’m paying a premium</div><br/><div id="37795966" class="c"><input type="checkbox" id="c-37795966" checked=""/><div class="controls bullet"><span class="by">hot_gril</span><span>|</span><a href="#37795689">parent</a><span>|</span><a href="#37794749">next</a><span>|</span><label class="collapse" for="c-37795966">[-]</label><label class="expand" for="c-37795966">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;m lazy, so it&#x27;s 99% for me. I don&#x27;t even mess with AMD CPUs; I know they&#x27;re not <i>exactly</i> the same instruction set as Intel, and more importantly they work with a different (and less mainstream) set of mobos, so I don&#x27;t want em. If AMD manages to pull more customers their way, that&#x27;s great, it just means lower Intel premium for me.</div><br/><div id="37799509" class="c"><input type="checkbox" id="c-37799509" checked=""/><div class="controls bullet"><span class="by">bornfreddy</span><span>|</span><a href="#37795689">root</a><span>|</span><a href="#37795966">parent</a><span>|</span><a href="#37799193">next</a><span>|</span><label class="collapse" for="c-37799509">[-]</label><label class="expand" for="c-37799509">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s an interesting take. AMD mobos are no &quot;less mainstream&quot; than Intel ones are... When you choose a CPU you are also choosing a compatible mobo chipset. The companies that make motherboards are mostly the same, so there should be no big difference between those.<p>Also, while the CPU instruction sets are not exactly equal, the same is true for Intel processors of different generations too. And it doesn&#x27;t matter one bit... Unless there is a bug in CPU you will never notice the difference, because it is taken care of at the compiler &#x2F; kernel level.<p>Intel does have some advantages (and disadvantages too) over AMD, just not those.</div><br/></div></div><div id="37799193" class="c"><input type="checkbox" id="c-37799193" checked=""/><div class="controls bullet"><span class="by">65a</span><span>|</span><a href="#37795689">root</a><span>|</span><a href="#37795966">parent</a><span>|</span><a href="#37799509">prev</a><span>|</span><a href="#37797720">next</a><span>|</span><label class="collapse" for="c-37799193">[-]</label><label class="expand" for="c-37799193">[1 more]</label></div><br/><div class="children"><div class="content">As an owner of some Sapphire Rapids parts, let me just direct you to: <a href="https:&#x2F;&#x2F;edc.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;design&#x2F;products-and-solutions&#x2F;processors-and-chipsets&#x2F;eagle-stream&#x2F;sapphire-rapids-specification-update&#x2F;001US&#x2F;errata-summary-table&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;edc.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;design&#x2F;products-and-...</a></div><br/></div></div></div></div></div></div><div id="37794749" class="c"><input type="checkbox" id="c-37794749" checked=""/><div class="controls bullet"><span class="by">ddtaylor</span><span>|</span><a href="#37795689">prev</a><span>|</span><a href="#37795992">next</a><span>|</span><label class="collapse" for="c-37794749">[-]</label><label class="expand" for="c-37794749">[4 more]</label></div><br/><div class="children"><div class="content">It&#x27;s worth noting that AMD also has a ROCm port of Tensorflow.</div><br/><div id="37794877" class="c"><input type="checkbox" id="c-37794877" checked=""/><div class="controls bullet"><span class="by">ginko</span><span>|</span><a href="#37794749">parent</a><span>|</span><a href="#37795992">next</a><span>|</span><label class="collapse" for="c-37794877">[-]</label><label class="expand" for="c-37794877">[3 more]</label></div><br/><div class="children"><div class="content">When I try to install rocm-ml-sdk on Arch linux it&#x27;ll tell me the total installed size would be about 18GB.<p>What can possibly explain this much bloat for what should essentially be a library on top of a graphics driver as well as some tools (compiler, profiler etc.)? 
A couple hundred MB I could understand if they come with graphical apps and demos, but not this..</div><br/><div id="37795518" class="c"><input type="checkbox" id="c-37795518" checked=""/><div class="controls bullet"><span class="by">tomsmeding</span><span>|</span><a href="#37794749">root</a><span>|</span><a href="#37794877">parent</a><span>|</span><a href="#37795992">next</a><span>|</span><label class="collapse" for="c-37795518">[-]</label><label class="expand" for="c-37795518">[2 more]</label></div><br/><div class="children"><div class="content">A regular TensorFlow installation, just the Python library, is an 184 MB wheel that unpacks to about 1.2 GB of stuff. I have no clue what mess goes in there, but it&#x27;s a lot.<p>Still, if you&#x27;re right that this package seems to take 18 GB disk size, something weird is going on.</div><br/><div id="37795941" class="c"><input type="checkbox" id="c-37795941" checked=""/><div class="controls bullet"><span class="by">slavik81</span><span>|</span><a href="#37794749">root</a><span>|</span><a href="#37795518">parent</a><span>|</span><a href="#37795992">next</a><span>|</span><label class="collapse" for="c-37795941">[-]</label><label class="expand" for="c-37795941">[1 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a lot of kernels that are specialized for particular sets of input parameters and tuned for improved performance on specific hardware, which makes the libraries a couple hundred megabytes per architecture. The ROCm libraries are huge because they are fat binaries containing native machine code for ~13 different GPU architectures.</div><br/></div></div></div></div></div></div></div></div><div id="37795992" class="c"><input type="checkbox" id="c-37795992" checked=""/><div class="controls bullet"><span class="by">the__alchemist</span><span>|</span><a href="#37794749">prev</a><span>|</span><a href="#37795823">next</a><span>|</span><label class="collapse" for="c-37795992">[-]</label><label class="expand" for="c-37795992">[1 more]</label></div><br/><div class="children"><div class="content">When coding using Vulkan, for graphics or compute (The latter is the relevant one here), you need to have CPU code (Written in C++, Rust etc), then serialize it as bytes, then have shaders which run on the graphics card. This 3-step process creates friction, much in the same way as backend&#x2F;serialization&#x2F;frontend does in web dev. Duplication of work, type checking not going across the bridge, the shader language being limited etc.<p>My understanding is CUDA&#x27;s main strength is avoiding this. Do you agree? Is that why it&#x27;s such a big deal? Ie, why this article was written, since you could always do compute shaders on AMD etc using Vulkan.</div><br/></div></div><div id="37795823" class="c"><input type="checkbox" id="c-37795823" checked=""/><div class="controls bullet"><span class="by">hot_gril</span><span>|</span><a href="#37795992">prev</a><span>|</span><a href="#37798025">next</a><span>|</span><label class="collapse" for="c-37795823">[-]</label><label class="expand" for="c-37795823">[4 more]</label></div><br/><div class="children"><div class="content">People complain about Nvidia being anticompetitive with CUDA, but I don&#x27;t really see it. They saw a gap in the standards for on-GPU compute and put tons of effort into a proprietary alternative. They tied CUDA to their own hardware, which sorta makes technical sense given the optimizations involved, but it&#x27;s their choice anyway. They still support the open standards, but many prefer CUDA and will pay the Nvidia premium for it because it&#x27;s actually nicer. They also don&#x27;t have CPU marketshare to tie things to.<p>Good for them. We can hope the open side catches up either by improving their standards, or adding more layers like this article describes.</div><br/><div id="37796593" class="c"><input type="checkbox" id="c-37796593" checked=""/><div class="controls bullet"><span class="by">zirgs</span><span>|</span><a href="#37795823">parent</a><span>|</span><a href="#37798025">next</a><span>|</span><label class="collapse" for="c-37796593">[-]</label><label class="expand" for="c-37796593">[3 more]</label></div><br/><div class="children"><div class="content">CUDA was released in 2007 and the development of it started even earlier - possibly even in the 90s. Back then nobody else cared about GPU compute. OpenCL came out 2 years after that.</div><br/><div id="37797359" class="c"><input type="checkbox" id="c-37797359" checked=""/><div class="controls bullet"><span class="by">killerstorm</span><span>|</span><a href="#37795823">root</a><span>|</span><a href="#37796593">parent</a><span>|</span><a href="#37798025">next</a><span>|</span><label class="collapse" for="c-37797359">[-]</label><label class="expand" for="c-37797359">[2 more]</label></div><br/><div class="children"><div class="content">Not true. People got interested in general-purpose GPU compute (GPGPU) in early 2000s when video cards with programmable shaders became available. <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;General-purpose_computing_on_graphics_processing_units#History" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;General-purpose_computing_on_g...</a><p>People made a programming language &amp; a compiler&#x2F;runtime for GPGPU in 2004: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;BrookGPU" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;BrookGPU</a></div><br/><div id="37797639" class="c"><input type="checkbox" id="c-37797639" checked=""/><div class="controls bullet"><span class="by">hot_gril</span><span>|</span><a href="#37795823">root</a><span>|</span><a href="#37797359">parent</a><span>|</span><a href="#37798025">next</a><span>|</span><label class="collapse" for="c-37797639">[-]</label><label class="expand" for="c-37797639">[1 more]</label></div><br/><div class="children"><div class="content">Everything has old beginnings that the specialists will remember, but GPU compute really reached mass popularity and became a large selling point for Nvidia in the 2010s.</div><br/></div></div></div></div></div></div></div></div><div id="37798025" class="c"><input type="checkbox" id="c-37798025" checked=""/><div class="controls bullet"><span class="by">tails4e</span><span>|</span><a href="#37795823">prev</a><span>|</span><a href="#37795569">next</a><span>|</span><label class="collapse" for="c-37798025">[-]</label><label class="expand" for="c-37798025">[1 more]</label></div><br/><div class="children"><div class="content">AMD playing catch up is a good thing, their SW solution is intended to run on any HW, and with hip being basically line for line compatible with cuda it makes porting very easy. They did it with FSR,and they are doing it with rocm. Hopefully it takes off as it&#x27;s a more open ecosystem for the industry. Necessity is the mother of invention and all that.</div><br/></div></div><div id="37795569" class="c"><input type="checkbox" id="c-37795569" checked=""/><div class="controls bullet"><span class="by">frnkng</span><span>|</span><a href="#37798025">prev</a><span>|</span><a href="#37795149">next</a><span>|</span><label class="collapse" for="c-37795569">[-]</label><label class="expand" for="c-37795569">[8 more]</label></div><br/><div class="children"><div class="content">As a former ETH miner I learned the hard way that saving a few bucks on hardware may not be worth operational issues.<p>I had a miner running with Nividia cards and a miner running with AMD cards. One of them had massive maintenance demand and the other did not. I will not state which brand was better imho.<p>Currently I estimate that running miners and running gpu servers has similar operational requirements and finally at scale similar financial considerations.<p>So, whatever is cheapest to operate in terms of time expenditure, hw cost, energy use,… will be used the most.<p>P.s.: I ran the mining operation not to earn money but mainly out of curiosity. And it was a small scale business powered by a pv system and a attached heat pump.</div><br/><div id="37795760" class="c"><input type="checkbox" id="c-37795760" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#37795569">parent</a><span>|</span><a href="#37795149">next</a><span>|</span><label class="collapse" for="c-37795760">[-]</label><label class="expand" for="c-37795760">[7 more]</label></div><br/><div class="children"><div class="content">I ran 150,000+ AMD cards for mining ETH. Once I fully automated all the vbios installs and individual card tuning, it ran beautifully. Took a lot of work to get there though!<p>Fact is that every single GPU chip is a snowflake. No two operate the same.</div><br/><div id="37795915" class="c"><input type="checkbox" id="c-37795915" checked=""/><div class="controls bullet"><span class="by">rottencupcakes</span><span>|</span><a href="#37795569">root</a><span>|</span><a href="#37795760">parent</a><span>|</span><a href="#37795149">next</a><span>|</span><label class="collapse" for="c-37795915">[-]</label><label class="expand" for="c-37795915">[6 more]</label></div><br/><div class="children"><div class="content">Have you ever written about this enterprise? This sounds super unique and I would be very interested in hearing about how it was run and how it turned out.</div><br/><div id="37796188" class="c"><input type="checkbox" id="c-37796188" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#37795569">root</a><span>|</span><a href="#37795915">parent</a><span>|</span><a href="#37795149">next</a><span>|</span><label class="collapse" for="c-37796188">[-]</label><label class="expand" for="c-37796188">[5 more]</label></div><br/><div class="children"><div class="content">It was unique, not many people on the planet, that I know of, who&#x27;ve run as many GPUs as I have. Especially not working for a giant company with large teams of people. For the tech team, it was just me and one other guy. Everything <i>had</i> to be automated because there was no way we could survive otherwise.<p>I&#x27;ve put a bunch of comments here on HN about the stuff I can talk about.<p>It no longer exists after PoS.</div><br/><div id="37796314" class="c"><input type="checkbox" id="c-37796314" checked=""/><div class="controls bullet"><span class="by">freedomben</span><span>|</span><a href="#37795569">root</a><span>|</span><a href="#37796188">parent</a><span>|</span><a href="#37795149">next</a><span>|</span><label class="collapse" for="c-37796314">[-]</label><label class="expand" for="c-37796314">[4 more]</label></div><br/><div class="children"><div class="content">what type of cards did you have?  what did you do with them after PoS?  How did you even buy so many cards?  Sorry, like the other commenter I&#x27;m extremely curious</div><br/><div id="37796396" class="c"><input type="checkbox" id="c-37796396" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#37795569">root</a><span>|</span><a href="#37796314">parent</a><span>|</span><a href="#37795149">next</a><span>|</span><label class="collapse" for="c-37796396">[-]</label><label class="expand" for="c-37796396">[3 more]</label></div><br/><div class="children"><div class="content">Primarily 470,480,570,580. We also ran a very large cluster of PS5 APU chips too.<p>Got the chips directly from AMD. Since these are 4-5 year old chips, they were not going to ever be used. It is more ROI efficient with ETH mining to use older cards than newer ones.<p>Had a couple OEM manufacture the cards specially for us with 8gb, heatsinks instead of fans (lower power usage) and no display ports (lower cost).<p>They will be recycled as there isn&#x27;t much use for them now.<p>I&#x27;m also no longer with the company.</div><br/><div id="37799043" class="c"><input type="checkbox" id="c-37799043" checked=""/><div class="controls bullet"><span class="by">xcdzvyn</span><span>|</span><a href="#37795569">root</a><span>|</span><a href="#37796396">parent</a><span>|</span><a href="#37795149">next</a><span>|</span><label class="collapse" for="c-37799043">[-]</label><label class="expand" for="c-37799043">[2 more]</label></div><br/><div class="children"><div class="content">Cool! Were the PS5 APUs actually attached to a PS5 motherboard, or were they repurposed entirely?</div><br/><div id="37799084" class="c"><input type="checkbox" id="c-37799084" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#37795569">root</a><span>|</span><a href="#37799043">parent</a><span>|</span><a href="#37795149">next</a><span>|</span><label class="collapse" for="c-37799084">[-]</label><label class="expand" for="c-37799084">[1 more]</label></div><br/><div class="children"><div class="content">Asrock bc-250. This is some hardware that I wouldn&#x27;t have purchased, if given the choice, especially that close to ETH PoS.<p>That said, I made it work, which was an insane amount of work, and it mined really well.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="37795149" class="c"><input type="checkbox" id="c-37795149" checked=""/><div class="controls bullet"><span class="by">pjmlp</span><span>|</span><a href="#37795569">prev</a><span>|</span><a href="#37795969">next</a><span>|</span><label class="collapse" for="c-37795149">[-]</label><label class="expand" for="c-37795149">[1 more]</label></div><br/><div class="children"><div class="content">Unless they get their act together regarding CUDA polyglot tooling, I seriously doubt it.</div><br/></div></div><div id="37795969" class="c"><input type="checkbox" id="c-37795969" checked=""/><div class="controls bullet"><span class="by">whywhywhywhy</span><span>|</span><a href="#37795149">prev</a><span>|</span><a href="#37798084">next</a><span>|</span><label class="collapse" for="c-37795969">[-]</label><label class="expand" for="c-37795969">[2 more]</label></div><br/><div class="children"><div class="content">Anyone who has to work in this ecosystem surely thinks this is a naive take</div><br/><div id="37796277" class="c"><input type="checkbox" id="c-37796277" checked=""/><div class="controls bullet"><span class="by">freedomben</span><span>|</span><a href="#37795969">parent</a><span>|</span><a href="#37798084">next</a><span>|</span><label class="collapse" for="c-37796277">[-]</label><label class="expand" for="c-37796277">[1 more]</label></div><br/><div class="children"><div class="content">For someone who doesn&#x27;t work in this ecosystem, can you elaborate?  What&#x27;s the real situation currently?</div><br/></div></div></div></div><div id="37798084" class="c"><input type="checkbox" id="c-37798084" checked=""/><div class="controls bullet"><span class="by">tormeh</span><span>|</span><a href="#37795969">prev</a><span>|</span><a href="#37797570">next</a><span>|</span><label class="collapse" for="c-37798084">[-]</label><label class="expand" for="c-37798084">[1 more]</label></div><br/><div class="children"><div class="content">For LLM inference, a shoutout to MLC LLM, which runs LLM models on basically any API that&#x27;s widely available: <a href="https:&#x2F;&#x2F;github.com&#x2F;mlc-ai&#x2F;mlc-llm">https:&#x2F;&#x2F;github.com&#x2F;mlc-ai&#x2F;mlc-llm</a></div><br/></div></div><div id="37797570" class="c"><input type="checkbox" id="c-37797570" checked=""/><div class="controls bullet"><span class="by">raggi</span><span>|</span><a href="#37798084">prev</a><span>|</span><a href="#37794442">next</a><span>|</span><label class="collapse" for="c-37797570">[-]</label><label class="expand" for="c-37797570">[2 more]</label></div><br/><div class="children"><div class="content">Can we just get wgsl compute good enough and over the line instead, and do away with these moats?</div><br/><div id="37800034" class="c"><input type="checkbox" id="c-37800034" checked=""/><div class="controls bullet"><span class="by">mschuetz</span><span>|</span><a href="#37797570">parent</a><span>|</span><a href="#37794442">next</a><span>|</span><label class="collapse" for="c-37800034">[-]</label><label class="expand" for="c-37800034">[1 more]</label></div><br/><div class="children"><div class="content">Not happening. WGSL wants to support the lowest common denominator, so it&#x27;ll always mainly be a 5-year old mobile-phone API. Also if you want to beat CUDA, you&#x27;ll need some functionality that&#x27;s completely missing in compute shaders, especially WGSL. Like pointers and pointer casting (and that glsl buffer reference extension is the worst emulation of that feature I&#x27;ve every seen).</div><br/></div></div></div></div><div id="37794442" class="c"><input type="checkbox" id="c-37794442" checked=""/><div class="controls bullet"><span class="by">superkuh</span><span>|</span><a href="#37797570">prev</a><span>|</span><a href="#37798030">next</a><span>|</span><label class="collapse" for="c-37794442">[-]</label><label class="expand" for="c-37794442">[7 more]</label></div><br/><div class="children"><div class="content">&gt;There is also a version of PyTorch that uses AMD ROCm, an open-source software stack for AMD GPU programming. Crossing the CUDA moat for AMD GPUs may be as easy as using PyTorch.<p>Unfortunately since the AMD firmware doesn&#x27;t reliably do what it&#x27;s supposed to those ROCm calls often don&#x27;t either. That&#x27;s if your AMD card is even still supported by ROCm: the AMD RX 580 I bought in 2021 (the great GPU shortage) had it&#x27;s ROCm support dropped in 2022 (4 years support total).<p>The only reliable interface in my experience has been via opencl.</div><br/><div id="37799236" class="c"><input type="checkbox" id="c-37799236" checked=""/><div class="controls bullet"><span class="by">65a</span><span>|</span><a href="#37794442">parent</a><span>|</span><a href="#37794495">next</a><span>|</span><label class="collapse" for="c-37799236">[-]</label><label class="expand" for="c-37799236">[1 more]</label></div><br/><div class="children"><div class="content">ROCm works fine on my 2016 Vega Frontier edition, for what it&#x27;s worth.</div><br/></div></div><div id="37794495" class="c"><input type="checkbox" id="c-37794495" checked=""/><div class="controls bullet"><span class="by">htrp</span><span>|</span><a href="#37794442">parent</a><span>|</span><a href="#37799236">prev</a><span>|</span><a href="#37794615">next</a><span>|</span><label class="collapse" for="c-37794495">[-]</label><label class="expand" for="c-37794495">[3 more]</label></div><br/><div class="children"><div class="content">has opencl actually improved enough to be competitive?</div><br/><div id="37794717" class="c"><input type="checkbox" id="c-37794717" checked=""/><div class="controls bullet"><span class="by">orangepurple</span><span>|</span><a href="#37794442">root</a><span>|</span><a href="#37794495">parent</a><span>|</span><a href="#37794615">next</a><span>|</span><label class="collapse" for="c-37794717">[-]</label><label class="expand" for="c-37794717">[2 more]</label></div><br/><div class="children"><div class="content">I thought ONNX is supposed to be the ultimate common denominator for machine learning model cross platform compatibility</div><br/></div></div></div></div><div id="37794615" class="c"><input type="checkbox" id="c-37794615" checked=""/><div class="controls bullet"><span class="by">zucker42</span><span>|</span><a href="#37794442">parent</a><span>|</span><a href="#37794495">prev</a><span>|</span><a href="#37798030">next</a><span>|</span><label class="collapse" for="c-37794615">[-]</label><label class="expand" for="c-37794615">[2 more]</label></div><br/><div class="children"><div class="content">Do you mean OpenCL using Rusticl or something else? And what DL framework, if any?</div><br/><div id="37794629" class="c"><input type="checkbox" id="c-37794629" checked=""/><div class="controls bullet"><span class="by">superkuh</span><span>|</span><a href="#37794442">root</a><span>|</span><a href="#37794615">parent</a><span>|</span><a href="#37798030">next</a><span>|</span><label class="collapse" for="c-37794629">[-]</label><label class="expand" for="c-37794629">[1 more]</label></div><br/><div class="children"><div class="content">I should clarify that I mean for human person uses. Not commercial or institutional. But, clBLAST via llama.cpp for LLM currently. Or far in the past just pure opencl for things with AMD cards.</div><br/></div></div></div></div></div></div><div id="37798030" class="c"><input type="checkbox" id="c-37798030" checked=""/><div class="controls bullet"><span class="by">spandextwins</span><span>|</span><a href="#37794442">prev</a><span>|</span><a href="#37795005">next</a><span>|</span><label class="collapse" for="c-37798030">[-]</label><label class="expand" for="c-37798030">[3 more]</label></div><br/><div class="children"><div class="content">That’s like saying Ford is gonna catch Tesla.</div><br/><div id="37798035" class="c"><input type="checkbox" id="c-37798035" checked=""/><div class="controls bullet"><span class="by">cantaloupe</span><span>|</span><a href="#37798030">parent</a><span>|</span><a href="#37798076">next</a><span>|</span><label class="collapse" for="c-37798035">[-]</label><label class="expand" for="c-37798035">[1 more]</label></div><br/><div class="children"><div class="content">Do you see that as an inevitability or an impossibility?</div><br/></div></div><div id="37798076" class="c"><input type="checkbox" id="c-37798076" checked=""/><div class="controls bullet"><span class="by">tpmx</span><span>|</span><a href="#37798030">parent</a><span>|</span><a href="#37798035">prev</a><span>|</span><a href="#37795005">next</a><span>|</span><label class="collapse" for="c-37798076">[-]</label><label class="expand" for="c-37798076">[1 more]</label></div><br/><div class="children"><div class="content">No, not really. They have similar enough silicon, they &quot;just&quot; need some software to make it work.</div><br/></div></div></div></div><div id="37795005" class="c"><input type="checkbox" id="c-37795005" checked=""/><div class="controls bullet"><span class="by">RcouF1uZ4gsC</span><span>|</span><a href="#37798030">prev</a><span>|</span><a href="#37796912">next</a><span>|</span><label class="collapse" for="c-37795005">[-]</label><label class="expand" for="c-37795005">[2 more]</label></div><br/><div class="children"><div class="content">I am not so sure.<p>Everyone knows that CUDA is a core competency of Nvidia and they have stuck to it for years and years refining it, fixing bugs, and making the experience smoother on Nvidia hardware.<p>On the other hand, AMD has not had the same level of commitment. They used to sing the praises of OpenCL. And then there is ROCm. Tomorrow, it might be something else.<p>Thus, Nvidia CUDA will get a lot more attention and tuning from even the portability layers because they know that their investment in it will reap dividends even years from now, whereas their investment in AMD might be obsolete in a few years.<p>In addition, even if there is theoretical support, getting specific driver support and working around driver bugs is likely to be more of a pain with AMD.</div><br/><div id="37795188" class="c"><input type="checkbox" id="c-37795188" checked=""/><div class="controls bullet"><span class="by">AnthonyMouse</span><span>|</span><a href="#37795005">parent</a><span>|</span><a href="#37796912">next</a><span>|</span><label class="collapse" for="c-37795188">[-]</label><label class="expand" for="c-37795188">[1 more]</label></div><br/><div class="children"><div class="content">This is what people complain about, but at the same time there aren&#x27;t enough cards, so the people with AMD cards want to use them. So they fix the bugs, or report them to AMD so they can fix them, and it gets better. Then more people use them and submit patches and bug reporters, and it gets better.<p>At some point the old complaints are no longer valid.</div><br/></div></div></div></div><div id="37796912" class="c"><input type="checkbox" id="c-37796912" checked=""/><div class="controls bullet"><span class="by">jiggawatts</span><span>|</span><a href="#37795005">prev</a><span>|</span><label class="collapse" for="c-37796912">[-]</label><label class="expand" for="c-37796912">[2 more]</label></div><br/><div class="children"><div class="content">Can I buy an MI300 or even rent one in a cloud?</div><br/><div id="37798268" class="c"><input type="checkbox" id="c-37798268" checked=""/><div class="controls bullet"><span class="by">arcanus</span><span>|</span><a href="#37796912">parent</a><span>|</span><label class="collapse" for="c-37798268">[-]</label><label class="expand" for="c-37798268">[1 more]</label></div><br/><div class="children"><div class="content">Soon. The card is coming in Q4. The early shipments are likely all going to LLNL&#x27;s El Capitan Exascale computer: <a href="https:&#x2F;&#x2F;www.tomshardware.com&#x2F;news&#x2F;amds-instinct-mi300-moves-into-el-capitan-installation" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.tomshardware.com&#x2F;news&#x2F;amds-instinct-mi300-moves-...</a></div><br/></div></div></div></div></div></div></div></div></div></body></html>