<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1705568482567" as="style"/><link rel="stylesheet" href="styles.css?v=1705568482567"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2307.01850">Self-Consuming Generative Models Go MAD</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>galaxyLogic</span> | <span>22 comments</span></div><br/><div><div id="39038914" class="c"><input type="checkbox" id="c-39038914" checked=""/><div class="controls bullet"><span class="by">Almondsetat</span><span>|</span><a href="#39039512">next</a><span>|</span><label class="collapse" for="c-39038914">[-]</label><label class="expand" for="c-39038914">[9 more]</label></div><br/><div class="children"><div class="content">If feeding AI generated stuff to train an AI is bad then what does that tell us about humans consuming AI generated stuff?<p>If I give a pig something to eat and he throws up I&#x27;m surely not touching that stuff myself</div><br/><div id="39039422" class="c"><input type="checkbox" id="c-39039422" checked=""/><div class="controls bullet"><span class="by">_nalply</span><span>|</span><a href="#39038914">parent</a><span>|</span><a href="#39038949">next</a><span>|</span><label class="collapse" for="c-39039422">[-]</label><label class="expand" for="c-39039422">[1 more]</label></div><br/><div class="children"><div class="content">You should better compare it with humans consuming stuff they created themselves. Or in a perhaps more sickening way, an author who only reads what they have written themself.<p>We know that there is more than one AI software but there is not as much variation as for humans.<p>And as someone else already wrote here, humans have something like a ratcheting mechanism. I would paraphrase it as &quot;Humans have learnt to stand on shoulders of giants&quot;. AI does not have this.</div><br/></div></div><div id="39038949" class="c"><input type="checkbox" id="c-39038949" checked=""/><div class="controls bullet"><span class="by">kvdveer</span><span>|</span><a href="#39038914">parent</a><span>|</span><a href="#39039422">prev</a><span>|</span><a href="#39039247">next</a><span>|</span><label class="collapse" for="c-39038949">[-]</label><label class="expand" for="c-39038949">[4 more]</label></div><br/><div class="children"><div class="content">Not sure you can compare humans and AI this way. It is quite possible that humans have coping mechanisms that AI does not (yet?) have.<p>Or to put it in your analogy, If I feed my cat milk, they&#x27;ll throw up. That doesn&#x27;t mean milk is unfit for human consumption.</div><br/><div id="39039137" class="c"><input type="checkbox" id="c-39039137" checked=""/><div class="controls bullet"><span class="by">Almondsetat</span><span>|</span><a href="#39038914">root</a><span>|</span><a href="#39038949">parent</a><span>|</span><a href="#39039224">next</a><span>|</span><label class="collapse" for="c-39039137">[-]</label><label class="expand" for="c-39039137">[1 more]</label></div><br/><div class="children"><div class="content">The example with the pig is because they are famous for eating basically anything without a hitch.<p>If AI generated stuff is so bad you cannot train AI with it why would humans use it to train themselves (aka learn stuff)?</div><br/></div></div><div id="39039224" class="c"><input type="checkbox" id="c-39039224" checked=""/><div class="controls bullet"><span class="by">mpweiher</span><span>|</span><a href="#39038914">root</a><span>|</span><a href="#39038949">parent</a><span>|</span><a href="#39039137">prev</a><span>|</span><a href="#39038996">next</a><span>|</span><label class="collapse" for="c-39039224">[-]</label><label class="expand" for="c-39039224">[1 more]</label></div><br/><div class="children"><div class="content">Just because I might have coping mechanisms to deal with awful stuff doesn&#x27;t mean I want to subject myself to awful stuff.</div><br/></div></div><div id="39038996" class="c"><input type="checkbox" id="c-39038996" checked=""/><div class="controls bullet"><span class="by">KeplerBoy</span><span>|</span><a href="#39038914">root</a><span>|</span><a href="#39038949">parent</a><span>|</span><a href="#39039224">prev</a><span>|</span><a href="#39039247">next</a><span>|</span><label class="collapse" for="c-39038996">[-]</label><label class="expand" for="c-39038996">[1 more]</label></div><br/><div class="children"><div class="content">milk is unfit for some humans though and I believe excessive AI interaction will come with it&#x27;s own set of issues for parts of our society. I am of the AI companion startups, that shit can&#x27;t and won&#x27;t end pretty.</div><br/></div></div></div></div><div id="39039247" class="c"><input type="checkbox" id="c-39039247" checked=""/><div class="controls bullet"><span class="by">Charon77</span><span>|</span><a href="#39038914">parent</a><span>|</span><a href="#39038949">prev</a><span>|</span><a href="#39038957">next</a><span>|</span><label class="collapse" for="c-39039247">[-]</label><label class="expand" for="c-39039247">[1 more]</label></div><br/><div class="children"><div class="content">We&#x27;ve been teaching humans with humans, using human made content.<p>Just because it doesn&#x27;t work with AI now, doesn&#x27;t mean it&#x27;s something that wouldn&#x27;t ever work.</div><br/></div></div><div id="39038957" class="c"><input type="checkbox" id="c-39038957" checked=""/><div class="controls bullet"><span class="by">yetihehe</span><span>|</span><a href="#39038914">parent</a><span>|</span><a href="#39039247">prev</a><span>|</span><a href="#39038941">next</a><span>|</span><label class="collapse" for="c-39038957">[-]</label><label class="expand" for="c-39038957">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s more like feeding humans to humans or cows to cows. They go mad too. Feeding cows to humans instead typically works, of course there are ALWAYS exceptions and it&#x27;s never perfect or without harms. Both with AI and cows.</div><br/></div></div><div id="39038941" class="c"><input type="checkbox" id="c-39038941" checked=""/><div class="controls bullet"><span class="by">BartjeD</span><span>|</span><a href="#39038914">parent</a><span>|</span><a href="#39038957">prev</a><span>|</span><a href="#39039512">next</a><span>|</span><label class="collapse" for="c-39038941">[-]</label><label class="expand" for="c-39038941">[1 more]</label></div><br/><div class="children"><div class="content">It reads to me that locking yourself &#x2F; AI up in a room with only yourself to converse with causes it to go MAD &#x2F; out of touch with reality.<p>If you interact with people &#x2F; fresh data regularly you&#x27;ll avoid that.<p>I&#x27;m not sure how much is down to evolving requirements and data formats vs degradation of the training data. 
The idea sounds common-sense but it&#x27;s also triggering a bullshit warning for me, because I&#x27;d expect more consistency per AI model, and differences to occur on new model interations - regardless of the dataset.</div><br/></div></div></div></div><div id="39039512" class="c"><input type="checkbox" id="c-39039512" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#39038914">prev</a><span>|</span><a href="#39039078">next</a><span>|</span><label class="collapse" for="c-39039512">[-]</label><label class="expand" for="c-39039512">[2 more]</label></div><br/><div class="children"><div class="content">This is like 6 months old. Is there a reason it&#x27;s suddenly being reposted?</div><br/><div id="39039550" class="c"><input type="checkbox" id="c-39039550" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#39039512">parent</a><span>|</span><a href="#39039078">next</a><span>|</span><label class="collapse" for="c-39039550">[-]</label><label class="expand" for="c-39039550">[1 more]</label></div><br/><div class="children"><div class="content">Previous post 5 months ago: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37021043">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37021043</a></div><br/></div></div></div></div><div id="39038921" class="c"><input type="checkbox" id="c-39038921" checked=""/><div class="controls bullet"><span class="by">_nalply</span><span>|</span><a href="#39039078">prev</a><span>|</span><a href="#39039267">next</a><span>|</span><label class="collapse" for="c-39038921">[-]</label><label class="expand" for="c-39038921">[1 more]</label></div><br/><div class="children"><div class="content">MAD = Model Autophagy Disorder<p>I find that catchy and descriptive on more than one level.</div><br/></div></div><div id="39039267" class="c"><input type="checkbox" id="c-39039267" checked=""/><div class="controls bullet"><span class="by">Frummy</span><span>|</span><a href="#39038921">prev</a><span>|</span><a href="#39038987">next</a><span>|</span><label class="collapse" for="c-39039267">[-]</label><label class="expand" for="c-39039267">[1 more]</label></div><br/><div class="children"><div class="content">I wonder if eventually, stronger models exposed to these loops can avoid going mad, like a human monk in isolation</div><br/></div></div><div id="39038987" class="c"><input type="checkbox" id="c-39038987" checked=""/><div class="controls bullet"><span class="by">fooker</span><span>|</span><a href="#39039267">prev</a><span>|</span><a href="#39039346">next</a><span>|</span><label class="collapse" for="c-39038987">[-]</label><label class="expand" for="c-39038987">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s articles like this that makes me think it&#x27;s plausible that we are a small N iterations away from stumbling into a transformer(&#x2F;newfangled thing) architecture that rivals the reasoning capacity of humans.</div><br/></div></div><div id="39039346" class="c"><input type="checkbox" id="c-39039346" checked=""/><div class="controls bullet"><span class="by">chewxy</span><span>|</span><a href="#39038987">prev</a><span>|</span><a href="#39038860">next</a><span>|</span><label class="collapse" for="c-39039346">[-]</label><label class="expand" for="c-39039346">[2 more]</label></div><br/><div class="children"><div class="content">We&#x27;re rebranding mode collapse as MAD now?</div><br/><div id="39039495" class="c"><input type="checkbox" id="c-39039495" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#39039346">parent</a><span>|</span><a href="#39038860">next</a><span>|</span><label class="collapse" for="c-39039495">[-]</label><label class="expand" for="c-39039495">[1 more]</label></div><br/><div class="children"><div class="content">Not &quot;now&quot; - this is like 6 months old. It came out right around the same time as the Stanford paper.</div><br/></div></div></div></div><div id="39038860" class="c"><input type="checkbox" id="c-39038860" checked=""/><div class="controls bullet"><span class="by">galaxyLogic</span><span>|</span><a href="#39039346">prev</a><span>|</span><a href="#39039025">next</a><span>|</span><label class="collapse" for="c-39038860">[-]</label><label class="expand" for="c-39038860">[2 more]</label></div><br/><div class="children"><div class="content">&quot;One doomsday scenario is that, if left uncontrolled for many generations, MAD could poison the data quality and diversity of the entire Internet... &quot;</div><br/><div id="39039016" class="c"><input type="checkbox" id="c-39039016" checked=""/><div class="controls bullet"><span class="by">t_tsonev</span><span>|</span><a href="#39038860">parent</a><span>|</span><a href="#39039025">next</a><span>|</span><label class="collapse" for="c-39039016">[-]</label><label class="expand" for="c-39039016">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s the most likely scenario.</div><br/></div></div></div></div><div id="39039025" class="c"><input type="checkbox" id="c-39039025" checked=""/><div class="controls bullet"><span class="by">tysam_and</span><span>|</span><a href="#39038860">prev</a><span>|</span><a href="#39039012">next</a><span>|</span><label class="collapse" for="c-39039025">[-]</label><label class="expand" for="c-39039025">[1 more]</label></div><br/><div class="children"><div class="content">This is, among other things, a very natural consequence of some of the equations surrounding and involved in Shannon&#x27;s original noisy channel capacity theorem, where the noise is (in many ways) conditioned upon the structure of the model itself.<p>It is not at all necessarily surprising, I think, from a purely high-level perspective, but I do personally think that I find that it is good to have the analysis. From a purely professional standpoint, I do not believe it is unique or distinctive enough as an individual method to need its own separate name for day-to-day use. From a personal perspective, however, I thought the mad cow disease reference was hilarious and applaud whoever came up with the acronym.<p>I find the benefit in the analysis, and the concerns presented about generated data being present in the data makes sense to me (and if in sufficient quantity, would make sense as biasing the models improperly in a rather significant kind of way).<p>I particularly enjoyed the humor of this line, the tongue-in-cheek nature is very funny&#x2F;nice to me here:<p>&quot;Ascertaining whether an autophagous loop has gone MAD or not (recall Definition 2.1) requires that we measure how far the synthesized data distribution Gt has drifted from the true data distribution Pr over the generations t.&quot;<p>I like their use of color in the paper, I saw a similar orange&#x2F;green color scheme earlier today and enjoyed it very much as an annotation method.<p>&quot;A fixed real dataset only slows generative model degradation&quot; is again also a natural consequence of Shannon&#x27;s noisy channel capacity theorem, one can say that with almost nearly perfect certainty that a limited neural network will not be able to perfectly fit the distribution of the data that it is training on, thus it will have bias, variance, or some combination of both, limited ultimately by the model&#x27;s capacity itself.<p>This w.r.t. the original dataset is noise, and we can choose between whether we want collapse, or recursively encoding the noise patterns of the previous model (which might happen to have an additive effect, or maybe not! Who knows! I do not know for sure here, I have not yet figured this one out myself yet).<p>w.r.t. the real data slowing down degradation, if we are sampling I.I.D. of course then proportionately we still should see some degradation as this is the nature of empirical risk minimization over maximum likelihood estimation. It is still good that they have shown this, however, I thinks.<p>The fresh data loop, I believe, would be an example of actually a kind of noise in and of itself, w.r.t. the original input dataset, and as long as this &#x27;noise&#x27; (from the perspective of the model) has a higher SNR than the (potentially slow) collapse of the model&#x27;s output distribution, then it should (in some kind of proportion at leasts) be constantly-playing &#x27;keep-up&#x27; with the fresh data.<p>&quot;First, we find that—regardless of the performance of early generations—the performance of later generations converges to a point that depends only on the amounts of real and synthetic data in the training loop. &quot; -- there we are (I saw this after making the SNR point, this makes sense within this framework of interpretation, then.<p>All in all, I found this paper very aware of itself and what it was studying, it was well-laid out and accessible, and while the points are not necessarily earth-shattering (though I still have to read through some of it, I think), having clear empirical evidence about this phenomenon, detailing it, and cutting away through the forest of (at-least-seemingly) untested battlegrounds is one that I appreciated.<p>Curious to hear what others think about this one. &lt;3 :&#x27;))))</div><br/></div></div><div id="39039012" class="c"><input type="checkbox" id="c-39039012" checked=""/><div class="controls bullet"><span class="by">Bayes7</span><span>|</span><a href="#39039025">prev</a><span>|</span><label class="collapse" for="c-39039012">[-]</label><label class="expand" for="c-39039012">[1 more]</label></div><br/><div class="children"><div class="content">Summarised by <a href="https:&#x2F;&#x2F;xkcd.com&#x2F;2494" rel="nofollow">https:&#x2F;&#x2F;xkcd.com&#x2F;2494</a></div><br/></div></div></div></div></div></div></div></body></html>