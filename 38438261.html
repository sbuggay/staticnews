<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1701162059392" as="style"/><link rel="stylesheet" href="styles.css?v=1701162059392"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.astralcodexten.com/p/god-help-us-lets-try-to-understand">Let&#x27;s try to understand AI monosemanticity</a> <span class="domain">(<a href="https://www.astralcodexten.com">www.astralcodexten.com</a>)</span></div><div class="subtext"><span>bananaflag</span> | <span>126 comments</span></div><br/><div><div id="38439314" class="c"><input type="checkbox" id="c-38439314" checked=""/><div class="controls bullet"><span class="by">lukev</span><span>|</span><a href="#38439553">next</a><span>|</span><label class="collapse" for="c-38439314">[-]</label><label class="expand" for="c-38439314">[14 more]</label></div><br/><div class="children"><div class="content">There&#x27;s actually a somewhat reasonable analogy to human cognitive processes here, I think, in the sense that humans tend to form concepts defined by their connectivity to other concepts (c.f. Ferdinand de Saussure &amp; structuralism).<p>Human brains are also a &quot;black box&quot; in the sense that you can&#x27;t scan&#x2F;dissect one to build a concept graph.<p>Neural nets do seem to have some sort of emergent structural concept graph, in the case of LLMs it&#x27;s largely informed by human language (because that&#x27;s what they&#x27;re trained on.) To an extent, we can observe this empirically through their output even if the first principles are opaque.</div><br/><div id="38440131" class="c"><input type="checkbox" id="c-38440131" checked=""/><div class="controls bullet"><span class="by">wahern</span><span>|</span><a href="#38439314">parent</a><span>|</span><a href="#38440858">next</a><span>|</span><label class="collapse" for="c-38440131">[-]</label><label class="expand" for="c-38440131">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Neural nets do seem to have some sort of emergent structural concept graph, in the case of LLMs it&#x27;s largely informed by human language (because that&#x27;s what they&#x27;re trained on.) To an extent, we can observe this empirically through their output even if the first principles are opaque.<p>Alternatively, what you&#x27;re seeing are the structures inherent within human culture as manifested through its literature[1], with LLMs simply being a new and useful tool which makes these structures more apparent.<p>[1] And also its engineers&#x27; training choices</div><br/></div></div><div id="38440858" class="c"><input type="checkbox" id="c-38440858" checked=""/><div class="controls bullet"><span class="by">Kapura</span><span>|</span><a href="#38439314">parent</a><span>|</span><a href="#38440131">prev</a><span>|</span><a href="#38439553">next</a><span>|</span><label class="collapse" for="c-38440858">[-]</label><label class="expand" for="c-38440858">[12 more]</label></div><br/><div class="children"><div class="content">This gets at the fundamental issue I have with AI: We&#x27;re trying to get machines to think, but we only have the barest understanding of how we, as humans, think. The concept of a &quot;neuron&quot; that can activate other neurons comes straight from real neurology, but given how complex the human brain is, it&#x27;s no surprise that we can only create something that is fundamentally &quot;lesser.&quot;<p>However, I think that real neurology and machine-learning can be mutually reinforcing fields: structures discovered in the one could be applied to the other, and vice versa. But thinking we can create &quot;AGI&quot; without first increasing our understanding of &quot;wet&quot; neural nets is the height of hubris.</div><br/><div id="38441936" class="c"><input type="checkbox" id="c-38441936" checked=""/><div class="controls bullet"><span class="by">hibikir</span><span>|</span><a href="#38439314">root</a><span>|</span><a href="#38440858">parent</a><span>|</span><a href="#38441261">next</a><span>|</span><label class="collapse" for="c-38441936">[-]</label><label class="expand" for="c-38441936">[4 more]</label></div><br/><div class="children"><div class="content">All the efforts to get computers to play chess mostly like humans do were made to look insufficient by a large enough neural network that took zero knowledge of human position evaluation, which just played against itself an outrageous number of games.<p>The plane is in many ways lesser than birds, and we barely understood aerodynamics when the wright brothers gave us a working plane: While the bird is efficient, we had a whole lot more thrust. Our planes are far better than they were back then, but not because we understand bird better, but because we focused on efficiency of the simpler designs we could build. The Formula 1 car doesn&#x27;t come from deep understanding of the efficient, graceful running movements of the cheetah.<p>Our results in medicine have been far and ahead of our understanding of biochemistry, DNA, and in general, how the human body works. The discovery of penicilin didn&#x27;t require a lot of understanding: It required luck. Ozempic and Viagra didn&#x27;t come from deep understanding of the human body: Resarchers were looking for one thing, and ended up with something useful that was straight out unintended.<p>We might be able to build AGI with what we know of brains, we might not. But either way, it&#x27;s not because we need to understand the human mind better: Do we have enough compute for a very crude systems that we know how to build to overcome our relative lack of understanding?<p>So if you ask me, what is the height of hubris is to think that working engineering solutions have to come from deep understanding of how nature solves the problem. The only reasonable bet, given the massive improvements in AI in the last decade, is to assume that the error bars of any prediction here are just huge. Will we get stuck, the way self-driving cars seem to have gotten stuck for a bit? Possibly. Will be go from a stochastic parrot into something better than humans, the same way that Go AIs went from pretty weak to better than any human in the blink of an eye? I&#x27;d not discount it either. I expect to be surprised either way.</div><br/><div id="38443328" class="c"><input type="checkbox" id="c-38443328" checked=""/><div class="controls bullet"><span class="by">PeterStuer</span><span>|</span><a href="#38439314">root</a><span>|</span><a href="#38441936">parent</a><span>|</span><a href="#38443160">next</a><span>|</span><label class="collapse" for="c-38443328">[-]</label><label class="expand" for="c-38443328">[1 more]</label></div><br/><div class="children"><div class="content">In AI research there are different branches with different goals and motivations. You have applied research, where the goal is to engineer systems that are better at soving certain needs. You also have scientific endeavours, usually interdisciplinary, typically crossed with biology, psychology and philosophy, where AI models are created and used as models of understanding human or animal intelligence.<p>The former is where as you describe where better understanding of biology might help but is not a prerequisite for progress, but in the latter it is not just needed but the goal.<p>Now I know this is a bit of a caricature as both of these disciplines are in practice poorly deliniated and often intermixed. It&#x27;s easy to find hubristic examples to mock on both sides, but there&#x27;s value and brilliance to be found in each respectively as well.</div><br/></div></div><div id="38443160" class="c"><input type="checkbox" id="c-38443160" checked=""/><div class="controls bullet"><span class="by">galaxyLogic</span><span>|</span><a href="#38439314">root</a><span>|</span><a href="#38441936">parent</a><span>|</span><a href="#38443328">prev</a><span>|</span><a href="#38441261">next</a><span>|</span><label class="collapse" for="c-38443160">[-]</label><label class="expand" for="c-38443160">[2 more]</label></div><br/><div class="children"><div class="content">&gt; height of hubris is to think that working engineering solutions have to come from deep understanding of how nature solves the problem.<p>It is basically understood how &quot;nature solves the problem&quot;. It does that by evolution. And what is machine learning and neural networks but artificial evolution?</div><br/><div id="38443210" class="c"><input type="checkbox" id="c-38443210" checked=""/><div class="controls bullet"><span class="by">valval</span><span>|</span><a href="#38439314">root</a><span>|</span><a href="#38443160">parent</a><span>|</span><a href="#38441261">next</a><span>|</span><label class="collapse" for="c-38443210">[-]</label><label class="expand" for="c-38443210">[1 more]</label></div><br/><div class="children"><div class="content">Nothing of the sort -- it&#x27;s best characterized as learning, like it says in the name.</div><br/></div></div></div></div></div></div><div id="38441261" class="c"><input type="checkbox" id="c-38441261" checked=""/><div class="controls bullet"><span class="by">sxg</span><span>|</span><a href="#38439314">root</a><span>|</span><a href="#38440858">parent</a><span>|</span><a href="#38441936">prev</a><span>|</span><a href="#38441509">next</a><span>|</span><label class="collapse" for="c-38441261">[-]</label><label class="expand" for="c-38441261">[5 more]</label></div><br/><div class="children"><div class="content">AGI and human intelligence don&#x27;t necessarily have to work the same way. While many of us assume that they must be very similar, I don&#x27;t think there&#x27;s any basis for that assumption.</div><br/><div id="38441371" class="c"><input type="checkbox" id="c-38441371" checked=""/><div class="controls bullet"><span class="by">swatcoder</span><span>|</span><a href="#38439314">root</a><span>|</span><a href="#38441261">parent</a><span>|</span><a href="#38443538">next</a><span>|</span><label class="collapse" for="c-38441371">[-]</label><label class="expand" for="c-38441371">[3 more]</label></div><br/><div class="children"><div class="content">That&#x27;s correct that they don&#x27;t need to work the same way. I would say most people that have earnestly thought about it assume they won&#x27;t.<p>But given the exponentially greater chemical, biological, and neurological complexity of human brains, the millions of years of evolutionary &quot;pre-training&quot; that they get imbued with, and the years of constant multimodal sensory input required for their culmination in what&#x27;s called human intelligence, it takes an <i>extremely</i> bold assumption to believe that equivalent ends can be met with a stream of fmadds driving through in an array of transistors.<p>It&#x27;s hard to overstate how big a gulf there is in both the hardware and software between living systems and what we currently see in our silicon projects. Neural network learning in general and LLM&#x27;s in particular are like crude paper airplanes compared to the flight capabilities of a dextrous bird. You can kinda squint and see that they both move some distance through the air, and even marvel at it for a bit, but there&#x27;s a long long long long way to go.</div><br/><div id="38442030" class="c"><input type="checkbox" id="c-38442030" checked=""/><div class="controls bullet"><span class="by">kelseyfrog</span><span>|</span><a href="#38439314">root</a><span>|</span><a href="#38441371">parent</a><span>|</span><a href="#38443538">next</a><span>|</span><label class="collapse" for="c-38442030">[-]</label><label class="expand" for="c-38442030">[2 more]</label></div><br/><div class="children"><div class="content">And yet no bird ever broke the sound barrier. Humans make things that exceed anything in nature. Thought will only be another example.</div><br/><div id="38443592" class="c"><input type="checkbox" id="c-38443592" checked=""/><div class="controls bullet"><span class="by">bomewish</span><span>|</span><a href="#38439314">root</a><span>|</span><a href="#38442030">parent</a><span>|</span><a href="#38443538">next</a><span>|</span><label class="collapse" for="c-38443592">[-]</label><label class="expand" for="c-38443592">[1 more]</label></div><br/><div class="children"><div class="content">Exceed in some regards but not others right? Like, let’s not imagine the plane is better at being a bird than the bird. It’s an evocative set of associations though. Makes me think we’ll end up creating the Concorde of intelligence but be no closer to understanding how our own works…</div><br/></div></div></div></div></div></div><div id="38443538" class="c"><input type="checkbox" id="c-38443538" checked=""/><div class="controls bullet"><span class="by">fuy</span><span>|</span><a href="#38439314">root</a><span>|</span><a href="#38441261">parent</a><span>|</span><a href="#38441371">prev</a><span>|</span><a href="#38441509">next</a><span>|</span><label class="collapse" for="c-38443538">[-]</label><label class="expand" for="c-38443538">[1 more]</label></div><br/><div class="children"><div class="content">This basis is our limited data set: we have only 1 example of human level intelligence, so it&#x27;s natural to assume there&#x27;s something unique, or at least significant about the way it works.</div><br/></div></div></div></div><div id="38441509" class="c"><input type="checkbox" id="c-38441509" checked=""/><div class="controls bullet"><span class="by">mitthrowaway2</span><span>|</span><a href="#38439314">root</a><span>|</span><a href="#38440858">parent</a><span>|</span><a href="#38441261">prev</a><span>|</span><a href="#38441633">next</a><span>|</span><label class="collapse" for="c-38441509">[-]</label><label class="expand" for="c-38441509">[1 more]</label></div><br/><div class="children"><div class="content">&gt; We&#x27;re trying to get machines to think, but we only have the barest understanding of how we, as humans, think... But thinking we can create &quot;AGI&quot; without first increasing our understanding of &quot;wet&quot; neural nets is the height of hubris.<p>That depends on the landscape of the solution space. If the techniques that work much better than anything else happen to be the same techniques used within our own brains, then we might find them, through aggressive experimentation and exploration, before we even realize that it&#x27;s also how our own brains work too.</div><br/></div></div><div id="38441633" class="c"><input type="checkbox" id="c-38441633" checked=""/><div class="controls bullet"><span class="by">dboreham</span><span>|</span><a href="#38439314">root</a><span>|</span><a href="#38440858">parent</a><span>|</span><a href="#38441509">prev</a><span>|</span><a href="#38439553">next</a><span>|</span><label class="collapse" for="c-38441633">[-]</label><label class="expand" for="c-38441633">[1 more]</label></div><br/><div class="children"><div class="content">Hard disagre. Imho humans suffer under a delusion that their thinking is like a computer whereas it is actually much more like an LLM. So if you ask humans[1] to figure our how they think, the answer will be a) long time coming and b) wrong.<p>[1] possibly bhudist monks excepted.</div><br/></div></div></div></div></div></div><div id="38439553" class="c"><input type="checkbox" id="c-38439553" checked=""/><div class="controls bullet"><span class="by">_as_text</span><span>|</span><a href="#38439314">prev</a><span>|</span><a href="#38439216">next</a><span>|</span><label class="collapse" for="c-38439553">[-]</label><label class="expand" for="c-38439553">[10 more]</label></div><br/><div class="children"><div class="content">I just skimmed through it for now, but it has seemed kinda natural to me for a few months now that there would be a deep connection between neural networks and differential or algebraic geometry.<p>Each ReLU layer is just a (quasi-)linear transformation, and a pass through two layers is basically also a linear transformation. If you say you want some piece of information to stay (numerically) intact as it passes through the network, you say you want that piece of information to be processed in the same way in each layer. The groups of linear transformations that &quot;all process information in the same way, and their compositions do, as well&quot; are basically the Lie groups. Anyone else ever had this thought?<p>I imagine if nothing catastrophic happens we&#x27;ll have a really beautiful theory of all this someday, which I won&#x27;t create, but maybe I&#x27;ll be able to understand it after a lot of hard work.</div><br/><div id="38440453" class="c"><input type="checkbox" id="c-38440453" checked=""/><div class="controls bullet"><span class="by">zozbot234</span><span>|</span><a href="#38439553">parent</a><span>|</span><a href="#38439953">next</a><span>|</span><label class="collapse" for="c-38440453">[-]</label><label class="expand" for="c-38440453">[2 more]</label></div><br/><div class="children"><div class="content">ReLU is quite far from linear, adding ReLU activations to a linear layer amounts to fitting a piecewise-segmented model of the underlying data.</div><br/><div id="38440873" class="c"><input type="checkbox" id="c-38440873" checked=""/><div class="controls bullet"><span class="by">drdeca</span><span>|</span><a href="#38439553">root</a><span>|</span><a href="#38440453">parent</a><span>|</span><a href="#38439953">next</a><span>|</span><label class="collapse" for="c-38440873">[-]</label><label class="expand" for="c-38440873">[1 more]</label></div><br/><div class="children"><div class="content">Well, at all but a finite number of points (specifically all but one point), there is a neighborhood of that point at which ReLU matches a linear function...<p>In one sense, that seems rather close to being linear. If you take a random point (according to a continuous probability distribution) , then with probability 1, if look in a small enough neighborhood of the selected point, it will be indistinguishable from linear within that neighborhood.<p>And, for a network made of ReLU gates and affine maps, still get that it looks indistinguishable from affine on any small enough region around any point outside of a set of measure zero.<p>So...
Depends what we mean by “almost linear” I think. I think one can make a reasonable case for saying that, <i>in a sense</i> it is “almost linear”.<p>But yes, of course I agree that in another important sense, it is far from linear. (E.g. it is not well approximated by any linear function)</div><br/></div></div></div></div><div id="38439953" class="c"><input type="checkbox" id="c-38439953" checked=""/><div class="controls bullet"><span class="by">KhoomeiK</span><span>|</span><a href="#38439553">parent</a><span>|</span><a href="#38440453">prev</a><span>|</span><a href="#38441604">next</a><span>|</span><label class="collapse" for="c-38439953">[-]</label><label class="expand" for="c-38439953">[1 more]</label></div><br/><div class="children"><div class="content">You might be interested in this workshop: <a href="https:&#x2F;&#x2F;www.neurreps.org&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.neurreps.org&#x2F;</a><p>And a possibly relevant paper from it:<p><a href="https:&#x2F;&#x2F;openreview.net&#x2F;forum?id=Ag8HcNFfDsg" rel="nofollow noreferrer">https:&#x2F;&#x2F;openreview.net&#x2F;forum?id=Ag8HcNFfDsg</a></div><br/></div></div><div id="38441604" class="c"><input type="checkbox" id="c-38441604" checked=""/><div class="controls bullet"><span class="by">blovescoffee</span><span>|</span><a href="#38439553">parent</a><span>|</span><a href="#38439953">prev</a><span>|</span><a href="#38440787">next</a><span>|</span><label class="collapse" for="c-38441604">[-]</label><label class="expand" for="c-38441604">[1 more]</label></div><br/><div class="children"><div class="content">What? The very point of neural networks is representing non-linear functions.</div><br/></div></div><div id="38440787" class="c"><input type="checkbox" id="c-38440787" checked=""/><div class="controls bullet"><span class="by">jimsimmons</span><span>|</span><a href="#38439553">parent</a><span>|</span><a href="#38441604">prev</a><span>|</span><a href="#38439824">next</a><span>|</span><label class="collapse" for="c-38440787">[-]</label><label class="expand" for="c-38440787">[4 more]</label></div><br/><div class="children"><div class="content">Everything is something. Question is what this nomenclature gymnastics buys you? Unless you answer that this is no different than claiming neural networks are a projection of my soul</div><br/><div id="38443452" class="c"><input type="checkbox" id="c-38443452" checked=""/><div class="controls bullet"><span class="by">rlupi</span><span>|</span><a href="#38439553">root</a><span>|</span><a href="#38440787">parent</a><span>|</span><a href="#38443224">next</a><span>|</span><label class="collapse" for="c-38443452">[-]</label><label class="expand" for="c-38443452">[1 more]</label></div><br/><div class="children"><div class="content">Could looking at NN through the lens of group theory unlock a lot of performance improvements?<p>If they have inner symmetries we are not aware of, you can avoid waste in searching in the wrong directions.<p>If you know that some concepts are necessarily independent, you can exploit that in your encoding to avoid superposition.<p>For example, I am using cyclic groups and dihedral groups, and prime powers to encode representations of what I know to be independent concepts in a NN for a small personal project.<p>I am working on a 32-bit (perhaps float) representation of mixtures of quantized Von Mises distributions (time of day patterns). I know there are enough bits to represent what I want, but I also want specific algebraic properties so that they will act as a probabilistic sketch: an accumulator or a Monad if you like.<p>I don&#x27;t know the exact formula for this probabilistic sketch operator, but I am positive it should exist. (I am just starting to learn group theory and category theory, to solve this problem; I suspect I want a specific semi-lattice structure, but I haven&#x27;t studied enough to know what properties I want)<p>My plan is to encode hourly buckets (location) as primes and how fuzzy they are (concentration) as their powers. I don&#x27;t know if this will work completely, but it will be the starting point for my next experiment: try to learn the probabilistic sketch I want.<p>I suspect that I will need different activation functions that you&#x27;d normally use in NN, because linear or ReLU or similar won&#x27;t be good to represent in finite space what I am searching for (likely a modular form or L-function). Looking at Koopman operator theory, I think I need to introduce non-linearity in the form of a Theta function neuron or Ramanujan Tau function (which is very connected to my problem).</div><br/></div></div><div id="38443224" class="c"><input type="checkbox" id="c-38443224" checked=""/><div class="controls bullet"><span class="by">seanhunter</span><span>|</span><a href="#38439553">root</a><span>|</span><a href="#38440787">parent</a><span>|</span><a href="#38443452">prev</a><span>|</span><a href="#38442798">next</a><span>|</span><label class="collapse" for="c-38443224">[-]</label><label class="expand" for="c-38443224">[1 more]</label></div><br/><div class="children"><div class="content">I would argue that there are a few fundamental ways to make progress in mathematics:<p>1. Proving that a thing or set of things is part of some grouping<p>2. Proving that a grouping has some property or set of properties (including connections to or relationships with other groupings)<p>These are extremely powerful tools and they buy you a lot because they allow you to connect new things in with mathematical work that has been done in the past. So for example if the GP surmises that something is a Lie group that buys them a bunch of results stretching back to the 18th century which can be applied to understand these neural nets even though they are a modern concept.</div><br/></div></div><div id="38442798" class="c"><input type="checkbox" id="c-38442798" checked=""/><div class="controls bullet"><span class="by">kdmccormick</span><span>|</span><a href="#38439553">root</a><span>|</span><a href="#38440787">parent</a><span>|</span><a href="#38443224">prev</a><span>|</span><a href="#38439824">next</a><span>|</span><label class="collapse" for="c-38442798">[-]</label><label class="expand" for="c-38442798">[1 more]</label></div><br/><div class="children"><div class="content">&gt; what this nomenclature gymnastics buys you?<p>???<p>Are you writing off all abstract mathematics as nomenclature gymnastics, or is there something about this connection that you think makes it particularly useless?</div><br/></div></div></div></div><div id="38439824" class="c"><input type="checkbox" id="c-38439824" checked=""/><div class="controls bullet"><span class="by">mathematicaster</span><span>|</span><a href="#38439553">parent</a><span>|</span><a href="#38440787">prev</a><span>|</span><a href="#38439216">next</a><span>|</span><label class="collapse" for="c-38439824">[-]</label><label class="expand" for="c-38439824">[1 more]</label></div><br/><div class="children"><div class="content">I did a little spelunking some time ago reacting to the same urge. Tropical geometry appears to be where the math talk is at.<p>Just dropping the reference here, I don&#x27;t grok the literature.</div><br/></div></div></div></div><div id="38439216" class="c"><input type="checkbox" id="c-38439216" checked=""/><div class="controls bullet"><span class="by">erikerikson</span><span>|</span><a href="#38439553">prev</a><span>|</span><a href="#38439660">next</a><span>|</span><label class="collapse" for="c-38439216">[-]</label><label class="expand" for="c-38439216">[9 more]</label></div><br/><div class="children"><div class="content">Before finishing my read, I need to register an objection to the opening which reads to me so as to imply it is the only means:<p>&gt; Researchers simulate a weird type of pseudo-neural-tissue, “reward” it a little every time it becomes a little more like the AI they want, and eventually it becomes the AI they want.<p>This isn&#x27;t the only way. Back propagation is a hack around the oversimplification of neural models.  By adding a sense of location into the network, you get linearly inseparable functions learned just fine.<p>Hopfield networks with Hebbian learning are sufficient and are implemented by the existing proofs of concept we have.</div><br/><div id="38439999" class="c"><input type="checkbox" id="c-38439999" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#38439216">parent</a><span>|</span><a href="#38441387">next</a><span>|</span><label class="collapse" for="c-38439999">[-]</label><label class="expand" for="c-38439999">[4 more]</label></div><br/><div class="children"><div class="content">This is true. We use backpropagation not because it’s the only way or because it’s biologically plausible (the brain doesn’t have any backward passes) but because it works. Neural networks aren’t special because of any sort of connection to the brain, we use them because we have hardware (GPUs) which can train them pretty quickly.<p>I feel the same way about transformers vs RNNs: even if RNNs are more “correct” in some sense of having theoretically infinite memory it takes forever to train them so transformers won. And then we developed techniques like Long LoRA which make theoretical disadvantages functionally irrelevant.</div><br/><div id="38442257" class="c"><input type="checkbox" id="c-38442257" checked=""/><div class="controls bullet"><span class="by">erikerikson</span><span>|</span><a href="#38439216">root</a><span>|</span><a href="#38439999">parent</a><span>|</span><a href="#38440384">next</a><span>|</span><label class="collapse" for="c-38442257">[-]</label><label class="expand" for="c-38442257">[1 more]</label></div><br/><div class="children"><div class="content">I agree that doing effective things is effective and we should do effective things when they prove valuable.  These things here are generally simplifications or even distillations which can be faster and more efficient which are excellent attributes of solutions.  My objection isn&#x27;t that we develop great systems but that we don&#x27;t forget that other branches in the solution space exist.  Particularly here because the more rich enough may help yield the next generation of solutions.</div><br/></div></div><div id="38440384" class="c"><input type="checkbox" id="c-38440384" checked=""/><div class="controls bullet"><span class="by">bilsbie</span><span>|</span><a href="#38439216">root</a><span>|</span><a href="#38439999">parent</a><span>|</span><a href="#38442257">prev</a><span>|</span><a href="#38441387">next</a><span>|</span><label class="collapse" for="c-38440384">[-]</label><label class="expand" for="c-38440384">[2 more]</label></div><br/><div class="children"><div class="content">&gt; developed techniques like Long LoRA which make theoretical disadvantages functionally irrelevant.<p>How’s that?</div><br/><div id="38440801" class="c"><input type="checkbox" id="c-38440801" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#38439216">root</a><span>|</span><a href="#38440384">parent</a><span>|</span><a href="#38441387">next</a><span>|</span><label class="collapse" for="c-38440801">[-]</label><label class="expand" for="c-38440801">[1 more]</label></div><br/><div class="children"><div class="content">Context windows used to be tiny e.g. 768 or 1024. That meant sliding windows of limited context if you had any reasonably sized input. If your context window is 32k or even 100k, a lot of inputs will fit into the context window entirely.<p>The reason huge context windows weren&#x27;t possible in the past is that memory requirements were quadratic with input length. Long LoRA lets us use less memory for our context windows, or use the same memory footprint for larger context windows.</div><br/></div></div></div></div></div></div><div id="38441387" class="c"><input type="checkbox" id="c-38441387" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#38439216">parent</a><span>|</span><a href="#38439999">prev</a><span>|</span><a href="#38441646">next</a><span>|</span><label class="collapse" for="c-38441387">[-]</label><label class="expand" for="c-38441387">[2 more]</label></div><br/><div class="children"><div class="content">I think it would be really exciting if somebody could show that ANNs that more resembled biological neurons could learn function approximation as well as (or better than!) current DNNs.  However, my understanding of math and engineering suggests that for the time being, the mechanisms we currently use and invest so much time and effort into will exceed more biologically inspired neurons, for utterly banal reasons.</div><br/><div id="38442265" class="c"><input type="checkbox" id="c-38442265" checked=""/><div class="controls bullet"><span class="by">erikerikson</span><span>|</span><a href="#38439216">root</a><span>|</span><a href="#38441387">parent</a><span>|</span><a href="#38441646">next</a><span>|</span><label class="collapse" for="c-38442265">[-]</label><label class="expand" for="c-38442265">[1 more]</label></div><br/><div class="children"><div class="content">Yet brains remain superior in many regards.  Still, by all means, let&#x27;s celebrate the advances we make!</div><br/></div></div></div></div><div id="38441646" class="c"><input type="checkbox" id="c-38441646" checked=""/><div class="controls bullet"><span class="by">blovescoffee</span><span>|</span><a href="#38439216">parent</a><span>|</span><a href="#38441387">prev</a><span>|</span><a href="#38439660">next</a><span>|</span><label class="collapse" for="c-38441646">[-]</label><label class="expand" for="c-38441646">[2 more]</label></div><br/><div class="children"><div class="content">Back propagation isn&#x27;t a hack. It&#x27;s a triumph. It&#x27;s powering the revolution we&#x27;re experiencing.</div><br/><div id="38442292" class="c"><input type="checkbox" id="c-38442292" checked=""/><div class="controls bullet"><span class="by">erikerikson</span><span>|</span><a href="#38439216">root</a><span>|</span><a href="#38441646">parent</a><span>|</span><a href="#38439660">next</a><span>|</span><label class="collapse" for="c-38442292">[-]</label><label class="expand" for="c-38442292">[1 more]</label></div><br/><div class="children"><div class="content">It has yield some excellent results that we should celebrate and use.  Yet the consensus of the field, when I was properly informed on such things, was that we developed back propagation while under a misunderstanding that linearly inseparable functions (e.g. XOR) could not be learned by Hebbian learning in Hopfield networks.  This was a correct result in the context of his assumptions but too limited and&#x2F;or over applied conclusion from Minsky&#x27;s work.</div><br/></div></div></div></div></div></div><div id="38439660" class="c"><input type="checkbox" id="c-38439660" checked=""/><div class="controls bullet"><span class="by">gmuslera</span><span>|</span><a href="#38439216">prev</a><span>|</span><a href="#38441231">next</a><span>|</span><label class="collapse" for="c-38439660">[-]</label><label class="expand" for="c-38439660">[1 more]</label></div><br/><div class="children"><div class="content">At least the first part reminded me of Hyperion and how AIs evolved there (I think the actual explanation is in The Fall of Hyperion), smaller but more interconnected &quot;code&quot;.<p>Not sure about actual implementation, but at least for us concepts or words are not pure nor isolated, they have multiple meanings that collapse into specific ones as you put several together</div><br/></div></div><div id="38441231" class="c"><input type="checkbox" id="c-38441231" checked=""/><div class="controls bullet"><span class="by">DonsDiscountGas</span><span>|</span><a href="#38439660">prev</a><span>|</span><a href="#38439728">next</a><span>|</span><label class="collapse" for="c-38441231">[-]</label><label class="expand" for="c-38441231">[1 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t this also(just?) a description of how high-dimensional embedding spaces work? Putting every kind of concept all in the same space is going to lead to some weird stuff. Different regions of the latent space will cover different concepts, with very uneven volumes, and local distances will generally be meaningful (red vs green) but long-distances won&#x27;t (red vs. ennui).<p>I guess we could also look at it the other way; embedding spaces work this way because the underlying neurons work this way.</div><br/></div></div><div id="38439728" class="c"><input type="checkbox" id="c-38439728" checked=""/><div class="controls bullet"><span class="by">daveguy</span><span>|</span><a href="#38441231">prev</a><span>|</span><a href="#38440295">next</a><span>|</span><label class="collapse" for="c-38439728">[-]</label><label class="expand" for="c-38439728">[53 more]</label></div><br/><div class="children"><div class="content">All this anthropomorphizing of activation networks strikes me as very odd. None of these neurons &quot;want&quot; to do anything. They respond to specific input. Maybe humans are the same, but in the case of artificial neural networks we at least know it&#x27;s a simple mathematical function. Also, an artificial neuron is nothing like a biological neuron. At the most basic -- artificial neurons don&#x27;t &quot;fire&quot; except in direct response to inputs. Biological neurons fire <i>because of their internal state</i>, state which is modified by biological signaling chemicals. It&#x27;s like comparing apples to gorillas.</div><br/><div id="38440842" class="c"><input type="checkbox" id="c-38440842" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#38439728">parent</a><span>|</span><a href="#38440134">next</a><span>|</span><label class="collapse" for="c-38440842">[-]</label><label class="expand" for="c-38440842">[7 more]</label></div><br/><div class="children"><div class="content">&gt;None of these neurons &quot;want&quot; to do anything. They respond to specific input.<p>Yes well, your neurons don&#x27;t &quot;want&quot; to do anything either.<p>&gt;Maybe humans are the same, but in the case of artificial neural networks we at least know it&#x27;s a simple mathematical function<p>So what, magic ? a soul ? If the brain is computing then the substrate is entirely irrelevant. Silicon, biology, pulleys and gears. all can be arranged to make the same or similar computations. If you genuinely believe the latter, it&#x27;s fine. The point is that &quot;simple&quot; mathematical function is kind of irrelevant. Either the brain computes and any substrate is fine or it doesn&#x27;t.<p>&gt;Also, an artificial neuron is nothing like a biological neuron.<p>They&#x27;re not the same but &quot;nothing like&quot; is pushing it a lot. They&#x27;re inspired by biological neurons and the only reason modern NNs aren&#x27;t closer to their biological counterparts is because they genuinely suffer for it, not because we can&#x27;t.<p>&gt;Biological neurons fire because of their internal state, state which is modified by biological signaling chemicals<p>Brains aren&#x27;t breaking break causality. The fire because of input.</div><br/><div id="38441046" class="c"><input type="checkbox" id="c-38441046" checked=""/><div class="controls bullet"><span class="by">airgapstopgap</span><span>|</span><a href="#38439728">root</a><span>|</span><a href="#38440842">parent</a><span>|</span><a href="#38440134">next</a><span>|</span><label class="collapse" for="c-38441046">[-]</label><label class="expand" for="c-38441046">[6 more]</label></div><br/><div class="children"><div class="content">Comments like this are incredibly grating. You condescend to the interlocutor for making a mistake which only exists in your own mistaken world model. Your confidence that neurons and ANN weights and «pulleys and gears» are all equivalent because there is, in theory, an intention to instantiate some computation, and to think otherwise is tantamount to belief in magic and broken causality, is just confused and born out of perusing popular-scientific materials instead of relying on scientific literature or hands-on experience.<p>&gt; The fire because of input.<p>No they do not fire because of input, they <i>modulate their firing probability</i> based on input, and there are different modalities of input with different effects. Neurons are self-contained biological units (descended, let me remind you, from standalone unicellular organisms, just like the rest of our cells), which actually have an independently developing internal state and even metabolic needs; they are not merely a system of logic gates even if you can approximate their role with a system of equations or an ANN. This is very different, mechanistically and teleologically. Hell, even spiking ANNs would be substantially different from currently dominant models.<p>&gt; So what, magic ? a soul ? If the brain is computing then the substrate is entirely irrelevant<p>Stop dumbing down complex arguments to some low-status culture war opinion you find it easy to dunk on.</div><br/><div id="38441243" class="c"><input type="checkbox" id="c-38441243" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#38439728">root</a><span>|</span><a href="#38441046">parent</a><span>|</span><a href="#38442465">next</a><span>|</span><label class="collapse" for="c-38441243">[-]</label><label class="expand" for="c-38441243">[3 more]</label></div><br/><div class="children"><div class="content">&gt;Your confidence that neurons and ANN weights and «pulleys and gears» are all equivalent because there is, in theory, an intention to instantiate some computation, and to think otherwise is tantamount to belief in magic and broken causality, is just confused and born out of perusing popular-scientific materials instead of relying on scientific literature or hands-on experience.<p>Computation is substrate independent. I&#x27;m not saying neurons and ANN weights and «pulleys and gears» are the same. I&#x27;m saying it does not matter because what you perform computation with does not change the results of the computation. If the brain computes, then it doesn&#x27;t matter what is doing the computation.<p>&gt;No they do not fire because of input, they modulate their firing probability based on input, and there are different modalities of input with different effects. Neurons are self-contained biological units (descended, let me remind you, from standalone unicellular organisms, just like the rest of our cells), which actually have an independently developing internal state and even metabolic needs; they are not merely a system of logic gates even if you can approximate their role with a system of equations or an ANN. This is very different, mechanistically and teleologically. Hell, even spiking ANNs would be substantially different from currently dominant models.<p>Yes, a neuron is firing because of input. To suggest otherwise is to suggest something beyond cause and effect directing the workings of the brain. If that is genuinely not the case then feel free to explain why, rather than an ad hominin attack on someone you don&#x27;t even know.<p>&gt; So what, magic ? a soul ? If the brain is computing then the substrate is entirely irrelevant<p>&gt;Stop dumbing down complex arguments to some low-status culture war opinion you find it easy to dunk on.<p>I personally don&#x27;t care if that&#x27;s what anyone believes. The intention is not to attack anyone.<p>If you believe in a soul or the non religious equivalent, that&#x27;s fine. We just have different axioms.<p>If you don&#x27;t believe in a soul(or the equivalent) but somehow think substrate matters then you need to explain why because it makes no sense.</div><br/><div id="38442882" class="c"><input type="checkbox" id="c-38442882" checked=""/><div class="controls bullet"><span class="by">kdmccormick</span><span>|</span><a href="#38439728">root</a><span>|</span><a href="#38441243">parent</a><span>|</span><a href="#38442465">next</a><span>|</span><label class="collapse" for="c-38442882">[-]</label><label class="expand" for="c-38442882">[2 more]</label></div><br/><div class="children"><div class="content">I am not well versed in any of this, but from reading the counterarguments, I think two good points are being made:<p>* Analogies aside, neurons are quite different than NN nodes, because each neuron has an incredibly complex internal cellular state, whereas an NN node just has an integer for state.<p>* A brain is not a &quot;function&quot; in the way that a trained LLM model is. Human life is not a series of input prompts and output prompts. Rather, we experience a fluid stream of stimuli, which our brain multiplexes and reacts to in a variety of ways (speaking, moving, storing memories, moving our pupils, releasing hormones, etc). That is NOT TO SAY a brain violates causality; it&#x27;s saying that the brain is <i>mechanically</i> doing so much more than an LLM, even if the LLM is better at raw computation.<p>None of this IMO precludes AGI from happening in the medium term future, but I do think we should be careful when making comparisons between AGI and the human brains.<p>Rather than comparing &quot;apples to gorillas&quot;, I&#x27;d say it&#x27;s like comparing a calculator to a tree. Yes, the calculator is SIGNIFICANTLY better at multiplication, but that doesn&#x27;t make it &quot;smarter&quot; than a tree, whatever that means.</div><br/><div id="38443558" class="c"><input type="checkbox" id="c-38443558" checked=""/><div class="controls bullet"><span class="by">airgapstopgap</span><span>|</span><a href="#38439728">root</a><span>|</span><a href="#38442882">parent</a><span>|</span><a href="#38442465">next</a><span>|</span><label class="collapse" for="c-38443558">[-]</label><label class="expand" for="c-38443558">[1 more]</label></div><br/><div class="children"><div class="content">I do not even think any of this has much of impact on AGI timelines. Human brain cells are not a superior substrate for computing &quot;intelligence&quot;.  They just are what they are; individual cells can somewhat meaningfully &quot;want&quot; stuff and be quasi-agents unto themselves, they do much more than integrate and fire. Weights in an ANN are purely terms in an equation without any inner process or content.</div><br/></div></div></div></div></div></div><div id="38442465" class="c"><input type="checkbox" id="c-38442465" checked=""/><div class="controls bullet"><span class="by">red75prime</span><span>|</span><a href="#38439728">root</a><span>|</span><a href="#38441046">parent</a><span>|</span><a href="#38441243">prev</a><span>|</span><a href="#38440134">next</a><span>|</span><label class="collapse" for="c-38442465">[-]</label><label class="expand" for="c-38442465">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Stop dumbing down complex arguments<p>It&#x27;s not dumbing down. It&#x27;s extracting the crux of the matter that the complexity of arguments is trying to hide, perhaps unintentionally. Either the brain implements a function that can be approximated by a neural network thanks to universal approximation theorem, or the function cannot be approximated (you need arguments for why it is the case), or magic.</div><br/><div id="38442732" class="c"><input type="checkbox" id="c-38442732" checked=""/><div class="controls bullet"><span class="by">morsecodist</span><span>|</span><a href="#38439728">root</a><span>|</span><a href="#38442465">parent</a><span>|</span><a href="#38440134">next</a><span>|</span><label class="collapse" for="c-38442732">[-]</label><label class="expand" for="c-38442732">[1 more]</label></div><br/><div class="children"><div class="content">This is technically true but kind of misses the point in my opinion. A neural network can approximate any function in theory but that doesn&#x27;t mean it has to do so in a reasonable amount of time and with a reasonable amount of resources. For example, take the function that gives you the prime factors of an integer. It is theoretically possible for a neural network to approximate this for an arbitrarily large fixed window but is provably infeasible to compute on current hardware. In theory, a quantum computer could compute this much faster.<p>This is not to say that the human brain leverages quantum effects. It&#x27;s just a well known example where the hardware and a specific algorithm can be shown to matter.<p>I also think it&#x27;s strange to describe the brain as implementing a function. Functions don&#x27;t exist. We made them up to help us think about building useful circuits (among other things). In this scenario, we would be implementing functions to help us simulate what is going on in brains.</div><br/></div></div></div></div></div></div></div></div><div id="38440134" class="c"><input type="checkbox" id="c-38440134" checked=""/><div class="controls bullet"><span class="by">Notatheist</span><span>|</span><a href="#38439728">parent</a><span>|</span><a href="#38440842">prev</a><span>|</span><a href="#38440103">next</a><span>|</span><label class="collapse" for="c-38440134">[-]</label><label class="expand" for="c-38440134">[10 more]</label></div><br/><div class="children"><div class="content">I feel anthropomorphizing is perfectly reasonable especially in this context. How would you like it described?</div><br/><div id="38440596" class="c"><input type="checkbox" id="c-38440596" checked=""/><div class="controls bullet"><span class="by">kibwen</span><span>|</span><a href="#38439728">root</a><span>|</span><a href="#38440134">parent</a><span>|</span><a href="#38440415">next</a><span>|</span><label class="collapse" for="c-38440596">[-]</label><label class="expand" for="c-38440596">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s one thing to deliberately anthropomorphize in order to construct an analogy.<p>It&#x27;s another thing to anthropomorphize by accident or by illusion, as per pareidolia: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Pareidolia" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Pareidolia</a> . Just as in pareidolia, where the human brain is primed to &quot;see&quot; a human face in a certain pattern of light and shapes, it seems that human brains are primed to &quot;see&quot; a human intelligence in the output of an LLM, because our brains are pattern-matching on &quot;things that look like human speech&quot;. But that&#x27;s a reason to <i>not</i> anthropomorphize LLMs, precisely because people are inclined to do so without thinking.</div><br/></div></div><div id="38440415" class="c"><input type="checkbox" id="c-38440415" checked=""/><div class="controls bullet"><span class="by">lacrimacida</span><span>|</span><a href="#38439728">root</a><span>|</span><a href="#38440134">parent</a><span>|</span><a href="#38440596">prev</a><span>|</span><a href="#38440275">next</a><span>|</span><label class="collapse" for="c-38440415">[-]</label><label class="expand" for="c-38440415">[1 more]</label></div><br/><div class="children"><div class="content">Anthropomorphizing is valid in relation with laguage, human language. Other than that anthropomorphizing would be somewhat valid with other human outputs but ideally it is understood it’s just one of the lenses in the toolkit, that it has pros and cons just like any idea or any tool.</div><br/></div></div><div id="38440275" class="c"><input type="checkbox" id="c-38440275" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#38439728">root</a><span>|</span><a href="#38440134">parent</a><span>|</span><a href="#38440415">prev</a><span>|</span><a href="#38440103">next</a><span>|</span><label class="collapse" for="c-38440275">[-]</label><label class="expand" for="c-38440275">[7 more]</label></div><br/><div class="children"><div class="content">There’s so much baggage attached with it. The only thing a neural network “wants” is to minimize its loss function. That’s all.</div><br/><div id="38441180" class="c"><input type="checkbox" id="c-38441180" checked=""/><div class="controls bullet"><span class="by">mitthrowaway2</span><span>|</span><a href="#38439728">root</a><span>|</span><a href="#38440275">parent</a><span>|</span><a href="#38440573">next</a><span>|</span><label class="collapse" for="c-38441180">[-]</label><label class="expand" for="c-38441180">[1 more]</label></div><br/><div class="children"><div class="content">The neural network doesn&#x27;t want to minimize its loss function any more than you want to maximize your inclusive genetic fitness.<p>The neural network <i>training process</i> wants to minimize the neural network&#x27;s loss function. The neural network, if it &quot;wants&quot; anything, will have such wants as were embedded in its weights through the process of minimize its loss function, which will mostly be &quot;wants&quot; whose satisfactions correlate with reduced loss function. Of course this is using the term &quot;want&quot; in a behaviourally-descriptive sense, not a subjective-experience sense.<p>For example, AlphaGo&#x27;s training routine wants to minimize AlphaGo&#x27;s loss function. AlphaGo wants to beat you at Go.</div><br/></div></div><div id="38440573" class="c"><input type="checkbox" id="c-38440573" checked=""/><div class="controls bullet"><span class="by">srcreigh</span><span>|</span><a href="#38439728">root</a><span>|</span><a href="#38440275">parent</a><span>|</span><a href="#38441180">prev</a><span>|</span><a href="#38441478">next</a><span>|</span><label class="collapse" for="c-38440573">[-]</label><label class="expand" for="c-38440573">[2 more]</label></div><br/><div class="children"><div class="content">You must not believe in human free will then. Or maybe you don’t believe the brain is the key aspect of human intelligence. Or believe the brain is more than just a organic NN</div><br/><div id="38440741" class="c"><input type="checkbox" id="c-38440741" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#38439728">root</a><span>|</span><a href="#38440573">parent</a><span>|</span><a href="#38441478">next</a><span>|</span><label class="collapse" for="c-38440741">[-]</label><label class="expand" for="c-38440741">[1 more]</label></div><br/><div class="children"><div class="content">I’m saying that neural networks are so different from brains that anthropomorphizing them runs a real risk of glossing over important differences.<p>Terms like “free will” and “intelligence” are too fuzzy to talk about precisely unless we’re on the exact same page regarding what we mean. And applying our imprecise definitions to machines is not doing us any favors.</div><br/></div></div></div></div><div id="38441478" class="c"><input type="checkbox" id="c-38441478" checked=""/><div class="controls bullet"><span class="by">bobbylarrybobby</span><span>|</span><a href="#38439728">root</a><span>|</span><a href="#38440275">parent</a><span>|</span><a href="#38440573">prev</a><span>|</span><a href="#38440451">next</a><span>|</span><label class="collapse" for="c-38441478">[-]</label><label class="expand" for="c-38441478">[1 more]</label></div><br/><div class="children"><div class="content">All humans “want” to do is obey the laws of physics.</div><br/></div></div><div id="38440451" class="c"><input type="checkbox" id="c-38440451" checked=""/><div class="controls bullet"><span class="by">csours</span><span>|</span><a href="#38439728">root</a><span>|</span><a href="#38440275">parent</a><span>|</span><a href="#38441478">prev</a><span>|</span><a href="#38440103">next</a><span>|</span><label class="collapse" for="c-38440451">[-]</label><label class="expand" for="c-38440451">[2 more]</label></div><br/><div class="children"><div class="content">Can we anthropomorphize the loss function then?</div><br/><div id="38440569" class="c"><input type="checkbox" id="c-38440569" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#38439728">root</a><span>|</span><a href="#38440451">parent</a><span>|</span><a href="#38440103">next</a><span>|</span><label class="collapse" for="c-38440569">[-]</label><label class="expand" for="c-38440569">[1 more]</label></div><br/><div class="children"><div class="content">That&#x27;s like anthropomorphizing utility functions in place of the humans that (are conjectured to) have them, isn&#x27;t it?</div><br/></div></div></div></div></div></div></div></div><div id="38440103" class="c"><input type="checkbox" id="c-38440103" checked=""/><div class="controls bullet"><span class="by">aatd86</span><span>|</span><a href="#38439728">parent</a><span>|</span><a href="#38440134">prev</a><span>|</span><a href="#38440961">next</a><span>|</span><label class="collapse" for="c-38440103">[-]</label><label class="expand" for="c-38440103">[8 more]</label></div><br/><div class="children"><div class="content">Do you mean that artificial neurons are inherently passive while biological neurons are inherently active i.e. they would act in spite of external input?<p>Just wondering if I understood you, I don&#x27;t know anything on the subject.</div><br/><div id="38440325" class="c"><input type="checkbox" id="c-38440325" checked=""/><div class="controls bullet"><span class="by">baobabKoodaa</span><span>|</span><a href="#38439728">root</a><span>|</span><a href="#38440103">parent</a><span>|</span><a href="#38440330">next</a><span>|</span><label class="collapse" for="c-38440325">[-]</label><label class="expand" for="c-38440325">[5 more]</label></div><br/><div class="children"><div class="content">That was already addressed in the comment you&#x27;re responding to. They wrote that in the case of artificial neural networks &quot;at least we know [there is no god inside the machine]&quot;. With regards to humans, we don&#x27;t know.</div><br/><div id="38441087" class="c"><input type="checkbox" id="c-38441087" checked=""/><div class="controls bullet"><span class="by">TacticalCoder</span><span>|</span><a href="#38439728">root</a><span>|</span><a href="#38440325">parent</a><span>|</span><a href="#38440808">next</a><span>|</span><label class="collapse" for="c-38441087">[-]</label><label class="expand" for="c-38441087">[3 more]</label></div><br/><div class="children"><div class="content">There are a great many possibilities between:<p>- a fully deterministic machine (even if the interface and the way OpenAI let people access ChatGPT make it seem like it&#x27;s non-deterministic, there are fully deterministic models out there, who not only only respond to inputs but also always respond the exact same thing given the same inputs [query, seed, ...]),<p>and:<p>- god exists<p>There could be chaos at work when human thinks. There may be interferences at play, say because whatever element that traveled trillion of kilometers just traversed our brain.<p>While a fully deterministic machine that always respond to the same input in the same way is just that: a deterministic machine.<p>P.S: I don&#x27;t know about other LLMs like Falcon 180b but image-generation models like StableDiffusion are fully deterministic. I think a model is using a broken design and shall quickly hit limitations if it cannot be queried in a deterministic way (and its usecases are certainly limited if repeatability is not achievable). If you want different answers, use a different seed or a different query. But the same query+seed should always give the exact same output.</div><br/><div id="38441552" class="c"><input type="checkbox" id="c-38441552" checked=""/><div class="controls bullet"><span class="by">syntaxfree</span><span>|</span><a href="#38439728">root</a><span>|</span><a href="#38441087">parent</a><span>|</span><a href="#38442119">next</a><span>|</span><label class="collapse" for="c-38441552">[-]</label><label class="expand" for="c-38441552">[1 more]</label></div><br/><div class="children"><div class="content">Also not all notions of God are dualistic — where you get to (notionally) talk to the guy using “you” and “I”. India’s Advaita Vedanta, Ibn Arabi’s version of Sufism and even some sects of Hasidism all hold that everything is <i>in</i> God. I haven’t found the Christianity that does this, but if it was discovered under Islam is probably thinkable here.<p>Are these different ideas of God entirely? Yes: in India there are however many gods and Brahman says these are lesser precisely because they don’t include the whole universe.</div><br/></div></div><div id="38442119" class="c"><input type="checkbox" id="c-38442119" checked=""/><div class="controls bullet"><span class="by">recursivecaveat</span><span>|</span><a href="#38439728">root</a><span>|</span><a href="#38441087">parent</a><span>|</span><a href="#38441552">prev</a><span>|</span><a href="#38440808">next</a><span>|</span><label class="collapse" for="c-38442119">[-]</label><label class="expand" for="c-38442119">[1 more]</label></div><br/><div class="children"><div class="content">For the record LLM are theoretically fully deterministic, but non-deterministic in practice. First, some randomness is deliberately set via &#x27;temperature&#x27;, and second some randomness comes from things like the order of floating point operations when you divide the model on your GPU(s) without being super careful about it.</div><br/></div></div></div></div><div id="38440808" class="c"><input type="checkbox" id="c-38440808" checked=""/><div class="controls bullet"><span class="by">aatd86</span><span>|</span><a href="#38439728">root</a><span>|</span><a href="#38440325">parent</a><span>|</span><a href="#38441087">prev</a><span>|</span><a href="#38440330">next</a><span>|</span><label class="collapse" for="c-38440808">[-]</label><label class="expand" for="c-38440808">[1 more]</label></div><br/><div class="children"><div class="content">That wasn&#x27;t clear to me because having internal state doesn&#x27;t mean that any process is running.
That&#x27;s the difference between memory and CPU.</div><br/></div></div></div></div><div id="38440330" class="c"><input type="checkbox" id="c-38440330" checked=""/><div class="controls bullet"><span class="by">pengstrom</span><span>|</span><a href="#38439728">root</a><span>|</span><a href="#38440103">parent</a><span>|</span><a href="#38440325">prev</a><span>|</span><a href="#38441183">next</a><span>|</span><label class="collapse" for="c-38440330">[-]</label><label class="expand" for="c-38440330">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;d say &#x27;in spite&#x27;-ness is pretty spot on for life in general</div><br/></div></div></div></div><div id="38440961" class="c"><input type="checkbox" id="c-38440961" checked=""/><div class="controls bullet"><span class="by">benchaney</span><span>|</span><a href="#38439728">parent</a><span>|</span><a href="#38440103">prev</a><span>|</span><a href="#38441389">next</a><span>|</span><label class="collapse" for="c-38440961">[-]</label><label class="expand" for="c-38440961">[1 more]</label></div><br/><div class="children"><div class="content">Can you be more specific about what particular anthropomorphizing you object to?  The only place the author uses the word want is in describing the wants of humans.</div><br/></div></div><div id="38441389" class="c"><input type="checkbox" id="c-38441389" checked=""/><div class="controls bullet"><span class="by">gremlinunderway</span><span>|</span><a href="#38439728">parent</a><span>|</span><a href="#38440961">prev</a><span>|</span><a href="#38440203">next</a><span>|</span><label class="collapse" for="c-38441389">[-]</label><label class="expand" for="c-38441389">[1 more]</label></div><br/><div class="children"><div class="content">I think you&#x27;re being a bit pedantic here.<p>Neurons also don&#x27;t &quot;respond&quot; to specific input either. They can&#x27;t speak or provide an answer to your input.<p>These are all just abstract metaphors and analogies. Literally everything in computer science at some point or another is an abstract metaphor or analogy.<p>When you look up the definition and etymology of &quot;input&quot;, it says to &quot;put on&quot; or &quot;impose&quot; or &quot;feed data into the machine&quot;. We&#x27;re not literally feeding the machine data, it doesn&#x27;t eat the data and subsist on it.<p>You could go on and on and nitpick every single one of these, and I don&#x27;t think the use of &quot;want&quot; (i.e. anthropomorphizing the networks to have intent) is all that bad.</div><br/></div></div><div id="38440203" class="c"><input type="checkbox" id="c-38440203" checked=""/><div class="controls bullet"><span class="by">ShamelessC</span><span>|</span><a href="#38439728">parent</a><span>|</span><a href="#38441389">prev</a><span>|</span><a href="#38439775">next</a><span>|</span><label class="collapse" for="c-38440203">[-]</label><label class="expand" for="c-38440203">[8 more]</label></div><br/><div class="children"><div class="content">Wait what are you referring to specifically? Any anthropomorphism in the article is _clearly_, clearly the author&#x27;s admitted simplification due to the incredible density of the subject matter.<p>Given that, I honestly can&#x27;t find anything too upsetting.<p>In any case, anthropomorphism is something I don&#x27;t mind, mostly. Is it misleading? For the layman. But the domain is one of modeling intelligence itself and there are many instances where an existing definition simply makes sense. This happens in lots of fields and causes similar amounts of frustration in those fields. So it goes.</div><br/><div id="38440370" class="c"><input type="checkbox" id="c-38440370" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#38439728">root</a><span>|</span><a href="#38440203">parent</a><span>|</span><a href="#38439775">next</a><span>|</span><label class="collapse" for="c-38440370">[-]</label><label class="expand" for="c-38440370">[7 more]</label></div><br/><div class="children"><div class="content">&gt; Humans also use neural nets to reason about concepts. We have a lot of neurons, but so does GPT-4.<p>I feel this is an abuse of the language. Biological neurons and ANN neurons aren’t the same or even all that similar. Brains don’t do backprop for example. Only forward passes. There’s a zoo of neurotransmitters which change the behavior of individual neurons or regions in the brain. Unused neurons in the brain can be repurposed for other things (for example if your arm is amputated).</div><br/><div id="38440863" class="c"><input type="checkbox" id="c-38440863" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#38439728">root</a><span>|</span><a href="#38440370">parent</a><span>|</span><a href="#38439775">next</a><span>|</span><label class="collapse" for="c-38440863">[-]</label><label class="expand" for="c-38440863">[6 more]</label></div><br/><div class="children"><div class="content">&gt;Biological neurons and ANN neurons aren’t the same or even all that similar.<p>They&#x27;re not the same but they&#x27;re definitely similar.<p>&gt;Brains don’t do backprop for example.<p>We&#x27;ve developed numerous different learning algorithms that are biologically plausible, but they all kinda work like backpropagation but worse, so we stuck with backpropagation. We&#x27;ve made more complicated neurons that better resemble biological neurons, but it is faster and works better if you just add extra simple neurons, so we do that instead. Spiking neural networks have connection patterns more similar to what you see in the brain, but they learn slower and are tougher to work with than regular layered neural networks, so we use layered neural networks instead.<p>The only reason modern NNs aren&#x27;t closer to their biological counterparts is because they genuinely suffer for it.<p>The secret of bird flight was wings. Not feathers. Not flapping.</div><br/><div id="38441084" class="c"><input type="checkbox" id="c-38441084" checked=""/><div class="controls bullet"><span class="by">tsimionescu</span><span>|</span><a href="#38439728">root</a><span>|</span><a href="#38440863">parent</a><span>|</span><a href="#38441122">next</a><span>|</span><label class="collapse" for="c-38441084">[-]</label><label class="expand" for="c-38441084">[1 more]</label></div><br/><div class="children"><div class="content">&gt; The only reason modern NNs aren&#x27;t closer to their biological counterparts is because they genuinely suffer for it.<p>And yet, there&#x27;s no ANN that&#x27;s as good at interacting with the real world as the simplest worms we&#x27;ve studied, despite having many times more neurons than those worms have cells.<p>We are clearly still missing some key pieces of the puzzle for intelligence, so claiming that the difference between ANNs and biological neurons is irrelevant is quite premature. We are far away from having an airfoil moment in AI research.</div><br/></div></div><div id="38441122" class="c"><input type="checkbox" id="c-38441122" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#38439728">root</a><span>|</span><a href="#38440863">parent</a><span>|</span><a href="#38441084">prev</a><span>|</span><a href="#38439775">next</a><span>|</span><label class="collapse" for="c-38441122">[-]</label><label class="expand" for="c-38441122">[4 more]</label></div><br/><div class="children"><div class="content">My point isn’t about whether ANNs work, it’s that they’re so fundamentally different (both locally at the level of the neuron and globally at the level of the brain) that calling them both “neurons” is pretty imprecise.<p>Silicon simulations of brains may “suffer” from being faithful but this also discounts the advantages that brains have. As I mentioned for example, brains can repurpose neurons for other tasks. Brains can also generalize from a single example, unlike neural networks which require thousands if not millions of examples.<p>Brains also generally do not suffer from catastrophic forgetting in the same way that our simulations tend to. If I ask you to study a textbook on cats you won’t suddenly forget the difference between cats and dogs.</div><br/><div id="38442096" class="c"><input type="checkbox" id="c-38442096" checked=""/><div class="controls bullet"><span class="by">greiskul</span><span>|</span><a href="#38439728">root</a><span>|</span><a href="#38441122">parent</a><span>|</span><a href="#38441357">next</a><span>|</span><label class="collapse" for="c-38442096">[-]</label><label class="expand" for="c-38442096">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Brains can also generalize from a single example, unlike neural networks which require thousands if not millions of examples.<p>Since the other comment already went for the evolution and structure angle, I&#x27;ll go for the other part. What single example? What test have you seen done on the brain capacity of few weeks old fetuses? Our brains start learning patterns in the world before we are even born. How much input does a baby receive every single second from it&#x27;s eyes and ears and every other sense?<p>Even when you are &quot;analyzing&quot; a new object for the first time, you receive a continuous stream of sensory input of it. Our brain even requires that to work, if you put a single frame different in a fast enough display, most times you won&#x27;t even notice the extra frame and your brain will just ignore it.</div><br/></div></div><div id="38441357" class="c"><input type="checkbox" id="c-38441357" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#38439728">root</a><span>|</span><a href="#38441122">parent</a><span>|</span><a href="#38442096">prev</a><span>|</span><a href="#38439775">next</a><span>|</span><label class="collapse" for="c-38441357">[-]</label><label class="expand" for="c-38441357">[2 more]</label></div><br/><div class="children"><div class="content">&gt;Brains can also generalize from a single example, unlike neural networks which require thousands if not millions of examples.<p>There is not a single brain on earth that is the blank slate a typical ANN is. &quot;Brains generalize from one example&quot; is pretty dubious. Millions of years of evolution matter.<p>&gt;As I mentioned for example, brains can repurpose neurons for other tasks.<p>Isn&#x27;t this just a matter of the practical distinction between training and inference and not some fundamental structural limitation ?<p>&gt;Brains also generally do not suffer from catastrophic forgetting in the same way that our simulations tend to.<p>This suggests CF may well be a simple matter of scale - <a href="https:&#x2F;&#x2F;palm-e.github.io&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;palm-e.github.io&#x2F;</a><p>Since individual anns are much closer to synapses, we don&#x27;t have anything near the scale of the brain yet.</div><br/><div id="38441894" class="c"><input type="checkbox" id="c-38441894" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#38439728">root</a><span>|</span><a href="#38441357">parent</a><span>|</span><a href="#38439775">next</a><span>|</span><label class="collapse" for="c-38441894">[-]</label><label class="expand" for="c-38441894">[1 more]</label></div><br/><div class="children"><div class="content">&gt; There is not a single brain on earth that is the blank slate a typical ANN is.<p>Of course structure matters, but biological neurons have far more degrees of freedom than those in ANNs. The fact that we even need to keep differentiating between the two is an indication that classifying both as “neurons” is not accurate.<p>&gt; Isn&#x27;t this just a matter of the practical distinction between training and inference and not some fundamental structural limitation?<p>It’s a difference in capabilities of the things themselves. A biological neuron organically seeks out new connections. Sure we could program that into an ANN somehow but the fact that nodes in an ANN don’t have this capability out of the box is a fundamental difference.<p>&gt; CF may well be a simple matter of scale<p>For a moment, a big enough network might be able to mirror an entire brain with the lottery ticket hypothesis. But if it takes two or ten or a thousand ANN neurons to simulate the degrees of freedom of a biological neuron, are they really the same?</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="38439775" class="c"><input type="checkbox" id="c-38439775" checked=""/><div class="controls bullet"><span class="by">lainga</span><span>|</span><a href="#38439728">parent</a><span>|</span><a href="#38440203">prev</a><span>|</span><a href="#38440626">next</a><span>|</span><label class="collapse" for="c-38439775">[-]</label><label class="expand" for="c-38439775">[6 more]</label></div><br/><div class="children"><div class="content">Wait till you find out how we talk about evolutionary adaptation... a couple analogies and elided concepts here and there and you&#x27;d swear Lamarck had smothered Darwin in his cradle</div><br/><div id="38440527" class="c"><input type="checkbox" id="c-38440527" checked=""/><div class="controls bullet"><span class="by">jjw1414</span><span>|</span><a href="#38439728">root</a><span>|</span><a href="#38439775">parent</a><span>|</span><a href="#38440626">next</a><span>|</span><label class="collapse" for="c-38440527">[-]</label><label class="expand" for="c-38440527">[5 more]</label></div><br/><div class="children"><div class="content">Yes, Lamarck is alive and well. I frequently hear colleagues in biology with years of experience making lazy statements such as, &quot;Humans evolved to walk because...&quot; as if evolution is a conscious act.  In many cases, these people are using the term evolution as shorthand for, &quot;Random mutations gave rise to new gene variants (alleles), and over numerous generations, natural selection leads to an increase in the prevalence of beneficial variants, specifically those that provide a reproductive advantage. The genetic variants that combined to enable bipedal locomotion provided such an advantage to the human population&quot;. Evolution is a passive, natural process driven by genetic variation, environmental changes, and the differential reproductive success of individuals with advantageous traits. This in turn results in population changes.  Evolution doesn&#x27;t involve a conscious choice or direction.  Evolution is the result of the cumulative effects of these factors over long periods of time.</div><br/><div id="38443579" class="c"><input type="checkbox" id="c-38443579" checked=""/><div class="controls bullet"><span class="by">Sharlin</span><span>|</span><a href="#38439728">root</a><span>|</span><a href="#38440527">parent</a><span>|</span><a href="#38441334">next</a><span>|</span><label class="collapse" for="c-38443579">[-]</label><label class="expand" for="c-38443579">[1 more]</label></div><br/><div class="children"><div class="content">Even in math, the most exact of all fields of study, people in practice are being nonrigorous and ambiguous and take shortcuts all the time. That’s just how communication works, because it would be incredibly grating and inefficient to try to be 100% unambiguous when everybody already knows what you mean.</div><br/></div></div><div id="38441334" class="c"><input type="checkbox" id="c-38441334" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#38439728">root</a><span>|</span><a href="#38440527">parent</a><span>|</span><a href="#38443579">prev</a><span>|</span><a href="#38443731">next</a><span>|</span><label class="collapse" for="c-38441334">[-]</label><label class="expand" for="c-38441334">[1 more]</label></div><br/><div class="children"><div class="content">While I generally agree with you, is it really &quot;lazy&quot; to say &quot;humans evolved to walk to&quot;?  Usually in the context, it&#x27;s not being used to claim there is some sort of intent or purpose, but rather to shortcut the (rather verbose) description you gave.<p>Also I can think of some counterpoints to yours: the people who bred teosinte into corn (or any wild grain into a domesticated one) appear to be making conscious choices or direction- that is, they used their intelligence and reasoning from observed examples of pairings to conclude that they could make improved specimens based on selective breeding (without knowing about random mutations of natural selection!).<p>And if we start to modify human germline then would also be an example of evolution with conscious choice or direction (assuming the modifications became fixed in the population).</div><br/></div></div><div id="38443731" class="c"><input type="checkbox" id="c-38443731" checked=""/><div class="controls bullet"><span class="by">hoseja</span><span>|</span><a href="#38439728">root</a><span>|</span><a href="#38440527">parent</a><span>|</span><a href="#38441334">prev</a><span>|</span><a href="#38441028">next</a><span>|</span><label class="collapse" for="c-38443731">[-]</label><label class="expand" for="c-38443731">[1 more]</label></div><br/><div class="children"><div class="content">Social, memetic evolution is at least partially Lamarckian.</div><br/></div></div></div></div></div></div><div id="38440626" class="c"><input type="checkbox" id="c-38440626" checked=""/><div class="controls bullet"><span class="by">drones</span><span>|</span><a href="#38439728">parent</a><span>|</span><a href="#38439775">prev</a><span>|</span><a href="#38440295">next</a><span>|</span><label class="collapse" for="c-38440626">[-]</label><label class="expand" for="c-38440626">[11 more]</label></div><br/><div class="children"><div class="content">All this anthropomorphizing of humans strikes me as very odd. None of these humans &quot;want&quot; to do anything. They respond to specific input. Maybe artificial neural networks are the same, but in the case of humans we at least know it&#x27;s a simple reaction to neurotransmitter signals.</div><br/><div id="38440788" class="c"><input type="checkbox" id="c-38440788" checked=""/><div class="controls bullet"><span class="by">arolihas</span><span>|</span><a href="#38439728">root</a><span>|</span><a href="#38440626">parent</a><span>|</span><a href="#38440805">next</a><span>|</span><label class="collapse" for="c-38440788">[-]</label><label class="expand" for="c-38440788">[5 more]</label></div><br/><div class="children"><div class="content">You&#x27;re probably just being tongue in cheek here, but I still wanted to add that we humans do actually &quot;want&quot;. Desires, intentions, these are real things. You can try to represent them as loss functions or rewards in some mathematical model but that isn&#x27;t the original thing. Consider this excerpt from <a href="https:&#x2F;&#x2F;scottaaronson.blog&#x2F;?p=7094#comment-1947377" rel="nofollow noreferrer">https:&#x2F;&#x2F;scottaaronson.blog&#x2F;?p=7094#comment-1947377</a><p>&gt; <i>Most animals are goal-directed, intentional, sensory-motor agents who grow interior representations of their environments during their lifetime which enables them to successfully navigate their environments. They are responsive to reasons their environments affords for action, because they can reason from their desires and beliefs towards actions.<p>In addition, animals like people, have complex representational abilities where we can reify the sensory-motor “concepts” which we develop as “abstract concepts” and give them symbolic representations which can then be communicated. We communicate because we have the capacity to form such representations, translate them symbolically, and use those symbols “on the right occasions” when we have the relevant mental states.<p>(Discrete mathematicians seem to have imparted a magical property to these symbols that *in them* is everything… no, when I use words its to represent my interior states… the words are </i>symptoms<i>, their patterns are coincidental and useful, but not where anything important lies).<p>In other words, we say “I like ice-cream” because: we are able to like things (desire, preference), we have tasted ice-cream, we have reflected on our preferences (via a capacity for self-modelling and self-directed emotional awareness), and so on. And when we say, “I like ice-cream” it’s *because* all of those things come together in radically complex ways to actually put us in a position to speak truthfully about ourselves. We really do like ice-cream.</i></div><br/><div id="38440933" class="c"><input type="checkbox" id="c-38440933" checked=""/><div class="controls bullet"><span class="by">Zambyte</span><span>|</span><a href="#38439728">root</a><span>|</span><a href="#38440788">parent</a><span>|</span><a href="#38441032">next</a><span>|</span><label class="collapse" for="c-38440933">[-]</label><label class="expand" for="c-38440933">[3 more]</label></div><br/><div class="children"><div class="content">&gt; but I still wanted to add that we humans do actually &quot;want&quot;.<p>I would also like to add that the subject of conversation is artificial and natural neurons, which humans, though contain some, are not.<p>If a NN is trained to do something, it can be equally considered as &quot;wanting&quot; to do that thing within the autonomy it is afforded, as much as any human.</div><br/><div id="38441222" class="c"><input type="checkbox" id="c-38441222" checked=""/><div class="controls bullet"><span class="by">TerrifiedMouse</span><span>|</span><a href="#38439728">root</a><span>|</span><a href="#38440933">parent</a><span>|</span><a href="#38441032">next</a><span>|</span><label class="collapse" for="c-38441222">[-]</label><label class="expand" for="c-38441222">[2 more]</label></div><br/><div class="children"><div class="content">&gt; If a NN is trained to do something, it can be equally considered as &quot;wanting&quot; to do that thing within the autonomy it is afforded, as much as any human.<p>Human wants are driven by instinct though - i.e. our preferences; if you like women, you like women, if you don’t, you don’t.<p>Our “output” is in service to those wants.<p>Current AI doesn’t have instinct &#x2F; preprogrammed goals - except for goal-driven AIs but the hyped up LLMs aren’t such AIs. Their output isn’t motivated by any goal - a LLM can’t deliberately lie to you to get you to do something; it lies because it doesn’t differentiate between what’s true and what’s false.</div><br/><div id="38442098" class="c"><input type="checkbox" id="c-38442098" checked=""/><div class="controls bullet"><span class="by">Zambyte</span><span>|</span><a href="#38439728">root</a><span>|</span><a href="#38441222">parent</a><span>|</span><a href="#38441032">next</a><span>|</span><label class="collapse" for="c-38442098">[-]</label><label class="expand" for="c-38442098">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Human wants are driven by instinct though<p>What is instinct physically?<p>&gt; a LLM can’t deliberately lie to you to get you to do something; it lies because it doesn’t differentiate between what’s true and what’s false.<p>A LLM also can&#x27;t do multi-step reasoning, yet here we are.</div><br/></div></div></div></div></div></div><div id="38441032" class="c"><input type="checkbox" id="c-38441032" checked=""/><div class="controls bullet"><span class="by">bongodongobob</span><span>|</span><a href="#38439728">root</a><span>|</span><a href="#38440788">parent</a><span>|</span><a href="#38440933">prev</a><span>|</span><a href="#38440805">next</a><span>|</span><label class="collapse" for="c-38441032">[-]</label><label class="expand" for="c-38441032">[1 more]</label></div><br/><div class="children"><div class="content">If my grandmother had wheels, she&#x27;d be a bike.<p>Fish don&#x27;t like ice cream and we don&#x27;t feel the need to spawn. It&#x27;s because of how we are built.</div><br/></div></div></div></div><div id="38440805" class="c"><input type="checkbox" id="c-38440805" checked=""/><div class="controls bullet"><span class="by">oivey</span><span>|</span><a href="#38439728">root</a><span>|</span><a href="#38440626">parent</a><span>|</span><a href="#38440788">prev</a><span>|</span><a href="#38442151">next</a><span>|</span><label class="collapse" for="c-38440805">[-]</label><label class="expand" for="c-38440805">[1 more]</label></div><br/><div class="children"><div class="content">This is the definition of anthropomorphism: “Anthropomorphism is the attribution of human traits, emotions, or intentions to non-human entities.” By definition the point you’re making doesn’t make sense.</div><br/></div></div><div id="38442151" class="c"><input type="checkbox" id="c-38442151" checked=""/><div class="controls bullet"><span class="by">EMM_386</span><span>|</span><a href="#38439728">root</a><span>|</span><a href="#38440626">parent</a><span>|</span><a href="#38440805">prev</a><span>|</span><a href="#38441439">next</a><span>|</span><label class="collapse" for="c-38442151">[-]</label><label class="expand" for="c-38442151">[2 more]</label></div><br/><div class="children"><div class="content">&gt; None of these humans &quot;want&quot; to do anything. They respond to specific input<p>Weird, because I&#x27;m pretty sure I had the choice whether to respond to this comment.<p>It wasn&#x27;t the light waves hitting my retina from an HN post, leading to nerves firing and neurotransmitters all coming together to post this.<p>I posted it because I have free will.  I almost didn&#x27;t.<p>Unless you truly feel that there is no free will, and that reality is just a bizarre movie we have to experience .... well, then ... we&#x27;ll disagree.</div><br/><div id="38442765" class="c"><input type="checkbox" id="c-38442765" checked=""/><div class="controls bullet"><span class="by">mitthrowaway2</span><span>|</span><a href="#38439728">root</a><span>|</span><a href="#38442151">parent</a><span>|</span><a href="#38441439">next</a><span>|</span><label class="collapse" for="c-38442765">[-]</label><label class="expand" for="c-38442765">[1 more]</label></div><br/><div class="children"><div class="content">At some point after the light waves hit your retina, did one or more of the atoms in your body <i>not</i> obey the exact same physical laws as the atoms outside your body?</div><br/></div></div></div></div></div></div></div></div><div id="38440295" class="c"><input type="checkbox" id="c-38440295" checked=""/><div class="controls bullet"><span class="by">nl</span><span>|</span><a href="#38439728">prev</a><span>|</span><a href="#38440059">next</a><span>|</span><label class="collapse" for="c-38440295">[-]</label><label class="expand" for="c-38440295">[2 more]</label></div><br/><div class="children"><div class="content">Personally I find the original paper much better written and easier to understand: <a href="https:&#x2F;&#x2F;transformer-circuits.pub&#x2F;2023&#x2F;monosemantic-features&#x2F;index.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;transformer-circuits.pub&#x2F;2023&#x2F;monosemantic-features&#x2F;...</a></div><br/><div id="38443309" class="c"><input type="checkbox" id="c-38443309" checked=""/><div class="controls bullet"><span class="by">zone411</span><span>|</span><a href="#38440295">parent</a><span>|</span><a href="#38440059">next</a><span>|</span><label class="collapse" for="c-38443309">[-]</label><label class="expand" for="c-38443309">[1 more]</label></div><br/><div class="children"><div class="content">If you have a background in ML, then yes, a paper is almost always better. I&#x27;ve recommended papers over sources like Towards Data Science etc. here. However, for laypeople, I doubt it would be as effective - they&#x27;d need to look up terms like MLP, ReLU, UMAP, Logit, or even what an activation function is, and they are the target audience of this post.</div><br/></div></div></div></div><div id="38440059" class="c"><input type="checkbox" id="c-38440059" checked=""/><div class="controls bullet"><span class="by">error9348</span><span>|</span><a href="#38440295">prev</a><span>|</span><a href="#38439616">next</a><span>|</span><label class="collapse" for="c-38440059">[-]</label><label class="expand" for="c-38440059">[1 more]</label></div><br/><div class="children"><div class="content">&gt; No one knows how it works. Researchers simulate a weird type of pseudo-neural-tissue, “reward” it a little every time it becomes a little more like the AI they want, and eventually it becomes the AI they want.<p>There is a distinction to be made in &quot;knowing how it works&quot; on architecture vs weights themselves.</div><br/></div></div><div id="38439616" class="c"><input type="checkbox" id="c-38439616" checked=""/><div class="controls bullet"><span class="by">shermantanktop</span><span>|</span><a href="#38440059">prev</a><span>|</span><a href="#38438946">next</a><span>|</span><label class="collapse" for="c-38439616">[-]</label><label class="expand" for="c-38439616">[1 more]</label></div><br/><div class="children"><div class="content">As described in the post, this seems quite analogous to the operation of a bloom filter, except each &quot;bit&quot; is more than a single bit&#x27;s worth of information, and the match detection has to do some thresholding&#x2F;ranking to select a winner.<p>That said, the post is itself clearly summarizing much more technical work, so my analogy is resting on shaky ground.</div><br/></div></div><div id="38438946" class="c"><input type="checkbox" id="c-38438946" checked=""/><div class="controls bullet"><span class="by">turtleyacht</span><span>|</span><a href="#38439616">prev</a><span>|</span><a href="#38443552">next</a><span>|</span><label class="collapse" for="c-38438946">[-]</label><label class="expand" for="c-38438946">[5 more]</label></div><br/><div class="children"><div class="content">By the same token, thinking in memes all the time may be a form of impoverished cognition.<p>Or, is it enhanced cognition, on the part of the interpreter having to unpack much from little?</div><br/><div id="38439196" class="c"><input type="checkbox" id="c-38439196" checked=""/><div class="controls bullet"><span class="by">throwanem</span><span>|</span><a href="#38438946">parent</a><span>|</span><a href="#38441431">next</a><span>|</span><label class="collapse" for="c-38439196">[-]</label><label class="expand" for="c-38439196">[2 more]</label></div><br/><div class="children"><div class="content">Darmok and Jalad at Mar-a-Lago.</div><br/><div id="38441358" class="c"><input type="checkbox" id="c-38441358" checked=""/><div class="controls bullet"><span class="by">kibwen</span><span>|</span><a href="#38438946">root</a><span>|</span><a href="#38439196">parent</a><span>|</span><a href="#38441431">next</a><span>|</span><label class="collapse" for="c-38441358">[-]</label><label class="expand" for="c-38441358">[1 more]</label></div><br/><div class="children"><div class="content">Shaka, paying to build the walls.</div><br/></div></div></div></div><div id="38441431" class="c"><input type="checkbox" id="c-38441431" checked=""/><div class="controls bullet"><span class="by">dekhn</span><span>|</span><a href="#38438946">parent</a><span>|</span><a href="#38439196">prev</a><span>|</span><a href="#38439062">next</a><span>|</span><label class="collapse" for="c-38441431">[-]</label><label class="expand" for="c-38441431">[1 more]</label></div><br/><div class="children"><div class="content">&gt;By the same token, thinking in memes all the time may be a form of impoverished cognition.<p>I would recast this: any thinking is a linear superposition of weighted tropes.  If you read TVTropes enough you&#x27;ll start to realize that the site doesn&#x27;t just describe TV plots, but basically all human interaction and thought, nicely clustered into nearly orthogonal topics.  Almost anything you can say can be expressed by taking a few tropes and combining them with weights.</div><br/></div></div><div id="38439062" class="c"><input type="checkbox" id="c-38439062" checked=""/><div class="controls bullet"><span class="by">aatd86</span><span>|</span><a href="#38438946">parent</a><span>|</span><a href="#38441431">prev</a><span>|</span><a href="#38443552">next</a><span>|</span><label class="collapse" for="c-38439062">[-]</label><label class="expand" for="c-38439062">[1 more]</label></div><br/><div class="children"><div class="content">Some kind of single context abstract interpretation maybe.</div><br/></div></div></div></div><div id="38443552" class="c"><input type="checkbox" id="c-38443552" checked=""/><div class="controls bullet"><span class="by">chrissnow2023</span><span>|</span><a href="#38438946">prev</a><span>|</span><a href="#38439292">next</a><span>|</span><label class="collapse" for="c-38443552">[-]</label><label class="expand" for="c-38443552">[1 more]</label></div><br/><div class="children"><div class="content">COOL~ interpreting a big AI with a bigger is like interpreting 42 with Earth~</div><br/></div></div><div id="38439292" class="c"><input type="checkbox" id="c-38439292" checked=""/><div class="controls bullet"><span class="by">s1gnp0st</span><span>|</span><a href="#38443552">prev</a><span>|</span><a href="#38440568">next</a><span>|</span><label class="collapse" for="c-38439292">[-]</label><label class="expand" for="c-38439292">[6 more]</label></div><br/><div class="children"><div class="content">&gt; Shouldn’t the AI be keeping the concept of God, Almighty Creator and Lord of the Universe, separate from God-<p>This seems wrong. God-zilla is using the concept of God as a superlative modifier. I would expect a neuron involved in the concept of godhood to activate whenever any metaphorical &quot;god-of-X&quot; concept is being used.</div><br/><div id="38439350" class="c"><input type="checkbox" id="c-38439350" checked=""/><div class="controls bullet"><span class="by">Sniffnoy</span><span>|</span><a href="#38439292">parent</a><span>|</span><a href="#38440568">next</a><span>|</span><label class="collapse" for="c-38439350">[-]</label><label class="expand" for="c-38439350">[5 more]</label></div><br/><div class="children"><div class="content">I mean, it&#x27;s not actually.  It&#x27;s just a somewhat unusual transcription (well, originally somewhat unusual, now obviously it&#x27;s the official English name) of what might be more usually transcribed as &quot;Gojira&quot;.</div><br/><div id="38439538" class="c"><input type="checkbox" id="c-38439538" checked=""/><div class="controls bullet"><span class="by">s1gnp0st</span><span>|</span><a href="#38439292">root</a><span>|</span><a href="#38439350">parent</a><span>|</span><a href="#38440568">next</a><span>|</span><label class="collapse" for="c-38439538">[-]</label><label class="expand" for="c-38439538">[4 more]</label></div><br/><div class="children"><div class="content">Ah, I thought the Japanese word was just &quot;jira&quot;. My mistake.</div><br/><div id="38439560" class="c"><input type="checkbox" id="c-38439560" checked=""/><div class="controls bullet"><span class="by">postmodest</span><span>|</span><a href="#38439292">root</a><span>|</span><a href="#38439538">parent</a><span>|</span><a href="#38439635">next</a><span>|</span><label class="collapse" for="c-38439560">[-]</label><label class="expand" for="c-38439560">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s an entirely different monster.</div><br/><div id="38439624" class="c"><input type="checkbox" id="c-38439624" checked=""/><div class="controls bullet"><span class="by">eichin</span><span>|</span><a href="#38439292">root</a><span>|</span><a href="#38439560">parent</a><span>|</span><a href="#38439635">next</a><span>|</span><label class="collapse" for="c-38439624">[-]</label><label class="expand" for="c-38439624">[1 more]</label></div><br/><div class="children"><div class="content">Indeed, but not an entirely unrelated one though - per <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Jira_(software)#Naming" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Jira_(software)#Naming</a> the inspiration path was Bugzilla -&gt; Godzilla -&gt; Gojira -&gt; Jira (which is why Confluence keeps correcting me when I try to spell it JIRA)</div><br/></div></div></div></div><div id="38439635" class="c"><input type="checkbox" id="c-38439635" checked=""/><div class="controls bullet"><span class="by">VinLucero</span><span>|</span><a href="#38439292">root</a><span>|</span><a href="#38439538">parent</a><span>|</span><a href="#38439560">prev</a><span>|</span><a href="#38440568">next</a><span>|</span><label class="collapse" for="c-38439635">[-]</label><label class="expand" for="c-38439635">[1 more]</label></div><br/><div class="children"><div class="content">I see what you did there.</div><br/></div></div></div></div></div></div></div></div><div id="38440568" class="c"><input type="checkbox" id="c-38440568" checked=""/><div class="controls bullet"><span class="by">csours</span><span>|</span><a href="#38439292">prev</a><span>|</span><a href="#38440674">next</a><span>|</span><label class="collapse" for="c-38440568">[-]</label><label class="expand" for="c-38440568">[1 more]</label></div><br/><div class="children"><div class="content">I feel like we&#x27;re a few more paradigm shifts away from self-driving cars, and this is one of them - being able to actually understand neural nets and modify them in a constructive way more directly - aka engineering.<p>Some more:<p><pre><code>    cheaper sensors (happening now)
    better sensor integration (happening now, kind of)
    better tools for ml grokking and intermediate engineering (this article, kind of)
    better tools for layering ml (probably the same thing as above)
    a new model for insurance&#x2F;responsibility&#x2F;something like this (unsure)
    better communication with people inside and outside the car (barely on the radar)</code></pre></div><br/></div></div><div id="38440674" class="c"><input type="checkbox" id="c-38440674" checked=""/><div class="controls bullet"><span class="by">Merrill</span><span>|</span><a href="#38440568">prev</a><span>|</span><a href="#38440242">next</a><span>|</span><label class="collapse" for="c-38440674">[-]</label><label class="expand" for="c-38440674">[3 more]</label></div><br/><div class="children"><div class="content">When LLMs are trained on text, are the words annotated to indicate the semantic meaning, or is the LLM training process expected to disambiguate the possibly hundreds of semantic meanings of an individual common word such as &quot;run&quot;?</div><br/><div id="38440719" class="c"><input type="checkbox" id="c-38440719" checked=""/><div class="controls bullet"><span class="by">OkayPhysicist</span><span>|</span><a href="#38440674">parent</a><span>|</span><a href="#38440242">next</a><span>|</span><label class="collapse" for="c-38440719">[-]</label><label class="expand" for="c-38440719">[2 more]</label></div><br/><div class="children"><div class="content">The task LLMs are trained on is &quot;predict the next word&quot;, which elegantly is included for free in your training set of text. Typically no annotation is provided, since that would involve a ton of human labor doing the annotations.</div><br/><div id="38441329" class="c"><input type="checkbox" id="c-38441329" checked=""/><div class="controls bullet"><span class="by">Merrill</span><span>|</span><a href="#38440674">root</a><span>|</span><a href="#38440719">parent</a><span>|</span><a href="#38440242">next</a><span>|</span><label class="collapse" for="c-38441329">[-]</label><label class="expand" for="c-38441329">[1 more]</label></div><br/><div class="children"><div class="content">Could you ask a specialized AI to define each of the words in a block of text to automate the meaning extraction?</div><br/></div></div></div></div></div></div><div id="38440242" class="c"><input type="checkbox" id="c-38440242" checked=""/><div class="controls bullet"><span class="by">kevdozer1</span><span>|</span><a href="#38440674">prev</a><span>|</span><a href="#38440182">next</a><span>|</span><label class="collapse" for="c-38440242">[-]</label><label class="expand" for="c-38440242">[1 more]</label></div><br/><div class="children"><div class="content">such a wonderful article, really enjoyed reading it</div><br/></div></div><div id="38440182" class="c"><input type="checkbox" id="c-38440182" checked=""/><div class="controls bullet"><span class="by">adamnemecek</span><span>|</span><a href="#38440242">prev</a><span>|</span><a href="#38442724">next</a><span>|</span><label class="collapse" for="c-38440182">[-]</label><label class="expand" for="c-38440182">[3 more]</label></div><br/><div class="children"><div class="content">Superposition makes sense when you understand all of ML as a convolution.</div><br/><div id="38441675" class="c"><input type="checkbox" id="c-38441675" checked=""/><div class="controls bullet"><span class="by">blovescoffee</span><span>|</span><a href="#38440182">parent</a><span>|</span><a href="#38442724">next</a><span>|</span><label class="collapse" for="c-38441675">[-]</label><label class="expand" for="c-38441675">[2 more]</label></div><br/><div class="children"><div class="content">You can just note the fact that two bits can represent four values.</div><br/><div id="38441825" class="c"><input type="checkbox" id="c-38441825" checked=""/><div class="controls bullet"><span class="by">adamnemecek</span><span>|</span><a href="#38440182">root</a><span>|</span><a href="#38441675">parent</a><span>|</span><a href="#38442724">next</a><span>|</span><label class="collapse" for="c-38441825">[-]</label><label class="expand" for="c-38441825">[1 more]</label></div><br/><div class="children"><div class="content">Can you explain machine learning through this lens?</div><br/></div></div></div></div></div></div><div id="38442724" class="c"><input type="checkbox" id="c-38442724" checked=""/><div class="controls bullet"><span class="by">lngnmn2</span><span>|</span><a href="#38440182">prev</a><span>|</span><a href="#38440118">next</a><span>|</span><label class="collapse" for="c-38442724">[-]</label><label class="expand" for="c-38442724">[1 more]</label></div><br/><div class="children"><div class="content">Every training set will produce a different set of weights, even the same training set with produce different weights with different initialization, leave alone slightly different architectures.<p>So what exactly is the point, except &quot;look at us, we are so clever&quot;?</div><br/></div></div><div id="38440118" class="c"><input type="checkbox" id="c-38440118" checked=""/><div class="controls bullet"><span class="by">asylteltine</span><span>|</span><a href="#38442724">prev</a><span>|</span><a href="#38442181">next</a><span>|</span><label class="collapse" for="c-38440118">[-]</label><label class="expand" for="c-38440118">[11 more]</label></div><br/><div class="children"><div class="content">I’ve never understood the hidden layers argument. Ultimately these models are executing code. You can examine the code. Why can’t that be done?</div><br/><div id="38440452" class="c"><input type="checkbox" id="c-38440452" checked=""/><div class="controls bullet"><span class="by">OkayPhysicist</span><span>|</span><a href="#38440118">parent</a><span>|</span><a href="#38440205">next</a><span>|</span><label class="collapse" for="c-38440452">[-]</label><label class="expand" for="c-38440452">[5 more]</label></div><br/><div class="children"><div class="content">Fundamentally, artificial neural networks boil down to some code that can be written, mathematically, as<p>σ(C*σ(B*σ(A*x)))... = y<p>repeated matrix-vector multiplication, separated by nonlinear functions, σ(). A,B,C etc here are your weights, x is your input vector, y is your output vector. Any nonlinear function will work, some behave better than others, but they must be present, otherwise you could simplify the problem by apply the associative property to the weights side of things, and you&#x27;d effectively have a single layer that can only simulate linear functions. A hidden layer is just any vector that is intermediate to that calculation, for example the result of σ(A*x) would be the first hidden layer.<p>So right off the bat, you obviously have a problem: &quot;Examining the code&quot; means examining the weights, which are just gigabyte&#x2F;terabyte sized matrices. Opaque is putting it mildly. The only sane approach is to start from one of our known-meaningful vectors (the input or output), and work our way inwards from there, either seeing what elements of hidden layer vectors have the most significant value when a certain input is applied, or determining what hidden layer vector values produce values closest resembling a desired output.<p>Then you start running into the problems described in the article.</div><br/><div id="38440510" class="c"><input type="checkbox" id="c-38440510" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#38440118">root</a><span>|</span><a href="#38440452">parent</a><span>|</span><a href="#38440205">next</a><span>|</span><label class="collapse" for="c-38440510">[-]</label><label class="expand" for="c-38440510">[4 more]</label></div><br/><div class="children"><div class="content">This is a really good explanation for a fully connected network. Most models will have more complex architectures so are even more complicated than this.</div><br/><div id="38440800" class="c"><input type="checkbox" id="c-38440800" checked=""/><div class="controls bullet"><span class="by">OkayPhysicist</span><span>|</span><a href="#38440118">root</a><span>|</span><a href="#38440510">parent</a><span>|</span><a href="#38440205">next</a><span>|</span><label class="collapse" for="c-38440800">[-]</label><label class="expand" for="c-38440800">[3 more]</label></div><br/><div class="children"><div class="content">I haven&#x27;t been great about keeping up with advances in the field, but to my understanding most if not all architectures in effect merely enforce symmetries upon the network. That is, they can be represented by a fully connected network but in that representation not all weights are free, in that some are fixed (0 or 1) or some are dependent (A(1,1) will also be equal to C(1,1)).</div><br/><div id="38440894" class="c"><input type="checkbox" id="c-38440894" checked=""/><div class="controls bullet"><span class="by">janalsncm</span><span>|</span><a href="#38440118">root</a><span>|</span><a href="#38440800">parent</a><span>|</span><a href="#38440205">next</a><span>|</span><label class="collapse" for="c-38440894">[-]</label><label class="expand" for="c-38440894">[2 more]</label></div><br/><div class="children"><div class="content">I don’t blame you, things are pretty diversified so I’m white knuckling it through my own subfield.<p>But as a simple example, convolution would be a little tedious to describe in the general notation you wrote above without getting into the image dimensions, stride, padding etc., not to mention residual layers or norm layers that are commonly used. Then there are things like stop gradient, dropout or even other training targets which are only used during training but not inference.</div><br/><div id="38441013" class="c"><input type="checkbox" id="c-38441013" checked=""/><div class="controls bullet"><span class="by">OkayPhysicist</span><span>|</span><a href="#38440118">root</a><span>|</span><a href="#38440894">parent</a><span>|</span><a href="#38440205">next</a><span>|</span><label class="collapse" for="c-38441013">[-]</label><label class="expand" for="c-38441013">[1 more]</label></div><br/><div class="children"><div class="content">Convolution actually can be pretty elegantly translated into multiplication by a  matrix with symmetries. It&#x27;s been a few years, but that was an example we had to work out by hand in my Deep Learning course at university.<p>It&#x27;s pretty much the quintessential example of an enforced symmetry, in that it introduces a symmetry against translation.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38440205" class="c"><input type="checkbox" id="c-38440205" checked=""/><div class="controls bullet"><span class="by">afthonos</span><span>|</span><a href="#38440118">parent</a><span>|</span><a href="#38440452">prev</a><span>|</span><a href="#38440565">next</a><span>|</span><label class="collapse" for="c-38440205">[-]</label><label class="expand" for="c-38440205">[2 more]</label></div><br/><div class="children"><div class="content">Think of it this way: looking at the neurons is like looking at the assembly and trying to understand how a video is played on YouTube. Yes, in theory it’s possible, but realistically no one would be able to do that. At least not in a reasonable amount of time.</div><br/><div id="38442023" class="c"><input type="checkbox" id="c-38442023" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#38440118">root</a><span>|</span><a href="#38440205">parent</a><span>|</span><a href="#38440565">next</a><span>|</span><label class="collapse" for="c-38442023">[-]</label><label class="expand" for="c-38442023">[1 more]</label></div><br/><div class="children"><div class="content">Video decoding works by reading and writing pixels to the same buffers you use for displaying them, so you could see a lot of how it works by watching the accesses there.</div><br/></div></div></div></div><div id="38440565" class="c"><input type="checkbox" id="c-38440565" checked=""/><div class="controls bullet"><span class="by">drdeca</span><span>|</span><a href="#38440118">parent</a><span>|</span><a href="#38440205">prev</a><span>|</span><a href="#38440750">next</a><span>|</span><label class="collapse" for="c-38440565">[-]</label><label class="expand" for="c-38440565">[1 more]</label></div><br/><div class="children"><div class="content">If by “code” you mean “the source code people wrote”, then, that source code by itself does not determine the behavior. The behavior is determined by the numbers.<p>Saying that understanding that code implies understanding how the network works, is like saying “John wrote this emulator for GBA games, therefore he must understand how the game [some GBA game] works”.<p>Except, instead of the game being written in assembly, it was written in malbolge.</div><br/></div></div><div id="38440750" class="c"><input type="checkbox" id="c-38440750" checked=""/><div class="controls bullet"><span class="by">cwillu</span><span>|</span><a href="#38440118">parent</a><span>|</span><a href="#38440565">prev</a><span>|</span><a href="#38441503">next</a><span>|</span><label class="collapse" for="c-38440750">[-]</label><label class="expand" for="c-38440750">[1 more]</label></div><br/><div class="children"><div class="content">“This is great for AIs but bad for interpreters. We hoped we could figure out what our AIs were doing just by looking at them. But it turns out they’re simulating much bigger and more complicated AIs, and if we want to know what’s going on, we have to look at those. But those AIs only exist in simulated abstract hyperdimensional spaces. Sounds hard to dissect!”<p>“Still, last month Anthropic’s interpretability team announced that they successfully dissected of one of the simulated AIs in its abstract hyperdimensional space.<p>(finally, we’re back to the monosemanticity paper!)<p>First the researchers trained a very simple 512-neuron AI to predict text, like a tiny version of GPT or Anthropic’s competing model Claude.<p>Then, they trained a second AI called an autoencoder to predict the activations of the first AI. They told it to posit a certain number of features (the experiments varied between ~2,000 and ~100,000), corresponding to the neurons of the higher-dimensional AI it was simulating. Then they made it predict how those features mapped onto the real neurons of the real AI.<p>They found that even though the original AI’s neurons weren’t comprehensible, the new AI’s simulated neurons (aka “features”) were! They were monosemantic, ie they meant one specific thing.<p>Here’s feature #2663 (remember, the original AI only had 512 neurons, but they’re treating it as simulating a larger AI with up to ~100,000 neuron-features).”</div><br/></div></div><div id="38441503" class="c"><input type="checkbox" id="c-38441503" checked=""/><div class="controls bullet"><span class="by">quickthrower2</span><span>|</span><a href="#38440118">parent</a><span>|</span><a href="#38440750">prev</a><span>|</span><a href="#38442181">next</a><span>|</span><label class="collapse" for="c-38441503">[-]</label><label class="expand" for="c-38441503">[1 more]</label></div><br/><div class="children"><div class="content">With these models, the Pytorch code is an interpreter, and the 100Gb of weights is the code. So you have say a 10G (10 weights = 1 LOC) LOC codebase, all in assembly, no comments. G&#x27;luck.<p>Oh and it doesn&#x27;t always use logic. So there are no ORs and ANDs and IFs, just +, *, exp, max, etc.</div><br/></div></div></div></div><div id="38442181" class="c"><input type="checkbox" id="c-38442181" checked=""/><div class="controls bullet"><span class="by">ggm</span><span>|</span><a href="#38440118">prev</a><span>|</span><label class="collapse" for="c-38442181">[-]</label><label class="expand" for="c-38442181">[1 more]</label></div><br/><div class="children"><div class="content">As long as you read it with the skeptics &quot;yes, but it&#x27;s not intelligence&quot; its a good read.<p>It&#x27;s when you read it with the &quot;at last, I can understand how reasoning and inference with meaning is going to emerge from this&quot; you have a problem.<p>It&#x27;s a great read but what you bring to it, informs what you take from it.</div><br/></div></div></div></div></div></div></div></body></html>