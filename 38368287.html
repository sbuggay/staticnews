<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1700643667669" as="style"/><link rel="stylesheet" href="styles.css?v=1700643667669"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://stability.ai/news/stable-video-diffusion-open-ai-video-model">Stable Video Diffusion</a> <span class="domain">(<a href="https://stability.ai">stability.ai</a>)</span></div><div class="subtext"><span>roborovskis</span> | <span>237 comments</span></div><br/><div><div id="38368607" class="c"><input type="checkbox" id="c-38368607" checked=""/><div class="controls bullet"><span class="by">btbuildem</span><span>|</span><a href="#38368561">next</a><span>|</span><label class="collapse" for="c-38368607">[-]</label><label class="expand" for="c-38368607">[82 more]</label></div><br/><div class="children"><div class="content">In the video towards the bottom of the page, there are two birds (blue jays), but in the background there are two identical buildings (which look a lot like the CN Tower). CN Tower is the main landmark of Toronto, whose baseball team happens to be the Blue Jays. It&#x27;s located near the main sportsball stadium downtown.<p>I vaguely understand how text-to-image works, and so it makes sense that the vector space for &quot;blue jays&quot; would be near &quot;toronto&quot; or &quot;cn tower&quot;. The improvements in scale and speed (image -&gt; now video) are impressive, but given how incredibly able the image generation models are, they simultaneously feel crippled and limited by their lack of editing &#x2F; iteration ability.<p>Has anyone come across a solution where model can iterate (eg, with prompts like &quot;move the bicycle to the left side of the photo&quot;)? It feels like we&#x27;re close.</div><br/><div id="38368981" class="c"><input type="checkbox" id="c-38368981" checked=""/><div class="controls bullet"><span class="by">TacticalCoder</span><span>|</span><a href="#38368607">parent</a><span>|</span><a href="#38368801">next</a><span>|</span><label class="collapse" for="c-38368981">[-]</label><label class="expand" for="c-38368981">[51 more]</label></div><br/><div class="children"><div class="content">&gt; Has anyone come across a solution where model can iterate (eg, with prompts like &quot;move the bicycle to the left side of the photo&quot;)? It feels like we&#x27;re close.<p>I feel like we&#x27;re close too, but for another reason.<p>For although I love SD and these video examples are great... It&#x27;s a flawed method: they never get lighting correctly and there are many incoherent things just about everywhere. Any 3D artist or photographer can immediately spot that.<p>However I&#x27;m willing to bet that we&#x27;ll soon have something <i>much</i> better: you&#x27;ll describe something and you&#x27;ll get a full 3D scene, with 3D models, source of lights set up, etc.<p>And the scene shall be sent into Blender and you&#x27;ll click on a button and have an actual rendering made by Blender, with correct lighting.<p>Wanna move that bicycle? Move it in the 3D scene exactly where you want.<p>That is coming.<p>And for audio it&#x27;s the same: why generate an audio file when soon models shall be able to generate the various tracks, with all the instruments and whatnots, allowing to create the audio file?<p>That is coming too.</div><br/><div id="38369556" class="c"><input type="checkbox" id="c-38369556" checked=""/><div class="controls bullet"><span class="by">epr</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38368981">parent</a><span>|</span><a href="#38369497">next</a><span>|</span><label class="collapse" for="c-38369556">[-]</label><label class="expand" for="c-38369556">[23 more]</label></div><br/><div class="children"><div class="content">&gt; you&#x27;ll describe something and you&#x27;ll get a full 3D scene, with 3D models, source of lights set up, etc.<p>I&#x27;m always confused why I don&#x27;t hear more about projects going in this direction. Controlnets are great, but there&#x27;s still quite a lot of hallucination and other tiny mistakes that a skilled human would never make.</div><br/><div id="38369775" class="c"><input type="checkbox" id="c-38369775" checked=""/><div class="controls bullet"><span class="by">boppo1</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38369556">parent</a><span>|</span><a href="#38372054">next</a><span>|</span><label class="collapse" for="c-38369775">[-]</label><label class="expand" for="c-38369775">[14 more]</label></div><br/><div class="children"><div class="content">Blender files are dramatically more complex than any image format, which are basically all just 2D arrays of 3-value vectors. The blender filetype uses a weird DNA&#x2F;RNA struct system that would probably require its own training run.<p>More on the Blender file format: <a href="https:&#x2F;&#x2F;fossies.org&#x2F;linux&#x2F;blender&#x2F;doc&#x2F;blender_file_format&#x2F;mystery_of_the_blend.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;fossies.org&#x2F;linux&#x2F;blender&#x2F;doc&#x2F;blender_file_format&#x2F;my...</a></div><br/><div id="38369912" class="c"><input type="checkbox" id="c-38369912" checked=""/><div class="controls bullet"><span class="by">mikepurvis</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38369775">parent</a><span>|</span><a href="#38370945">next</a><span>|</span><label class="collapse" for="c-38369912">[-]</label><label class="expand" for="c-38369912">[9 more]</label></div><br/><div class="children"><div class="content">But surely you wouldn&#x27;t try to emit that format directly, but rather some higher level scene description? Or even just a set of instructions for how to manipulate the UI to create the imagined scene?</div><br/><div id="38374718" class="c"><input type="checkbox" id="c-38374718" checked=""/><div class="controls bullet"><span class="by">numpad0</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38369912">parent</a><span>|</span><a href="#38370331">next</a><span>|</span><label class="collapse" for="c-38374718">[-]</label><label class="expand" for="c-38374718">[2 more]</label></div><br/><div class="children"><div class="content">It sure feels weird to me as well, that GenAI is always supposed to be end-to-end with everything done inside NN blackbox. No one seems to be doing image output as SVG or .ai.</div><br/><div id="38376550" class="c"><input type="checkbox" id="c-38376550" checked=""/><div class="controls bullet"><span class="by">HammadB</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38374718">parent</a><span>|</span><a href="#38370331">next</a><span>|</span><label class="collapse" for="c-38376550">[-]</label><label class="expand" for="c-38376550">[1 more]</label></div><br/><div class="children"><div class="content">There is a fundamental disconnect between industry and academia here.</div><br/></div></div></div></div><div id="38370331" class="c"><input type="checkbox" id="c-38370331" checked=""/><div class="controls bullet"><span class="by">BirdieNZ</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38369912">parent</a><span>|</span><a href="#38374718">prev</a><span>|</span><a href="#38374178">next</a><span>|</span><label class="collapse" for="c-38370331">[-]</label><label class="expand" for="c-38370331">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve seen this but producing Python scripts that you run in Blender, e.g. <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=x60zHw_z4NM" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=x60zHw_z4NM</a> (but I saw something marginally more impressive, not sure where though!)</div><br/></div></div><div id="38374178" class="c"><input type="checkbox" id="c-38374178" checked=""/><div class="controls bullet"><span class="by">mikebelanger</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38369912">parent</a><span>|</span><a href="#38370331">prev</a><span>|</span><a href="#38370392">next</a><span>|</span><label class="collapse" for="c-38374178">[-]</label><label class="expand" for="c-38374178">[4 more]</label></div><br/><div class="children"><div class="content">Yeah I&#x27;d imagine that&#x27;s the best way. Lots of LLMs can generate workable Python code too, so code that jives with Blender&#x27;s Python API doesn&#x27;t seem like too much of a leap.<p>The only trick is that there has to be enough Blender Python code to train the LLM on.</div><br/><div id="38374499" class="c"><input type="checkbox" id="c-38374499" checked=""/><div class="controls bullet"><span class="by">arcticbull</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38374178">parent</a><span>|</span><a href="#38370392">next</a><span>|</span><label class="collapse" for="c-38374499">[-]</label><label class="expand" for="c-38374499">[3 more]</label></div><br/><div class="children"><div class="content">Maybe something like OpenSCAD is a good middle ground. Procedural code-like format for specifying 3D objects that can then be converted and imported in Blender.</div><br/><div id="38375201" class="c"><input type="checkbox" id="c-38375201" checked=""/><div class="controls bullet"><span class="by">lightedman</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38374499">parent</a><span>|</span><a href="#38370392">next</a><span>|</span><label class="collapse" for="c-38375201">[-]</label><label class="expand" for="c-38375201">[2 more]</label></div><br/><div class="children"><div class="content">I tried all the AI stuff that I could on OpenSCAD.<p>While it generates a lot of code that initially makes sense, when you use the code, you get a jumbled block.</div><br/><div id="38376564" class="c"><input type="checkbox" id="c-38376564" checked=""/><div class="controls bullet"><span class="by">regularfry</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38375201">parent</a><span>|</span><a href="#38370392">next</a><span>|</span><label class="collapse" for="c-38376564">[-]</label><label class="expand" for="c-38376564">[1 more]</label></div><br/><div class="children"><div class="content">This. I think problem is that the LLMs really struggle with 3d scene understanding, so what you would need to do is generate code that generates code.<p>But also I suspect there just isn&#x27;t that much openscad code in the training data, and the semantics are different enough to python or any of the other languages that are well-represented that it struggles.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38370945" class="c"><input type="checkbox" id="c-38370945" checked=""/><div class="controls bullet"><span class="by">guyomes</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38369775">parent</a><span>|</span><a href="#38369912">prev</a><span>|</span><a href="#38372575">next</a><span>|</span><label class="collapse" for="c-38370945">[-]</label><label class="expand" for="c-38370945">[1 more]</label></div><br/><div class="children"><div class="content">Voxel files could be a simpler step for 3D images.</div><br/></div></div><div id="38369960" class="c"><input type="checkbox" id="c-38369960" checked=""/><div class="controls bullet"><span class="by">Keyframe</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38369775">parent</a><span>|</span><a href="#38370391">prev</a><span>|</span><a href="#38372054">next</a><span>|</span><label class="collapse" for="c-38369960">[-]</label><label class="expand" for="c-38369960">[1 more]</label></div><br/><div class="children"><div class="content">Scene layouts, models and their attributes are a result of user input (ok and sometimes program output). One avenue to take there would be to train on input expecting an output. Like teaching a model to draw instead of generate images.. which in a sense we already did by broadly painting out silhouettes and then rendering details.</div><br/></div></div></div></div><div id="38372054" class="c"><input type="checkbox" id="c-38372054" checked=""/><div class="controls bullet"><span class="by">lairv</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38369556">parent</a><span>|</span><a href="#38369775">prev</a><span>|</span><a href="#38371735">next</a><span>|</span><label class="collapse" for="c-38372054">[-]</label><label class="expand" for="c-38372054">[1 more]</label></div><br/><div class="children"><div class="content">I think the bottleneck is data<p>For single 3D object the biggest dataset is ObjaverseXL with 10M samples<p>For full 3D scenes you could at best get ~1000 scenes with datasets like ScanNet  I guess<p>Text2Image models are trained on datasets with 5 billion samples</div><br/></div></div><div id="38371735" class="c"><input type="checkbox" id="c-38371735" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38369556">parent</a><span>|</span><a href="#38372054">prev</a><span>|</span><a href="#38370744">next</a><span>|</span><label class="collapse" for="c-38371735">[-]</label><label class="expand" for="c-38371735">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;m always confused why I don&#x27;t hear more about projects going in this direction.<p>Probably because they aren&#x27;t as advanced and the demos aren&#x27;t as impressive to nontechnical audiences who don&#x27;t understand the implications: there’s lots of work on text-to-3d-model generation, and even plugins for some stable diffusion UIs (e.g., MotionDiff for ComyUI.)</div><br/></div></div><div id="38370744" class="c"><input type="checkbox" id="c-38370744" checked=""/><div class="controls bullet"><span class="by">jowday</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38369556">parent</a><span>|</span><a href="#38371735">prev</a><span>|</span><a href="#38373883">next</a><span>|</span><label class="collapse" for="c-38370744">[-]</label><label class="expand" for="c-38370744">[3 more]</label></div><br/><div class="children"><div class="content">There&#x27;s a lot of issues with it, but perhaps the biggest is that there aren&#x27;t just troves of easily scrapable and digestible 3D models lying around on the internet to train on top of like we have with text, images, and video.<p>Almost all of the generative 3D models you see are actually generative image models that essentially (very crude simplification) perform something like photogrammetry to generate a 3D model - &#x27;does this 3D object, rendered from 25 different views, match the text prompt as evaluated by this model trained on text-image pairs&#x27;?<p>This is a shitty way to generate 3D models, and it&#x27;s why they almost all look kind of malformed.</div><br/><div id="38371119" class="c"><input type="checkbox" id="c-38371119" checked=""/><div class="controls bullet"><span class="by">sterlind</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38370744">parent</a><span>|</span><a href="#38373883">next</a><span>|</span><label class="collapse" for="c-38371119">[-]</label><label class="expand" for="c-38371119">[2 more]</label></div><br/><div class="children"><div class="content">If reinforcement learning were farther along, you could have it learn to reproduce scenes as 3D models. Each episode&#x27;s task is to mimic an image, each step is a command mutating the scene (adding a polygon, or rotating the camera, etc.), and the reward signal is image similarity. You can even start by training it with synthetic data: generate small random scenes and make them increasingly sophisticated, then later switch over to trying to mimic images.<p>You wouldn&#x27;t need any models to learn from. But my intuition is that RL is still quite weak, and that the model would flounder after learning to mimic background color and placing a few spheres.</div><br/><div id="38371200" class="c"><input type="checkbox" id="c-38371200" checked=""/><div class="controls bullet"><span class="by">skdotdan</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38371119">parent</a><span>|</span><a href="#38373883">next</a><span>|</span><label class="collapse" for="c-38371200">[-]</label><label class="expand" for="c-38371200">[1 more]</label></div><br/><div class="children"><div class="content">Deepmind tried something similar in 2018 <a href="https:&#x2F;&#x2F;deepmind.google&#x2F;discover&#x2F;blog&#x2F;learning-to-write-programs-that-generate-images&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;deepmind.google&#x2F;discover&#x2F;blog&#x2F;learning-to-write-prog...</a></div><br/></div></div></div></div></div></div><div id="38373883" class="c"><input type="checkbox" id="c-38373883" checked=""/><div class="controls bullet"><span class="by">eigenvalue</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38369556">parent</a><span>|</span><a href="#38370744">prev</a><span>|</span><a href="#38372511">next</a><span>|</span><label class="collapse" for="c-38373883">[-]</label><label class="expand" for="c-38373883">[1 more]</label></div><br/><div class="children"><div class="content">I think this recent Gaussian Splatting technique could end up working really well for generative models, at least once there is a big corpus of high quality scenes to train on. Seems almost ideal for the task because it gets photorealistic results from any angle, but in a sparse, data efficient way, and it doesn’t require a separate rendering pipeline.</div><br/></div></div><div id="38372511" class="c"><input type="checkbox" id="c-38372511" checked=""/><div class="controls bullet"><span class="by">insanitybit</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38369556">parent</a><span>|</span><a href="#38373883">prev</a><span>|</span><a href="#38370235">next</a><span>|</span><label class="collapse" for="c-38372511">[-]</label><label class="expand" for="c-38372511">[1 more]</label></div><br/><div class="children"><div class="content">I assume because it&#x27;s still extremely early.</div><br/></div></div><div id="38370235" class="c"><input type="checkbox" id="c-38370235" checked=""/><div class="controls bullet"><span class="by">bozhark</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38369556">parent</a><span>|</span><a href="#38372511">prev</a><span>|</span><a href="#38369497">next</a><span>|</span><label class="collapse" for="c-38370235">[-]</label><label class="expand" for="c-38370235">[1 more]</label></div><br/><div class="children"><div class="content">One was on the front page the other day, I’ll search for a link</div><br/></div></div></div></div><div id="38369497" class="c"><input type="checkbox" id="c-38369497" checked=""/><div class="controls bullet"><span class="by">bob1029</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38368981">parent</a><span>|</span><a href="#38369556">prev</a><span>|</span><a href="#38370567">next</a><span>|</span><label class="collapse" for="c-38369497">[-]</label><label class="expand" for="c-38369497">[1 more]</label></div><br/><div class="children"><div class="content">&gt; However I&#x27;m willing to bet that we&#x27;ll soon have something much better: you&#x27;ll describe something and you&#x27;ll get a full 3D scene, with 3D models, source of lights set up, etc.<p>I agree with this philosophy - Teach the AI to work with the same tools the human does. We already have a lot of human experts to refer to. Training material is everywhere.<p>There isn&#x27;t a &quot;text-to-video&quot; expert we can query to help us refine the capabilities around SD. It&#x27;s a one-shot, Jupiter-scale model with incomprehensible inertia. Contrast this with an expert-tuned model (i.e. natural language instructions) that can be nuanced precisely and to the the point of imperceptibility with a single sentence.<p>The other cool thing about the &quot;use existing tools&quot; path is that if the AI fails part way through, it&#x27;s actually possible for a human operator to step in and attempt recovery.</div><br/></div></div><div id="38370567" class="c"><input type="checkbox" id="c-38370567" checked=""/><div class="controls bullet"><span class="by">coldtea</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38368981">parent</a><span>|</span><a href="#38369497">prev</a><span>|</span><a href="#38369301">next</a><span>|</span><label class="collapse" for="c-38370567">[-]</label><label class="expand" for="c-38370567">[2 more]</label></div><br/><div class="children"><div class="content">&gt;<i>For although I love SD and these video examples are great... It&#x27;s a flawed method: they never get lighting correctly and there are many incoherent things just about everywhere. Any 3D artist or photographer can immediately spot that.</i><p>The question is whether the 99% of the audience would even care...</div><br/><div id="38374834" class="c"><input type="checkbox" id="c-38374834" checked=""/><div class="controls bullet"><span class="by">COAGULOPATH</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38370567">parent</a><span>|</span><a href="#38369301">next</a><span>|</span><label class="collapse" for="c-38374834">[-]</label><label class="expand" for="c-38374834">[1 more]</label></div><br/><div class="children"><div class="content">Of course they would. The internet spent a solid month laughing at the Sonic the Hedgehog movie because Sonic had weird-looking teeth.</div><br/></div></div></div></div><div id="38369301" class="c"><input type="checkbox" id="c-38369301" checked=""/><div class="controls bullet"><span class="by">atentaten</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38368981">parent</a><span>|</span><a href="#38370567">prev</a><span>|</span><a href="#38371576">next</a><span>|</span><label class="collapse" for="c-38369301">[-]</label><label class="expand" for="c-38369301">[11 more]</label></div><br/><div class="children"><div class="content">Whats your reasoning for feeling that we&#x27;re close?</div><br/><div id="38369571" class="c"><input type="checkbox" id="c-38369571" checked=""/><div class="controls bullet"><span class="by">cptaj</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38369301">parent</a><span>|</span><a href="#38371576">next</a><span>|</span><label class="collapse" for="c-38369571">[-]</label><label class="expand" for="c-38369571">[10 more]</label></div><br/><div class="children"><div class="content">We do it for text, audio and bitmapped images. A 3D scene file format is no different, you could train a model to output a blender file format instead of a bitmap.<p>It can learn anything you have data for.<p>Heck, we do it with geospatial data already, generating segmentation vectors. Why not 3D?</div><br/><div id="38369803" class="c"><input type="checkbox" id="c-38369803" checked=""/><div class="controls bullet"><span class="by">boppo1</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38369571">parent</a><span>|</span><a href="#38371670">next</a><span>|</span><label class="collapse" for="c-38369803">[-]</label><label class="expand" for="c-38369803">[3 more]</label></div><br/><div class="children"><div class="content">&gt;3D scene file format is no different<p>Not in theory, but the level of complexity is way higher and the amount of data available is much smaller.<p>Compare bitmaps to this:
<a href="https:&#x2F;&#x2F;fossies.org&#x2F;linux&#x2F;blender&#x2F;doc&#x2F;blender_file_format&#x2F;mystery_of_the_blend.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;fossies.org&#x2F;linux&#x2F;blender&#x2F;doc&#x2F;blender_file_format&#x2F;my...</a></div><br/><div id="38370363" class="c"><input type="checkbox" id="c-38370363" checked=""/><div class="controls bullet"><span class="by">kaibee</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38369803">parent</a><span>|</span><a href="#38371670">next</a><span>|</span><label class="collapse" for="c-38370363">[-]</label><label class="expand" for="c-38370363">[2 more]</label></div><br/><div class="children"><div class="content">Also the level of fault tolerance... if your pixels are a bit blurry, chances are no one notices at a high enough resolution.  If your json is a bit blurry you have problems.</div><br/><div id="38373489" class="c"><input type="checkbox" id="c-38373489" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38370363">parent</a><span>|</span><a href="#38371670">next</a><span>|</span><label class="collapse" for="c-38373489">[-]</label><label class="expand" for="c-38373489">[1 more]</label></div><br/><div class="children"><div class="content">You can do &quot;constrained decoding&quot; on a code model which keeps it grammatically correct.<p>But we haven&#x27;t gotten diffusion working well for text&#x2F;code, so generating long files is a problem.</div><br/></div></div></div></div></div></div><div id="38371670" class="c"><input type="checkbox" id="c-38371670" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38369571">parent</a><span>|</span><a href="#38369803">prev</a><span>|</span><a href="#38371603">next</a><span>|</span><label class="collapse" for="c-38371670">[-]</label><label class="expand" for="c-38371670">[1 more]</label></div><br/><div class="children"><div class="content">We do it for 3D, too.<p><a href="https:&#x2F;&#x2F;guytevet.github.io&#x2F;mdm-page&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;guytevet.github.io&#x2F;mdm-page&#x2F;</a></div><br/></div></div><div id="38371603" class="c"><input type="checkbox" id="c-38371603" checked=""/><div class="controls bullet"><span class="by">jncfhnb</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38369571">parent</a><span>|</span><a href="#38371670">prev</a><span>|</span><a href="#38371576">next</a><span>|</span><label class="collapse" for="c-38371603">[-]</label><label class="expand" for="c-38371603">[5 more]</label></div><br/><div class="children"><div class="content">Text, audio, and bitmapped images are data. Numbers and tokens.<p>A 3D scene is vastly more complex, and the way you consume it is tangential to the rendering of it we use to interpret. It is a collection of arbitrary data structures.<p>We’ll need a new approach for this kind of problem</div><br/><div id="38371688" class="c"><input type="checkbox" id="c-38371688" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38371603">parent</a><span>|</span><a href="#38371576">next</a><span>|</span><label class="collapse" for="c-38371688">[-]</label><label class="expand" for="c-38371688">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Text, audio, and bitmapped images are data. Numbers and tokens.<p>&gt; A 3D scene is vastly more complex<p>3D scenes, in fact, are also data, numbers and tokens. (Well, numbers, but so are tokens.)</div><br/><div id="38372831" class="c"><input type="checkbox" id="c-38372831" checked=""/><div class="controls bullet"><span class="by">jncfhnb</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38371688">parent</a><span>|</span><a href="#38371576">next</a><span>|</span><label class="collapse" for="c-38372831">[-]</label><label class="expand" for="c-38372831">[3 more]</label></div><br/><div class="children"><div class="content">As I stated and you selectively omitted, 3D scenes are collections of many arbitrary data structures.<p>Not at all the same as fixed sized arrays representing images.</div><br/><div id="38372863" class="c"><input type="checkbox" id="c-38372863" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38372831">parent</a><span>|</span><a href="#38371576">next</a><span>|</span><label class="collapse" for="c-38372863">[-]</label><label class="expand" for="c-38372863">[2 more]</label></div><br/><div class="children"><div class="content">Text gen, one of the things you contrast 3d to, similarly isn&#x27;t fixed size (capped in most models, but not <i>fixed</i>.)<p>In fact, the data structures of a 3D scene can be serialized as text, and a properly trained text gen system could generate such a representation directly, though that&#x27;s probably not the best route to decent text-to-3d.</div><br/><div id="38373570" class="c"><input type="checkbox" id="c-38373570" checked=""/><div class="controls bullet"><span class="by">jncfhnb</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38372863">parent</a><span>|</span><a href="#38371576">next</a><span>|</span><label class="collapse" for="c-38373570">[-]</label><label class="expand" for="c-38373570">[1 more]</label></div><br/><div class="children"><div class="content">Text is a standard sized embedding vector that gets passed one at a time to an LLM. All tokens have the same shape. Each token is processed one at a time. All tokens also have a pre defined order. It is very different and vastly simpler.<p>Serializing 3D models as text is not going to work for negligibly non trivial circumstances.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div id="38371576" class="c"><input type="checkbox" id="c-38371576" checked=""/><div class="controls bullet"><span class="by">Kuinox</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38368981">parent</a><span>|</span><a href="#38369301">prev</a><span>|</span><a href="#38371440">next</a><span>|</span><label class="collapse" for="c-38371576">[-]</label><label class="expand" for="c-38371576">[1 more]</label></div><br/><div class="children"><div class="content">This isn&#x27;t coming, it&#x27;s already here.  
<a href="https:&#x2F;&#x2F;github.com&#x2F;gsgen3d&#x2F;gsgen">https:&#x2F;&#x2F;github.com&#x2F;gsgen3d&#x2F;gsgen</a>  
Yes, it&#x27;s just 3D models for now, but it can do whole scenes generations, it&#x27;s just not great yet at it.  
The tech is there but just need to improve.</div><br/></div></div><div id="38371440" class="c"><input type="checkbox" id="c-38371440" checked=""/><div class="controls bullet"><span class="by">sheepscreek</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38368981">parent</a><span>|</span><a href="#38371576">prev</a><span>|</span><a href="#38369162">next</a><span>|</span><label class="collapse" for="c-38371440">[-]</label><label class="expand" for="c-38371440">[1 more]</label></div><br/><div class="children"><div class="content">Excellent point.<p>Perhaps a more computationally expensive but better looking method will be to pull all objects in the scene from a 3D model library, then programmatically set the scene and render it.</div><br/></div></div><div id="38369162" class="c"><input type="checkbox" id="c-38369162" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38368981">parent</a><span>|</span><a href="#38371440">prev</a><span>|</span><a href="#38370338">next</a><span>|</span><label class="collapse" for="c-38369162">[-]</label><label class="expand" for="c-38369162">[5 more]</label></div><br/><div class="children"><div class="content">Are you working on all that?</div><br/><div id="38369544" class="c"><input type="checkbox" id="c-38369544" checked=""/><div class="controls bullet"><span class="by">cptaj</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38369162">parent</a><span>|</span><a href="#38370338">next</a><span>|</span><label class="collapse" for="c-38369544">[-]</label><label class="expand" for="c-38369544">[4 more]</label></div><br/><div class="children"><div class="content">Probably not. But there does seem to be a clear path to it.<p>The main  issue is going to be having the right dataset. You basically need to record user actions in something like blender (ie: moving a model of a bike to the left of a scene), match it to a text description of the action (ie; &quot;move bike to the left&quot;) and match those to before&#x2F;after snapshots of the resulting file format.<p>You need a whole metric fuckton of these.<p>After that, you train your model to produce those 3d scene files instead of image bitmaps.<p>You can do this for a lot of other tasks. These general purpose models can learn anything that you can usefully represent in data.<p>I can imagine AGI being, at least in part, a large set of these purpose trained  models. Heck, maybe our brains work this way. When we learn to throw a ball, we train a model in a subset of our brain to do just this and then this model is called on by our general consciousness when needed.<p>Sorry, I&#x27;m just rambling here but its very exciting stuff.</div><br/><div id="38371197" class="c"><input type="checkbox" id="c-38371197" checked=""/><div class="controls bullet"><span class="by">sterlind</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38369544">parent</a><span>|</span><a href="#38372249">next</a><span>|</span><label class="collapse" for="c-38371197">[-]</label><label class="expand" for="c-38371197">[2 more]</label></div><br/><div class="children"><div class="content">The hard part of AGI is the self-training and few examples. Your parents didn&#x27;t attach strings to your body and puppeteer you through a few hundred thousand games of baseball. And the humans that invented baseball had zero training data to go on.</div><br/><div id="38373082" class="c"><input type="checkbox" id="c-38373082" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38371197">parent</a><span>|</span><a href="#38372249">next</a><span>|</span><label class="collapse" for="c-38373082">[-]</label><label class="expand" for="c-38373082">[1 more]</label></div><br/><div class="children"><div class="content">Your body is a result of a billion year old evolutionary optimization process. GPT-4 was trained from scratch in a few months.</div><br/></div></div></div></div><div id="38372249" class="c"><input type="checkbox" id="c-38372249" checked=""/><div class="controls bullet"><span class="by">filipezf</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38369544">parent</a><span>|</span><a href="#38371197">prev</a><span>|</span><a href="#38370338">next</a><span>|</span><label class="collapse" for="c-38372249">[-]</label><label class="expand" for="c-38372249">[1 more]</label></div><br/><div class="children"><div class="content">I have for some time planning to do a &#x27;Wikipedia for AI&#x27; (even bought a domain), where people could contribute all sorts of these skills ( not only 3d video, but also manual skills, or anything). Given the current climate of &#x27;AI will save&#x2F;doom us&#x27;, and that users would in some sense be training their own replacements, I don&#x27;t know how much love such site would have, though.</div><br/></div></div></div></div></div></div><div id="38370338" class="c"><input type="checkbox" id="c-38370338" checked=""/><div class="controls bullet"><span class="by">internet101010</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38368981">parent</a><span>|</span><a href="#38369162">prev</a><span>|</span><a href="#38369962">next</a><span>|</span><label class="collapse" for="c-38370338">[-]</label><label class="expand" for="c-38370338">[1 more]</label></div><br/><div class="children"><div class="content">I am guessing it will be similar to inpainting in normal stable diffusion, which is easy when using the workflow feature InvokeAI ui.</div><br/></div></div><div id="38369962" class="c"><input type="checkbox" id="c-38369962" checked=""/><div class="controls bullet"><span class="by">a_bouncing_bean</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38368981">parent</a><span>|</span><a href="#38370338">prev</a><span>|</span><a href="#38374736">next</a><span>|</span><label class="collapse" for="c-38369962">[-]</label><label class="expand" for="c-38369962">[1 more]</label></div><br/><div class="children"><div class="content">Thanks! this is exactly what I have been thinking, only you&#x27;ve expressed it much more eloquently than I would be able.</div><br/></div></div><div id="38374736" class="c"><input type="checkbox" id="c-38374736" checked=""/><div class="controls bullet"><span class="by">jwoodbridge</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38368981">parent</a><span>|</span><a href="#38369962">prev</a><span>|</span><a href="#38372866">next</a><span>|</span><label class="collapse" for="c-38374736">[-]</label><label class="expand" for="c-38374736">[2 more]</label></div><br/><div class="children"><div class="content">we&#x27;re working on this if you want to give it a try - dream3d.com</div><br/><div id="38375565" class="c"><input type="checkbox" id="c-38375565" checked=""/><div class="controls bullet"><span class="by">hackerlight</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38374736">parent</a><span>|</span><a href="#38372866">next</a><span>|</span><label class="collapse" for="c-38375565">[-]</label><label class="expand" for="c-38375565">[1 more]</label></div><br/><div class="children"><div class="content">You should put a demo on the landing page</div><br/></div></div></div></div><div id="38372866" class="c"><input type="checkbox" id="c-38372866" checked=""/><div class="controls bullet"><span class="by">solarkraft</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38368981">parent</a><span>|</span><a href="#38374736">prev</a><span>|</span><a href="#38374556">next</a><span>|</span><label class="collapse" for="c-38372866">[-]</label><label class="expand" for="c-38372866">[1 more]</label></div><br/><div class="children"><div class="content">Where is the training data coming from?</div><br/></div></div></div></div><div id="38368801" class="c"><input type="checkbox" id="c-38368801" checked=""/><div class="controls bullet"><span class="by">xianshou</span><span>|</span><a href="#38368607">parent</a><span>|</span><a href="#38368981">prev</a><span>|</span><a href="#38369456">next</a><span>|</span><label class="collapse" for="c-38368801">[-]</label><label class="expand" for="c-38368801">[2 more]</label></div><br/><div class="children"><div class="content">Emu edit should be exactly what you&#x27;re looking for: <a href="https:&#x2F;&#x2F;ai.meta.com&#x2F;blog&#x2F;emu-text-to-video-generation-image-editing-research&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;ai.meta.com&#x2F;blog&#x2F;emu-text-to-video-generation-image-...</a></div><br/><div id="38370244" class="c"><input type="checkbox" id="c-38370244" checked=""/><div class="controls bullet"><span class="by">smcleod</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38368801">parent</a><span>|</span><a href="#38369456">next</a><span>|</span><label class="collapse" for="c-38370244">[-]</label><label class="expand" for="c-38370244">[1 more]</label></div><br/><div class="children"><div class="content">It doesn’t look like the code for that is available anywhere though?</div><br/></div></div></div></div><div id="38369456" class="c"><input type="checkbox" id="c-38369456" checked=""/><div class="controls bullet"><span class="by">amoshebb</span><span>|</span><a href="#38368607">parent</a><span>|</span><a href="#38368801">prev</a><span>|</span><a href="#38375241">next</a><span>|</span><label class="collapse" for="c-38369456">[-]</label><label class="expand" for="c-38369456">[1 more]</label></div><br/><div class="children"><div class="content">I wonder what other odd connections are made due to city-name almost certainly being the most common word next to sportsball-name.<p>Do the parameters think that Jazz musicians are mormon? Padres often surf? Wizards like the Lincoln Memorial?</div><br/></div></div><div id="38375241" class="c"><input type="checkbox" id="c-38375241" checked=""/><div class="controls bullet"><span class="by">zeckalpha</span><span>|</span><a href="#38368607">parent</a><span>|</span><a href="#38369456">prev</a><span>|</span><a href="#38374805">next</a><span>|</span><label class="collapse" for="c-38375241">[-]</label><label class="expand" for="c-38375241">[1 more]</label></div><br/><div class="children"><div class="content">I see that as a reference to the AI generated Toronto Blue Jays advertisement gone wrong that went viral earlier this year. <a href="https:&#x2F;&#x2F;www.blogto.com&#x2F;sports_play&#x2F;2023&#x2F;06&#x2F;ai-generated-toronto-blue-jays-commercial&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.blogto.com&#x2F;sports_play&#x2F;2023&#x2F;06&#x2F;ai-generated-toro...</a></div><br/></div></div><div id="38374805" class="c"><input type="checkbox" id="c-38374805" checked=""/><div class="controls bullet"><span class="by">COAGULOPATH</span><span>|</span><a href="#38368607">parent</a><span>|</span><a href="#38375241">prev</a><span>|</span><a href="#38369188">next</a><span>|</span><label class="collapse" for="c-38374805">[-]</label><label class="expand" for="c-38374805">[1 more]</label></div><br/><div class="children"><div class="content">&gt; they simultaneously feel crippled and limited by their lack of editing &#x2F; iteration ability.<p>Yeah. They&#x27;re not &quot;videos&quot; so much as images that move around a bit.<p>This doesn&#x27;t really look any better than those Midjourney + RunwayML videos we had half a year ago.<p>&gt;Has anyone come across a solution where model can iterate (eg, with prompts like &quot;move the bicycle to the left side of the photo&quot;)? It feels like we&#x27;re close.<p>Google has a model called Phenaki that supposedly allows for that kind of stuff. But the public can&#x27;t use it so it&#x27;s hard to say how good it actually is.</div><br/></div></div><div id="38369188" class="c"><input type="checkbox" id="c-38369188" checked=""/><div class="controls bullet"><span class="by">filterfiber</span><span>|</span><a href="#38368607">parent</a><span>|</span><a href="#38374805">prev</a><span>|</span><a href="#38373461">next</a><span>|</span><label class="collapse" for="c-38369188">[-]</label><label class="expand" for="c-38369188">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Has anyone come across a solution where model can iterate (eg, with prompts like &quot;move the bicycle to the left side of the photo&quot;)? It feels like we&#x27;re close.<p>Emu can do that.<p>The bluejay&#x2F;toronto thing may be addressable later (I suspect via more detailed annotations a la dalle3) - these current video models are highly focused on figuring out temporal coherence</div><br/></div></div><div id="38373461" class="c"><input type="checkbox" id="c-38373461" checked=""/><div class="controls bullet"><span class="by">achileas</span><span>|</span><a href="#38368607">parent</a><span>|</span><a href="#38369188">prev</a><span>|</span><a href="#38373960">next</a><span>|</span><label class="collapse" for="c-38373461">[-]</label><label class="expand" for="c-38373461">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Has anyone come across a solution where model can iterate (eg, with prompts like &quot;move the bicycle to the left side of the photo&quot;)? It feels like we&#x27;re close.<p>Nearly all of the available models have this, even the highly commercialized ones like in Adobe Firefly and Canva, it’s called inpainting in most tools.</div><br/></div></div><div id="38373960" class="c"><input type="checkbox" id="c-38373960" checked=""/><div class="controls bullet"><span class="by">dsmmcken</span><span>|</span><a href="#38368607">parent</a><span>|</span><a href="#38373461">prev</a><span>|</span><a href="#38372913">next</a><span>|</span><label class="collapse" for="c-38373960">[-]</label><label class="expand" for="c-38373960">[1 more]</label></div><br/><div class="children"><div class="content">Adobe is doing some great work here in my opinion in terms of building AI tools that make sense for artist workflows. This &quot;sneak peak&quot; demo from the recent Adobe Max conference is pretty much exactly what you described, actually better because you can just click on an object in the image and drag it.<p>See video: <a href="https:&#x2F;&#x2F;www.adobe.com&#x2F;max&#x2F;2023&#x2F;sessions&#x2F;project-stardust-gs6-11.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.adobe.com&#x2F;max&#x2F;2023&#x2F;sessions&#x2F;project-stardust-gs6...</a></div><br/></div></div><div id="38372913" class="c"><input type="checkbox" id="c-38372913" checked=""/><div class="controls bullet"><span class="by">01100011</span><span>|</span><a href="#38368607">parent</a><span>|</span><a href="#38373960">prev</a><span>|</span><a href="#38369173">next</a><span>|</span><label class="collapse" for="c-38372913">[-]</label><label class="expand" for="c-38372913">[1 more]</label></div><br/><div class="children"><div class="content">I recently tried to generate clip art for a presentation using GPT-4&#x2F;DALL-E 3.  I found it could handle some updates but the output generally varied wildly as I tried to refine the image.  For instance, I&#x27;d have a cartoon character checking its watch and also wearing a pocket watch.  Trying to remove the pocket watch resulted in an entirely new cartoon with little stylistic continuity to the first.<p>Also, I originally tried to get the 3 characters in the image to be generated simultaneously, but eventually gave up as DALL-E had a hard time understanding how I wanted them positioned relative to each other.  I just generated 3 separate characters and positioned them in the same image using Gimp.</div><br/></div></div><div id="38369173" class="c"><input type="checkbox" id="c-38369173" checked=""/><div class="controls bullet"><span class="by">treesciencebot</span><span>|</span><a href="#38368607">parent</a><span>|</span><a href="#38372913">prev</a><span>|</span><a href="#38369300">next</a><span>|</span><label class="collapse" for="c-38369173">[-]</label><label class="expand" for="c-38369173">[1 more]</label></div><br/><div class="children"><div class="content">Have you seen fal.ai&#x2F;dynamic where you can perform image to image synthesis (basically editing an existing image with the help of diffusion process) using LCMs to provide a real time UI?</div><br/></div></div><div id="38369300" class="c"><input type="checkbox" id="c-38369300" checked=""/><div class="controls bullet"><span class="by">JoshTriplett</span><span>|</span><a href="#38368607">parent</a><span>|</span><a href="#38369173">prev</a><span>|</span><a href="#38368705">next</a><span>|</span><label class="collapse" for="c-38369300">[-]</label><label class="expand" for="c-38369300">[1 more]</label></div><br/><div class="children"><div class="content">I also wonder if the model takes capitalization into account. Capitalized &quot;Blue Jays&quot; seems more likely to reference the sports team; the birds would be lowercase.</div><br/></div></div><div id="38368705" class="c"><input type="checkbox" id="c-38368705" checked=""/><div class="controls bullet"><span class="by">appplication</span><span>|</span><a href="#38368607">parent</a><span>|</span><a href="#38369300">prev</a><span>|</span><a href="#38368709">next</a><span>|</span><label class="collapse" for="c-38368705">[-]</label><label class="expand" for="c-38368705">[5 more]</label></div><br/><div class="children"><div class="content">I don’t spend a lot of time keeping up with the space, but I could have sworn I’ve seen a demo that allowed you to iterate in the way you’re suggesting. Maybe someone else can link it.</div><br/><div id="38369040" class="c"><input type="checkbox" id="c-38369040" checked=""/><div class="controls bullet"><span class="by">ssalka</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38368705">parent</a><span>|</span><a href="#38369515">next</a><span>|</span><label class="collapse" for="c-38369040">[-]</label><label class="expand" for="c-38369040">[2 more]</label></div><br/><div class="children"><div class="content">My guess is you&#x27;re thinking of InstructPix2Pix[1], with prompts like &quot;make the sky green&quot; or &quot;replace the fruits with cake&quot;<p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;timothybrooks&#x2F;instruct-pix2pix">https:&#x2F;&#x2F;github.com&#x2F;timothybrooks&#x2F;instruct-pix2pix</a></div><br/><div id="38371395" class="c"><input type="checkbox" id="c-38371395" checked=""/><div class="controls bullet"><span class="by">appplication</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38369040">parent</a><span>|</span><a href="#38369515">next</a><span>|</span><label class="collapse" for="c-38371395">[-]</label><label class="expand" for="c-38371395">[1 more]</label></div><br/><div class="children"><div class="content">This is exactly it!</div><br/></div></div></div></div><div id="38369515" class="c"><input type="checkbox" id="c-38369515" checked=""/><div class="controls bullet"><span class="by">tjoff</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38368705">parent</a><span>|</span><a href="#38369040">prev</a><span>|</span><a href="#38368989">next</a><span>|</span><label class="collapse" for="c-38369515">[-]</label><label class="expand" for="c-38369515">[1 more]</label></div><br/><div class="children"><div class="content">Emu-Edit is the closest I&#x27;ve seen.<p><a href="https:&#x2F;&#x2F;emu-edit.metademolab.com&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;emu-edit.metademolab.com&#x2F;</a><p><a href="https:&#x2F;&#x2F;ai.meta.com&#x2F;blog&#x2F;emu-text-to-video-generation-image-editing-research&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;ai.meta.com&#x2F;blog&#x2F;emu-text-to-video-generation-image-...</a></div><br/></div></div><div id="38368989" class="c"><input type="checkbox" id="c-38368989" checked=""/><div class="controls bullet"><span class="by">accrual</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38368705">parent</a><span>|</span><a href="#38369515">prev</a><span>|</span><a href="#38368709">next</a><span>|</span><label class="collapse" for="c-38368989">[-]</label><label class="expand" for="c-38368989">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not exactly like GP described (e.g. move bike to the left) but there is a more advanced SD technique called inpainting [0] that allows you to manually recompose parts of the image, e.g. to fix bad eyes and hands.<p>[0] <a href="https:&#x2F;&#x2F;stable-diffusion-art.com&#x2F;inpainting_basics&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;stable-diffusion-art.com&#x2F;inpainting_basics&#x2F;</a></div><br/></div></div></div></div><div id="38368709" class="c"><input type="checkbox" id="c-38368709" checked=""/><div class="controls bullet"><span class="by">kshacker</span><span>|</span><a href="#38368607">parent</a><span>|</span><a href="#38368705">prev</a><span>|</span><a href="#38373315">next</a><span>|</span><label class="collapse" for="c-38368709">[-]</label><label class="expand" for="c-38368709">[3 more]</label></div><br/><div class="children"><div class="content">Assuming we can post links, you mean this video: <a href="https:&#x2F;&#x2F;youtu.be&#x2F;G7mihAy691g?si=o2KCmR2Uh_97UQ0N" rel="nofollow noreferrer">https:&#x2F;&#x2F;youtu.be&#x2F;G7mihAy691g?si=o2KCmR2Uh_97UQ0N</a><p>Also, maybe you can&#x27;t edit post facto, but when you give prompts, would you not be able to say : two blue jays but no CN tower</div><br/><div id="38368742" class="c"><input type="checkbox" id="c-38368742" checked=""/><div class="controls bullet"><span class="by">FrozenTuna</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38368709">parent</a><span>|</span><a href="#38373315">next</a><span>|</span><label class="collapse" for="c-38368742">[-]</label><label class="expand" for="c-38368742">[2 more]</label></div><br/><div class="children"><div class="content">Yes, its called a negative prompt. Idk if txt2video has it, but both llms and stable-diffusion have it so I&#x27;d assume its good to go.</div><br/><div id="38368873" class="c"><input type="checkbox" id="c-38368873" checked=""/><div class="controls bullet"><span class="by">nottheengineer</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38368742">parent</a><span>|</span><a href="#38373315">next</a><span>|</span><label class="collapse" for="c-38368873">[-]</label><label class="expand" for="c-38368873">[1 more]</label></div><br/><div class="children"><div class="content">Haven&#x27;t implemented negative prompts yet, but from what I can tell it&#x27;s as simple as substracting from the prompt in embedding space.</div><br/></div></div></div></div></div></div><div id="38373315" class="c"><input type="checkbox" id="c-38373315" checked=""/><div class="controls bullet"><span class="by">stevage</span><span>|</span><a href="#38368607">parent</a><span>|</span><a href="#38368709">prev</a><span>|</span><a href="#38368735">next</a><span>|</span><label class="collapse" for="c-38373315">[-]</label><label class="expand" for="c-38373315">[1 more]</label></div><br/><div class="children"><div class="content">I wondered similarly whether the astronaut&#x27;s weird gait was because it was kind of &quot;moonwalking&quot; on the moon.</div><br/></div></div><div id="38368735" class="c"><input type="checkbox" id="c-38368735" checked=""/><div class="controls bullet"><span class="by">FrozenTuna</span><span>|</span><a href="#38368607">parent</a><span>|</span><a href="#38373315">prev</a><span>|</span><a href="#38369495">next</a><span>|</span><label class="collapse" for="c-38368735">[-]</label><label class="expand" for="c-38368735">[1 more]</label></div><br/><div class="children"><div class="content">Not <i>exactly</i> what you&#x27;re asking for, but AnimateDiff has introduced creating gifs to SD. Still takes quite a bit of tweaking IME.</div><br/></div></div><div id="38369495" class="c"><input type="checkbox" id="c-38369495" checked=""/><div class="controls bullet"><span class="by">ProfessorZoom</span><span>|</span><a href="#38368607">parent</a><span>|</span><a href="#38368735">prev</a><span>|</span><a href="#38374553">next</a><span>|</span><label class="collapse" for="c-38369495">[-]</label><label class="expand" for="c-38369495">[1 more]</label></div><br/><div class="children"><div class="content">that sounds like v0 by vercel, you can iterate just like you asked, to combine that type of iteration with video would be really awesome</div><br/></div></div><div id="38369334" class="c"><input type="checkbox" id="c-38369334" checked=""/><div class="controls bullet"><span class="by">psunavy03</span><span>|</span><a href="#38368607">parent</a><span>|</span><a href="#38371382">prev</a><span>|</span><a href="#38368561">next</a><span>|</span><label class="collapse" for="c-38369334">[-]</label><label class="expand" for="c-38369334">[6 more]</label></div><br/><div class="children"><div class="content">&gt; sportsball<p>This is not the flex you think it is.  You don&#x27;t have to like sports, but snarking on people who do doesn&#x27;t make you intellectual, it just makes you come across as a douchebag, no different than a sports fan making fun of &quot;D&amp;D nerds&quot; or something.</div><br/><div id="38369539" class="c"><input type="checkbox" id="c-38369539" checked=""/><div class="controls bullet"><span class="by">Zetaphor</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38369334">parent</a><span>|</span><a href="#38375962">next</a><span>|</span><label class="collapse" for="c-38369539">[-]</label><label class="expand" for="c-38369539">[2 more]</label></div><br/><div class="children"><div class="content">This has become a colloquial term for describing all sports, not the insult you&#x27;re perceiving it to be.<p>Rather than projecting your own hangups and calling people names, try instead assuming that they&#x27;re not trying to offend you personally and are just using common vernacular.</div><br/><div id="38373446" class="c"><input type="checkbox" id="c-38373446" checked=""/><div class="controls bullet"><span class="by">achileas</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38369539">parent</a><span>|</span><a href="#38375962">next</a><span>|</span><label class="collapse" for="c-38373446">[-]</label><label class="expand" for="c-38373446">[1 more]</label></div><br/><div class="children"><div class="content">If only there was an existing way to refer to sports generally! And OP was referring to a specific sport (baseball), not sports generally.</div><br/></div></div></div></div><div id="38375962" class="c"><input type="checkbox" id="c-38375962" checked=""/><div class="controls bullet"><span class="by">jojobas</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38369334">parent</a><span>|</span><a href="#38369539">prev</a><span>|</span><a href="#38371252">next</a><span>|</span><label class="collapse" for="c-38375962">[-]</label><label class="expand" for="c-38375962">[1 more]</label></div><br/><div class="children"><div class="content">Would you get incensed by &quot;petrolhead&quot;, &quot;greenfingers&quot; or &quot;trekkie&quot;? Is that what you choose to be emotional about?</div><br/></div></div><div id="38369415" class="c"><input type="checkbox" id="c-38369415" checked=""/><div class="controls bullet"><span class="by">chaps</span><span>|</span><a href="#38368607">root</a><span>|</span><a href="#38369334">parent</a><span>|</span><a href="#38371252">prev</a><span>|</span><a href="#38368561">next</a><span>|</span><label class="collapse" for="c-38369415">[-]</label><label class="expand" for="c-38369415">[1 more]</label></div><br/><div class="children"><div class="content">Ah, Mr. Kettle, I see you&#x27;ve met my friend, Mr. Pot!</div><br/></div></div></div></div></div></div><div id="38368561" class="c"><input type="checkbox" id="c-38368561" checked=""/><div class="controls bullet"><span class="by">valine</span><span>|</span><a href="#38368607">prev</a><span>|</span><a href="#38373277">next</a><span>|</span><label class="collapse" for="c-38368561">[-]</label><label class="expand" for="c-38368561">[34 more]</label></div><br/><div class="children"><div class="content">The rate of progress in ML this past year has been breath taking.<p>I can’t wait to see what people do with this once controlnet is properly adapted to video. Generating videos from scratch is cool, but the real utility of this will be the temporal consistency. Getting stable video out of stable diffusion typically involves lots of manual post processing to remove flicker.</div><br/><div id="38369658" class="c"><input type="checkbox" id="c-38369658" checked=""/><div class="controls bullet"><span class="by">alberth</span><span>|</span><a href="#38368561">parent</a><span>|</span><a href="#38372630">next</a><span>|</span><label class="collapse" for="c-38369658">[-]</label><label class="expand" for="c-38369658">[18 more]</label></div><br/><div class="children"><div class="content">What was the big “unlock” that allowed so much progress this past year?<p>I ask as a noob in this area.</div><br/><div id="38369823" class="c"><input type="checkbox" id="c-38369823" checked=""/><div class="controls bullet"><span class="by">4death4</span><span>|</span><a href="#38368561">root</a><span>|</span><a href="#38369658">parent</a><span>|</span><a href="#38369947">next</a><span>|</span><label class="collapse" for="c-38369823">[-]</label><label class="expand" for="c-38369823">[6 more]</label></div><br/><div class="children"><div class="content">I think these are the main drivers behind the progress:<p>- Unsupervised learning techniques, e.g. transformers and diffusion models. You need unsupervised techniques in order to utilize enough data. There have been other unsupervised techniques in the past, e.g. GANs, but they don&#x27;t work as well.<p>- Massive amounts of training data.<p>- The belief that training these models will produce something valuable. It costs between hundreds of thousands to millions of dollars to train these models. The people doing the training need to believe they&#x27;re going to get something interesting out at the end. More and more people and teams are starting to see training a large model as something worth pursuing.<p>- Better GPUs, which enables training larger models.<p>- Honestly the fall of crypto probably also contributed, because miners were eating a lot of GPU time.</div><br/><div id="38370199" class="c"><input type="checkbox" id="c-38370199" checked=""/><div class="controls bullet"><span class="by">mkaic</span><span>|</span><a href="#38368561">root</a><span>|</span><a href="#38369823">parent</a><span>|</span><a href="#38369947">next</a><span>|</span><label class="collapse" for="c-38370199">[-]</label><label class="expand" for="c-38370199">[5 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think transformers or diffusion models are inherently &quot;unsupervised&quot;, especially not the way they&#x27;re used in Stable Diffusion and related models (which are very much trained in a supervised fashion). I agree with the rest of your points though.</div><br/><div id="38370456" class="c"><input type="checkbox" id="c-38370456" checked=""/><div class="controls bullet"><span class="by">ebalit</span><span>|</span><a href="#38368561">root</a><span>|</span><a href="#38370199">parent</a><span>|</span><a href="#38374970">next</a><span>|</span><label class="collapse" for="c-38370456">[-]</label><label class="expand" for="c-38370456">[3 more]</label></div><br/><div class="children"><div class="content">Generative methods have usually been considered unsupervised.<p>You&#x27;re right that conditional generation start to blur the lines though.</div><br/><div id="38372300" class="c"><input type="checkbox" id="c-38372300" checked=""/><div class="controls bullet"><span class="by">n2d4</span><span>|</span><a href="#38368561">root</a><span>|</span><a href="#38370456">parent</a><span>|</span><a href="#38374970">next</a><span>|</span><label class="collapse" for="c-38372300">[-]</label><label class="expand" for="c-38372300">[2 more]</label></div><br/><div class="children"><div class="content">&quot;Generative AI&quot; is a misnomer; it&#x27;s not the same kind of &quot;generative&quot; as the G in GAN.<p>While you&#x27;re right about GANs, diffusion models as transformers as transformers are most commonly trained with supervised learning.</div><br/><div id="38372407" class="c"><input type="checkbox" id="c-38372407" checked=""/><div class="controls bullet"><span class="by">ebalit</span><span>|</span><a href="#38368561">root</a><span>|</span><a href="#38372300">parent</a><span>|</span><a href="#38374970">next</a><span>|</span><label class="collapse" for="c-38372407">[-]</label><label class="expand" for="c-38372407">[1 more]</label></div><br/><div class="children"><div class="content">I disagree. Diffusion models are trained to generate the probability distribution of their training dataset, like other generative models (GAN, VAE, etc). The fact that the architecture is a Transformer (or a CNN with attention like in Stable Diffusion) is orthogonal to the generative vs discriminative divide.<p>Unsupervised is a confusing term as there is always an underlying loss being optimized and working as a supervision signal, even for good old kmeans. But generative models are generally considered to be part of unsupervised methods.</div><br/></div></div></div></div></div></div><div id="38374970" class="c"><input type="checkbox" id="c-38374970" checked=""/><div class="controls bullet"><span class="by">valec</span><span>|</span><a href="#38368561">root</a><span>|</span><a href="#38370199">parent</a><span>|</span><a href="#38370456">prev</a><span>|</span><a href="#38369947">next</a><span>|</span><label class="collapse" for="c-38374970">[-]</label><label class="expand" for="c-38374970">[1 more]</label></div><br/><div class="children"><div class="content">self-supervised is a better term</div><br/></div></div></div></div></div></div><div id="38369947" class="c"><input type="checkbox" id="c-38369947" checked=""/><div class="controls bullet"><span class="by">Cyphase</span><span>|</span><a href="#38368561">root</a><span>|</span><a href="#38369658">parent</a><span>|</span><a href="#38369823">prev</a><span>|</span><a href="#38369700">next</a><span>|</span><label class="collapse" for="c-38369947">[-]</label><label class="expand" for="c-38369947">[1 more]</label></div><br/><div class="children"><div class="content">One factor is that Stable Diffusion and ChatGPT were released within 3 months of each other – August 22, 2022 and November 3, 2022, respectively. That brought a lot of attention and excitement to the field. More excitement, more people, more work being done, more progress.<p>Of course those two releases didn&#x27;t fall out of the sky.</div><br/></div></div><div id="38369700" class="c"><input type="checkbox" id="c-38369700" checked=""/><div class="controls bullet"><span class="by">mlboss</span><span>|</span><a href="#38368561">root</a><span>|</span><a href="#38369658">parent</a><span>|</span><a href="#38369947">prev</a><span>|</span><a href="#38372111">next</a><span>|</span><label class="collapse" for="c-38369700">[-]</label><label class="expand" for="c-38369700">[8 more]</label></div><br/><div class="children"><div class="content">Stable diffusion open source release and llama release</div><br/><div id="38369821" class="c"><input type="checkbox" id="c-38369821" checked=""/><div class="controls bullet"><span class="by">alberth</span><span>|</span><a href="#38368561">root</a><span>|</span><a href="#38369700">parent</a><span>|</span><a href="#38372111">next</a><span>|</span><label class="collapse" for="c-38369821">[-]</label><label class="expand" for="c-38369821">[7 more]</label></div><br/><div class="children"><div class="content">But what technically allowed for so much progress?<p>There’s been open source AI&#x2F;ML for 20+ years.<p>Nothing comes close to the massive milestones over the past year.</div><br/><div id="38370272" class="c"><input type="checkbox" id="c-38370272" checked=""/><div class="controls bullet"><span class="by">kmeisthax</span><span>|</span><a href="#38368561">root</a><span>|</span><a href="#38369821">parent</a><span>|</span><a href="#38375057">next</a><span>|</span><label class="collapse" for="c-38370272">[-]</label><label class="expand" for="c-38370272">[1 more]</label></div><br/><div class="children"><div class="content">Attention, transformers, diffusion. Prior image synthesis techniques - i.e. GANs - had problems that made it difficult to scale them up, whereas the current techniques seem to have no limit other than the amount of RAM in your GPU.</div><br/></div></div><div id="38375057" class="c"><input type="checkbox" id="c-38375057" checked=""/><div class="controls bullet"><span class="by">fragmede</span><span>|</span><a href="#38368561">root</a><span>|</span><a href="#38369821">parent</a><span>|</span><a href="#38370272">prev</a><span>|</span><a href="#38370136">next</a><span>|</span><label class="collapse" for="c-38375057">[-]</label><label class="expand" for="c-38375057">[1 more]</label></div><br/><div class="children"><div class="content">the Transformers are all you need paper from Google, which may end up being a larger contribution to society than Google search, is foundational.<p>Emad Mostaque and his investment in stable diffusion, and his decision to release it to the world.<p>I&#x27;m sure there are others, but those are the two that stick out to me.</div><br/></div></div><div id="38370136" class="c"><input type="checkbox" id="c-38370136" checked=""/><div class="controls bullet"><span class="by">Chabsff</span><span>|</span><a href="#38368561">root</a><span>|</span><a href="#38369821">parent</a><span>|</span><a href="#38375057">prev</a><span>|</span><a href="#38370401">next</a><span>|</span><label class="collapse" for="c-38370136">[-]</label><label class="expand" for="c-38370136">[1 more]</label></div><br/><div class="children"><div class="content">Public availability of large transformer-based foundation models trained at great expense, which is what OP is referring  to, is definitely unprecedented.</div><br/></div></div><div id="38370401" class="c"><input type="checkbox" id="c-38370401" checked=""/><div class="controls bullet"><span class="by">jasonjmcghee</span><span>|</span><a href="#38368561">root</a><span>|</span><a href="#38369821">parent</a><span>|</span><a href="#38370136">prev</a><span>|</span><a href="#38373169">next</a><span>|</span><label class="collapse" for="c-38370401">[-]</label><label class="expand" for="c-38370401">[2 more]</label></div><br/><div class="children"><div class="content">People figuring out how to train and scale newer architectures (like transfomers) effectively, to be wildly larger than ever before.<p>Take AlexNet - the major &quot;oh shit&quot; moment in image classification.<p>It had an absolutely mind-blowing number of parameters at a whopping 62 million.<p>Holy shit, what a large network, right?<p>Absolutely unprecedented.<p>Now, for language models, anything under 1B parameters is a toy that barely works.<p>Stable diffusion has around 1B or so - or the early models did, I&#x27;m sure they&#x27;re larger now.<p>A whole lot of smart people had to do a bunch of cool stuff to be able to keep networks working at all at that size.<p>Many, many times over the years, people have tried to make larger networks, which fail to converge (read: learn to do something useful) in all sorts of crazy ways.<p>At this size, it&#x27;s also expensive to train these things from scratch, and takes a shit-ton of data, so research&#x2F;discovery of new things is slow and difficult.<p>But, we kind of climbed over a cliff, and now things are absolutely taking off in all the fields around this kind of stuff.<p>Take a look at XTTSv2 for example, a leading open source text-to-speech model. It uses multiple models in its architecture, but one of them is GPT.<p>There are a few key models that are still being used in a bunch of different modalities like CLIP, U-Net, GPT, etc. or similar variants. When they were released &#x2F; made available, people jumped on them and started experimenting.</div><br/><div id="38370474" class="c"><input type="checkbox" id="c-38370474" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#38368561">root</a><span>|</span><a href="#38370401">parent</a><span>|</span><a href="#38373169">next</a><span>|</span><label class="collapse" for="c-38370474">[-]</label><label class="expand" for="c-38370474">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Stable diffusion has around 1B or so - or the early models did, I&#x27;m sure they&#x27;re larger now.<p>SDXL is 6.6 billion.</div><br/></div></div></div></div><div id="38373169" class="c"><input type="checkbox" id="c-38373169" checked=""/><div class="controls bullet"><span class="by">mschuster91</span><span>|</span><a href="#38368561">root</a><span>|</span><a href="#38369821">parent</a><span>|</span><a href="#38370401">prev</a><span>|</span><a href="#38372111">next</a><span>|</span><label class="collapse" for="c-38373169">[-]</label><label class="expand" for="c-38373169">[1 more]</label></div><br/><div class="children"><div class="content">&gt; But what technically allowed for so much progress?<p>The availability of GPU compute time. Up until the Russian invasion into Ukraine, interest rates were low AF so everyone and their dog thought it would be a cool idea to mine one or another sort of shitcoin. Once rising interest rates killed that business model for good, miners dumped their GPUs on the open market, and an awful lot of cloud computing capacity suddenly went free.</div><br/></div></div></div></div></div></div><div id="38372111" class="c"><input type="checkbox" id="c-38372111" checked=""/><div class="controls bullet"><span class="by">marricks</span><span>|</span><a href="#38368561">root</a><span>|</span><a href="#38369658">parent</a><span>|</span><a href="#38369700">prev</a><span>|</span><a href="#38372635">next</a><span>|</span><label class="collapse" for="c-38372111">[-]</label><label class="expand" for="c-38372111">[1 more]</label></div><br/><div class="children"><div class="content">I mean, you probably didn&#x27;t pay much attention to battery capacity before phones, laptops, and electric cars, right? Battery capacity has probably increased though at some rate before you paid attention. It&#x27;s just when something actually becomes relevant that we notice.<p>Not that more advances don&#x27;t happen with sustained hype, just there&#x27;s some sort of tipping point involving usefulness based either on improvement of the thing in question or it&#x27;s utility elsewhere.</div><br/></div></div><div id="38372635" class="c"><input type="checkbox" id="c-38372635" checked=""/><div class="controls bullet"><span class="by">throwaway290</span><span>|</span><a href="#38368561">root</a><span>|</span><a href="#38369658">parent</a><span>|</span><a href="#38372111">prev</a><span>|</span><a href="#38372630">next</a><span>|</span><label class="collapse" for="c-38372635">[-]</label><label class="expand" for="c-38372635">[1 more]</label></div><br/><div class="children"><div class="content">MS subsidizing it with 10 billions USD and (un)healthy contempt towards copyright.</div><br/></div></div></div></div><div id="38372630" class="c"><input type="checkbox" id="c-38372630" checked=""/><div class="controls bullet"><span class="by">kornesh</span><span>|</span><a href="#38368561">parent</a><span>|</span><a href="#38369658">prev</a><span>|</span><a href="#38368828">next</a><span>|</span><label class="collapse" for="c-38372630">[-]</label><label class="expand" for="c-38372630">[1 more]</label></div><br/><div class="children"><div class="content">Yeah, solving the flickering problem and achieving temporal consistency will be the key to realize the full potential of generative video models.<p>Right now, AnimateDiff is leading the way in consistency but I&#x27;m really excited to see what people will do with this new model.</div><br/></div></div><div id="38368828" class="c"><input type="checkbox" id="c-38368828" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#38368561">parent</a><span>|</span><a href="#38372630">prev</a><span>|</span><a href="#38369819">next</a><span>|</span><label class="collapse" for="c-38368828">[-]</label><label class="expand" for="c-38368828">[12 more]</label></div><br/><div class="children"><div class="content">Controlnet is adapted to video today, the issues are that it&#x27;s very slow. Haven&#x27;t you seen the insane quality of videos on civitai?</div><br/><div id="38368870" class="c"><input type="checkbox" id="c-38368870" checked=""/><div class="controls bullet"><span class="by">valine</span><span>|</span><a href="#38368561">root</a><span>|</span><a href="#38368828">parent</a><span>|</span><a href="#38369431">next</a><span>|</span><label class="collapse" for="c-38368870">[-]</label><label class="expand" for="c-38368870">[7 more]</label></div><br/><div class="children"><div class="content">I have seen them, the workflows to create those videos are extremely labor intensive. Control net lets you maintain poses between frames, it doesn’t solve the temporal consistency of small details.</div><br/><div id="38368947" class="c"><input type="checkbox" id="c-38368947" checked=""/><div class="controls bullet"><span class="by">mattnewton</span><span>|</span><a href="#38368561">root</a><span>|</span><a href="#38368870">parent</a><span>|</span><a href="#38369431">next</a><span>|</span><label class="collapse" for="c-38368947">[-]</label><label class="expand" for="c-38368947">[6 more]</label></div><br/><div class="children"><div class="content">People use animatediff’s motion module (or other models that have cross frame attention layers). Consistency is close to being solved.</div><br/><div id="38369110" class="c"><input type="checkbox" id="c-38369110" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#38368561">root</a><span>|</span><a href="#38368947">parent</a><span>|</span><a href="#38369077">next</a><span>|</span><label class="collapse" for="c-38369110">[-]</label><label class="expand" for="c-38369110">[4 more]</label></div><br/><div class="children"><div class="content">Temporal consistency is improving, but “close to being solved” is very optimistic.</div><br/><div id="38369399" class="c"><input type="checkbox" id="c-38369399" checked=""/><div class="controls bullet"><span class="by">mattnewton</span><span>|</span><a href="#38368561">root</a><span>|</span><a href="#38369110">parent</a><span>|</span><a href="#38369077">next</a><span>|</span><label class="collapse" for="c-38369399">[-]</label><label class="expand" for="c-38369399">[3 more]</label></div><br/><div class="children"><div class="content">No I think we’re actually close. My source is I’m working on this problem and the incredible progress of our tiny 3 person team at drip.art (<a href="http:&#x2F;&#x2F;api.drip.art" rel="nofollow noreferrer">http:&#x2F;&#x2F;api.drip.art</a>) - we can generate a lot of frames that are consistent, and with interpolation between them, smoothly restyle even long videos. Cross-frame attention works for most cases, it just needs to be scaled up.<p>And that’s just for diffusion focused approaches like ours. There are probably other techniques from the token flow or nerf family of approaches close to breakout levels of quality, tons of talented researchers working on that too.</div><br/><div id="38376019" class="c"><input type="checkbox" id="c-38376019" checked=""/><div class="controls bullet"><span class="by">Hard_Space</span><span>|</span><a href="#38368561">root</a><span>|</span><a href="#38369399">parent</a><span>|</span><a href="#38375397">next</a><span>|</span><label class="collapse" for="c-38376019">[-]</label><label class="expand" for="c-38376019">[1 more]</label></div><br/><div class="children"><div class="content">Once a video can show a person twisting round, and their belt buckle is the same at the end as it was at the start of the turn, it&#x27;s solved. VFX pipelines need consistency. TC is a long, long way from being solved, except by hitching it to 3DMMs and SMPL models (and even then, the results are not fabulous yet).</div><br/></div></div><div id="38375397" class="c"><input type="checkbox" id="c-38375397" checked=""/><div class="controls bullet"><span class="by">ryukoposting</span><span>|</span><a href="#38368561">root</a><span>|</span><a href="#38369399">parent</a><span>|</span><a href="#38376019">prev</a><span>|</span><a href="#38369077">next</a><span>|</span><label class="collapse" for="c-38375397">[-]</label><label class="expand" for="c-38375397">[1 more]</label></div><br/><div class="children"><div class="content">The demo clips on the site are cool, but when you call it a &quot;solved problem,&quot; I&#x27;d expect to see panning, rotating, and zooming within a cohesive scene with multiple subjects.</div><br/></div></div></div></div></div></div><div id="38369077" class="c"><input type="checkbox" id="c-38369077" checked=""/><div class="controls bullet"><span class="by">valine</span><span>|</span><a href="#38368561">root</a><span>|</span><a href="#38368947">parent</a><span>|</span><a href="#38369110">prev</a><span>|</span><a href="#38369431">next</a><span>|</span><label class="collapse" for="c-38369077">[-]</label><label class="expand" for="c-38369077">[1 more]</label></div><br/><div class="children"><div class="content">Hopefully this new model will be a step beyond what you can do with animatediff</div><br/></div></div></div></div></div></div><div id="38369431" class="c"><input type="checkbox" id="c-38369431" checked=""/><div class="controls bullet"><span class="by">capableweb</span><span>|</span><a href="#38368561">root</a><span>|</span><a href="#38368828">parent</a><span>|</span><a href="#38368870">prev</a><span>|</span><a href="#38369819">next</a><span>|</span><label class="collapse" for="c-38369431">[-]</label><label class="expand" for="c-38369431">[4 more]</label></div><br/><div class="children"><div class="content">&gt; Haven&#x27;t you seen the insane quality of videos on civitai?<p>I have not, so I went to <a href="https:&#x2F;&#x2F;civitai.com&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;civitai.com&#x2F;</a> which I guess is what you&#x27;re talking about? But I cannot find a single video there, just images and models.</div><br/><div id="38371744" class="c"><input type="checkbox" id="c-38371744" checked=""/><div class="controls bullet"><span class="by">Kevin09210</span><span>|</span><a href="#38368561">root</a><span>|</span><a href="#38369431">parent</a><span>|</span><a href="#38371802">next</a><span>|</span><label class="collapse" for="c-38371744">[-]</label><label class="expand" for="c-38371744">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;shorts&#x2F;ZN-NbdFwfNQ" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.youtube.com&#x2F;shorts&#x2F;ZN-NbdFwfNQ</a><p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=3WWy98ylLT4" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=3WWy98ylLT4</a><p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;shorts&#x2F;1vqOjYWEF84" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.youtube.com&#x2F;shorts&#x2F;1vqOjYWEF84</a><p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;shorts&#x2F;jOIb9QbrhZ8" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.youtube.com&#x2F;shorts&#x2F;jOIb9QbrhZ8</a><p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;shorts&#x2F;C3F_YI84TXA" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.youtube.com&#x2F;shorts&#x2F;C3F_YI84TXA</a><p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;shorts&#x2F;4IqJHozY4F0" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.youtube.com&#x2F;shorts&#x2F;4IqJHozY4F0</a><p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;shorts&#x2F;h3OmBLlm5-g" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.youtube.com&#x2F;shorts&#x2F;h3OmBLlm5-g</a><p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;shorts&#x2F;ZT7tuIgSDRk" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.youtube.com&#x2F;shorts&#x2F;ZT7tuIgSDRk</a><p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;shorts&#x2F;WnUYbsOMyvs" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.youtube.com&#x2F;shorts&#x2F;WnUYbsOMyvs</a><p><a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;shorts&#x2F;BKKqX2aMlSg" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.youtube.com&#x2F;shorts&#x2F;BKKqX2aMlSg</a><p>The inconsistencies are what&#x27;s most interesting in these videos in fact</div><br/></div></div><div id="38371802" class="c"><input type="checkbox" id="c-38371802" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#38368561">root</a><span>|</span><a href="#38369431">parent</a><span>|</span><a href="#38371744">prev</a><span>|</span><a href="#38372251">next</a><span>|</span><label class="collapse" for="c-38371802">[-]</label><label class="expand" for="c-38371802">[1 more]</label></div><br/><div class="children"><div class="content">A small percentage of the images are animations. This id (for obvious reasons) particularly common for images used on the catalog pages for animation-related tools and models, but also its not uncommon for (AnimateDiff-based, mostly) animations to be used to demo the output of other models.</div><br/></div></div><div id="38372251" class="c"><input type="checkbox" id="c-38372251" checked=""/><div class="controls bullet"><span class="by">adventured</span><span>|</span><a href="#38368561">root</a><span>|</span><a href="#38369431">parent</a><span>|</span><a href="#38371802">prev</a><span>|</span><a href="#38369819">next</a><span>|</span><label class="collapse" for="c-38372251">[-]</label><label class="expand" for="c-38372251">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;civitai.com&#x2F;images" rel="nofollow noreferrer">https:&#x2F;&#x2F;civitai.com&#x2F;images</a><p>Go there, in the top right of the content area it has two drop-downs: Most Reactions | Filters<p>Under filters, change the media setting to video.<p>Civitai has a notoriously poor layout for finding&#x2F;browsing things unfortunately.</div><br/></div></div></div></div></div></div><div id="38369819" class="c"><input type="checkbox" id="c-38369819" checked=""/><div class="controls bullet"><span class="by">hanniabu</span><span>|</span><a href="#38368561">parent</a><span>|</span><a href="#38368828">prev</a><span>|</span><a href="#38373277">next</a><span>|</span><label class="collapse" for="c-38369819">[-]</label><label class="expand" for="c-38369819">[2 more]</label></div><br/><div class="children"><div class="content">&gt; but the real utility of this will be the temporal consistency<p>The main utility will me misinformation</div><br/></div></div></div></div><div id="38373277" class="c"><input type="checkbox" id="c-38373277" checked=""/><div class="controls bullet"><span class="by">firefoxd</span><span>|</span><a href="#38368561">prev</a><span>|</span><a href="#38368562">next</a><span>|</span><label class="collapse" for="c-38373277">[-]</label><label class="expand" for="c-38373277">[6 more]</label></div><br/><div class="children"><div class="content">I understand the magnitude of innovation that&#x27;s going on here. But still feel like we are generating these videos with both hands tied behind our backs. In other words, it&#x27;s nearly impossible to edit the videos in this constraints. (Imagine trying to edit the blue Jays to get the perfect view).<p>Since videos are rarely consumed raw, what if this becomes a pipeline in Blender instead? (Blender the 3d software). Now the video becomes a complete scene with all the key elements of the text input animated. You have your textures, you have your animation, you have your camera, you have all the objects in place. We can even have the render engine in the pipeline to increase the speed of video generation.<p>It may sound like I&#x27;m complaining, but I&#x27;m just ask making a feature request...</div><br/><div id="38374724" class="c"><input type="checkbox" id="c-38374724" checked=""/><div class="controls bullet"><span class="by">jwoodbridge</span><span>|</span><a href="#38373277">parent</a><span>|</span><a href="#38373396">next</a><span>|</span><label class="collapse" for="c-38374724">[-]</label><label class="expand" for="c-38374724">[1 more]</label></div><br/><div class="children"><div class="content">we&#x27;re working on this - dream3d.com</div><br/></div></div><div id="38373396" class="c"><input type="checkbox" id="c-38373396" checked=""/><div class="controls bullet"><span class="by">huytersd</span><span>|</span><a href="#38373277">parent</a><span>|</span><a href="#38374724">prev</a><span>|</span><a href="#38368562">next</a><span>|</span><label class="collapse" for="c-38373396">[-]</label><label class="expand" for="c-38373396">[4 more]</label></div><br/><div class="children"><div class="content">What would solve all these issues is full generation of 3D models that we hopefully get a chance to see over the next decade. I’ve been advocating for a solid LiDAR camera on the iPhone so there is a lot of training data for these LLMs.</div><br/><div id="38373740" class="c"><input type="checkbox" id="c-38373740" checked=""/><div class="controls bullet"><span class="by">ricardobeat</span><span>|</span><a href="#38373277">root</a><span>|</span><a href="#38373396">parent</a><span>|</span><a href="#38368562">next</a><span>|</span><label class="collapse" for="c-38373740">[-]</label><label class="expand" for="c-38373740">[3 more]</label></div><br/><div class="children"><div class="content">&gt; I’ve been advocating for a solid LiDAR camera on the iPhone<p>What do you mean by “advocating”? The iPhone has had a LiDAR camera since 2020.</div><br/><div id="38373849" class="c"><input type="checkbox" id="c-38373849" checked=""/><div class="controls bullet"><span class="by">xvector</span><span>|</span><a href="#38373277">root</a><span>|</span><a href="#38373740">parent</a><span>|</span><a href="#38368562">next</a><span>|</span><label class="collapse" for="c-38373849">[-]</label><label class="expand" for="c-38373849">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s probably why they qualified with &quot;solid&quot;, the iPhone&#x27;s LiDAR camera is quite terrible.</div><br/><div id="38374611" class="c"><input type="checkbox" id="c-38374611" checked=""/><div class="controls bullet"><span class="by">huytersd</span><span>|</span><a href="#38373277">root</a><span>|</span><a href="#38373849">parent</a><span>|</span><a href="#38368562">next</a><span>|</span><label class="collapse" for="c-38374611">[-]</label><label class="expand" for="c-38374611">[1 more]</label></div><br/><div class="children"><div class="content">Yes, exactly.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38368562" class="c"><input type="checkbox" id="c-38368562" checked=""/><div class="controls bullet"><span class="by">ericpauley</span><span>|</span><a href="#38373277">prev</a><span>|</span><a href="#38372183">next</a><span>|</span><label class="collapse" for="c-38368562">[-]</label><label class="expand" for="c-38368562">[20 more]</label></div><br/><div class="children"><div class="content">I&#x27;m still puzzled as to how these &quot;non-commercial&quot; model licenses are supposed to be enforceable. Software licenses govern the redistribution of the <i>software</i>, not products produced with it. An image isn&#x27;t GPL&#x27;d because it was produced with GIMP.</div><br/><div id="38368858" class="c"><input type="checkbox" id="c-38368858" checked=""/><div class="controls bullet"><span class="by">yorwba</span><span>|</span><a href="#38368562">parent</a><span>|</span><a href="#38370889">next</a><span>|</span><label class="collapse" for="c-38368858">[-]</label><label class="expand" for="c-38368858">[5 more]</label></div><br/><div class="children"><div class="content">The license is a contract that allows you to use the software provided you fulfill some conditions. If you do not fulfill the conditions, you have no <i>right</i> to a <i>copy</i> of the software and can be sued. This enforcement mechanism is the same whether the conditions are that you include source code with copies you redistribute, or that you may only use it for evil, or that you must pay a monthly fee. Of course this enforcement mechanism may turn out to be ineffective if it&#x27;s hard to discover that you&#x27;re violating the conditions.</div><br/><div id="38369175" class="c"><input type="checkbox" id="c-38369175" checked=""/><div class="controls bullet"><span class="by">comex</span><span>|</span><a href="#38368562">root</a><span>|</span><a href="#38368858">parent</a><span>|</span><a href="#38370889">next</a><span>|</span><label class="collapse" for="c-38369175">[-]</label><label class="expand" for="c-38369175">[4 more]</label></div><br/><div class="children"><div class="content">It also somewhat depends on open legal questions like whether models are copyrightable and, if so, whether model outputs are derivative works of the model.  Suppose that models are not copyrightable, due to their not being the product of human creativity (this is debatable).  Then the creator can still require people to agree to contractual terms before downloading the model from them, presumably including the usage limitations as well as an agreement not to redistribute the model to anyone else who does not also agree.  Agreement can happen explicitly by pressing a button, or potentially implicitly just by downloading the model from them, if the terms are clearly disclosed beforehand.  But if someone decides on their own (not induced by you in any way) to violate the contract by uploading it somewhere else, and you passively download it from there, then you may be in the clear.</div><br/><div id="38370720" class="c"><input type="checkbox" id="c-38370720" checked=""/><div class="controls bullet"><span class="by">ronsor</span><span>|</span><a href="#38368562">root</a><span>|</span><a href="#38369175">parent</a><span>|</span><a href="#38370889">next</a><span>|</span><label class="collapse" for="c-38370720">[-]</label><label class="expand" for="c-38370720">[3 more]</label></div><br/><div class="children"><div class="content">&gt; Then the creator can still require people to agree to contractual terms before downloading the model from them, presumably including the usage limitations as well as an agreement not to redistribute the model to anyone else who does not also agree.<p>I don&#x27;t think it&#x27;s possible to invent copyright-like rights.</div><br/><div id="38371839" class="c"><input type="checkbox" id="c-38371839" checked=""/><div class="controls bullet"><span class="by">yorwba</span><span>|</span><a href="#38368562">root</a><span>|</span><a href="#38370720">parent</a><span>|</span><a href="#38370889">next</a><span>|</span><label class="collapse" for="c-38371839">[-]</label><label class="expand" for="c-38371839">[2 more]</label></div><br/><div class="children"><div class="content">Why not? Two willing parties can agree to bind themselves to all kinds of obligations in a contract as long as they&#x27;re not explicitly illegal.<p>Copyleft is an example of someone successfully inventing a copyright-like right by bootstrapping off existing copyright with a specially engineered contract.</div><br/><div id="38373979" class="c"><input type="checkbox" id="c-38373979" checked=""/><div class="controls bullet"><span class="by">frognumber</span><span>|</span><a href="#38368562">root</a><span>|</span><a href="#38371839">parent</a><span>|</span><a href="#38370889">next</a><span>|</span><label class="collapse" for="c-38373979">[-]</label><label class="expand" for="c-38373979">[1 more]</label></div><br/><div class="children"><div class="content">There are a few problems:<p>1) You and I invent our own private &quot;copyright&quot; for data (which is not copyrightable)<p>2) Everything is fine until my wife walks up to my computer and makes a copy of the data. She&#x27;s not bound by our private &quot;copyright.&quot; She doesn&#x27;t even know it exists, and shares the data with her bestie.<p>And... our private pseudo-copyright is dead.<p>Also: Licenses are not the same as contracts. There are times when something can be both, one, or the other. But there are a lot of limits on how far they reach. The output of a program is rarely copyrightable by the author (as opposed to the user).</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38370889" class="c"><input type="checkbox" id="c-38370889" checked=""/><div class="controls bullet"><span class="by">SXX</span><span>|</span><a href="#38368562">parent</a><span>|</span><a href="#38368858">prev</a><span>|</span><a href="#38370094">next</a><span>|</span><label class="collapse" for="c-38370889">[-]</label><label class="expand" for="c-38370889">[2 more]</label></div><br/><div class="children"><div class="content">It doesn&#x27;t have to be enforceable. This licensing model works exactly the same as Microsoft Windows licensing or WinRAR licensing. Lots and lots of people have pirated Windows or just buy some cheap keys off Ebay, but no one of them in their sane mind would use anything like that at their company.<p>The same way you can easily violate any &quot;non-commercial&quot; clauses of models like this one as private person or as some tiny startup, but company that decide to use them for their business will more likely just go and pay.<p>So it&#x27;s possible to ignore license, but legal and financial risks are not worth it for businesses.</div><br/><div id="38372613" class="c"><input type="checkbox" id="c-38372613" checked=""/><div class="controls bullet"><span class="by">taberiand</span><span>|</span><a href="#38368562">root</a><span>|</span><a href="#38370889">parent</a><span>|</span><a href="#38370094">next</a><span>|</span><label class="collapse" for="c-38372613">[-]</label><label class="expand" for="c-38372613">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve heard companies also intentionally do not go after individuals pirating software e.g., Adobe Photoshop - it benefits them to have students pirate and skill up on their software and then enter companies that buy Photoshop because their employees know it, over locking down and having those students, and then the businesses, switch to open source.</div><br/></div></div></div></div><div id="38370094" class="c"><input type="checkbox" id="c-38370094" checked=""/><div class="controls bullet"><span class="by">kmeisthax</span><span>|</span><a href="#38368562">parent</a><span>|</span><a href="#38370889">prev</a><span>|</span><a href="#38368623">next</a><span>|</span><label class="collapse" for="c-38370094">[-]</label><label class="expand" for="c-38370094">[3 more]</label></div><br/><div class="children"><div class="content">So, there&#x27;s a few different things interacting here that are a little confusing.<p>First off, you have copyright law, which grants monopolies on the act of copying to the creators of the original. In order to legally make use of that work you need to either have permission to do so (a license), or you need to own a copy of the work that was made by someone with permission to make and sell copies (a sale). For the purposes of computer software, you will almost always get rights to the software through a license and <i>not</i> a sale. In fact, there is an argument that usage of computer software requires a license and that a sale wouldn&#x27;t be enough because you wouldn&#x27;t have permission to load it into RAM[0].<p>Licenses are, at least under US law, contracts. These are Turing-complete priestly rites written in a special register of English that legally bind people to do or not do certain things. A license can grant rights, or, confusingly, take them away. For example, you could write a license that takes away your fair use rights[1], and courts will actually respect that. So you can also have a license that says you&#x27;re only allowed to use software for specific listed purposes but not others.<p>In copyright you also have the notion of a derivative work. This was invented whole-cloth by the US Supreme Court, who needed a reason to prosecute someone for making a SSSniperWolf-tier abridgement[2] of someone else&#x27;s George Washington biography. Normal copyright infringement is evidenced by substantial similarity and access: i.e. you saw the original, then you made something that&#x27;s nearly identical, ergo infringement. The law regarding derivative works goes a step further and counts hypothetical works that an author <i>might</i> make - like sequels, translations, remakes, abridgements, and so on - as requiring permission in order to make. Without that permission, you don&#x27;t own anything and your work has no right to exist.<p>The GPL is the anticopyright &quot;judo move&quot;, invented by a really ornery computer programmer that was angry about not being able to fix their printer drivers. It disclaims <i>almost</i> the entire copyright monopoly, but it leaves behind one license restriction, called a &quot;copyleft&quot;: any derivative work must be licensed under the GPL. So if you modify the software and distribute it, you have to distribute your changes under GPL terms, thus locking the software in the commons.<p>Images made with software are not derivative works of the software, nor do they contain a substantially similar copy of the software in them. Ergo, the GPL copyleft does not trip. In fact, <i>even if it did trip</i>, your image is still not a derivative work of the software, so you don&#x27;t lose ownership over the image because you didn&#x27;t get permission. This also applies to model licenses on AI software, insamuch as the AI companies don&#x27;t own their training data[3].<p>However, there&#x27;s still something that licenses can take away: your right to use the software. If you use the model for &quot;commercial&quot; purposes - whatever those would be - you&#x27;d be in breach of the license. What happens next is also determined by the license. It could be written to take away your noncommercial rights if you breach the license, or it could preserve them. In either case, however, the primary enforcement mechanism would be a court of law, and courts usually award money damages. If particularly justified, they <i>could</i> demand you destroy all copies of the software.<p>If it went to SCOTUS (unlikely), they might even decide that images made by software are derivative works of the software after all, just to spite you. The Betamax case said that advertising a copying device with potentially infringing scenarios was fine as long as that device could be used in a non-infringing manner, but then the Grokster case said it was &quot;inducement&quot; and overturned it. Static, unchanging rules are ultimately a polite fiction, and the law can change behind your back if the people in power want or need it to. This is why you don&#x27;t talk about the law in terms of something being legal or illegal, you talk about it in terms of risk.<p>[0] Yes, this is a real argument that courts have actually made. Or at least the Ninth Circuit.<p>The actual facts of the case are even more insane - basically a company trying to sue former employees for fixing it&#x27;s customers computers. Imagine if Apple sued Louis Rossman for pirating macOS every time he turned on a customer laptop. The only reason why they <i>can&#x27;t</i> is because Congress actually created a special exemption for computer repair and made it part of the DMCA.<p>[1] For example, one of the things you agree to when you buy Oracle database software is to give up your right to benchmark the software. I&#x27;m serious! The tech industry is evil and needs to burn down to the ground!<p>[2] They took 300 pages worth of material from 12 books and copied it into a separate, 2 volume work.<p>[3] Whether or not copyright on the training data images flows through to make generated images a derivative work is a separate legal question in active litigation.</div><br/><div id="38370270" class="c"><input type="checkbox" id="c-38370270" checked=""/><div class="controls bullet"><span class="by">dragonwriter</span><span>|</span><a href="#38368562">root</a><span>|</span><a href="#38370094">parent</a><span>|</span><a href="#38373490">next</a><span>|</span><label class="collapse" for="c-38370270">[-]</label><label class="expand" for="c-38370270">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Licenses are, at least under US law, contracts<p>Not necessarily; gratuitous licenses are not contracts. Licenses which happen to also meet the requirements for contracts (or be embedded in agreements that do) are contracts or components of contracts, but that&#x27;s not all licenses.</div><br/></div></div><div id="38373490" class="c"><input type="checkbox" id="c-38373490" checked=""/><div class="controls bullet"><span class="by">rperez333</span><span>|</span><a href="#38368562">root</a><span>|</span><a href="#38370094">parent</a><span>|</span><a href="#38370270">prev</a><span>|</span><a href="#38368623">next</a><span>|</span><label class="collapse" for="c-38373490">[-]</label><label class="expand" for="c-38373490">[1 more]</label></div><br/><div class="children"><div class="content">If a company train the model from scratch, on its own dataset, could the resulting model be used commercially?</div><br/></div></div></div></div><div id="38368623" class="c"><input type="checkbox" id="c-38368623" checked=""/><div class="controls bullet"><span class="by">cubefox</span><span>|</span><a href="#38368562">parent</a><span>|</span><a href="#38370094">prev</a><span>|</span><a href="#38373330">next</a><span>|</span><label class="collapse" for="c-38368623">[-]</label><label class="expand" for="c-38368623">[3 more]</label></div><br/><div class="children"><div class="content">Nobody claimed otherwise?</div><br/><div id="38368790" class="c"><input type="checkbox" id="c-38368790" checked=""/><div class="controls bullet"><span class="by">not2b</span><span>|</span><a href="#38368562">root</a><span>|</span><a href="#38368623">parent</a><span>|</span><a href="#38368874">next</a><span>|</span><label class="collapse" for="c-38368790">[-]</label><label class="expand" for="c-38368790">[1 more]</label></div><br/><div class="children"><div class="content">There are sites that make Stable Diffusion-derived models available, along with GPU resources, and they sell the service of generating images from the models. The company isn&#x27;t permitting that use, and it seems that they could find violators and shut them down.</div><br/></div></div><div id="38368874" class="c"><input type="checkbox" id="c-38368874" checked=""/><div class="controls bullet"><span class="by">littlethoughts</span><span>|</span><a href="#38368562">root</a><span>|</span><a href="#38368623">parent</a><span>|</span><a href="#38368790">prev</a><span>|</span><a href="#38373330">next</a><span>|</span><label class="collapse" for="c-38368874">[-]</label><label class="expand" for="c-38368874">[1 more]</label></div><br/><div class="children"><div class="content">Fantasy.ai was subject to controversy for attempting to license models.</div><br/></div></div></div></div><div id="38373330" class="c"><input type="checkbox" id="c-38373330" checked=""/><div class="controls bullet"><span class="by">stevage</span><span>|</span><a href="#38368562">parent</a><span>|</span><a href="#38368623">prev</a><span>|</span><a href="#38368901">next</a><span>|</span><label class="collapse" for="c-38373330">[-]</label><label class="expand" for="c-38373330">[1 more]</label></div><br/><div class="children"><div class="content">A software licence can definitely govern who can use it and what they can do with it.<p>&gt; An image isn&#x27;t GPL&#x27;d because it was produced with GIMP.<p>That&#x27;s because of how the GPL is written, not because of some limitation of software licences.</div><br/></div></div><div id="38368901" class="c"><input type="checkbox" id="c-38368901" checked=""/><div class="controls bullet"><span class="by">dist-epoch</span><span>|</span><a href="#38368562">parent</a><span>|</span><a href="#38373330">prev</a><span>|</span><a href="#38368813">next</a><span>|</span><label class="collapse" for="c-38368901">[-]</label><label class="expand" for="c-38368901">[4 more]</label></div><br/><div class="children"><div class="content">Visual Studio Community (and many other products) only allows &quot;non-commercial&quot; usage. Sounds like it limits what you can do with what you produce with it.<p>At the end of the day, a license is a legal contract. If you agree that an image which you produce with some software will be GPL&#x27;ed, it&#x27;s enforceable.<p>As an example, see the Creative Commons license, ShareAlike clause:<p>&gt; If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.</div><br/><div id="38369337" class="c"><input type="checkbox" id="c-38369337" checked=""/><div class="controls bullet"><span class="by">blibble</span><span>|</span><a href="#38368562">root</a><span>|</span><a href="#38368901">parent</a><span>|</span><a href="#38371521">next</a><span>|</span><label class="collapse" for="c-38369337">[-]</label><label class="expand" for="c-38369337">[1 more]</label></div><br/><div class="children"><div class="content">&gt; At the end of the day, a license is a legal contract. If you agree that an image which you produce with some software will be GPL&#x27;ed, it&#x27;s enforceable.<p>you can put whatever you want in a contract, doesn&#x27;t mean it&#x27;s enforceable</div><br/></div></div><div id="38371521" class="c"><input type="checkbox" id="c-38371521" checked=""/><div class="controls bullet"><span class="by">antonyt</span><span>|</span><a href="#38368562">root</a><span>|</span><a href="#38368901">parent</a><span>|</span><a href="#38369337">prev</a><span>|</span><a href="#38368813">next</a><span>|</span><label class="collapse" for="c-38371521">[-]</label><label class="expand" for="c-38371521">[2 more]</label></div><br/><div class="children"><div class="content">Do you have link for the VS Community terms you&#x27;re describing? What I&#x27;ve found is directly contradictory: &quot;Any individual developer can use Visual Studio Community to create their own free or paid apps.&quot; From <a href="https:&#x2F;&#x2F;visualstudio.microsoft.com&#x2F;vs&#x2F;community&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;visualstudio.microsoft.com&#x2F;vs&#x2F;community&#x2F;</a></div><br/><div id="38371672" class="c"><input type="checkbox" id="c-38371672" checked=""/><div class="controls bullet"><span class="by">dist-epoch</span><span>|</span><a href="#38368562">root</a><span>|</span><a href="#38371521">parent</a><span>|</span><a href="#38368813">next</a><span>|</span><label class="collapse" for="c-38371672">[-]</label><label class="expand" for="c-38371672">[1 more]</label></div><br/><div class="children"><div class="content">Enterprise organizations are not allowed to use VS Community for commercial purposes:<p>&gt; <i>In enterprise organizations (meaning those with &gt;250 PCs or &gt;$1 Million US Dollars in annual revenue), no use is permitted beyond the open source, academic research, and classroom learning environment scenarios described above.</i></div><br/></div></div></div></div></div></div><div id="38368813" class="c"><input type="checkbox" id="c-38368813" checked=""/><div class="controls bullet"><span class="by">Der_Einzige</span><span>|</span><a href="#38368562">parent</a><span>|</span><a href="#38368901">prev</a><span>|</span><a href="#38372183">next</a><span>|</span><label class="collapse" for="c-38368813">[-]</label><label class="expand" for="c-38368813">[1 more]</label></div><br/><div class="children"><div class="content">They&#x27;re not enforceable.</div><br/></div></div></div></div><div id="38372183" class="c"><input type="checkbox" id="c-38372183" checked=""/><div class="controls bullet"><span class="by">shaileshm</span><span>|</span><a href="#38368562">prev</a><span>|</span><a href="#38368689">next</a><span>|</span><label class="collapse" for="c-38372183">[-]</label><label class="expand" for="c-38372183">[1 more]</label></div><br/><div class="children"><div class="content">This field moves so fast. Blink an eye and there is another new paper. This is really cool and the learning speed of us humans is insane! Really excited on using it for downstream tasks! I wonder how easy it is to integrate animatediff with this model?<p>Also, can someone benchmark it on m3 devices? It would be cool to see if it is worth getting on to run these diffusion inferences and development. If m3 pro can allow finetuning it would be amazing to use it on downstream tasks!</div><br/></div></div><div id="38368689" class="c"><input type="checkbox" id="c-38368689" checked=""/><div class="controls bullet"><span class="by">accrual</span><span>|</span><a href="#38372183">prev</a><span>|</span><a href="#38373953">next</a><span>|</span><label class="collapse" for="c-38368689">[-]</label><label class="expand" for="c-38368689">[2 more]</label></div><br/><div class="children"><div class="content">Fascinating leap forward.<p>It makes me think of the difference between ancestral and non-ancestral samplers, e.g. Euler vs Euler Ancestral. With Euler, the output is somewhat deterministic and doesn&#x27;t vary with increasing sampling steps, but with Ancestral, noise is added to each step which creates more variety but is more random&#x2F;stochastic.<p>I assume to create video, the sampler needs to lean heavily on the previous frame while injecting some kind of sub-prompt, like rotate &lt;object&gt; to the left by 5 degrees, etc. I like the phrase another commenter used, &quot;temporal consistency&quot;.<p>Edit: Indeed the special sauce is &quot;temporal layers&quot;. [0]<p>&gt; Recently, latent diffusion models trained for 2D image synthesis have been turned into generative video models by inserting temporal layers and finetuning them on small, high-quality video datasets<p>[0] <a href="https:&#x2F;&#x2F;stability.ai&#x2F;research&#x2F;stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets" rel="nofollow noreferrer">https:&#x2F;&#x2F;stability.ai&#x2F;research&#x2F;stable-video-diffusion-scaling...</a></div><br/><div id="38369082" class="c"><input type="checkbox" id="c-38369082" checked=""/><div class="controls bullet"><span class="by">adventured</span><span>|</span><a href="#38368689">parent</a><span>|</span><a href="#38373953">next</a><span>|</span><label class="collapse" for="c-38369082">[-]</label><label class="expand" for="c-38369082">[1 more]</label></div><br/><div class="children"><div class="content">The hardest problem the Stable Diffusion community has dealt with in terms of quality has been in the video space, largely in relation to the consistency between frames. It&#x27;s probably the most commonly discussed problem for example on r&#x2F;stablediffusion. Temporal consistency is the popular term for that.<p>So this example was posted an hour ago, and it&#x27;s jumping all over the place frame to frame (somewhat weak temporal consistency). The author appears to have used pretty straight-forward text2img + Animatediff:<p><a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;StableDiffusion&#x2F;comments&#x2F;180no09&#x2F;on_the_swing_animatediff_text2image_only&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;StableDiffusion&#x2F;comments&#x2F;180no09&#x2F;on...</a><p>Fixing that frame to frame jitter related to animation is probably the most in-demand thing around Stable Diffusion right now.<p>Animatediff motion painting made a splash the other day:<p><a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;StableDiffusion&#x2F;comments&#x2F;17xnqn7&#x2F;roll_your_own_motion_brush_with_animatediff_and&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;StableDiffusion&#x2F;comments&#x2F;17xnqn7&#x2F;ro...</a><p>It&#x27;s definitely an exciting time around SD + animation. You can see how close it is to reaching the next level of generation.</div><br/></div></div></div></div><div id="38373953" class="c"><input type="checkbox" id="c-38373953" checked=""/><div class="controls bullet"><span class="by">rbhuta</span><span>|</span><a href="#38368689">prev</a><span>|</span><a href="#38368712">next</a><span>|</span><label class="collapse" for="c-38373953">[-]</label><label class="expand" for="c-38373953">[2 more]</label></div><br/><div class="children"><div class="content">VRAM requirements are big for this launch.
We&#x27;re hosting this for free at <a href="https:&#x2F;&#x2F;app.decoherence.co&#x2F;stablevideo">https:&#x2F;&#x2F;app.decoherence.co&#x2F;stablevideo</a>. 
Disclaimer: Google log-in required to help us reduce spam.</div><br/><div id="38374414" class="c"><input type="checkbox" id="c-38374414" checked=""/><div class="controls bullet"><span class="by">xena</span><span>|</span><a href="#38373953">parent</a><span>|</span><a href="#38368712">next</a><span>|</span><label class="collapse" for="c-38374414">[-]</label><label class="expand" for="c-38374414">[1 more]</label></div><br/><div class="children"><div class="content">How big is big?</div><br/></div></div></div></div><div id="38368712" class="c"><input type="checkbox" id="c-38368712" checked=""/><div class="controls bullet"><span class="by">awongh</span><span>|</span><a href="#38373953">prev</a><span>|</span><a href="#38376353">next</a><span>|</span><label class="collapse" for="c-38368712">[-]</label><label class="expand" for="c-38368712">[3 more]</label></div><br/><div class="children"><div class="content">It makes sense that they had to take out all of the cuts and fades from the training data to improve results.<p>I’m the background section of the research paper they mention “temporal convolution layers”, can anyone explain what that is? What sort of training data is the input to represent temporal states between images that make up a video? Or does that mean something else?</div><br/><div id="38371785" class="c"><input type="checkbox" id="c-38371785" checked=""/><div class="controls bullet"><span class="by">flaghacker</span><span>|</span><a href="#38368712">parent</a><span>|</span><a href="#38370161">next</a><span>|</span><label class="collapse" for="c-38371785">[-]</label><label class="expand" for="c-38371785">[1 more]</label></div><br/><div class="children"><div class="content">It means that instead of (only) doing convolution in spatial dimensions, it also(&#x2F;instead) happens in the temporal dimension.<p>A good resource for the &quot;instead&quot; case: <a href="https:&#x2F;&#x2F;unit8.com&#x2F;resources&#x2F;temporal-convolutional-networks-and-forecasting&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;unit8.com&#x2F;resources&#x2F;temporal-convolutional-networks-...</a><p>The &quot;also&quot; case is an example of 3D convolution, an example of a paper that uses it: <a href="https:&#x2F;&#x2F;www.cv-foundation.org&#x2F;openaccess&#x2F;content_iccv_2015&#x2F;papers&#x2F;Tran_Learning_Spatiotemporal_Features_ICCV_2015_paper.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.cv-foundation.org&#x2F;openaccess&#x2F;content_iccv_2015&#x2F;p...</a></div><br/></div></div><div id="38370161" class="c"><input type="checkbox" id="c-38370161" checked=""/><div class="controls bullet"><span class="by">machinekob</span><span>|</span><a href="#38368712">parent</a><span>|</span><a href="#38371785">prev</a><span>|</span><a href="#38376353">next</a><span>|</span><label class="collapse" for="c-38370161">[-]</label><label class="expand" for="c-38370161">[1 more]</label></div><br/><div class="children"><div class="content">I would assume is something similar to joining multiple frames&#x2F;attentions? in channel dimension and then moving values inside so convolution will have access to some channels from other video frames.<p>I was working on similar idea few years ago using this paper as reference and it was working extremely well for consistency also helping with flicker.
<a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1811.08383" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1811.08383</a></div><br/></div></div></div></div><div id="38376353" class="c"><input type="checkbox" id="c-38376353" checked=""/><div class="controls bullet"><span class="by">renlo</span><span>|</span><a href="#38368712">prev</a><span>|</span><a href="#38368810">next</a><span>|</span><label class="collapse" for="c-38376353">[-]</label><label class="expand" for="c-38376353">[1 more]</label></div><br/><div class="children"><div class="content">How much longer will it be until we can play &quot;video games&quot; which consist of user-input streamed to an AI that generates video output and streams it to the player&#x27;s screen?</div><br/></div></div><div id="38368810" class="c"><input type="checkbox" id="c-38368810" checked=""/><div class="controls bullet"><span class="by">epiccoleman</span><span>|</span><a href="#38376353">prev</a><span>|</span><a href="#38375141">next</a><span>|</span><label class="collapse" for="c-38368810">[-]</label><label class="expand" for="c-38368810">[1 more]</label></div><br/><div class="children"><div class="content">This is really, really cool. A few months ago I was playing with some of the &quot;video&quot; generation models on Replicate, and I got some really neat results[1], but it was very clear that the resulting videos were made from prompting each &quot;frame&quot; with the previous one. This looks like it can actually figure out how to make something that has a higher level context to it.<p>It&#x27;s crazy to see this level of progress in just a bit over half a year.<p>[1]: <a href="https:&#x2F;&#x2F;epiccoleman.com&#x2F;posts&#x2F;2023-03-05-deforum-stable-diffusion" rel="nofollow noreferrer">https:&#x2F;&#x2F;epiccoleman.com&#x2F;posts&#x2F;2023-03-05-deforum-stable-diff...</a></div><br/></div></div><div id="38375141" class="c"><input type="checkbox" id="c-38375141" checked=""/><div class="controls bullet"><span class="by">keiferski</span><span>|</span><a href="#38368810">prev</a><span>|</span><a href="#38375900">next</a><span>|</span><label class="collapse" for="c-38375141">[-]</label><label class="expand" for="c-38375141">[1 more]</label></div><br/><div class="children"><div class="content">Question for anyone more familiar with this space: are there any high-quality tools which take an image and make it into a short video? For example, an image of a tree becomes a video of a tree swaying in the wind.<p>I have googled for it but mostly just get low quality web tools.</div><br/></div></div><div id="38375900" class="c"><input type="checkbox" id="c-38375900" checked=""/><div class="controls bullet"><span class="by">devdiary</span><span>|</span><a href="#38375141">prev</a><span>|</span><a href="#38368752">next</a><span>|</span><label class="collapse" for="c-38375900">[-]</label><label class="expand" for="c-38375900">[1 more]</label></div><br/><div class="children"><div class="content">A default glitch effect in the video can make the distortions a &quot;feature not a bug&quot;</div><br/></div></div><div id="38368752" class="c"><input type="checkbox" id="c-38368752" checked=""/><div class="controls bullet"><span class="by">spaceman_2020</span><span>|</span><a href="#38375900">prev</a><span>|</span><a href="#38369332">next</a><span>|</span><label class="collapse" for="c-38368752">[-]</label><label class="expand" for="c-38368752">[17 more]</label></div><br/><div class="children"><div class="content">A seemingly off topic question, but with enough compute and optimization, could you eventually simulate “reality”?<p>Like, at this point, what are the technical counters to the assertion that our world is a simulation?</div><br/><div id="38369122" class="c"><input type="checkbox" id="c-38369122" checked=""/><div class="controls bullet"><span class="by">KineticLensman</span><span>|</span><a href="#38368752">parent</a><span>|</span><a href="#38368984">next</a><span>|</span><label class="collapse" for="c-38369122">[-]</label><label class="expand" for="c-38369122">[7 more]</label></div><br/><div class="children"><div class="content">(disclaimer: worked in the sim industry for 25 years, still active in terms of physics-based rendering).<p>First off, there are zero technical proofs that we are in a sim, just a number of philosophical arguments.<p>In practical terms, we cannot yet simulate a single human cell at the molecular level, given the massive number of interactions that occur every microsecond. Simulating our entire universe is not technically possible within the lifetime of our universe, according to our current understanding of computation and physics. You either have to assume that ‘the sim’ is very narrowly focussed in scope and fidelity, and &#x2F; or that the outer universe that hosts ‘the sim’ has laws of physics that are essentially magic from our perspective. In which case the simulation hypothesis is essentially a religious argument, where the creator typed &#x27;let there be light&#x27; into his computer. If there isn&#x27;t such a creator, the sim hypothesis &#x27;merely&#x27; suggests that our universe, at its lowest levels, looks somewhat computational, which is an entirely different argument.</div><br/><div id="38369481" class="c"><input type="checkbox" id="c-38369481" checked=""/><div class="controls bullet"><span class="by">freedomben</span><span>|</span><a href="#38368752">root</a><span>|</span><a href="#38369122">parent</a><span>|</span><a href="#38373067">next</a><span>|</span><label class="collapse" for="c-38369481">[-]</label><label class="expand" for="c-38369481">[5 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think you would need to simulate the entire universe, just enough of it that the consciousness receiving sense data can&#x27;t encounter any missing info or &quot;glitches&quot; in the metaphorical matrix.  Still hard of course, but substantially less compute intensive than every molecule in the universe.</div><br/><div id="38370138" class="c"><input type="checkbox" id="c-38370138" checked=""/><div class="controls bullet"><span class="by">gcanyon</span><span>|</span><a href="#38368752">root</a><span>|</span><a href="#38369481">parent</a><span>|</span><a href="#38370212">next</a><span>|</span><label class="collapse" for="c-38370138">[-]</label><label class="expand" for="c-38370138">[1 more]</label></div><br/><div class="children"><div class="content">And if you’re in charge of the simulation, you get to decide how many “consciousnesses” there are, constraining them to be within your available compute. Maybe that’s ~8 billion — maybe it’s 1. Yeah, I’m feeling pretty Boltzmann-ish right now…</div><br/></div></div><div id="38370212" class="c"><input type="checkbox" id="c-38370212" checked=""/><div class="controls bullet"><span class="by">KineticLensman</span><span>|</span><a href="#38368752">root</a><span>|</span><a href="#38369481">parent</a><span>|</span><a href="#38370138">prev</a><span>|</span><a href="#38370585">next</a><span>|</span><label class="collapse" for="c-38370212">[-]</label><label class="expand" for="c-38370212">[1 more]</label></div><br/><div class="children"><div class="content">&gt; but substantially less compute intensive than every molecule in the universe<p>Very true, but to me this view of the universe and one&#x27;s existence within it as a sort of second-rate solipsist bodge isn&#x27;t a satisfyingly profound answer to the question of life the universe and everything.<p>Although put like that it explains quite a lot.<p>[Edit] There is also a sense in which the sim-as-a-focussed-mini-universe view is even less falsifiable, because sim proponents address any doubt about the sim by moving the goal posts to accommodate what they claim is actually achievable by the putative creator&#x2F;hacker on Planet Tharg or similar.</div><br/></div></div><div id="38370585" class="c"><input type="checkbox" id="c-38370585" checked=""/><div class="controls bullet"><span class="by">kaashif</span><span>|</span><a href="#38368752">root</a><span>|</span><a href="#38369481">parent</a><span>|</span><a href="#38370212">prev</a><span>|</span><a href="#38373067">next</a><span>|</span><label class="collapse" for="c-38370585">[-]</label><label class="expand" for="c-38370585">[2 more]</label></div><br/><div class="children"><div class="content">And you don&#x27;t have to simulate it in real time, maybe 1 second here takes years or centuries to simulate outside the simulation. It&#x27;s not like we&#x27;d have any way to tell.</div><br/><div id="38370918" class="c"><input type="checkbox" id="c-38370918" checked=""/><div class="controls bullet"><span class="by">hackerlight</span><span>|</span><a href="#38368752">root</a><span>|</span><a href="#38370585">parent</a><span>|</span><a href="#38373067">next</a><span>|</span><label class="collapse" for="c-38370918">[-]</label><label class="expand" for="c-38370918">[1 more]</label></div><br/><div class="children"><div class="content">These are all open questions in philosophy of mind. Nobody knows what causes consciousness&#x2F;qualia so nobody knows if it&#x27;s substrate dependent or not and therefore nobody knows if it can be simulated in a computer, or if it can nobody knows what type of computer is required for consciousness to be a property of the resulting simulation.</div><br/></div></div></div></div></div></div><div id="38373067" class="c"><input type="checkbox" id="c-38373067" checked=""/><div class="controls bullet"><span class="by">jdaxe</span><span>|</span><a href="#38368752">root</a><span>|</span><a href="#38369122">parent</a><span>|</span><a href="#38369481">prev</a><span>|</span><a href="#38368984">next</a><span>|</span><label class="collapse" for="c-38373067">[-]</label><label class="expand" for="c-38373067">[1 more]</label></div><br/><div class="children"><div class="content">Maybe something like quantum mechanics are an &quot;optimization&quot; of the sim, i.e the sim doesn&#x27;t actually compute the locations, spin etc of subatomic particles but instead just uses probabilities to simulate it. Only when a consciousness decides to look more closely does it retroactively decide what those properties really were.<p>Kind of like how video games won&#x27;t render the full resolution textures when the character is far away or zoomed out.<p>I&#x27;m sure I&#x27;m not the first person to have thought this.</div><br/></div></div></div></div><div id="38368984" class="c"><input type="checkbox" id="c-38368984" checked=""/><div class="controls bullet"><span class="by">tracerbulletx</span><span>|</span><a href="#38368752">parent</a><span>|</span><a href="#38369122">prev</a><span>|</span><a href="#38368907">next</a><span>|</span><label class="collapse" for="c-38368984">[-]</label><label class="expand" for="c-38368984">[3 more]</label></div><br/><div class="children"><div class="content">The brain does simulate reality in the sense that what you experience isn&#x27;t direct sensory input, but more like a dream being generated to predict what it thinks is happening based on conflicting and imperfect sensory input.</div><br/><div id="38369107" class="c"><input type="checkbox" id="c-38369107" checked=""/><div class="controls bullet"><span class="by">accrual</span><span>|</span><a href="#38368752">root</a><span>|</span><a href="#38368984">parent</a><span>|</span><a href="#38369228">next</a><span>|</span><label class="collapse" for="c-38369107">[-]</label><label class="expand" for="c-38369107">[1 more]</label></div><br/><div class="children"><div class="content">To illustrate your point, an easily accessible example of this is how the second hand on clocks appears to freeze for longer than a second when you quickly glance at it. The brain is predicting&#x2F;interpolating what it expects to see, creating the illusion of a delay.<p><a href="https:&#x2F;&#x2F;www.popsci.com&#x2F;how-time-seems-to-stop&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.popsci.com&#x2F;how-time-seems-to-stop&#x2F;</a></div><br/></div></div><div id="38369228" class="c"><input type="checkbox" id="c-38369228" checked=""/><div class="controls bullet"><span class="by">danielbln</span><span>|</span><a href="#38368752">root</a><span>|</span><a href="#38368984">parent</a><span>|</span><a href="#38369107">prev</a><span>|</span><a href="#38368907">next</a><span>|</span><label class="collapse" for="c-38369228">[-]</label><label class="expand" for="c-38369228">[1 more]</label></div><br/><div class="children"><div class="content">Example vision: comes in from the optic nerve warped and upside down and as small patches of high resolution captured by the eyes zigzagging across the visual field (saccades), all of which is assembled and integrated into a coherent field of vision by our trusty old grey blob.</div><br/></div></div></div></div><div id="38368907" class="c"><input type="checkbox" id="c-38368907" checked=""/><div class="controls bullet"><span class="by">2-718-281-828</span><span>|</span><a href="#38368752">parent</a><span>|</span><a href="#38368984">prev</a><span>|</span><a href="#38371023">next</a><span>|</span><label class="collapse" for="c-38368907">[-]</label><label class="expand" for="c-38368907">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Like, at this point, what are the technical counters to the assertion that our world is a simulation?<p>How about this theory is neither verifiable nor falsifiable.</div><br/><div id="38369292" class="c"><input type="checkbox" id="c-38369292" checked=""/><div class="controls bullet"><span class="by">vidarh</span><span>|</span><a href="#38368752">root</a><span>|</span><a href="#38368907">parent</a><span>|</span><a href="#38371023">next</a><span>|</span><label class="collapse" for="c-38369292">[-]</label><label class="expand" for="c-38369292">[1 more]</label></div><br/><div class="children"><div class="content">The <i>general concept</i> is not falsifiable, but many variations might be, or their inverse might be. E.g. the theory that we are <i>not</i> in a simulation would in general be falsifiable by finding an &quot;escape&quot; from a simulation and so showing we are in one (but not finding an escape of course tells us nothing).<p>It&#x27;s not a very useful endeavour to worry about, but it can be fun to speculate about what might give rise to testable hypotheses and what that might tell us about the world.</div><br/></div></div></div></div><div id="38371023" class="c"><input type="checkbox" id="c-38371023" checked=""/><div class="controls bullet"><span class="by">SXX</span><span>|</span><a href="#38368752">parent</a><span>|</span><a href="#38368907">prev</a><span>|</span><a href="#38373362">next</a><span>|</span><label class="collapse" for="c-38371023">[-]</label><label class="expand" for="c-38371023">[1 more]</label></div><br/><div class="children"><div class="content">Actually it was already done by sentdex with GAN Theft Auto:<p><a href="https:&#x2F;&#x2F;youtu.be&#x2F;udPY5rQVoW0" rel="nofollow noreferrer">https:&#x2F;&#x2F;youtu.be&#x2F;udPY5rQVoW0</a><p>To an extent...<p>PS: Video is 2 years old, but still really impressive.</div><br/></div></div><div id="38373362" class="c"><input type="checkbox" id="c-38373362" checked=""/><div class="controls bullet"><span class="by">justanotherjoe</span><span>|</span><a href="#38368752">parent</a><span>|</span><a href="#38371023">prev</a><span>|</span><a href="#38369104">next</a><span>|</span><label class="collapse" for="c-38373362">[-]</label><label class="expand" for="c-38373362">[1 more]</label></div><br/><div class="children"><div class="content">That theory was never meant to be so airtight such that it &#x27;needs&#x27; to be refuted.</div><br/></div></div><div id="38369104" class="c"><input type="checkbox" id="c-38369104" checked=""/><div class="controls bullet"><span class="by">beepbooptheory</span><span>|</span><a href="#38368752">parent</a><span>|</span><a href="#38373362">prev</a><span>|</span><a href="#38368843">next</a><span>|</span><label class="collapse" for="c-38369104">[-]</label><label class="expand" for="c-38369104">[1 more]</label></div><br/><div class="children"><div class="content">Why does it matter? Not trying to dismiss, but truly, what would it mean to you if you could somehow verify the &quot;simulation&quot;?<p>If it <i>would</i> mean something drastic to you, I would be very curious to hear your preexisting existential beliefs&#x2F;commitments.<p>People say this sometimes and its kind of slowly revealed to me that its just a new kind of geocentrism: its not <i>just</i> a simulation people have in mind, but one where earth&#x2F;humans are centered, and the rest of the universe is just for the benefit of &quot;our&quot; part of the simulation.<p>Which is a fine theory I guess, but is also just essentially wanting God to exist with extra steps!</div><br/></div></div><div id="38368843" class="c"><input type="checkbox" id="c-38368843" checked=""/><div class="controls bullet"><span class="by">refulgentis</span><span>|</span><a href="#38368752">parent</a><span>|</span><a href="#38369104">prev</a><span>|</span><a href="#38369332">next</a><span>|</span><label class="collapse" for="c-38368843">[-]</label><label class="expand" for="c-38368843">[1 more]</label></div><br/><div class="children"><div class="content">A little too freshman&#x27;s first bit off a bong for me. There is, of course, substantial differences between video and reality.<p>Let&#x27;s steel-man — you mean 3D VR. Let&#x27;s stipulate there&#x27;s a headset today that renders 3D visually indistinguishable from reality. We&#x27;re still short the other 4 senses<p>Much like faith, there&#x27;s always a way to sort of escape the traps here and say &quot;can you PROVE this is base reality&quot;<p>The general technical argument against &quot;brain in a vat being stimulated&quot; would be the computation expense of doing such, but you can also write that off with the equivalent of foveated rendering but for all senses &#x2F; entities</div><br/></div></div></div></div><div id="38369332" class="c"><input type="checkbox" id="c-38369332" checked=""/><div class="controls bullet"><span class="by">aliljet</span><span>|</span><a href="#38368752">prev</a><span>|</span><a href="#38368704">next</a><span>|</span><label class="collapse" for="c-38369332">[-]</label><label class="expand" for="c-38369332">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been following this space very very closely and the killer feature would be to be able to generate these full featured videos for longer than a few seconds with consistently shaped &quot;characters&quot; (e.g., flowers, and grass, and houses, and cars, actors, etc.). Right now, it&#x27;s not clear to me that this is achieving that objective. This feels like it could be great to create short GIFs, but at what cost?<p>To be clear, this remains wicked, wicked, wicked exciting.</div><br/></div></div><div id="38368704" class="c"><input type="checkbox" id="c-38368704" checked=""/><div class="controls bullet"><span class="by">torginus</span><span>|</span><a href="#38369332">prev</a><span>|</span><a href="#38372248">next</a><span>|</span><label class="collapse" for="c-38368704">[-]</label><label class="expand" for="c-38368704">[3 more]</label></div><br/><div class="children"><div class="content">I admit I&#x27;m ignorant about these model&#x27;s inner workings, but I don&#x27;t understand why text is the chosen input format for these models.<p>It was the same for image generation, where one needed to produce text prompts to create the image, and stuff like img2img and Controlnet that allowed things like controlling poses and inpainting, or having multiple prompts with masks controlling which part of the image is influenced by which prompt.</div><br/><div id="38368811" class="c"><input type="checkbox" id="c-38368811" checked=""/><div class="controls bullet"><span class="by">gorbypark</span><span>|</span><a href="#38368704">parent</a><span>|</span><a href="#38369704">next</a><span>|</span><label class="collapse" for="c-38368811">[-]</label><label class="expand" for="c-38368811">[1 more]</label></div><br/><div class="children"><div class="content">According to the GitHub repo this is an &quot;image-to-video model&quot;.  They tease of an upcoming &quot;text to video&quot; interface on the linked landing page, though.  My guess is that interface will use a text-to-image model and then feed that into the image-to-video model.</div><br/></div></div><div id="38369704" class="c"><input type="checkbox" id="c-38369704" checked=""/><div class="controls bullet"><span class="by">pizzafeelsright</span><span>|</span><a href="#38368704">parent</a><span>|</span><a href="#38368811">prev</a><span>|</span><a href="#38372248">next</a><span>|</span><label class="collapse" for="c-38369704">[-]</label><label class="expand" for="c-38369704">[1 more]</label></div><br/><div class="children"><div class="content">Imago Deo?  The Word is what is spoken when we create.<p>The input eventually becomes meanings mapped to reality.</div><br/></div></div></div></div><div id="38372248" class="c"><input type="checkbox" id="c-38372248" checked=""/><div class="controls bullet"><span class="by">nuclearsugar</span><span>|</span><a href="#38368704">prev</a><span>|</span><a href="#38369380">next</a><span>|</span><label class="collapse" for="c-38372248">[-]</label><label class="expand" for="c-38372248">[2 more]</label></div><br/><div class="children"><div class="content">Very excited to play with this. Some of my latest experiments - <a href="https:&#x2F;&#x2F;www.jasonfletcher.info&#x2F;vjloops&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.jasonfletcher.info&#x2F;vjloops&#x2F;</a></div><br/><div id="38374022" class="c"><input type="checkbox" id="c-38374022" checked=""/><div class="controls bullet"><span class="by">rbhuta</span><span>|</span><a href="#38372248">parent</a><span>|</span><a href="#38369380">next</a><span>|</span><label class="collapse" for="c-38374022">[-]</label><label class="expand" for="c-38374022">[1 more]</label></div><br/><div class="children"><div class="content">We&#x27;re hosting this free (no credit card needed) at <a href="https:&#x2F;&#x2F;app.decoherence.co&#x2F;stablevideo">https:&#x2F;&#x2F;app.decoherence.co&#x2F;stablevideo</a> Disclaimer: Google log-in required to help us reduce spam.
Let me know what you think of it! It works best on landscape images from my tests.</div><br/></div></div></div></div><div id="38369380" class="c"><input type="checkbox" id="c-38369380" checked=""/><div class="controls bullet"><span class="by">speedgoose</span><span>|</span><a href="#38372248">prev</a><span>|</span><a href="#38368384">next</a><span>|</span><label class="collapse" for="c-38369380">[-]</label><label class="expand" for="c-38369380">[7 more]</label></div><br/><div class="children"><div class="content">Has anyone managed to run the thing? I got the streamlit demo to start after fighting with pytorch, mamba, and pip for half an hour, but the demo runs out of GPU memory after a little while. I have 24GB on GPU on the machine I used, does it need more?</div><br/><div id="38370340" class="c"><input type="checkbox" id="c-38370340" checked=""/><div class="controls bullet"><span class="by">skonteam</span><span>|</span><a href="#38369380">parent</a><span>|</span><a href="#38370233">next</a><span>|</span><label class="collapse" for="c-38370340">[-]</label><label class="expand" for="c-38370340">[2 more]</label></div><br/><div class="children"><div class="content">Yeah, got a 24GB 4090, try to reduce the number of frames decoded to something like 4 or 8. Although, keep in mind it caps the 24Gb and goes to RAM (with the latest nvidia drivers).</div><br/><div id="38370427" class="c"><input type="checkbox" id="c-38370427" checked=""/><div class="controls bullet"><span class="by">speedgoose</span><span>|</span><a href="#38369380">root</a><span>|</span><a href="#38370340">parent</a><span>|</span><a href="#38370233">next</a><span>|</span><label class="collapse" for="c-38370427">[-]</label><label class="expand" for="c-38370427">[1 more]</label></div><br/><div class="children"><div class="content">Oh yes it works, thanks!</div><br/></div></div></div></div><div id="38370233" class="c"><input type="checkbox" id="c-38370233" checked=""/><div class="controls bullet"><span class="by">mkaic</span><span>|</span><a href="#38369380">parent</a><span>|</span><a href="#38370340">prev</a><span>|</span><a href="#38370937">next</a><span>|</span><label class="collapse" for="c-38370233">[-]</label><label class="expand" for="c-38370233">[3 more]</label></div><br/><div class="children"><div class="content">Have heard from others attempting it that it needs 40GB, so basically an A100&#x2F;A6000&#x2F;H100 or other large card. Or an Apple Silicon Mac with a bunch of unified memory, I guess.</div><br/><div id="38370284" class="c"><input type="checkbox" id="c-38370284" checked=""/><div class="controls bullet"><span class="by">speedgoose</span><span>|</span><a href="#38369380">root</a><span>|</span><a href="#38370233">parent</a><span>|</span><a href="#38370281">next</a><span>|</span><label class="collapse" for="c-38370284">[-]</label><label class="expand" for="c-38370284">[1 more]</label></div><br/><div class="children"><div class="content">Alright thanks for the information. I will try to justify using one A100 for my &quot;very important&quot; research activities.</div><br/></div></div><div id="38370281" class="c"><input type="checkbox" id="c-38370281" checked=""/><div class="controls bullet"><span class="by">mlboss</span><span>|</span><a href="#38369380">root</a><span>|</span><a href="#38370233">parent</a><span>|</span><a href="#38370284">prev</a><span>|</span><a href="#38370937">next</a><span>|</span><label class="collapse" for="c-38370281">[-]</label><label class="expand" for="c-38370281">[1 more]</label></div><br/><div class="children"><div class="content">Give it a week.</div><br/></div></div></div></div><div id="38370937" class="c"><input type="checkbox" id="c-38370937" checked=""/><div class="controls bullet"><span class="by">nwoli</span><span>|</span><a href="#38369380">parent</a><span>|</span><a href="#38370233">prev</a><span>|</span><a href="#38368384">next</a><span>|</span><label class="collapse" for="c-38370937">[-]</label><label class="expand" for="c-38370937">[1 more]</label></div><br/><div class="children"><div class="content">Is the checkpoint default fp16 or fp32?</div><br/></div></div></div></div><div id="38368384" class="c"><input type="checkbox" id="c-38368384" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#38369380">prev</a><span>|</span><a href="#38370808">next</a><span>|</span><label class="collapse" for="c-38368384">[-]</label><label class="expand" for="c-38368384">[5 more]</label></div><br/><div class="children"><div class="content">Model weights (two variations, each 10GB) are available without waitlist&#x2F;approval: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;stabilityai&#x2F;stable-video-diffusion-img2vid-xt" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;stabilityai&#x2F;stable-video-diffusion-im...</a><p>The LICENSE is a special non-commercial one: <a href="https:&#x2F;&#x2F;huggingface.co&#x2F;stabilityai&#x2F;stable-video-diffusion-img2vid-xt&#x2F;blob&#x2F;main&#x2F;LICENSE" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;stabilityai&#x2F;stable-video-diffusion-im...</a><p>It&#x27;s unclear how exactly to run it easily: diffusers has video generation support now but need to see if it plugs in seamlessly.</div><br/><div id="38368555" class="c"><input type="checkbox" id="c-38368555" checked=""/><div class="controls bullet"><span class="by">chankstein38</span><span>|</span><a href="#38368384">parent</a><span>|</span><a href="#38368522">next</a><span>|</span><label class="collapse" for="c-38368555">[-]</label><label class="expand" for="c-38368555">[2 more]</label></div><br/><div class="children"><div class="content">It looks like the huggingface page links their github that seems to have python scripts to run these: <a href="https:&#x2F;&#x2F;github.com&#x2F;Stability-AI&#x2F;generative-models">https:&#x2F;&#x2F;github.com&#x2F;Stability-AI&#x2F;generative-models</a></div><br/><div id="38368663" class="c"><input type="checkbox" id="c-38368663" checked=""/><div class="controls bullet"><span class="by">minimaxir</span><span>|</span><a href="#38368384">root</a><span>|</span><a href="#38368555">parent</a><span>|</span><a href="#38368522">next</a><span>|</span><label class="collapse" for="c-38368663">[-]</label><label class="expand" for="c-38368663">[1 more]</label></div><br/><div class="children"><div class="content">Those scripts aren&#x27;t as easy to use or iterate upon since they are CLI apps instead of a REPL like a Colab&#x2F;Jupyter Notebook (although these models probably will not run in a normal Colab without shenanigans).<p>They can be hacked into a Jupyter Notebook but it&#x27;s really not fun.</div><br/></div></div></div></div><div id="38368522" class="c"><input type="checkbox" id="c-38368522" checked=""/><div class="controls bullet"><span class="by">ronsor</span><span>|</span><a href="#38368384">parent</a><span>|</span><a href="#38368555">prev</a><span>|</span><a href="#38368527">next</a><span>|</span><label class="collapse" for="c-38368522">[-]</label><label class="expand" for="c-38368522">[1 more]</label></div><br/><div class="children"><div class="content">Regular reminder that it is very likely that model weights can&#x27;t be copyrighted (and thus can&#x27;t be licensed).</div><br/></div></div></div></div><div id="38370808" class="c"><input type="checkbox" id="c-38370808" checked=""/><div class="controls bullet"><span class="by">neaumusic</span><span>|</span><a href="#38368384">prev</a><span>|</span><a href="#38368568">next</a><span>|</span><label class="collapse" for="c-38370808">[-]</label><label class="expand" for="c-38370808">[4 more]</label></div><br/><div class="children"><div class="content">It&#x27;s funny that still don&#x27;t really have video wallpapers on most devices (I&#x27;m only aware of Wallpaper Engine on Windows)</div><br/><div id="38376661" class="c"><input type="checkbox" id="c-38376661" checked=""/><div class="controls bullet"><span class="by">spupy</span><span>|</span><a href="#38370808">parent</a><span>|</span><a href="#38371778">next</a><span>|</span><label class="collapse" for="c-38376661">[-]</label><label class="expand" for="c-38376661">[1 more]</label></div><br/><div class="children"><div class="content">Mplayer&#x2F;MPV used to be able to play videos in the X root window like a wallpaper. No idea if it still works nowadays.</div><br/></div></div><div id="38371778" class="c"><input type="checkbox" id="c-38371778" checked=""/><div class="controls bullet"><span class="by">Sohcahtoa82</span><span>|</span><a href="#38370808">parent</a><span>|</span><a href="#38376661">prev</a><span>|</span><a href="#38368568">next</a><span>|</span><label class="collapse" for="c-38371778">[-]</label><label class="expand" for="c-38371778">[2 more]</label></div><br/><div class="children"><div class="content">I had a video wallpaper on my Motorola Droid back in 2010.</div><br/><div id="38376333" class="c"><input type="checkbox" id="c-38376333" checked=""/><div class="controls bullet"><span class="by">tetris11</span><span>|</span><a href="#38370808">root</a><span>|</span><a href="#38371778">parent</a><span>|</span><a href="#38368568">next</a><span>|</span><label class="collapse" for="c-38376333">[-]</label><label class="expand" for="c-38376333">[1 more]</label></div><br/><div class="children"><div class="content">and a battery life of...?<p>I do wonder if there have been any codec studies that measure power usage with respect to RAM</div><br/></div></div></div></div></div></div><div id="38368568" class="c"><input type="checkbox" id="c-38368568" checked=""/><div class="controls bullet"><span class="by">helpmenotok</span><span>|</span><a href="#38370808">prev</a><span>|</span><a href="#38371753">next</a><span>|</span><label class="collapse" for="c-38368568">[-]</label><label class="expand" for="c-38368568">[18 more]</label></div><br/><div class="children"><div class="content">Can this be used for porn?</div><br/><div id="38376708" class="c"><input type="checkbox" id="c-38376708" checked=""/><div class="controls bullet"><span class="by">alkonaut</span><span>|</span><a href="#38368568">parent</a><span>|</span><a href="#38368701">next</a><span>|</span><label class="collapse" for="c-38376708">[-]</label><label class="expand" for="c-38376708">[1 more]</label></div><br/><div class="children"><div class="content">The answer to that question is always &quot;yes&quot;, regardless what &quot;this&quot; is.<p>Diffusion models for moving images are already used to a limited extent for this. And I&#x27;m sure it will be <i>the</i> use case, not just an edge case.</div><br/></div></div><div id="38368701" class="c"><input type="checkbox" id="c-38368701" checked=""/><div class="controls bullet"><span class="by">1024core</span><span>|</span><a href="#38368568">parent</a><span>|</span><a href="#38376708">prev</a><span>|</span><a href="#38369171">next</a><span>|</span><label class="collapse" for="c-38368701">[-]</label><label class="expand" for="c-38368701">[1 more]</label></div><br/><div class="children"><div class="content">The question reminded me of this classic: <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=YRgNOyCnbqg" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=YRgNOyCnbqg</a></div><br/></div></div><div id="38369171" class="c"><input type="checkbox" id="c-38369171" checked=""/><div class="controls bullet"><span class="by">hbn</span><span>|</span><a href="#38368568">parent</a><span>|</span><a href="#38368701">prev</a><span>|</span><a href="#38369244">next</a><span>|</span><label class="collapse" for="c-38369171">[-]</label><label class="expand" for="c-38369171">[3 more]</label></div><br/><div class="children"><div class="content">Depends on whether trains, cars, and&#x2F;or black cowboys tickle your fancy.</div><br/><div id="38369915" class="c"><input type="checkbox" id="c-38369915" checked=""/><div class="controls bullet"><span class="by">boppo1</span><span>|</span><a href="#38368568">root</a><span>|</span><a href="#38369171">parent</a><span>|</span><a href="#38369501">next</a><span>|</span><label class="collapse" for="c-38369915">[-]</label><label class="expand" for="c-38369915">[1 more]</label></div><br/><div class="children"><div class="content">Whatever this is:<p><a href="https:&#x2F;&#x2F;i.4cdn.org&#x2F;g&#x2F;1700595378919869.png" rel="nofollow noreferrer">https:&#x2F;&#x2F;i.4cdn.org&#x2F;g&#x2F;1700595378919869.png</a></div><br/></div></div></div></div><div id="38369244" class="c"><input type="checkbox" id="c-38369244" checked=""/><div class="controls bullet"><span class="by">artursapek</span><span>|</span><a href="#38368568">parent</a><span>|</span><a href="#38369171">prev</a><span>|</span><a href="#38370978">next</a><span>|</span><label class="collapse" for="c-38369244">[-]</label><label class="expand" for="c-38369244">[1 more]</label></div><br/><div class="children"><div class="content">Porn will be one of the main use cases for this technology. Porn sites pioneered video streaming technologies back in the day, and drove a lot of the innovation there.</div><br/></div></div><div id="38370978" class="c"><input type="checkbox" id="c-38370978" checked=""/><div class="controls bullet"><span class="by">SXX</span><span>|</span><a href="#38368568">parent</a><span>|</span><a href="#38369244">prev</a><span>|</span><a href="#38368839">next</a><span>|</span><label class="collapse" for="c-38370978">[-]</label><label class="expand" for="c-38370978">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s already posted to Unstable Diffusion discord so soon we&#x27;ll know.<p>After all fine-tuning wouldn&#x27;t take that long.</div><br/></div></div><div id="38368839" class="c"><input type="checkbox" id="c-38368839" checked=""/><div class="controls bullet"><span class="by">Racing0461</span><span>|</span><a href="#38368568">parent</a><span>|</span><a href="#38370978">prev</a><span>|</span><a href="#38368610">next</a><span>|</span><label class="collapse" for="c-38368839">[-]</label><label class="expand" for="c-38368839">[1 more]</label></div><br/><div class="children"><div class="content">Nope, all commercial models are severly gated.</div><br/></div></div><div id="38368610" class="c"><input type="checkbox" id="c-38368610" checked=""/><div class="controls bullet"><span class="by">theodric</span><span>|</span><a href="#38368568">parent</a><span>|</span><a href="#38368839">prev</a><span>|</span><a href="#38368635">next</a><span>|</span><label class="collapse" for="c-38368610">[-]</label><label class="expand" for="c-38368610">[1 more]</label></div><br/><div class="children"><div class="content">If it can&#x27;t, someone will massage it until it can. Porn, and probably also stock video to sell to YouTubers.</div><br/></div></div><div id="38368635" class="c"><input type="checkbox" id="c-38368635" checked=""/><div class="controls bullet"><span class="by">citrusui</span><span>|</span><a href="#38368568">parent</a><span>|</span><a href="#38368610">prev</a><span>|</span><a href="#38371753">next</a><span>|</span><label class="collapse" for="c-38368635">[-]</label><label class="expand" for="c-38368635">[8 more]</label></div><br/><div class="children"><div class="content">Very unusual comment.<p>I do not think so as the chance of constructing a fleshy eldritch horror is quite high.</div><br/><div id="38368730" class="c"><input type="checkbox" id="c-38368730" checked=""/><div class="controls bullet"><span class="by">johndevor</span><span>|</span><a href="#38368568">root</a><span>|</span><a href="#38368635">parent</a><span>|</span><a href="#38368778">next</a><span>|</span><label class="collapse" for="c-38368730">[-]</label><label class="expand" for="c-38368730">[4 more]</label></div><br/><div class="children"><div class="content">How is that not the first question to ask? Porn has proven to be a fantastic litmus test of fast market penetration when it comes to new technologies.</div><br/><div id="38369096" class="c"><input type="checkbox" id="c-38369096" checked=""/><div class="controls bullet"><span class="by">xanderlewis</span><span>|</span><a href="#38368568">root</a><span>|</span><a href="#38368730">parent</a><span>|</span><a href="#38369074">next</a><span>|</span><label class="collapse" for="c-38369096">[-]</label><label class="expand" for="c-38369096">[1 more]</label></div><br/><div class="children"><div class="content">Market what?</div><br/></div></div><div id="38369074" class="c"><input type="checkbox" id="c-38369074" checked=""/><div class="controls bullet"><span class="by">citrusui</span><span>|</span><a href="#38368568">root</a><span>|</span><a href="#38368730">parent</a><span>|</span><a href="#38369096">prev</a><span>|</span><a href="#38369083">next</a><span>|</span><label class="collapse" for="c-38369074">[-]</label><label class="expand" for="c-38369074">[1 more]</label></div><br/><div class="children"><div class="content">This is true. I was hoping my educated guess of the outcome would minimize the possibility of anyone attempting this. And yet, here we are - the only losing strategy in the technology sector is to not try at all.</div><br/></div></div><div id="38369083" class="c"><input type="checkbox" id="c-38369083" checked=""/><div class="controls bullet"><span class="by">throwaway743</span><span>|</span><a href="#38368568">root</a><span>|</span><a href="#38368730">parent</a><span>|</span><a href="#38369074">prev</a><span>|</span><a href="#38368778">next</a><span>|</span><label class="collapse" for="c-38369083">[-]</label><label class="expand" for="c-38369083">[1 more]</label></div><br/><div class="children"><div class="content">No pun intended?</div><br/></div></div></div></div><div id="38368778" class="c"><input type="checkbox" id="c-38368778" checked=""/><div class="controls bullet"><span class="by">crtasm</span><span>|</span><a href="#38368568">root</a><span>|</span><a href="#38368635">parent</a><span>|</span><a href="#38368730">prev</a><span>|</span><a href="#38369176">next</a><span>|</span><label class="collapse" for="c-38368778">[-]</label><label class="expand" for="c-38368778">[1 more]</label></div><br/><div class="children"><div class="content">That didn&#x27;t stop people using PornPen for images and it wouldn&#x27;t stop them using something else for video.</div><br/></div></div><div id="38369176" class="c"><input type="checkbox" id="c-38369176" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#38368568">root</a><span>|</span><a href="#38368635">parent</a><span>|</span><a href="#38368778">prev</a><span>|</span><a href="#38368671">next</a><span>|</span><label class="collapse" for="c-38369176">[-]</label><label class="expand" for="c-38369176">[1 more]</label></div><br/><div class="children"><div class="content">A surprisingly large number of people are into fleshy eldritch horrors.</div><br/></div></div><div id="38368671" class="c"><input type="checkbox" id="c-38368671" checked=""/><div class="controls bullet"><span class="by">tstrimple</span><span>|</span><a href="#38368568">root</a><span>|</span><a href="#38368635">parent</a><span>|</span><a href="#38369176">prev</a><span>|</span><a href="#38371753">next</a><span>|</span><label class="collapse" for="c-38368671">[-]</label><label class="expand" for="c-38368671">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I do not think so as the chance of constructing a fleshy eldritch horror is quite high.<p>There is a market for everything!</div><br/></div></div></div></div></div></div><div id="38371753" class="c"><input type="checkbox" id="c-38371753" checked=""/><div class="controls bullet"><span class="by">chrononaut</span><span>|</span><a href="#38368568">prev</a><span>|</span><a href="#38368329">next</a><span>|</span><label class="collapse" for="c-38371753">[-]</label><label class="expand" for="c-38371753">[1 more]</label></div><br/><div class="children"><div class="content">Much like in static images, the subtle unintended imperfections are quite interesting to observe.<p>For example, the man in the cowboy hat seems he is almost gagging. In the train video the tracks seem to be too wide while the train ice skates across them.</div><br/></div></div><div id="38368660" class="c"><input type="checkbox" id="c-38368660" checked=""/><div class="controls bullet"><span class="by">dinvlad</span><span>|</span><a href="#38368329">prev</a><span>|</span><a href="#38368590">next</a><span>|</span><label class="collapse" for="c-38368660">[-]</label><label class="expand" for="c-38368660">[2 more]</label></div><br/><div class="children"><div class="content">Seems relatively unimpressive tbh - it&#x27;s not really a video, and we&#x27;ve seen this kind of thing for a few months now</div><br/><div id="38368895" class="c"><input type="checkbox" id="c-38368895" checked=""/><div class="controls bullet"><span class="by">accrual</span><span>|</span><a href="#38368660">parent</a><span>|</span><a href="#38368590">next</a><span>|</span><label class="collapse" for="c-38368895">[-]</label><label class="expand" for="c-38368895">[1 more]</label></div><br/><div class="children"><div class="content">It seems like the breakthrough is that the video generating method is now baked into the model and generator. I&#x27;ve seen several fairly impressive AI animations as well, but until now, I assumed they were tediously cobbled together by hacking on the still-image SD models.</div><br/></div></div></div></div><div id="38368590" class="c"><input type="checkbox" id="c-38368590" checked=""/><div class="controls bullet"><span class="by">christkv</span><span>|</span><a href="#38368660">prev</a><span>|</span><a href="#38372643">next</a><span>|</span><label class="collapse" for="c-38368590">[-]</label><label class="expand" for="c-38368590">[13 more]</label></div><br/><div class="children"><div class="content">Looks like I&#x27;m still good for my bet with some friends that before 2028 a team of 5-10 people will create a blockbuster style movie that today costs 100+ million USD on a shoestring budget and we won&#x27;t be able to tell.</div><br/><div id="38369330" class="c"><input type="checkbox" id="c-38369330" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#38368590">parent</a><span>|</span><a href="#38369579">next</a><span>|</span><label class="collapse" for="c-38369330">[-]</label><label class="expand" for="c-38369330">[4 more]</label></div><br/><div class="children"><div class="content">I wouldn&#x27;t bet either way.<p>Back in the mid 90s to 2010 or so, graphical improvements were hailed as photorealistic only to be improved upon with each subsequent blockbuster game.<p>I think we&#x27;re in a similar phase with AI[0]: every new release in $category is better, gets hailed as super fantastic world changing, is improved upon in the subsequent Two Minute Papers video on $category, and the cycle repeats.<p>[0] all of them: LLMs, image generators, cars, robots, voice recognition and synthesis, scientific research, …</div><br/><div id="38370110" class="c"><input type="checkbox" id="c-38370110" checked=""/><div class="controls bullet"><span class="by">Keyframe</span><span>|</span><a href="#38368590">root</a><span>|</span><a href="#38369330">parent</a><span>|</span><a href="#38371853">next</a><span>|</span><label class="collapse" for="c-38370110">[-]</label><label class="expand" for="c-38370110">[2 more]</label></div><br/><div class="children"><div class="content">Your comment reminded me of this: <a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;gaming&#x2F;comments&#x2F;ktyr1&#x2F;unreal_yes_this_is_an_actual_pc_game_screenshot&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;gaming&#x2F;comments&#x2F;ktyr1&#x2F;unreal_yes_th...</a><p>Many more examples, of course.</div><br/><div id="38370571" class="c"><input type="checkbox" id="c-38370571" checked=""/><div class="controls bullet"><span class="by">ben_w</span><span>|</span><a href="#38368590">root</a><span>|</span><a href="#38370110">parent</a><span>|</span><a href="#38371853">next</a><span>|</span><label class="collapse" for="c-38370571">[-]</label><label class="expand" for="c-38370571">[1 more]</label></div><br/><div class="children"><div class="content">Yup, that castle flyby, those reflections. I remember being mesmerised by the sequence as a teenager.<p>Big quality improvement over Marathon 2 on a mid-90s Mac, which itself was a substantial boost over the Commodore 64 and NES I&#x27;d been playing on before that.</div><br/></div></div></div></div><div id="38371853" class="c"><input type="checkbox" id="c-38371853" checked=""/><div class="controls bullet"><span class="by">Sohcahtoa82</span><span>|</span><a href="#38368590">root</a><span>|</span><a href="#38369330">parent</a><span>|</span><a href="#38370110">prev</a><span>|</span><a href="#38369579">next</a><span>|</span><label class="collapse" for="c-38371853">[-]</label><label class="expand" for="c-38371853">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Back in the mid 90s to 2010 or so, graphical improvements were hailed as photorealistic<p>Whenever I saw anybody calling those graphics &quot;photorealistic&quot;, I always had to roll my eyes and question if those people were legally blind.<p>Like, c&#x27;mon.  Yeah, they could be large leaps ahead of the previous generation, but photorealistic?  Get real.<p>Even today, I&#x27;m not sure there&#x27;s a single game that I would say has photo-realistic graphics.</div><br/></div></div></div></div><div id="38369579" class="c"><input type="checkbox" id="c-38369579" checked=""/><div class="controls bullet"><span class="by">deckard1</span><span>|</span><a href="#38368590">parent</a><span>|</span><a href="#38369330">prev</a><span>|</span><a href="#38369528">next</a><span>|</span><label class="collapse" for="c-38369579">[-]</label><label class="expand" for="c-38369579">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m imagining more of an AI that takes a standard movie screenplay and a sidecar file, similar to a CSS file for the web and generates the movie. This sidecar file would contain the &quot;director&quot; of the movie, with camera angles, shot length and speed, color grading, etc. Don&#x27;t like how the new Dune movie looks? Edit the stylesheet and make it your own. Personalized remixed blockbusters.<p>On a more serious note, I don&#x27;t think Roger Deakins has anything to worry about right now. Or maybe ever. We&#x27;ve been here before. DAWs opened up an entire world of audio production to people that could afford a laptop and some basic gear. But we certainly do not have a thousand Beatles out there. It still requires talent and effort.</div><br/><div id="38369953" class="c"><input type="checkbox" id="c-38369953" checked=""/><div class="controls bullet"><span class="by">timeon</span><span>|</span><a href="#38368590">root</a><span>|</span><a href="#38369579">parent</a><span>|</span><a href="#38369528">next</a><span>|</span><label class="collapse" for="c-38369953">[-]</label><label class="expand" for="c-38369953">[1 more]</label></div><br/><div class="children"><div class="content">&gt; thousand Beatles out there. It still requires talent and effort<p>As well as marketing.</div><br/></div></div></div></div><div id="38369528" class="c"><input type="checkbox" id="c-38369528" checked=""/><div class="controls bullet"><span class="by">marcusverus</span><span>|</span><a href="#38368590">parent</a><span>|</span><a href="#38369579">prev</a><span>|</span><a href="#38368799">next</a><span>|</span><label class="collapse" for="c-38369528">[-]</label><label class="expand" for="c-38369528">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;m pumped for this future, but I&#x27;m not sure that I buy your optimistic timeline. If the history of AI has taught us anything, it is that the last 1% of of progress is the hardest half. And given the unforgiving nature of the uncanny valley, the video produced by such a system will be worthless until it is damn-near perfect. That&#x27;s a tall order!</div><br/></div></div><div id="38368799" class="c"><input type="checkbox" id="c-38368799" checked=""/><div class="controls bullet"><span class="by">accrual</span><span>|</span><a href="#38368590">parent</a><span>|</span><a href="#38369528">prev</a><span>|</span><a href="#38368693">next</a><span>|</span><label class="collapse" for="c-38368799">[-]</label><label class="expand" for="c-38368799">[3 more]</label></div><br/><div class="children"><div class="content">The first full-length AI generated movie will be an important milestone for sure, and will probably become a &quot;required watch&quot; for future AI history classes. I wonder what the Rotten Tomatoes page will look like.</div><br/><div id="38368997" class="c"><input type="checkbox" id="c-38368997" checked=""/><div class="controls bullet"><span class="by">jjkaczor</span><span>|</span><a href="#38368590">root</a><span>|</span><a href="#38368799">parent</a><span>|</span><a href="#38369274">next</a><span>|</span><label class="collapse" for="c-38368997">[-]</label><label class="expand" for="c-38368997">[1 more]</label></div><br/><div class="children"><div class="content">As per the reviews - it will be hard to say, as both positive and negative takes will be uploaded by ChatGPT bots (or it&#x27;s myriad of descendents).</div><br/></div></div><div id="38369274" class="c"><input type="checkbox" id="c-38369274" checked=""/><div class="controls bullet"><span class="by">qiine</span><span>|</span><a href="#38368590">root</a><span>|</span><a href="#38368799">parent</a><span>|</span><a href="#38368997">prev</a><span>|</span><a href="#38368693">next</a><span>|</span><label class="collapse" for="c-38369274">[-]</label><label class="expand" for="c-38369274">[1 more]</label></div><br/><div class="children"><div class="content">&quot;I wonder what the Rotten Tomatoes page will look like&quot;<p>Surely it will be written using machine vision and llms !</div><br/></div></div></div></div><div id="38368693" class="c"><input type="checkbox" id="c-38368693" checked=""/><div class="controls bullet"><span class="by">CamperBob2</span><span>|</span><a href="#38368590">parent</a><span>|</span><a href="#38368799">prev</a><span>|</span><a href="#38369111">next</a><span>|</span><label class="collapse" for="c-38368693">[-]</label><label class="expand" for="c-38368693">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;ll happen, but I think you&#x27;re early.  2038 for sure, unless something drastic happens to stop it (or is forced to happen.)</div><br/></div></div><div id="38369111" class="c"><input type="checkbox" id="c-38369111" checked=""/><div class="controls bullet"><span class="by">throwaway743</span><span>|</span><a href="#38368590">parent</a><span>|</span><a href="#38368693">prev</a><span>|</span><a href="#38372643">next</a><span>|</span><label class="collapse" for="c-38369111">[-]</label><label class="expand" for="c-38369111">[1 more]</label></div><br/><div class="children"><div class="content">Definitely a big first for benchmarks. After that hyper personalized content&#x2F;media generated on-demand</div><br/></div></div></div></div><div id="38372643" class="c"><input type="checkbox" id="c-38372643" checked=""/><div class="controls bullet"><span class="by">didip</span><span>|</span><a href="#38368590">prev</a><span>|</span><a href="#38372964">next</a><span>|</span><label class="collapse" for="c-38372643">[-]</label><label class="expand" for="c-38372643">[1 more]</label></div><br/><div class="children"><div class="content">Stability.ai, please make sure your board is sane.</div><br/></div></div><div id="38372964" class="c"><input type="checkbox" id="c-38372964" checked=""/><div class="controls bullet"><span class="by">RandomBK</span><span>|</span><a href="#38372643">prev</a><span>|</span><a href="#38371061">next</a><span>|</span><label class="collapse" for="c-38372964">[-]</label><label class="expand" for="c-38372964">[1 more]</label></div><br/><div class="children"><div class="content">Needs 40GB VRAM, down to 24GB by reducing the number of frames processed in parallel.</div><br/></div></div><div id="38371061" class="c"><input type="checkbox" id="c-38371061" checked=""/><div class="controls bullet"><span class="by">pcj-github</span><span>|</span><a href="#38372964">prev</a><span>|</span><a href="#38373357">next</a><span>|</span><label class="collapse" for="c-38371061">[-]</label><label class="expand" for="c-38371061">[1 more]</label></div><br/><div class="children"><div class="content">Soon the hollywood strike won&#x27;t even matter, won&#x27;t need any of those jobs.  Entire west coast economy obliterated.</div><br/></div></div><div id="38373357" class="c"><input type="checkbox" id="c-38373357" checked=""/><div class="controls bullet"><span class="by">Eduard</span><span>|</span><a href="#38371061">prev</a><span>|</span><a href="#38373672">next</a><span>|</span><label class="collapse" for="c-38373357">[-]</label><label class="expand" for="c-38373357">[1 more]</label></div><br/><div class="children"><div class="content">cannot join the waiting list (nor opt in for marketing newsletter), because the sign-up form checkboxes don&#x27;t toggle on android mobile Chrome or Firefox.</div><br/></div></div><div id="38373672" class="c"><input type="checkbox" id="c-38373672" checked=""/><div class="controls bullet"><span class="by">gregorymichael</span><span>|</span><a href="#38373357">prev</a><span>|</span><a href="#38371622">next</a><span>|</span><label class="collapse" for="c-38373672">[-]</label><label class="expand" for="c-38373672">[3 more]</label></div><br/><div class="children"><div class="content">How long until Replicate has this available?</div><br/><div id="38374020" class="c"><input type="checkbox" id="c-38374020" checked=""/><div class="controls bullet"><span class="by">rbhuta</span><span>|</span><a href="#38373672">parent</a><span>|</span><a href="#38374823">next</a><span>|</span><label class="collapse" for="c-38374020">[-]</label><label class="expand" for="c-38374020">[1 more]</label></div><br/><div class="children"><div class="content">We&#x27;re hosting this free (no credit card needed) at <a href="https:&#x2F;&#x2F;app.decoherence.co&#x2F;stablevideo">https:&#x2F;&#x2F;app.decoherence.co&#x2F;stablevideo</a>
Disclaimer: Google log-in required to help us reduce spam.<p>Let me know what you think of it! It works best on landscape images from my tests.</div><br/></div></div><div id="38374823" class="c"><input type="checkbox" id="c-38374823" checked=""/><div class="controls bullet"><span class="by">radicality</span><span>|</span><a href="#38373672">parent</a><span>|</span><a href="#38374020">prev</a><span>|</span><a href="#38371622">next</a><span>|</span><label class="collapse" for="c-38374823">[-]</label><label class="expand" for="c-38374823">[1 more]</label></div><br/><div class="children"><div class="content">Looks like there is a WIP here: <a href="https:&#x2F;&#x2F;replicate.com&#x2F;lucataco&#x2F;svd">https:&#x2F;&#x2F;replicate.com&#x2F;lucataco&#x2F;svd</a></div><br/></div></div></div></div><div id="38371622" class="c"><input type="checkbox" id="c-38371622" checked=""/><div class="controls bullet"><span class="by">jonplackett</span><span>|</span><a href="#38373672">prev</a><span>|</span><label class="collapse" for="c-38371622">[-]</label><label class="expand" for="c-38371622">[1 more]</label></div><br/><div class="children"><div class="content">Is this available in the stability API any time soon?</div><br/></div></div></div></div></div></div></div></body></html>