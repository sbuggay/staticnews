<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1690966863055" as="style"/><link rel="stylesheet" href="styles.css?v=1690966863055"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2307.08378">eGPU: A 750 MHz Class Soft GPGPU for FPGA</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>matt_d</span> | <span>38 comments</span></div><br/><div><div id="36963716" class="c"><input type="checkbox" id="c-36963716" checked=""/><div class="controls bullet"><span class="by">Lramseyer</span><span>|</span><a href="#36966915">next</a><span>|</span><label class="collapse" for="c-36963716">[-]</label><label class="expand" for="c-36963716">[5 more]</label></div><br/><div class="children"><div class="content">Full Disclosure, I work for an FPGA company.<p>The mind blowing part of all of this is the fact that they were able to close timing at 771MHz. That is insanely fast for an FPGA. For perspective, most modern FPGAs run their designs at around 300MHz* While most of the heavy lifting in this design use hardened components like DSPs and FPUs, it&#x27;s still very impressive to see!<p>What I didn&#x27;t see talked about much was how memory is loaded in and out of the processor. I&#x27;m curious to see what the memory bandwidth numbers look like as well as the resource utilization of the higher level routing.<p>*For most hardware designs that aren&#x27;t things like CPUs and GPUs, you don&#x27;t always need a super high clock speed. You have a lot more flexibility to compute in space rather than in time (think more threads running slower.) The pros and cons of such tradeoffs are a bit of a complicated topic, but should at least be noted.</div><br/><div id="36964099" class="c"><input type="checkbox" id="c-36964099" checked=""/><div class="controls bullet"><span class="by">mathisfun123</span><span>|</span><a href="#36963716">parent</a><span>|</span><a href="#36966915">next</a><span>|</span><label class="collapse" for="c-36964099">[-]</label><label class="expand" for="c-36964099">[4 more]</label></div><br/><div class="children"><div class="content">&gt; The mind blowing part of all of this is the fact that they were able to close timing at 771MHz<p>It&#x27;s true but I mean this is Intel in-house research right? If they can&#x27;t get absolute peak fmax on their own parts that would be a really bad look right? Plus these stratix parts have hard FP blocks (not just DSPs) so they&#x27;re basically mostly scheduling stuff rather building the whole datapath. But admittedly I haven&#x27;t read the paper...<p>&gt;Full Disclosure, I work for an FPGA company<p>I currently do too (as an intern, maybe even the same one as you) and I haven&#x27;t looked very hard but I&#x27;m sure we have similar fmax achieving projects (maybe even GPUs since we&#x27;re fighting hard to compete with Nvidia...).</div><br/><div id="36964580" class="c"><input type="checkbox" id="c-36964580" checked=""/><div class="controls bullet"><span class="by">aseipp</span><span>|</span><a href="#36963716">root</a><span>|</span><a href="#36964099">parent</a><span>|</span><a href="#36966915">next</a><span>|</span><label class="collapse" for="c-36964580">[-]</label><label class="expand" for="c-36964580">[3 more]</label></div><br/><div class="children"><div class="content">I think it&#x27;s more that the compile was unconstrained, so no floorplanning or handholding synthesis anything (aside from the usual bullshit to e.g. make sure DSPs map correctly). Which is pretty good; yeah, the architecture is also good and you&#x27;d expect that at minimum from a team like this, but they also didn&#x27;t have to beat the tool into submission either which seems nice.<p>IME, until fairly recently at least, Quartus loved to get fiddly on even the most basic designs when using Stratix class parts where you wanted to rely on, say, hyper-registers or retiming. Like a thing that should clearly be pipelined just... doesn&#x27;t, and falls over from a gust of wind. It seems to have gotten quite a bit better in the last few versions, though, so seeing an unconstrained compile go this high is pretty good I think. Too bad they don&#x27;t seem to have released their code, though.</div><br/><div id="36964772" class="c"><input type="checkbox" id="c-36964772" checked=""/><div class="controls bullet"><span class="by">mathisfun123</span><span>|</span><a href="#36963716">root</a><span>|</span><a href="#36964580">parent</a><span>|</span><a href="#36966915">next</a><span>|</span><label class="collapse" for="c-36964772">[-]</label><label class="expand" for="c-36964772">[2 more]</label></div><br/><div class="children"><div class="content">&gt; I think it&#x27;s more that the compile was unconstrained, so no floorplanning or handholding synthesis anything<p>i mean they don&#x27;t say anything about how long they let it run for - constraints are to narrow the search space for human-time-scale benefit right? i&#x27;m sure if i let vivado run (i&#x27;m not an intel person) until heat death of the universe it would also give me some fantastic fmax numbers too.<p>&gt; Too bad they don&#x27;t seem to have released their code, though.<p>and they never will. i don&#x27;t know what it is about this kind of research (or how you can even get away with this) but it&#x27;s almost unheard of for the commercial groups to release their code&#x2F;rtl&#x2F;tcl scripts. i have emailed around before and i just continually get shined on - &quot;oh we&#x27;re planning to release but it has to go through legal&quot;. and this is from my capacity as a phd student ie not direct competitor. and it&#x27;s funny to me because if you&#x27;re selling these boards&#x2F;part&#x2F;fpgas aren&#x27;t you strongly incenvitized to throw as much sample code over the fence as possible so that people buy your hardware in order to use your code?</div><br/><div id="36965305" class="c"><input type="checkbox" id="c-36965305" checked=""/><div class="controls bullet"><span class="by">minetest2048</span><span>|</span><a href="#36963716">root</a><span>|</span><a href="#36964772">parent</a><span>|</span><a href="#36966915">next</a><span>|</span><label class="collapse" for="c-36965305">[-]</label><label class="expand" for="c-36965305">[1 more]</label></div><br/><div class="children"><div class="content">&gt;if you&#x27;re selling these boards&#x2F;part&#x2F;fpgas aren&#x27;t you strongly incenvitized to throw as much sample code over the fence as possible so that people buy your hardware in order to use your code?<p>I wish... this is a problem for general embedded things. We just recently bought a cubesat bus and the only way to interface our hardware with it is to send CAN bus frames to it. We asked them for an SDK and they don&#x27;t have it. Instead they sent us a bunch of ICDs with screenshots of readthedocs snippets containing the frame structures.<p>The unfortunate answer to that question is that unfortunately we don&#x27;t make purchasing decisions based on whether sample code exists, or detailed documentation quality for niche embedded hardware. Maybe because everyone is not giving those things.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="36966915" class="c"><input type="checkbox" id="c-36966915" checked=""/><div class="controls bullet"><span class="by">tasty_freeze</span><span>|</span><a href="#36963716">prev</a><span>|</span><a href="#36963195">next</a><span>|</span><label class="collapse" for="c-36966915">[-]</label><label class="expand" for="c-36966915">[2 more]</label></div><br/><div class="children"><div class="content">The &quot;G&quot; in &quot;GPU&quot; or the second &quot;G&quot; in &quot;GPGPU&quot; stands for &quot;graphics&quot;. The paper describes a compute-oriented processor that doesn&#x27;t appear to contain any gfx-specific hardware.<p>For instance, in conventional GPUs, there is dedicated hardware for rasterizing the pixels covered by a triangle; there is dedicated hardware for doing texture map calculations and sample filtering. The instruction set disclosed in the paper doesn&#x27;t contain anything to help accelerate those tasks. With enough code I&#x27;m sure one could get it to draw something, but it will be notably inefficient while doing so.</div><br/><div id="36967843" class="c"><input type="checkbox" id="c-36967843" checked=""/><div class="controls bullet"><span class="by">iamtedd</span><span>|</span><a href="#36966915">parent</a><span>|</span><a href="#36963195">next</a><span>|</span><label class="collapse" for="c-36967843">[-]</label><label class="expand" for="c-36967843">[1 more]</label></div><br/><div class="children"><div class="content">Also, the &#x27;e&#x27; in eGPU usually means that the card is housed externally to the rest of the computer - in its own enclosure.<p>They don&#x27;t actually explain anywhere in their paper what &#x27;eGPU&#x27; means. Is it &#x27;emulated&#x27;? I thought circuits implemented in FPGAs are not considered emulation?</div><br/></div></div></div></div><div id="36963195" class="c"><input type="checkbox" id="c-36963195" checked=""/><div class="controls bullet"><span class="by">dragontamer</span><span>|</span><a href="#36966915">prev</a><span>|</span><a href="#36963093">next</a><span>|</span><label class="collapse" for="c-36963195">[-]</label><label class="expand" for="c-36963195">[10 more]</label></div><br/><div class="children"><div class="content">For a GPU circuit, it basically comes down to the number of hardware multipliers on the FPGA, does it not?<p>I remember synthesizing a 16-bit Wallace tree in a lab exercise back in college. I think that single multiplier used up 70% of my LUTs.<p>You only will get massive amounts of hardware parallel multipliers if the underlying circuit has a ton of hardware multipliers (Like Xilinx&#x27;s VLIW SIMD AI chips)<p>-------<p>At all computer sizes, a GPU probably will have more multiply circuits than an equivalent cost FPGA, with exception of maybe those AI chips from Xilinx (where the individual cores are basically presynthesized with hardcoded ISA).<p>Ex: at under 500mW power usage you probably will prefer some ARM NEON SIMD or TI DSP &#x2F; VLIW. At cell phone levels you&#x27;d prefer a cell phone GPU, and at desktop&#x2F;server levels you&#x27;d prefer a desktop GPU.</div><br/><div id="36963600" class="c"><input type="checkbox" id="c-36963600" checked=""/><div class="controls bullet"><span class="by">danhor</span><span>|</span><a href="#36963195">parent</a><span>|</span><a href="#36963093">next</a><span>|</span><label class="collapse" for="c-36963600">[-]</label><label class="expand" for="c-36963600">[9 more]</label></div><br/><div class="children"><div class="content">&gt; At all computer sizes, a GPU probably will have more multiply circuits than an equivalent cost FPGA<p>Very likely yes, but FPGAs often have hundreds to thousands of hardware multipliers, as part of the DSP blocks. Here for example newer AMD FPGAs: <a href="https:&#x2F;&#x2F;eu.mouser.com&#x2F;datasheet&#x2F;2&#x2F;903&#x2F;ds890_ultrascale_overview-1591529.pdf" rel="nofollow noreferrer">https:&#x2F;&#x2F;eu.mouser.com&#x2F;datasheet&#x2F;2&#x2F;903&#x2F;ds890_ultrascale_overv...</a></div><br/><div id="36963887" class="c"><input type="checkbox" id="c-36963887" checked=""/><div class="controls bullet"><span class="by">mathisfun123</span><span>|</span><a href="#36963195">root</a><span>|</span><a href="#36963600">parent</a><span>|</span><a href="#36964623">next</a><span>|</span><label class="collapse" for="c-36963887">[-]</label><label class="expand" for="c-36963887">[5 more]</label></div><br/><div class="children"><div class="content">I wish people would stop quoting marketing material as some kind representation of what they know.<p>You&#x27;re giving completely the wrong impression about dsp slices - it is absolutely not 1 dsp slice per FP operator at any precision that you would want to do floating point arithmetic. It&#x27;s definitely at least 2 plus a whole bunch of LUTs (~500) for FP16 with 4 stages or something like that. And if you want faster (fewer stages) then you need more slices. On alveo u280, which is an ultrascale part, I have never been able to effectively utilize more than ~4000 dsp slices (out of 9024) for 5,4 mults and that cost basically 99% of clbs in SLR1 and SLR2.<p>And even then, disconnected FPUs are completely meaningless without a datapath implementing eg matmul and boy oh boy do you have no clue what you&#x27;re in for there.<p>Takeaway: it&#x27;s pointless to compare raw specsheet numbers when <i>everything</i> comes down to datapath.</div><br/><div id="36967392" class="c"><input type="checkbox" id="c-36967392" checked=""/><div class="controls bullet"><span class="by">Ballas</span><span>|</span><a href="#36963195">root</a><span>|</span><a href="#36963887">parent</a><span>|</span><a href="#36965110">next</a><span>|</span><label class="collapse" for="c-36967392">[-]</label><label class="expand" for="c-36967392">[1 more]</label></div><br/><div class="children"><div class="content">The Intel Agilex (that was used in this paper) DSP blocks natively support single and half precision floating point.</div><br/></div></div><div id="36965110" class="c"><input type="checkbox" id="c-36965110" checked=""/><div class="controls bullet"><span class="by">gsmecher</span><span>|</span><a href="#36963195">root</a><span>|</span><a href="#36963887">parent</a><span>|</span><a href="#36967392">prev</a><span>|</span><a href="#36964623">next</a><span>|</span><label class="collapse" for="c-36965110">[-]</label><label class="expand" for="c-36965110">[3 more]</label></div><br/><div class="children"><div class="content">&gt; everything comes down to datapath.<p>Ain&#x27;t that the truth.<p>However: Xilinx&#x27;s DSP58 blocks (Versal devices), and older Intel DSPs, do integrate floating point into the DSP tiles - which does narrow the gap between the datasheet&#x27;s DSP count and the number of floating-point operations achievable per clock.</div><br/><div id="36965790" class="c"><input type="checkbox" id="c-36965790" checked=""/><div class="controls bullet"><span class="by">mathisfun123</span><span>|</span><a href="#36963195">root</a><span>|</span><a href="#36965110">parent</a><span>|</span><a href="#36964623">next</a><span>|</span><label class="collapse" for="c-36965790">[-]</label><label class="expand" for="c-36965790">[2 more]</label></div><br/><div class="children"><div class="content">&gt; Xilinx&#x27;s DSP58 blocks (Versal devices)<p>this is correct (i am currently working on designs that target these parts) but those things are more expensive than comparably sized&#x2F;equipped nvidia gpus so (in the context of this discussion) it&#x27;s moot.</div><br/><div id="36967625" class="c"><input type="checkbox" id="c-36967625" checked=""/><div class="controls bullet"><span class="by">rzzzt</span><span>|</span><a href="#36963195">root</a><span>|</span><a href="#36965790">parent</a><span>|</span><a href="#36964623">next</a><span>|</span><label class="collapse" for="c-36967625">[-]</label><label class="expand" for="c-36967625">[1 more]</label></div><br/><div class="children"><div class="content">Ballas mentions in a neighboring reply that the eGPU from the paper is using an Agilex 7: <a href="https:&#x2F;&#x2F;ark.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;ark&#x2F;products&#x2F;217645&#x2F;intel-agilex-7-fpga-fseries-014-r24b.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;ark.intel.com&#x2F;content&#x2F;www&#x2F;us&#x2F;en&#x2F;ark&#x2F;products&#x2F;217645&#x2F;...</a></div><br/></div></div></div></div></div></div></div></div><div id="36964623" class="c"><input type="checkbox" id="c-36964623" checked=""/><div class="controls bullet"><span class="by">dragontamer</span><span>|</span><a href="#36963195">root</a><span>|</span><a href="#36963600">parent</a><span>|</span><a href="#36963887">prev</a><span>|</span><a href="#36963759">next</a><span>|</span><label class="collapse" for="c-36964623">[-]</label><label class="expand" for="c-36964623">[1 more]</label></div><br/><div class="children"><div class="content">And my 6-year-old Consumer GPU (Vega64) has 4096 32-bit multipliers at 1500 MHz clock. (And I bought it a year or two late, so it was only $400). With 8GB HBM2 RAM at 512GBps (yes, Gigabytes per second) throughput. You absolutely are not getting anything close to a consumer GPU in terms of performance or cost on any other platform.<p>---------<p>FPGAs advantage is that systolic array arrangement. I&#x27;m pretty sure GPUs will win in a &quot;hardware multiplier&quot; war, but it might be harder to get all the data into a typical GPU<p>There&#x27;s a fair number of programs that will never utilize a GPU with any level of efficiency. That&#x27;s where FPGAs come in, utilization will be better because of LUTs and Routers and all that magic.<p>But if people are looking for a TFLOP fight with raw numbers of multipliers? My bet is on the GPU and SIMD-processing in general. Those architectures are just crazy.<p>--------------<p>BTW: That top end DSP slice is documented here: <a href="https:&#x2F;&#x2F;docs.xilinx.com&#x2F;v&#x2F;u&#x2F;en-US&#x2F;ug579-ultrascale-dsp" rel="nofollow noreferrer">https:&#x2F;&#x2F;docs.xilinx.com&#x2F;v&#x2F;u&#x2F;en-US&#x2F;ug579-ultrascale-dsp</a><p>That&#x27;s a 27-bit x 18-bit multiplier per dsp.<p>In contrast, a 32-bit floating point operation is 24-bit x 24-bit multiplier plus a bit of extra stuff for the exponent bits. I&#x27;m pretty sure that the 4096-shader Vega64 actually had 32-bit multipliers but I&#x27;m not 100% sure on that.<p>But the float 24-bit x 24-bit multiplier would need more than one dsp to replicate, maybe two of them. AMD&#x27;s newest consumer 7900 xt GPUs are 6144-shader... but those shaders can execute _TWO_ multiplies per clock tick for a total of 12288 hardware multipliers operating at 2500 MHz clock.</div><br/></div></div><div id="36963759" class="c"><input type="checkbox" id="c-36963759" checked=""/><div class="controls bullet"><span class="by">UncleOxidant</span><span>|</span><a href="#36963195">root</a><span>|</span><a href="#36963600">parent</a><span>|</span><a href="#36964623">prev</a><span>|</span><a href="#36963732">next</a><span>|</span><label class="collapse" for="c-36963759">[-]</label><label class="expand" for="c-36963759">[1 more]</label></div><br/><div class="children"><div class="content">The FPGAs with enough multipliers to be competitive against an actual GPU are going to be quite a bit more expensive than a GPU aren&#x27;t they?</div><br/></div></div><div id="36963732" class="c"><input type="checkbox" id="c-36963732" checked=""/><div class="controls bullet"><span class="by">pkaye</span><span>|</span><a href="#36963195">root</a><span>|</span><a href="#36963600">parent</a><span>|</span><a href="#36963759">prev</a><span>|</span><a href="#36963093">next</a><span>|</span><label class="collapse" for="c-36963732">[-]</label><label class="expand" for="c-36963732">[1 more]</label></div><br/><div class="children"><div class="content">How much would that FPGA cost?</div><br/></div></div></div></div></div></div><div id="36963093" class="c"><input type="checkbox" id="c-36963093" checked=""/><div class="controls bullet"><span class="by">stefanpie</span><span>|</span><a href="#36963195">prev</a><span>|</span><a href="#36964188">next</a><span>|</span><label class="collapse" for="c-36963093">[-]</label><label class="expand" for="c-36963093">[3 more]</label></div><br/><div class="children"><div class="content">One group at Georgia Tech in our building has also been working on open source GPU designs that can also target FPGAs and interoperate with RISCV. They have several publications on the work they have built up. Thought I might share since it’s not referenced in the submission paper.<p><a href="https:&#x2F;&#x2F;vortex.cc.gatech.edu&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;vortex.cc.gatech.edu&#x2F;</a></div><br/><div id="36964341" class="c"><input type="checkbox" id="c-36964341" checked=""/><div class="controls bullet"><span class="by">mepian</span><span>|</span><a href="#36963093">parent</a><span>|</span><a href="#36964188">next</a><span>|</span><label class="collapse" for="c-36964341">[-]</label><label class="expand" for="c-36964341">[2 more]</label></div><br/><div class="children"><div class="content">They still haven&#x27;t published the source code for their Skybox project, I wonder why. Unless I missed it in their repository? <a href="https:&#x2F;&#x2F;github.com&#x2F;vortexgpgpu">https:&#x2F;&#x2F;github.com&#x2F;vortexgpgpu</a></div><br/><div id="36966928" class="c"><input type="checkbox" id="c-36966928" checked=""/><div class="controls bullet"><span class="by">stefanpie</span><span>|</span><a href="#36963093">root</a><span>|</span><a href="#36964341">parent</a><span>|</span><a href="#36964188">next</a><span>|</span><label class="collapse" for="c-36966928">[-]</label><label class="expand" for="c-36966928">[1 more]</label></div><br/><div class="children"><div class="content">Not sure, my best advice would be to just email the authors if you are interested. Seems like they presented to ASPLOS so maybe the students disappeared over the summer for internships or vacation.</div><br/></div></div></div></div></div></div><div id="36964188" class="c"><input type="checkbox" id="c-36964188" checked=""/><div class="controls bullet"><span class="by">gsmecher</span><span>|</span><a href="#36963093">prev</a><span>|</span><a href="#36967267">next</a><span>|</span><label class="collapse" for="c-36964188">[-]</label><label class="expand" for="c-36964188">[1 more]</label></div><br/><div class="children"><div class="content">Also discussed here: <a href="https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;FPGA&#x2F;comments&#x2F;15fnb6u&#x2F;egpu_a_750_mhz_class_soft_gpgpu_for_fpga&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;FPGA&#x2F;comments&#x2F;15fnb6u&#x2F;egpu_a_750_mh...</a></div><br/></div></div><div id="36964299" class="c"><input type="checkbox" id="c-36964299" checked=""/><div class="controls bullet"><span class="by">unwind</span><span>|</span><a href="#36967267">prev</a><span>|</span><a href="#36964565">next</a><span>|</span><label class="collapse" for="c-36964299">[-]</label><label class="expand" for="c-36964299">[2 more]</label></div><br/><div class="children"><div class="content">Uh, non-native question: what is the word &quot;class&quot; doing in the title?<p>Is a hyphen missing, so it should be &quot;750 MHz-class&quot;? I searched the linked page but the word only appears in the title, sans hyphen.</div><br/><div id="36966164" class="c"><input type="checkbox" id="c-36966164" checked=""/><div class="controls bullet"><span class="by">WantonQuantum</span><span>|</span><a href="#36964299">parent</a><span>|</span><a href="#36964565">next</a><span>|</span><label class="collapse" for="c-36966164">[-]</label><label class="expand" for="c-36966164">[1 more]</label></div><br/><div class="children"><div class="content">Yes, the title is saying that the &quot;soft GPU&quot; being executed on the FPGA is running like (in the same &quot;class&quot; as) a 750 MHz GPU.</div><br/></div></div></div></div><div id="36964565" class="c"><input type="checkbox" id="c-36964565" checked=""/><div class="controls bullet"><span class="by">kristianp</span><span>|</span><a href="#36964299">prev</a><span>|</span><a href="#36963031">next</a><span>|</span><label class="collapse" for="c-36964565">[-]</label><label class="expand" for="c-36964565">[1 more]</label></div><br/><div class="children"><div class="content">This page isn&#x27;t loading for me. Mirrored at: <a href="https:&#x2F;&#x2F;archive.is&#x2F;jewT4" rel="nofollow noreferrer">https:&#x2F;&#x2F;archive.is&#x2F;jewT4</a></div><br/></div></div><div id="36963031" class="c"><input type="checkbox" id="c-36963031" checked=""/><div class="controls bullet"><span class="by">avmich</span><span>|</span><a href="#36964565">prev</a><span>|</span><label class="collapse" for="c-36963031">[-]</label><label class="expand" for="c-36963031">[12 more]</label></div><br/><div class="children"><div class="content">Wonder it this could help to alleviate the momentary shortage of GPUs on the market.</div><br/><div id="36963355" class="c"><input type="checkbox" id="c-36963355" checked=""/><div class="controls bullet"><span class="by">latchkey</span><span>|</span><a href="#36963031">parent</a><span>|</span><a href="#36963089">next</a><span>|</span><label class="collapse" for="c-36963355">[-]</label><label class="expand" for="c-36963355">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t think this will be momentary. Reality is that there have been shortages of GPUs for a long time now and demand isn&#x27;t going down. People are signing 3 year contracts with lambda now.</div><br/></div></div><div id="36963089" class="c"><input type="checkbox" id="c-36963089" checked=""/><div class="controls bullet"><span class="by">ZiiS</span><span>|</span><a href="#36963031">parent</a><span>|</span><a href="#36963355">prev</a><span>|</span><a href="#36963105">next</a><span>|</span><label class="collapse" for="c-36963089">[-]</label><label class="expand" for="c-36963089">[8 more]</label></div><br/><div class="children"><div class="content">10 year old entry level GPUs have 100 750Mhz cores</div><br/><div id="36963145" class="c"><input type="checkbox" id="c-36963145" checked=""/><div class="controls bullet"><span class="by">monocasa</span><span>|</span><a href="#36963031">root</a><span>|</span><a href="#36963089">parent</a><span>|</span><a href="#36964613">next</a><span>|</span><label class="collapse" for="c-36963145">[-]</label><label class="expand" for="c-36963145">[6 more]</label></div><br/><div class="children"><div class="content">&#x27;Cores&#x27; are really overstated in GPUs.  CUDA cores are really SIMD lanes and if you counted it the same way as a CPU does, you&#x27;d get somewhere in the dozens of cores range even for modern GPUs.</div><br/><div id="36968337" class="c"><input type="checkbox" id="c-36968337" checked=""/><div class="controls bullet"><span class="by">deaddodo</span><span>|</span><a href="#36963031">root</a><span>|</span><a href="#36963145">parent</a><span>|</span><a href="#36964212">next</a><span>|</span><label class="collapse" for="c-36968337">[-]</label><label class="expand" for="c-36968337">[1 more]</label></div><br/><div class="children"><div class="content">If you want to count &quot;cores&quot; in a GPU, the best equivalency is streaming multiprocessors.<p>While it&#x27;s true that they&#x27;re in the dozens, strictly speaking, we&#x27;ve got GPUs with well over 100 now (the RTX 4090, for instance) and 64+ is a pretty common configuration.</div><br/></div></div><div id="36964212" class="c"><input type="checkbox" id="c-36964212" checked=""/><div class="controls bullet"><span class="by">xigency</span><span>|</span><a href="#36963031">root</a><span>|</span><a href="#36963145">parent</a><span>|</span><a href="#36968337">prev</a><span>|</span><a href="#36964359">next</a><span>|</span><label class="collapse" for="c-36964212">[-]</label><label class="expand" for="c-36964212">[3 more]</label></div><br/><div class="children"><div class="content">That seems backwards to me. Sure, a GPU core is less general, but in terms of concurrent execution, memory bandwidth, and FLOPS I would expect hundreds to thousands of cores for all new GPU offerings. Apple’s double-digit GPU core counts for instance sound extremely understated.</div><br/><div id="36964392" class="c"><input type="checkbox" id="c-36964392" checked=""/><div class="controls bullet"><span class="by">monocasa</span><span>|</span><a href="#36963031">root</a><span>|</span><a href="#36964212">parent</a><span>|</span><a href="#36968121">next</a><span>|</span><label class="collapse" for="c-36964392">[-]</label><label class="expand" for="c-36964392">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s not.  The best comparison is the SM count for Nvidia hardware, or the wavefront count for AMD hardware.  So a 4070 has 46 cores as you&#x27;d count them on a CPU.</div><br/></div></div><div id="36968121" class="c"><input type="checkbox" id="c-36968121" checked=""/><div class="controls bullet"><span class="by">imtringued</span><span>|</span><a href="#36963031">root</a><span>|</span><a href="#36964212">parent</a><span>|</span><a href="#36964392">prev</a><span>|</span><a href="#36964359">next</a><span>|</span><label class="collapse" for="c-36968121">[-]</label><label class="expand" for="c-36968121">[1 more]</label></div><br/><div class="children"><div class="content">If you have a truck that can carry 64 boxes, then according to Nvidia marketing each box would be considered its own core even though the boxes all go the same way.<p>Meanwhile if you had 64 passengers cars, then each of them would be able to drive a different route at the same time and if you counted the seats you would actually end up with 320 &quot;Nvidia cores&quot;.</div><br/></div></div></div></div><div id="36964359" class="c"><input type="checkbox" id="c-36964359" checked=""/><div class="controls bullet"><span class="by">codedokode</span><span>|</span><a href="#36963031">root</a><span>|</span><a href="#36963145">parent</a><span>|</span><a href="#36964212">prev</a><span>|</span><a href="#36964613">next</a><span>|</span><label class="collapse" for="c-36964359">[-]</label><label class="expand" for="c-36964359">[1 more]</label></div><br/><div class="children"><div class="content">A proper method is counting ALUs instead of vague &quot;cores&quot;.</div><br/></div></div></div></div><div id="36964613" class="c"><input type="checkbox" id="c-36964613" checked=""/><div class="controls bullet"><span class="by">kristianp</span><span>|</span><a href="#36963031">root</a><span>|</span><a href="#36963089">parent</a><span>|</span><a href="#36963145">prev</a><span>|</span><a href="#36963105">next</a><span>|</span><label class="collapse" for="c-36964613">[-]</label><label class="expand" for="c-36964613">[1 more]</label></div><br/><div class="children"><div class="content">From the abstract: &quot;The eGPU architecture is a streaming multiprocessor (SM) machine with 512 threads. Each SM contains 16 scalar processors (SP).&quot;</div><br/></div></div></div></div><div id="36963105" class="c"><input type="checkbox" id="c-36963105" checked=""/><div class="controls bullet"><span class="by">monocasa</span><span>|</span><a href="#36963031">parent</a><span>|</span><a href="#36963089">prev</a><span>|</span><a href="#36963103">next</a><span>|</span><label class="collapse" for="c-36963105">[-]</label><label class="expand" for="c-36963105">[1 more]</label></div><br/><div class="children"><div class="content">If it&#x27;s on an FPGA then it doesn&#x27;t really compete with GPUs you can buy from just about any perspective other than openness.</div><br/></div></div></div></div></div></div></div></div></div></body></html>