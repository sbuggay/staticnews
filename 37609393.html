<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1695459660752" as="style"/><link rel="stylesheet" href="styles.css?v=1695459660752"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://vgel.me/posts/handmade-transformer/">I made a transformer to predict a simple sequence manually</a> <span class="domain">(<a href="https://vgel.me">vgel.me</a>)</span></div><div class="subtext"><span>lukastyrychtr</span> | <span>82 comments</span></div><br/><div><div id="37615122" class="c"><input type="checkbox" id="c-37615122" checked=""/><div class="controls bullet"><span class="by">teddykoker</span><span>|</span><a href="#37620871">next</a><span>|</span><label class="collapse" for="c-37615122">[-]</label><label class="expand" for="c-37615122">[3 more]</label></div><br/><div class="children"><div class="content">A related line of work is &quot;Thinking Like Transformers&quot; [1]. They introduce a primitive programming language, RASP, which is composed of operations capable of being modeled with transformer components, and demonstrate how different programs can be written with it, e.g. histograms, sorting. Sasha Rush and Gail Weiss have an excellent blog post on it as well [2]. Follow on work actually demonstrated how RASP-like programs could actually be compiled into model weights without training [3].<p>[1] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2106.06981" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2106.06981</a><p>[2] <a href="https:&#x2F;&#x2F;srush.github.io&#x2F;raspy&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;srush.github.io&#x2F;raspy&#x2F;</a><p>[3] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2301.05062" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2301.05062</a></div><br/><div id="37618386" class="c"><input type="checkbox" id="c-37618386" checked=""/><div class="controls bullet"><span class="by">newhouseb</span><span>|</span><a href="#37615122">parent</a><span>|</span><a href="#37619761">next</a><span>|</span><label class="collapse" for="c-37618386">[-]</label><label class="expand" for="c-37618386">[1 more]</label></div><br/><div class="children"><div class="content">Huge fan of RASP et al. If you enjoy this space, might be fun to take a glance at some of my work on HandCrafted Transformers [1] wherein I hand-pick the weights in a transformer model to do long-handed addition similar to how humans learn to do it in gradeshcool.<p>[1] <a href="https:&#x2F;&#x2F;colab.research.google.com&#x2F;github&#x2F;newhouseb&#x2F;handcrafted&#x2F;blob&#x2F;main&#x2F;HandCrafted.ipynb" rel="nofollow noreferrer">https:&#x2F;&#x2F;colab.research.google.com&#x2F;github&#x2F;newhouseb&#x2F;handcraft...</a></div><br/></div></div><div id="37619761" class="c"><input type="checkbox" id="c-37619761" checked=""/><div class="controls bullet"><span class="by">theGnuMe</span><span>|</span><a href="#37615122">parent</a><span>|</span><a href="#37618386">prev</a><span>|</span><a href="#37620871">next</a><span>|</span><label class="collapse" for="c-37619761">[-]</label><label class="expand" for="c-37619761">[1 more]</label></div><br/><div class="children"><div class="content">It seems like a functional language like Haskell would be the right tool for this.<p>Also going from a net to code would be super interesting in terms of explain-ability.</div><br/></div></div></div></div><div id="37620871" class="c"><input type="checkbox" id="c-37620871" checked=""/><div class="controls bullet"><span class="by">thomasahle</span><span>|</span><a href="#37615122">prev</a><span>|</span><a href="#37619139">next</a><span>|</span><label class="collapse" for="c-37620871">[-]</label><label class="expand" for="c-37620871">[1 more]</label></div><br/><div class="children"><div class="content">I thought I understood transformers well, even though I had never implemented them. Then one day I implemented them, and they didn&#x27;t work&#x2F;train nearly as well as the standard pytorch transformer.<p>I eventually realized that I had ignored the dropout, because I thought my data  could never overfit. (I trained the transformer to add numbers, and I never showed it the same pair twice.) Turns out dropout has a much bigger role than I had realized.<p>TLDR, just go and implement a transformer.
The more from scratch the better.
Everyone I know who tried it, ended up learning something they hadn&#x27;t expected.
From how training is parallelized over tokens down to how backprop really works.
It&#x27;s different for every person.</div><br/></div></div><div id="37619139" class="c"><input type="checkbox" id="c-37619139" checked=""/><div class="controls bullet"><span class="by">mode80</span><span>|</span><a href="#37620871">prev</a><span>|</span><a href="#37616678">next</a><span>|</span><label class="collapse" for="c-37619139">[-]</label><label class="expand" for="c-37619139">[1 more]</label></div><br/><div class="children"><div class="content">I love Karpathy et al but this is what finally made it click for me: <a href="https:&#x2F;&#x2F;youtu.be&#x2F;kWLed8o5M2Y?si=SJT5_lCJ0hSR7Z_k" rel="nofollow noreferrer">https:&#x2F;&#x2F;youtu.be&#x2F;kWLed8o5M2Y?si=SJT5_lCJ0hSR7Z_k</a></div><br/></div></div><div id="37616678" class="c"><input type="checkbox" id="c-37616678" checked=""/><div class="controls bullet"><span class="by">Supply5411</span><span>|</span><a href="#37619139">prev</a><span>|</span><a href="#37618521">next</a><span>|</span><label class="collapse" for="c-37616678">[-]</label><label class="expand" for="c-37616678">[15 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve been kicking around a similar idea for awhile. Why can&#x27;t we have an intuitive interface to the weights of a model, that a domain expert can tweak by hand to accelerate training? For example, in a vision model, they can increase the &quot;orangeness&quot; collection of weights when detecting traffic cones. That way, instead of requiring thousands&#x2F;millions more examples to calibrate &quot;orangeness&quot; right, it&#x27;s accelerated by a human expert. The difficulty is obviously having this interface map to the collections of weights that mean different things, but is there a technical reason this can&#x27;t be done?</div><br/><div id="37621602" class="c"><input type="checkbox" id="c-37621602" checked=""/><div class="controls bullet"><span class="by">PoignardAzur</span><span>|</span><a href="#37616678">parent</a><span>|</span><a href="#37617044">next</a><span>|</span><label class="collapse" for="c-37621602">[-]</label><label class="expand" for="c-37621602">[1 more]</label></div><br/><div class="children"><div class="content">The reason you&#x27;re looking for is called &quot;The Bitter Lesson&quot;.<p>The short version is, trying to give human assistance to AIs is almost always less cost-effective than making them run on more computing power.<p>By the time your human expect has calibrated your weight layers to detect orange traffic cones, your GPU cluster has trained its AI to detect traffic cones, traffic ligts, trees, other cars, traffic cones with a slightly different shade of orange, etc.</div><br/></div></div><div id="37617044" class="c"><input type="checkbox" id="c-37617044" checked=""/><div class="controls bullet"><span class="by">elesiuta</span><span>|</span><a href="#37616678">parent</a><span>|</span><a href="#37621602">prev</a><span>|</span><a href="#37616761">next</a><span>|</span><label class="collapse" for="c-37617044">[-]</label><label class="expand" for="c-37617044">[5 more]</label></div><br/><div class="children"><div class="content">&gt; weights of a model, that a domain expert can tweak by hand<p>This sounds similar to how image recognition was done before deep learning [1]<p>[1] <a href="https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=8SF_h3xF3cE&amp;t=1358s">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=8SF_h3xF3cE&amp;t=1358s</a></div><br/><div id="37617167" class="c"><input type="checkbox" id="c-37617167" checked=""/><div class="controls bullet"><span class="by">Supply5411</span><span>|</span><a href="#37616678">root</a><span>|</span><a href="#37617044">parent</a><span>|</span><a href="#37616761">next</a><span>|</span><label class="collapse" for="c-37617167">[-]</label><label class="expand" for="c-37617167">[4 more]</label></div><br/><div class="children"><div class="content">Great example. Right, the deep learning approach uncovers all kinds of hidden features and relationships automatically that a team of humans might miss.<p>I guess I&#x27;m thinking about this problem from the perspective of these GPT models requiring more training data than a normal person can acquire. Currently, it seems you need the entire internet worth of training data (and a lot of money) to get something that can communicate reasonably well. But most people can communicate reasonably well, so it would be cool if that basic communication knowledge could be somehow used to accelerate training and minimize the reliance on training data.</div><br/><div id="37617785" class="c"><input type="checkbox" id="c-37617785" checked=""/><div class="controls bullet"><span class="by">EricMausler</span><span>|</span><a href="#37616678">root</a><span>|</span><a href="#37617167">parent</a><span>|</span><a href="#37618782">next</a><span>|</span><label class="collapse" for="c-37617785">[-]</label><label class="expand" for="c-37617785">[2 more]</label></div><br/><div class="children"><div class="content">I am still learning transformers, but I believe part of the issue may be that the weights do not necessarily correlate to things like &quot;orangeness&quot;<p>Instead of a transformer for each color, you have like 5 to 100 weights that represent some arbitrary combination of colors.  Literally the arbitrariness is defined by the dataset and the number of weights allocated.<p>They may even represent more than just color.<p>So I am not sure if a weight is actually a &quot;dial&quot; like you are describing it, where you can turn up or down different qualities.  I think the relationship between weights and features is relatively chaotic.<p>Like you may increase orangeness but decrease &quot;cone shapedness&quot; or accidentally make it identify deer as trees or something, all by just changing 1 value on 1 weight</div><br/><div id="37619336" class="c"><input type="checkbox" id="c-37619336" checked=""/><div class="controls bullet"><span class="by">space_boy</span><span>|</span><a href="#37616678">root</a><span>|</span><a href="#37617785">parent</a><span>|</span><a href="#37618782">next</a><span>|</span><label class="collapse" for="c-37619336">[-]</label><label class="expand" for="c-37619336">[1 more]</label></div><br/><div class="children"><div class="content">It is possible that the parameters, like weights in a machine learning model, interact to yield outcomes in a manner analogous to the interactions between genes in biological systems, which produce traits. These interactions involve complex interdependencies, so there really aren&#x27;t 1 to 1 dials.</div><br/></div></div></div></div><div id="37618782" class="c"><input type="checkbox" id="c-37618782" checked=""/><div class="controls bullet"><span class="by">mistrial9</span><span>|</span><a href="#37616678">root</a><span>|</span><a href="#37617167">parent</a><span>|</span><a href="#37617785">prev</a><span>|</span><a href="#37616761">next</a><span>|</span><label class="collapse" for="c-37618782">[-]</label><label class="expand" for="c-37618782">[1 more]</label></div><br/><div class="children"><div class="content">&gt; the deep learning approach uncovers all kinds of hidden features and relationships automatically that a team of humans might miss<p>sitting in a lecture from a decent DeepLearning practitioner, there were two questions from the audience (among others). The first question asked &quot;How can we check the results using other models, so that computers will catch the errors that humans miss?&quot;<p>The second question was more like &quot;when a model is built across a non-trivial input space, the features and classes that come out are one set of possibilities, but there are many more possibilities. How can we discover more about the model that is built, knowing that there are inherent epistemological conflicts in any model?&quot;<p>I also thought it was interesting that the two questioners were from large but very different demographic groups, and at different stages of learning and practice (the second question was from a senior coder).</div><br/></div></div></div></div></div></div><div id="37616761" class="c"><input type="checkbox" id="c-37616761" checked=""/><div class="controls bullet"><span class="by">amilios</span><span>|</span><a href="#37616678">parent</a><span>|</span><a href="#37617044">prev</a><span>|</span><a href="#37616808">next</a><span>|</span><label class="collapse" for="c-37616761">[-]</label><label class="expand" for="c-37616761">[6 more]</label></div><br/><div class="children"><div class="content">The technical reason it can&#x27;t be done (or would be very difficult to do) is that weights are typically very uninterpretable. There aren&#x27;t specific clusters of neurons that map to one concept or another, everything kind of does everything.</div><br/><div id="37616885" class="c"><input type="checkbox" id="c-37616885" checked=""/><div class="controls bullet"><span class="by">Supply5411</span><span>|</span><a href="#37616678">root</a><span>|</span><a href="#37616761">parent</a><span>|</span><a href="#37616808">next</a><span>|</span><label class="collapse" for="c-37616885">[-]</label><label class="expand" for="c-37616885">[5 more]</label></div><br/><div class="children"><div class="content">I wonder if an expert can &quot;impose&quot; weights onto a model and the model will opt to continue with them when it resumes training. For example, in the vision example, the expert may not know where &quot;orangeness&quot; currently exists, but if they impose their own collection of weight adjustments that represent orangeness, will the model continue to use these weights as the path of least resistance when continuing to optimize? Just spitballing, but if we can&#x27;t pick out which neurons do what, the alternative would seem to be to encourage the model to adopt a control interface of neurons.</div><br/><div id="37618833" class="c"><input type="checkbox" id="c-37618833" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#37616678">root</a><span>|</span><a href="#37616885">parent</a><span>|</span><a href="#37616808">next</a><span>|</span><label class="collapse" for="c-37618833">[-]</label><label class="expand" for="c-37618833">[4 more]</label></div><br/><div class="children"><div class="content">That would make it less efficient - since learning is compression, a less compressed model will also learn less at the same size.</div><br/><div id="37620290" class="c"><input type="checkbox" id="c-37620290" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#37616678">root</a><span>|</span><a href="#37618833">parent</a><span>|</span><a href="#37621141">next</a><span>|</span><label class="collapse" for="c-37620290">[-]</label><label class="expand" for="c-37620290">[2 more]</label></div><br/><div class="children"><div class="content">Humans have much more compressed models, trying to transfer human learning to machine learning could potentially be a way to get more efficient models.<p>The way we do that currently is by labeling data for training, but maybe there are better ways to do it. Like some semi code to write with hints for the model. Like, instead of labeled data, could have a series of &quot;lectures&quot; of labeled data that would lead to a good end state, instead of training on all the data in parallel.<p>You don&#x27;t teach a child calculus by showing them a million calculus problems after all, you ramp up starting with simple numbers and then slowly ramping up with more concepts. But to do that we would need to change how we train models.<p>Edit: By doing it that way you could see the skill of the model after each lecture, and update the lecture to try to make the model learn better. Not sure how to do that, but such ways to work with parts of models is a potential way forward.</div><br/><div id="37620404" class="c"><input type="checkbox" id="c-37620404" checked=""/><div class="controls bullet"><span class="by">astrange</span><span>|</span><a href="#37616678">root</a><span>|</span><a href="#37620290">parent</a><span>|</span><a href="#37621141">next</a><span>|</span><label class="collapse" for="c-37620404">[-]</label><label class="expand" for="c-37620404">[1 more]</label></div><br/><div class="children"><div class="content">There are techniques for this called &quot;curriculum learning&quot; and &quot;textbooks&quot;.<p>I&#x27;m not sure exactly what is in the textbooks since I admit to not reading the papers yet.<p>I&#x27;m personally wondering if you could increase the reliability of training on web data by labeling it with where each document came from, so it knows different authors disagree on things. But this brings back the issue where people don&#x27;t like it if a model can write &quot;in the style of Author Name&quot;…</div><br/></div></div></div></div><div id="37621141" class="c"><input type="checkbox" id="c-37621141" checked=""/><div class="controls bullet"><span class="by">Supply5411</span><span>|</span><a href="#37616678">root</a><span>|</span><a href="#37618833">parent</a><span>|</span><a href="#37620290">prev</a><span>|</span><a href="#37616808">next</a><span>|</span><label class="collapse" for="c-37621141">[-]</label><label class="expand" for="c-37621141">[1 more]</label></div><br/><div class="children"><div class="content">Might be a worthwhile tradeoff. Less efficient model but a clear control interface, versus more efficient model but it&#x27;s a blackbox.</div><br/></div></div></div></div></div></div></div></div><div id="37616808" class="c"><input type="checkbox" id="c-37616808" checked=""/><div class="controls bullet"><span class="by">jarde</span><span>|</span><a href="#37616678">parent</a><span>|</span><a href="#37616761">prev</a><span>|</span><a href="#37616765">next</a><span>|</span><label class="collapse" for="c-37616808">[-]</label><label class="expand" for="c-37616808">[1 more]</label></div><br/><div class="children"><div class="content">The number of layers and weights is really not at a scale we can handle updating manually, and even if we could the downstream effects of modifying weights are way too hard to manage. Say you are updating the picture to be better at orange, but unless you can monitor all the other colours for correctness at the same time you probably are creating issues for other colours without realizing it</div><br/></div></div><div id="37616765" class="c"><input type="checkbox" id="c-37616765" checked=""/><div class="controls bullet"><span class="by">klysm</span><span>|</span><a href="#37616678">parent</a><span>|</span><a href="#37616808">prev</a><span>|</span><a href="#37618521">next</a><span>|</span><label class="collapse" for="c-37616765">[-]</label><label class="expand" for="c-37616765">[1 more]</label></div><br/><div class="children"><div class="content">The attention mechanisms present in transformers don’t seem easy to map to semantics that humans can understand. There are too many parameters involved</div><br/></div></div></div></div><div id="37618521" class="c"><input type="checkbox" id="c-37618521" checked=""/><div class="controls bullet"><span class="by">tostadora1</span><span>|</span><a href="#37616678">prev</a><span>|</span><a href="#37614164">next</a><span>|</span><label class="collapse" for="c-37618521">[-]</label><label class="expand" for="c-37618521">[1 more]</label></div><br/><div class="children"><div class="content">I always wanted to at least have a shallow understanding of Transformers but the paper was way too technical for me.<p>This really helped me understand how they work! Or at least I understood your example, it was very clear. And I also got to brush up my matrix stuff from uni lol.<p>Thanks!</div><br/></div></div><div id="37614164" class="c"><input type="checkbox" id="c-37614164" checked=""/><div class="controls bullet"><span class="by">PaulHoule</span><span>|</span><a href="#37618521">prev</a><span>|</span><a href="#37617048">next</a><span>|</span><label class="collapse" for="c-37614164">[-]</label><label class="expand" for="c-37614164">[19 more]</label></div><br/><div class="children"><div class="content">It&#x27;s some kind of abstract machine,  like a turing machine or the machine that parses regexes,  isn&#x27;t it?</div><br/><div id="37617789" class="c"><input type="checkbox" id="c-37617789" checked=""/><div class="controls bullet"><span class="by">thewataccount</span><span>|</span><a href="#37614164">parent</a><span>|</span><a href="#37616179">next</a><span>|</span><label class="collapse" for="c-37617789">[-]</label><label class="expand" for="c-37617789">[1 more]</label></div><br/><div class="children"><div class="content">This is simplified a bit - It&#x27;s just a &quot;machine&quot; that maps [set of inputs] -&gt; [set of probabilities of the next output]<p>First you define a list of tokens - lets say 24 letters because that&#x27;s easier.<p>They are a machine that takes an input sequence of tokens, does a deterministic series of matrix operations, and outputs what is a list of the probability of every token.<p>&quot;learning&quot; is just the process of setting some of the numbers inside of a matrix(s) used for some of the operations.<p>Notice that there&#x27;s only a single &quot;if&quot; statement in their final code, and it&#x27;s for evaluating the result&#x27;s accuracy. All of the &quot;logic&quot; is from the result of these matrix operations.</div><br/></div></div><div id="37616179" class="c"><input type="checkbox" id="c-37616179" checked=""/><div class="controls bullet"><span class="by">nerdponx</span><span>|</span><a href="#37614164">parent</a><span>|</span><a href="#37617789">prev</a><span>|</span><a href="#37618545">next</a><span>|</span><label class="collapse" for="c-37616179">[-]</label><label class="expand" for="c-37616179">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s kind of hard to interpret these things as &quot;automata&quot; in the sense that one might usually think of them.<p>Everything is usually a little fuzzy in a neural network. There&#x27;s rarely anything like an if&#x2F;else statement, although (as in the transformer example) you have some cases of &quot;masking&quot; values with 0 or -∞. The output is almost always fuzzy as well, being a collection of scores or probabilities. For example, a model that distinguishes cat pictures and dog pictures might emit a result like &quot;dog:0.95 cat:0.05&quot;, and we say that it predicted a cat because the dog score is higher than the cat score.<p>In fact, the core of the transformer, the attention mechanism, is based on a kind of &quot;soft lookup&quot; operation. In a non-fuzzy system, you might want to do something like loop through each token in the sequence, check if that token is relevant to the current token, and take some action if it&#x27;s relevant. But in a transformer, relevance is not a binary decision. Instead, the attention mechanism computes a continuous relevance score between each pair of tokens in the sequence, and uses those scores to take further action.<p>But some things are <i>not</i> easily generalized directly from a system based on of binary decisions. For example, those relevance scores are used as weights to compute a weighted average over tokens in the vocabulary, and thereby obtain an &quot;average token&quot; for the current position in the sequence. I don&#x27;t think there&#x27;s an easy way to interpret this as an extension of some process based on branching logic.</div><br/></div></div><div id="37618545" class="c"><input type="checkbox" id="c-37618545" checked=""/><div class="controls bullet"><span class="by">ntonozzi</span><span>|</span><a href="#37614164">parent</a><span>|</span><a href="#37616179">prev</a><span>|</span><a href="#37614406">next</a><span>|</span><label class="collapse" for="c-37618545">[-]</label><label class="expand" for="c-37618545">[2 more]</label></div><br/><div class="children"><div class="content">Yes! Check out this paper describing how Linear Transformers are secretly Fast Weight Programmers: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2102.11174" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2102.11174</a>.</div><br/><div id="37621285" class="c"><input type="checkbox" id="c-37621285" checked=""/><div class="controls bullet"><span class="by">KirillPanov</span><span>|</span><a href="#37614164">root</a><span>|</span><a href="#37618545">parent</a><span>|</span><a href="#37614406">next</a><span>|</span><label class="collapse" for="c-37621285">[-]</label><label class="expand" for="c-37621285">[1 more]</label></div><br/><div class="children"><div class="content">Jürgen Schmidhuber reminds me of Richard Feynman in one very specific way: while everybody else in their respective fields uses math to hide the deep insights they&#x27;ve been mining for publications, Schmidhuber and Feynman just simply <i>tell you the big insight</i>, and then proceed to refine and illuminate it with math.</div><br/></div></div></div></div><div id="37614406" class="c"><input type="checkbox" id="c-37614406" checked=""/><div class="controls bullet"><span class="by">enriquto</span><span>|</span><a href="#37614164">parent</a><span>|</span><a href="#37618545">prev</a><span>|</span><a href="#37614501">next</a><span>|</span><label class="collapse" for="c-37614406">[-]</label><label class="expand" for="c-37614406">[13 more]</label></div><br/><div class="children"><div class="content">Neural networks <i>are</i> Turing machines.  You can make them perform any computation by carefully setting up their weights.  It would be nice to have compilers for them that were not based on approximation, though.</div><br/><div id="37618498" class="c"><input type="checkbox" id="c-37618498" checked=""/><div class="controls bullet"><span class="by">shawntan</span><span>|</span><a href="#37614164">root</a><span>|</span><a href="#37614406">parent</a><span>|</span><a href="#37617269">next</a><span>|</span><label class="collapse" for="c-37618498">[-]</label><label class="expand" for="c-37618498">[3 more]</label></div><br/><div class="children"><div class="content">Would I be able to solve the Travelling Salesman Problem with a Transformer with the appropriately assigned weights? That would be an achievement. You&#x27;d beat some known bounds of the complexity of TSP.</div><br/><div id="37619561" class="c"><input type="checkbox" id="c-37619561" checked=""/><div class="controls bullet"><span class="by">fooker</span><span>|</span><a href="#37614164">root</a><span>|</span><a href="#37618498">parent</a><span>|</span><a href="#37617269">next</a><span>|</span><label class="collapse" for="c-37619561">[-]</label><label class="expand" for="c-37619561">[2 more]</label></div><br/><div class="children"><div class="content">We don&#x27;t know.<p>And even if we could try to solve this problem, there is no known way of verifying the solution would be correct in general.</div><br/><div id="37620875" class="c"><input type="checkbox" id="c-37620875" checked=""/><div class="controls bullet"><span class="by">shawntan</span><span>|</span><a href="#37614164">root</a><span>|</span><a href="#37619561">parent</a><span>|</span><a href="#37617269">next</a><span>|</span><label class="collapse" for="c-37620875">[-]</label><label class="expand" for="c-37620875">[1 more]</label></div><br/><div class="children"><div class="content">TSPs are not unsolvable.<p>My point was that Transformers and neural networks as they are now are not Turing machines if you don&#x27;t allow for the model to grow with the input size. That said, it has to grow in &quot;depth&quot; not just parameters. The fact that people think fixed depth computation can universally compute everything is a worrying trend.</div><br/></div></div></div></div></div></div><div id="37617269" class="c"><input type="checkbox" id="c-37617269" checked=""/><div class="controls bullet"><span class="by">ComputerGuru</span><span>|</span><a href="#37614164">root</a><span>|</span><a href="#37614406">parent</a><span>|</span><a href="#37618498">prev</a><span>|</span><a href="#37614425">next</a><span>|</span><label class="collapse" for="c-37617269">[-]</label><label class="expand" for="c-37617269">[1 more]</label></div><br/><div class="children"><div class="content">The whole point is that we don’t know what rules to encode and we want the system to derive the weights for itself as part of the training process. We have compilers that can do deterministic code - that’s the normal approach.</div><br/></div></div><div id="37614425" class="c"><input type="checkbox" id="c-37614425" checked=""/><div class="controls bullet"><span class="by">PartiallyTyped</span><span>|</span><a href="#37614164">root</a><span>|</span><a href="#37614406">parent</a><span>|</span><a href="#37617269">prev</a><span>|</span><a href="#37614501">next</a><span>|</span><label class="collapse" for="c-37614425">[-]</label><label class="expand" for="c-37614425">[8 more]</label></div><br/><div class="children"><div class="content">&gt;  It would be nice to have compilers for them that were not based on approximation, though.<p>Could you elaborate?</div><br/><div id="37614521" class="c"><input type="checkbox" id="c-37614521" checked=""/><div class="controls bullet"><span class="by">enriquto</span><span>|</span><a href="#37614164">root</a><span>|</span><a href="#37614425">parent</a><span>|</span><a href="#37614501">next</a><span>|</span><label class="collapse" for="c-37614521">[-]</label><label class="expand" for="c-37614521">[7 more]</label></div><br/><div class="children"><div class="content">People typically set the weights of a neural network using heuristic approximation algorithms, by looking at a large set of example inputs&#x2F;outputs and trying to find weights that perform the needed computation as accurately as possible. This approximation process is called <i>training</i>.  But this approximation happens because nobody really knows how to set the weights otherwise.  It would be nice if we had &quot;compilers&quot; for neural networks, where you write an algorithm in a programming language, and you get a neural network (architecture+weights) that performs the same computation.<p>TFA is a beautiful step in that direction.  What I want is an automated way to do this, without having to hire vgel every time.</div><br/><div id="37615076" class="c"><input type="checkbox" id="c-37615076" checked=""/><div class="controls bullet"><span class="by">nyrikki</span><span>|</span><a href="#37614164">root</a><span>|</span><a href="#37614521">parent</a><span>|</span><a href="#37617835">next</a><span>|</span><label class="collapse" for="c-37615076">[-]</label><label class="expand" for="c-37615076">[2 more]</label></div><br/><div class="children"><div class="content">A turing complete system doesn&#x27;t necessarily mean it&#x27;s useful, it just means that it&#x27;s equivalent with a turing machine. The ability to describe any possible algorithm is not that powerful in itself.<p>As an example, algebraic type systems are often TC simply because general recursion is allowed.<p>Feed forward networks are effectively DAGs and while you may be able to express any algorithms using them they are also pairwise linear in respect to inputs.<p>Statistical learning is powerful in finding and matching patterns, but graph rewriting, which is what your doing with initial random weights and training is not trivial.<p>More importantly it doesn&#x27;t make issues like the halting problem decidable.<p>I don&#x27;t see why the same limits in graph rewriting languages which were explored in the 90s won&#x27;t hit using feed forward networks as computation systems outside of the application of nation-state scale computing power.<p>But I am open to understanding where I am wrong.</div><br/><div id="37618703" class="c"><input type="checkbox" id="c-37618703" checked=""/><div class="controls bullet"><span class="by">shawntan</span><span>|</span><a href="#37614164">root</a><span>|</span><a href="#37615076">parent</a><span>|</span><a href="#37617835">next</a><span>|</span><label class="collapse" for="c-37618703">[-]</label><label class="expand" for="c-37618703">[1 more]</label></div><br/><div class="children"><div class="content">Short version of this without the caveats: It&#x27;s not even Turing complete.<p>I review a few papers on the topic here: <a href="https:&#x2F;&#x2F;blog.wtf.sg&#x2F;posts&#x2F;2023-02-03-the-new-xor-problem&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;blog.wtf.sg&#x2F;posts&#x2F;2023-02-03-the-new-xor-problem&#x2F;</a></div><br/></div></div></div></div><div id="37617835" class="c"><input type="checkbox" id="c-37617835" checked=""/><div class="controls bullet"><span class="by">Legend2440</span><span>|</span><a href="#37614164">root</a><span>|</span><a href="#37614521">parent</a><span>|</span><a href="#37615076">prev</a><span>|</span><a href="#37615137">next</a><span>|</span><label class="collapse" for="c-37617835">[-]</label><label class="expand" for="c-37617835">[1 more]</label></div><br/><div class="children"><div class="content">The point of training is to create computer programs through optimization, because there are many problems (like understanding language) that we just don&#x27;t know how to write programs to do.<p>It&#x27;s not that we don&#x27;t know how to set the weights - neural networks are only designed with weights because it makes them easy to optimize.<p>There is no reason to use them if you plan to write your own code for them. You won&#x27;t be able to do anything that you couldn&#x27;t do in a normal programming language, because what makes NNs special <i>is</i> the training process.</div><br/></div></div><div id="37615137" class="c"><input type="checkbox" id="c-37615137" checked=""/><div class="controls bullet"><span class="by">justo-rivera</span><span>|</span><a href="#37614164">root</a><span>|</span><a href="#37614521">parent</a><span>|</span><a href="#37617835">prev</a><span>|</span><a href="#37618764">next</a><span>|</span><label class="collapse" for="c-37615137">[-]</label><label class="expand" for="c-37615137">[1 more]</label></div><br/><div class="children"><div class="content">That makes no entropy or cyber-netic sense at all. You would just get a neural network that outputs the exact formula, or algo. Like, if you would just do a sine it would be a taylor series encoded into neurons<p>Its like going from computing PI as a constant to computing it as a giantic float.<p>You lose info</div><br/></div></div><div id="37618764" class="c"><input type="checkbox" id="c-37618764" checked=""/><div class="controls bullet"><span class="by">shawntan</span><span>|</span><a href="#37614164">root</a><span>|</span><a href="#37614521">parent</a><span>|</span><a href="#37615137">prev</a><span>|</span><a href="#37614664">next</a><span>|</span><label class="collapse" for="c-37618764">[-]</label><label class="expand" for="c-37618764">[1 more]</label></div><br/><div class="children"><div class="content">Something close exists:<p>RASP <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2106.06981" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2106.06981</a><p>python implementation: <a href="https:&#x2F;&#x2F;srush.github.io&#x2F;raspy&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;srush.github.io&#x2F;raspy&#x2F;</a></div><br/></div></div><div id="37614664" class="c"><input type="checkbox" id="c-37614664" checked=""/><div class="controls bullet"><span class="by">PartiallyTyped</span><span>|</span><a href="#37614164">root</a><span>|</span><a href="#37614521">parent</a><span>|</span><a href="#37618764">prev</a><span>|</span><a href="#37614501">next</a><span>|</span><label class="collapse" for="c-37614664">[-]</label><label class="expand" for="c-37614664">[1 more]</label></div><br/><div class="children"><div class="content">Why would you do that when it&#x27;s better to do the opposite? Given a model quantize it and compile it to direct code objects that do the same thing much much much faster?<p>The generality of the approach [NNs] implies that they are effectively a union of all programs that may be represented, and as such there needs to be the capacity for that, this capacity is in size, which makes them wasteful for exact solutions.<p>it is fairly trivial to create FFNNs that behave as decision trees using just relus if you can encode your problem as a continuous problem with a finite set of inputs. Then you can very well say that this decision tree is, well, a program, and there you have it.<p>The actual problem is the encoding, which is why NNs are so powerful, that is, they learn the encodings themselves through grad descent and variants.</div><br/></div></div></div></div></div></div></div></div><div id="37614501" class="c"><input type="checkbox" id="c-37614501" checked=""/><div class="controls bullet"><span class="by">taneq</span><span>|</span><a href="#37614164">parent</a><span>|</span><a href="#37614406">prev</a><span>|</span><a href="#37617048">next</a><span>|</span><label class="collapse" for="c-37614501">[-]</label><label class="expand" for="c-37614501">[1 more]</label></div><br/><div class="children"><div class="content">It’s just any pile of linear algebra that’s been in contact with the AllSpark, right?</div><br/></div></div></div></div><div id="37617048" class="c"><input type="checkbox" id="c-37617048" checked=""/><div class="controls bullet"><span class="by">utopcell</span><span>|</span><a href="#37614164">prev</a><span>|</span><a href="#37614445">next</a><span>|</span><label class="collapse" for="c-37617048">[-]</label><label class="expand" for="c-37617048">[1 more]</label></div><br/><div class="children"><div class="content">Darn it Theia: I can&#x27;t stop playing your hoverator game!</div><br/></div></div><div id="37614445" class="c"><input type="checkbox" id="c-37614445" checked=""/><div class="controls bullet"><span class="by">tayo42</span><span>|</span><a href="#37617048">prev</a><span>|</span><a href="#37618437">next</a><span>|</span><label class="collapse" for="c-37614445">[-]</label><label class="expand" for="c-37614445">[6 more]</label></div><br/><div class="children"><div class="content">&gt; maybe even feel inspired to make your own model by hand as well!<p>Other then a learning exercise to satisfy your curiosity what are you doing with this? I&#x27;m starting to get the feeling that anything complex with ml models is unreasonable for a at home blog reader?</div><br/><div id="37619105" class="c"><input type="checkbox" id="c-37619105" checked=""/><div class="controls bullet"><span class="by">sterlind</span><span>|</span><a href="#37614445">parent</a><span>|</span><a href="#37616320">next</a><span>|</span><label class="collapse" for="c-37619105">[-]</label><label class="expand" for="c-37619105">[1 more]</label></div><br/><div class="children"><div class="content">In nanoGPT, you pre-train a model on Shakespeare, and in 3 minutes it gets to a Lewis Carroll&#x27;s Jabberwocky level of fidelity on the source material. It makes up lots of plausible-seeming old English words, learns the basics of English grammar, the layout of the plays, etc. I was pretty amazed that it got that good in such a short period of time.<p>I think locally training a bunch of models to the fidelity of Shakespeare-from-Wish.com might tell you when you&#x27;ve hit on a winning architecture, and when to try scaling up.</div><br/></div></div><div id="37616320" class="c"><input type="checkbox" id="c-37616320" checked=""/><div class="controls bullet"><span class="by">onemoresoop</span><span>|</span><a href="#37614445">parent</a><span>|</span><a href="#37619105">prev</a><span>|</span><a href="#37616190">next</a><span>|</span><label class="collapse" for="c-37616320">[-]</label><label class="expand" for="c-37616320">[2 more]</label></div><br/><div class="children"><div class="content">Author states in the first paragraph of their blog post:<p>&quot;I&#x27;ve been wanting to understand transformers and attention better for awhile now—I&#x27;d read The Illustrated Transformer, but still didn&#x27;t feel like I had an intuitive understanding of what the various pieces of attention were doing. What&#x27;s the difference between q and k? And don&#x27;t even get me started on v!&quot;</div><br/><div id="37618443" class="c"><input type="checkbox" id="c-37618443" checked=""/><div class="controls bullet"><span class="by">tayo42</span><span>|</span><a href="#37614445">root</a><span>|</span><a href="#37616320">parent</a><span>|</span><a href="#37616190">next</a><span>|</span><label class="collapse" for="c-37618443">[-]</label><label class="expand" for="c-37618443">[1 more]</label></div><br/><div class="children"><div class="content">lol three people said the same thing, i get that. thats why i said other than learning and satisfying curiosity...</div><br/></div></div></div></div><div id="37616190" class="c"><input type="checkbox" id="c-37616190" checked=""/><div class="controls bullet"><span class="by">nerdponx</span><span>|</span><a href="#37614445">parent</a><span>|</span><a href="#37616320">prev</a><span>|</span><a href="#37614550">next</a><span>|</span><label class="collapse" for="c-37616190">[-]</label><label class="expand" for="c-37616190">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s an <i>excellent</i> learning exercise, not just to satisfy curiosity but to develop and deepen understanding.</div><br/></div></div><div id="37614550" class="c"><input type="checkbox" id="c-37614550" checked=""/><div class="controls bullet"><span class="by">taneq</span><span>|</span><a href="#37614445">parent</a><span>|</span><a href="#37616190">prev</a><span>|</span><a href="#37618437">next</a><span>|</span><label class="collapse" for="c-37614550">[-]</label><label class="expand" for="c-37614550">[1 more]</label></div><br/><div class="children"><div class="content">I dunno, maybe they actually enjoy hacking on projects like this? Weird I know.</div><br/></div></div></div></div><div id="37618437" class="c"><input type="checkbox" id="c-37618437" checked=""/><div class="controls bullet"><span class="by">dang</span><span>|</span><a href="#37614445">prev</a><span>|</span><label class="collapse" for="c-37618437">[-]</label><label class="expand" for="c-37618437">[34 more]</label></div><br/><div class="children"><div class="content">[stub for sweeping offtopicness under the rug]</div><br/><div id="37618092" class="c"><input type="checkbox" id="c-37618092" checked=""/><div class="controls bullet"><span class="by">b20000</span><span>|</span><a href="#37618437">parent</a><span>|</span><a href="#37618002">next</a><span>|</span><label class="collapse" for="c-37618092">[-]</label><label class="expand" for="c-37618092">[2 more]</label></div><br/><div class="children"><div class="content">i thought, here is a guy building an electrical transformer by hand, but no, more AI shite</div><br/><div id="37618098" class="c"><input type="checkbox" id="c-37618098" checked=""/><div class="controls bullet"><span class="by">artursapek</span><span>|</span><a href="#37618437">root</a><span>|</span><a href="#37618092">parent</a><span>|</span><a href="#37618002">next</a><span>|</span><label class="collapse" for="c-37618098">[-]</label><label class="expand" for="c-37618098">[1 more]</label></div><br/><div class="children"><div class="content">same lol but this post actually looks rly good</div><br/></div></div></div></div><div id="37613592" class="c"><input type="checkbox" id="c-37613592" checked=""/><div class="controls bullet"><span class="by">seabass-labrax</span><span>|</span><a href="#37618437">parent</a><span>|</span><a href="#37618002">prev</a><span>|</span><a href="#37614139">next</a><span>|</span><label class="collapse" for="c-37613592">[-]</label><label class="expand" for="c-37613592">[14 more]</label></div><br/><div class="children"><div class="content">Minor request: can we have &#x27;neural network&#x27; or something in the title? This is related to the machine learning &#x27;transformer&#x27; architecture, rather than the bundle of coils that couples two circuits electromagnetically.</div><br/><div id="37613792" class="c"><input type="checkbox" id="c-37613792" checked=""/><div class="controls bullet"><span class="by">u_name</span><span>|</span><a href="#37618437">root</a><span>|</span><a href="#37613592">parent</a><span>|</span><a href="#37613712">next</a><span>|</span><label class="collapse" for="c-37613792">[-]</label><label class="expand" for="c-37613792">[4 more]</label></div><br/><div class="children"><div class="content">And not about &quot;alien robots who can disguise themselves by transforming into everyday machinery, primarily vehicles&quot; [1] either. :)<p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Transformers_(film)" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Transformers_(film)</a></div><br/><div id="37613978" class="c"><input type="checkbox" id="c-37613978" checked=""/><div class="controls bullet"><span class="by">HankB99</span><span>|</span><a href="#37618437">root</a><span>|</span><a href="#37613792">parent</a><span>|</span><a href="#37613824">next</a><span>|</span><label class="collapse" for="c-37613978">[-]</label><label class="expand" for="c-37613978">[1 more]</label></div><br/><div class="children"><div class="content">&quot;Transformers: More than meets the eye!&quot;<p>That was my second thought. My first thought was coils of wire wrapping iron cores done by someone who does not know what they&#x27;re doing.</div><br/></div></div><div id="37613824" class="c"><input type="checkbox" id="c-37613824" checked=""/><div class="controls bullet"><span class="by">seabass-labrax</span><span>|</span><a href="#37618437">root</a><span>|</span><a href="#37613792">parent</a><span>|</span><a href="#37613978">prev</a><span>|</span><a href="#37613712">next</a><span>|</span><label class="collapse" for="c-37613824">[-]</label><label class="expand" for="c-37613824">[2 more]</label></div><br/><div class="children"><div class="content">Ha, I didn&#x27;t think of that. But if it was, that might be more impressive than either coiling wire or writing Python code :)</div><br/><div id="37615773" class="c"><input type="checkbox" id="c-37615773" checked=""/><div class="controls bullet"><span class="by">cycomanic</span><span>|</span><a href="#37618437">root</a><span>|</span><a href="#37613824">parent</a><span>|</span><a href="#37613712">next</a><span>|</span><label class="collapse" for="c-37615773">[-]</label><label class="expand" for="c-37615773">[1 more]</label></div><br/><div class="children"><div class="content">You might appreciate this then <a href="https:&#x2F;&#x2F;m.youtube.com&#x2F;watch?v=uFmV0Xxae18">https:&#x2F;&#x2F;m.youtube.com&#x2F;watch?v=uFmV0Xxae18</a></div><br/></div></div></div></div></div></div><div id="37613712" class="c"><input type="checkbox" id="c-37613712" checked=""/><div class="controls bullet"><span class="by">sottol</span><span>|</span><a href="#37618437">root</a><span>|</span><a href="#37613592">parent</a><span>|</span><a href="#37613792">prev</a><span>|</span><a href="#37616016">next</a><span>|</span><label class="collapse" for="c-37613712">[-]</label><label class="expand" for="c-37613712">[6 more]</label></div><br/><div class="children"><div class="content">&quot;no training&quot; gave it away for me.</div><br/><div id="37613848" class="c"><input type="checkbox" id="c-37613848" checked=""/><div class="controls bullet"><span class="by">seabass-labrax</span><span>|</span><a href="#37618437">root</a><span>|</span><a href="#37613712">parent</a><span>|</span><a href="#37616016">next</a><span>|</span><label class="collapse" for="c-37613848">[-]</label><label class="expand" for="c-37613848">[5 more]</label></div><br/><div class="children"><div class="content">I misinterpreted that as &quot;I have no training; I am a beginner&quot; rather than &quot;I am not going to train this thing&quot; :)</div><br/><div id="37614881" class="c"><input type="checkbox" id="c-37614881" checked=""/><div class="controls bullet"><span class="by">chungy</span><span>|</span><a href="#37618437">root</a><span>|</span><a href="#37613848">parent</a><span>|</span><a href="#37616621">next</a><span>|</span><label class="collapse" for="c-37614881">[-]</label><label class="expand" for="c-37614881">[2 more]</label></div><br/><div class="children"><div class="content">That&#x27;s how I interpreted as well.<p>&quot;Electrical transformer without any formal training in electric engineering&quot; is basically how the title read to me. Followed by a lot of confusion when the article was not that.<p>Good on him, it&#x27;s just... an odd way to phrase it :)</div><br/><div id="37616451" class="c"><input type="checkbox" id="c-37616451" checked=""/><div class="controls bullet"><span class="by">eichin</span><span>|</span><a href="#37618437">root</a><span>|</span><a href="#37614881">parent</a><span>|</span><a href="#37616621">next</a><span>|</span><label class="collapse" for="c-37616451">[-]</label><label class="expand" for="c-37616451">[1 more]</label></div><br/><div class="children"><div class="content">Heh, I went a step further and thought &quot;ooh, a novice EE project, but titling it like that everyone&#x27;s going to be confused about it involving machine learning&quot; until I realized I was the one that got it backwards...</div><br/></div></div></div></div><div id="37616621" class="c"><input type="checkbox" id="c-37616621" checked=""/><div class="controls bullet"><span class="by">kulahan</span><span>|</span><a href="#37618437">root</a><span>|</span><a href="#37613848">parent</a><span>|</span><a href="#37614881">prev</a><span>|</span><a href="#37614075">next</a><span>|</span><label class="collapse" for="c-37616621">[-]</label><label class="expand" for="c-37616621">[1 more]</label></div><br/><div class="children"><div class="content">I didn&#x27;t even consider that it COULD be about LLMs until I saw the comment above.</div><br/></div></div><div id="37614075" class="c"><input type="checkbox" id="c-37614075" checked=""/><div class="controls bullet"><span class="by">poniko</span><span>|</span><a href="#37618437">root</a><span>|</span><a href="#37613848">parent</a><span>|</span><a href="#37616621">prev</a><span>|</span><a href="#37616016">next</a><span>|</span><label class="collapse" for="c-37614075">[-]</label><label class="expand" for="c-37614075">[1 more]</label></div><br/><div class="children"><div class="content">Though he was a novis and had no training in the field.</div><br/></div></div></div></div></div></div><div id="37616016" class="c"><input type="checkbox" id="c-37616016" checked=""/><div class="controls bullet"><span class="by">cypress66</span><span>|</span><a href="#37618437">root</a><span>|</span><a href="#37613592">parent</a><span>|</span><a href="#37613712">prev</a><span>|</span><a href="#37613989">next</a><span>|</span><label class="collapse" for="c-37616016">[-]</label><label class="expand" for="c-37616016">[1 more]</label></div><br/><div class="children"><div class="content">Funny how even though I studied EE, it didn&#x27;t cross my mind it could be an electrical transformer.</div><br/></div></div><div id="37613989" class="c"><input type="checkbox" id="c-37613989" checked=""/><div class="controls bullet"><span class="by">freecodyx</span><span>|</span><a href="#37618437">root</a><span>|</span><a href="#37613592">parent</a><span>|</span><a href="#37616016">prev</a><span>|</span><a href="#37618142">next</a><span>|</span><label class="collapse" for="c-37613989">[-]</label><label class="expand" for="c-37613989">[1 more]</label></div><br/><div class="children"><div class="content">I also thought the physical transformer</div><br/></div></div><div id="37618142" class="c"><input type="checkbox" id="c-37618142" checked=""/><div class="controls bullet"><span class="by">3abiton</span><span>|</span><a href="#37618437">root</a><span>|</span><a href="#37613592">parent</a><span>|</span><a href="#37613989">prev</a><span>|</span><a href="#37614139">next</a><span>|</span><label class="collapse" for="c-37618142">[-]</label><label class="expand" for="c-37618142">[1 more]</label></div><br/><div class="children"><div class="content">It&#x27;s always mildly annoying when different technologies have the same name or acronym</div><br/></div></div></div></div><div id="37614139" class="c"><input type="checkbox" id="c-37614139" checked=""/><div class="controls bullet"><span class="by">p1esk</span><span>|</span><a href="#37618437">parent</a><span>|</span><a href="#37613592">prev</a><span>|</span><label class="collapse" for="c-37614139">[-]</label><label class="expand" for="c-37614139">[16 more]</label></div><br/><div class="children"><div class="content">Somehow I expected he made it out of mechanical parts. Like literally “by hand”.</div><br/><div id="37614363" class="c"><input type="checkbox" id="c-37614363" checked=""/><div class="controls bullet"><span class="by">jebarker</span><span>|</span><a href="#37618437">root</a><span>|</span><a href="#37614139">parent</a><span>|</span><a href="#37614142">next</a><span>|</span><label class="collapse" for="c-37614363">[-]</label><label class="expand" for="c-37614363">[14 more]</label></div><br/><div class="children"><div class="content">Nitpick: the author is female</div><br/><div id="37614534" class="c"><input type="checkbox" id="c-37614534" checked=""/><div class="controls bullet"><span class="by">melenaboija</span><span>|</span><a href="#37618437">root</a><span>|</span><a href="#37614363">parent</a><span>|</span><a href="#37617679">prev</a><span>|</span><a href="#37614142">next</a><span>|</span><label class="collapse" for="c-37614534">[-]</label><label class="expand" for="c-37614534">[9 more]</label></div><br/><div class="children"><div class="content">Yo, Twitter is not closed is just that they renamed it X. These colorful comments deserve to have the right audience.</div><br/><div id="37614557" class="c"><input type="checkbox" id="c-37614557" checked=""/><div class="controls bullet"><span class="by">jebarker</span><span>|</span><a href="#37618437">root</a><span>|</span><a href="#37614534">parent</a><span>|</span><a href="#37614683">next</a><span>|</span><label class="collapse" for="c-37614557">[-]</label><label class="expand" for="c-37614557">[6 more]</label></div><br/><div class="children"><div class="content">This isn&#x27;t a color comment. It&#x27;s a small correction to a factual error. No culture war to be had here.</div><br/></div></div><div id="37614683" class="c"><input type="checkbox" id="c-37614683" checked=""/><div class="controls bullet"><span class="by">jacoblambda</span><span>|</span><a href="#37618437">root</a><span>|</span><a href="#37614534">parent</a><span>|</span><a href="#37614557">prev</a><span>|</span><a href="#37614861">next</a><span>|</span><label class="collapse" for="c-37614683">[-]</label><label class="expand" for="c-37614683">[1 more]</label></div><br/><div class="children"><div class="content">They aren&#x27;t being mean. They are just nitting that s&#x2F;he&#x2F;she&#x2F;g</div><br/></div></div><div id="37614861" class="c"><input type="checkbox" id="c-37614861" checked=""/><div class="controls bullet"><span class="by">beebeepka</span><span>|</span><a href="#37618437">root</a><span>|</span><a href="#37614534">parent</a><span>|</span><a href="#37614683">prev</a><span>|</span><a href="#37614142">next</a><span>|</span><label class="collapse" for="c-37614861">[-]</label><label class="expand" for="c-37614861">[1 more]</label></div><br/><div class="children"><div class="content">What the hell are you talking about?</div><br/></div></div></div></div></div></div><div id="37614142" class="c"><input type="checkbox" id="c-37614142" checked=""/><div class="controls bullet"><span class="by">sitzkrieg</span><span>|</span><a href="#37618437">root</a><span>|</span><a href="#37614139">parent</a><span>|</span><a href="#37614363">prev</a><span>|</span><label class="collapse" for="c-37614142">[-]</label><label class="expand" for="c-37614142">[1 more]</label></div><br/><div class="children"><div class="content">me too, i was hoping to see a flyback transformer wound with a power drill or something</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>