<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1693472463117" as="style"/><link rel="stylesheet" href="styles.css?v=1693472463117"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://www.evanjones.ca/random-load-balancing-is-uneven.html">Random Load Balancing Is Unevenly Distributed</a> <span class="domain">(<a href="https://www.evanjones.ca">www.evanjones.ca</a>)</span></div><div class="subtext"><span>zdw</span> | <span>13 comments</span></div><br/><div><div id="37332914" class="c"><input type="checkbox" id="c-37332914" checked=""/><div class="controls bullet"><span class="by">phamilton</span><span>|</span><a href="#37332890">next</a><span>|</span><label class="collapse" for="c-37332914">[-]</label><label class="expand" for="c-37332914">[2 more]</label></div><br/><div class="children"><div class="content">We&#x27;ve used both round robin and least outstanding connections in AWS and found many really weird phenomena over the years.<p>In general, LOC is more forgiving for uneven workloads, especially for Ruby services. If a slow request hits, that machine gets less traffic than the others and gets some room to recover and burn off its queue.<p>Where we found weird problems was at the extremes: very low concurrency and exceptionally high concurrency.<p>Low concurrency meant that one host consistently had 2 connections and another consistently had 3. That&#x27;s 50% more load on one host. I believe AWS has fixed this and now tie-breakers are resolved randomly, but it was kinda painful to deal with. Our solution was to scale vertically and run fewer hosts. 19 on one host vs 20 on another was less jarring.<p>The other extreme was 1k+ websocket connections per host. When we autoscaled, it would add a new host. That host would have 0 connections and so the next 1k connections would all go to the new host. For that system, we changed it back to round robin.</div><br/><div id="37333441" class="c"><input type="checkbox" id="c-37333441" checked=""/><div class="controls bullet"><span class="by">saghm</span><span>|</span><a href="#37332914">parent</a><span>|</span><a href="#37332890">next</a><span>|</span><label class="collapse" for="c-37333441">[-]</label><label class="expand" for="c-37333441">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Low concurrency meant that one host consistently had 2 connections and another consistently had 3. That&#x27;s 50% more load on one host. I believe AWS has fixed this and now tie-breakers are resolved randomly, but it was kinda painful to deal with. Our solution was to scale vertically and run fewer hosts. 19 on one host vs 20 on another was less jarring.<p>I assume you&#x27;re saying that this happened over a period much longer than the lifetime of a connection? If all of the connections are super long lived, I&#x27;m not really sure what else the load balancer could do, other than forcibly close one and make the client swap to the other server, but naively that doesn&#x27;t really seem like something I&#x27;d want a load balancer to do.</div><br/></div></div></div></div><div id="37332890" class="c"><input type="checkbox" id="c-37332890" checked=""/><div class="controls bullet"><span class="by">upon_drumhead</span><span>|</span><a href="#37332914">prev</a><span>|</span><a href="#37333670">next</a><span>|</span><label class="collapse" for="c-37332890">[-]</label><label class="expand" for="c-37332890">[1 more]</label></div><br/><div class="children"><div class="content">The &quot;Power of Two Random Choices&quot; was previously (and much better) discussed at <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37143376">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37143376</a></div><br/></div></div><div id="37333670" class="c"><input type="checkbox" id="c-37333670" checked=""/><div class="controls bullet"><span class="by">plasma</span><span>|</span><a href="#37332890">prev</a><span>|</span><a href="#37332785">next</a><span>|</span><label class="collapse" for="c-37333670">[-]</label><label class="expand" for="c-37333670">[1 more]</label></div><br/><div class="children"><div class="content">Worth noting the difference between an AWS Application Load Balancer (ALB) that is HTTP request aware, and Network Load Balancer (NLB) which is not, when load balancing HTTP traffic.<p>AWS ALB (and others I&#x27;m sure) can balance by &quot;Least outstanding requests&quot; [1] which means a server with the least in-flight HTTP requests (not keep-alive network connections!) to an app server will be chosen.<p>If the balancer operates on the network level (eg NLB) and it maintains keep-alive connections to servers, the balancing won&#x27;t be as even from a HTTP request perspective because a keep-alive connection may or may not be processing a request right now and so the request will be routed based on number of TCP connections to app servers, not current HTTP request activity to them.<p>[1] <a href="https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;elasticloadbalancing&#x2F;latest&#x2F;application&#x2F;load-balancer-target-groups.html#modify-routing-algorithm" rel="nofollow noreferrer">https:&#x2F;&#x2F;docs.aws.amazon.com&#x2F;elasticloadbalancing&#x2F;latest&#x2F;appl...</a></div><br/></div></div><div id="37332785" class="c"><input type="checkbox" id="c-37332785" checked=""/><div class="controls bullet"><span class="by">ggm</span><span>|</span><a href="#37333670">prev</a><span>|</span><a href="#37334088">next</a><span>|</span><label class="collapse" for="c-37332785">[-]</label><label class="expand" for="c-37332785">[2 more]</label></div><br/><div class="children"><div class="content">All things which have too few bins under random are at risk of asymmetry. Some LB situations take some approximation to load&#x2F;responsiveness, and back off the counts as a function of observed pace of response. TCP RTT will do, size of the pending Queue for async responses too, time in queue, there&#x27;s lots of measures. Your liveness check to the backends probably provides this simply by how responsive to heartbeat it is.<p>Given tasks do not run in identical time, there&#x27;s going to be variance come what may.<p>I would suggest a pool of 3 should tend to be more stable. But, your platform people may be recommending that 3 and below need to be sized to work as 1, only 4 and above can start to assume a minimum cohort (2?) exist.<p>Sharding is not LB. It&#x27;s just a hash approximation to grouping things. Maybe you&#x27;re telling a story about using a random() to emulate how far out of 50&#x2F;50 a 2 part sharding story can get?<p>Routers doing LB may well do simple prefix triage. You may be stuck on one side of the LB because of who you are, and nothings going to change it.<p>(I don&#x27;t do this for a living, wiser people may tell you I&#x27;m wrong)</div><br/><div id="37333103" class="c"><input type="checkbox" id="c-37333103" checked=""/><div class="controls bullet"><span class="by">beardedwizard</span><span>|</span><a href="#37332785">parent</a><span>|</span><a href="#37334088">next</a><span>|</span><label class="collapse" for="c-37333103">[-]</label><label class="expand" for="c-37333103">[1 more]</label></div><br/><div class="children"><div class="content">Typically network LB will look at the entire layer 4 tuple to overcome things like NAT clustering traffic from many users into one place. Not to say IP lb doesn&#x27;t happen, but with NAT being de-facto almost nobody does it IME.</div><br/></div></div></div></div><div id="37334088" class="c"><input type="checkbox" id="c-37334088" checked=""/><div class="controls bullet"><span class="by">bullen</span><span>|</span><a href="#37332785">prev</a><span>|</span><a href="#37333367">next</a><span>|</span><label class="collapse" for="c-37334088">[-]</label><label class="expand" for="c-37334088">[1 more]</label></div><br/><div class="children"><div class="content">DNS should perform round-robin in the order of the IPs in the UDP packet. Otherwise you have no control.<p>Just like JSON (and js) hashmaps should really be treemaps and retain order.<p>Last but not least you should be able to make a TCP stream loose order. So you get UDP speed without changing pipes&#x2F;ports and all that jazz.<p>It&#x27;s all about doing that job when you design a protocol.</div><br/></div></div><div id="37333367" class="c"><input type="checkbox" id="c-37333367" checked=""/><div class="controls bullet"><span class="by">mratsim</span><span>|</span><a href="#37334088">prev</a><span>|</span><a href="#37332836">next</a><span>|</span><label class="collapse" for="c-37333367">[-]</label><label class="expand" for="c-37333367">[1 more]</label></div><br/><div class="children"><div class="content">Work-stealing has been there since 1996 and is optimal if greedy. Original Cilk paper: <a href="http:&#x2F;&#x2F;supertech.csail.mit.edu&#x2F;papers&#x2F;cilkjpdc96.pdf" rel="nofollow noreferrer">http:&#x2F;&#x2F;supertech.csail.mit.edu&#x2F;papers&#x2F;cilkjpdc96.pdf</a><p>There are other techniques that are optimal like Parallel Depth First scheduling from Blelloch.<p>Since then there has been a lot of work for distributed work-stealing.<p>Boggles my mind that at &quot;cloud-scale&quot; people still use round robin and assume uniform workloads and that at the server level there is uniform load as well.</div><br/></div></div><div id="37332836" class="c"><input type="checkbox" id="c-37332836" checked=""/><div class="controls bullet"><span class="by">tedunangst</span><span>|</span><a href="#37333367">prev</a><span>|</span><a href="#37332629">next</a><span>|</span><label class="collapse" for="c-37332836">[-]</label><label class="expand" for="c-37332836">[2 more]</label></div><br/><div class="children"><div class="content">The more unrealistic assumption is that every work item or request takes uniform processing. Things really blow up when you have a pile of p99 slow requests.</div><br/><div id="37333068" class="c"><input type="checkbox" id="c-37333068" checked=""/><div class="controls bullet"><span class="by">beardedwizard</span><span>|</span><a href="#37332836">parent</a><span>|</span><a href="#37332629">next</a><span>|</span><label class="collapse" for="c-37333068">[-]</label><label class="expand" for="c-37333068">[1 more]</label></div><br/><div class="children"><div class="content">This. Load balancing based on count assumes every request is served in equal time and resources.<p>I think what the article misses highlighting is that this is about stateless load balancing which improves throughput at the LB, at the cost of potentially suboptimal distribution.<p>The other nit I have is calling hashing random, it&#x27;s not, and a lot goes into selecting hash algorithms for LB that don&#x27;t result in mass movement of requests every time a server is added or removed from the pool.<p>If the author wants stateless balanced load and assumes all requests are equal, why not use an approach like round robin?<p>Beyond that, you can get into more stateful methods that look at CPU use on servers, or any other metric you like to load balance on the resource you care about, ie memory or CPU vs number of open sockets.</div><br/></div></div></div></div><div id="37332629" class="c"><input type="checkbox" id="c-37332629" checked=""/><div class="controls bullet"><span class="by">hinkley</span><span>|</span><a href="#37332836">prev</a><span>|</span><a href="#37333119">next</a><span>|</span><label class="collapse" for="c-37332629">[-]</label><label class="expand" for="c-37332629">[1 more]</label></div><br/><div class="children"><div class="content">Why does the first case have a chart and the second case just vomits out numbers, which incidentally don&#x27;t fit into the page template?</div><br/></div></div><div id="37333119" class="c"><input type="checkbox" id="c-37333119" checked=""/><div class="controls bullet"><span class="by">chewbacha</span><span>|</span><a href="#37332629">prev</a><span>|</span><label class="collapse" for="c-37333119">[-]</label><label class="expand" for="c-37333119">[1 more]</label></div><br/><div class="children"><div class="content">So, the real problem isn’t noted until towards the end which is that these methods require keeping some state, thus making the requests more expensive.<p>It’s a good trade off when throughput is low and the impact of lumpy traffic is high. At higher volumes, random allows stateless load balancing.<p>Use the tool that makes sense for your project. Even at the same system I’ll have some services that operate at low volumes with high latency and others with high volume and low latency.</div><br/></div></div></div></div></div></div></div></body></html>