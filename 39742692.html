<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1710838876782" as="style"/><link rel="stylesheet" href="styles.css?v=1710838876782"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://lwn.net/SubscriberLink/964735/8b795f23495af1d4/">Cranelift code generation comes to Rust</a> <span class="domain">(<a href="https://lwn.net">lwn.net</a>)</span></div><div class="subtext"><span>ridruejo</span> | <span>100 comments</span></div><br/><div><div id="39745528" class="c"><input type="checkbox" id="c-39745528" checked=""/><div class="controls bullet"><span class="by">CodesInChaos</span><span>|</span><a href="#39743558">next</a><span>|</span><label class="collapse" for="c-39745528">[-]</label><label class="expand" for="c-39745528">[13 more]</label></div><br/><div class="children"><div class="content">You can use different backends and optimization for different crates. It often makes sense to use optimized LLVM builds for dependencies, and debug LLVM or even Cranelift for your own code.<p>See <a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;rust&#x2F;comments&#x2F;1bhpfeb&#x2F;vastly_improved_recompile_times_in_rust_with&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;rust&#x2F;comments&#x2F;1bhpfeb&#x2F;vastly_improv...</a></div><br/><div id="39747136" class="c"><input type="checkbox" id="c-39747136" checked=""/><div class="controls bullet"><span class="by">zozbot234</span><span>|</span><a href="#39745528">parent</a><span>|</span><a href="#39743558">next</a><span>|</span><label class="collapse" for="c-39747136">[-]</label><label class="expand" for="c-39747136">[12 more]</label></div><br/><div class="children"><div class="content">I would not expect this to work without issues, as Rust does not support a stable binary ABI across different compiler versions.  How can we be sure that the two &quot;codegen backends&quot; will always be implementing the same binary ABI?</div><br/><div id="39749112" class="c"><input type="checkbox" id="c-39749112" checked=""/><div class="controls bullet"><span class="by">aseipp</span><span>|</span><a href="#39745528">root</a><span>|</span><a href="#39747136">parent</a><span>|</span><a href="#39747669">next</a><span>|</span><label class="collapse" for="c-39749112">[-]</label><label class="expand" for="c-39749112">[1 more]</label></div><br/><div class="children"><div class="content">Because the ABI is defined and implemented by shared code in the runtime and compiler. There is no need for cross version compatibility between the two backends, only compatibility within the same version. This isn&#x27;t particularly new either, FWIW. The Glasgow Haskell Compiler also has an unstable ABI that is not standardized, but LLVM compiled code interoperates seamlessly with code generated by the non-LLVM compiler backend (the mechanism by which that is achieved is likely different though.)</div><br/></div></div><div id="39747669" class="c"><input type="checkbox" id="c-39747669" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#39745528">root</a><span>|</span><a href="#39747136">parent</a><span>|</span><a href="#39749112">prev</a><span>|</span><a href="#39749026">next</a><span>|</span><label class="collapse" for="c-39747669">[-]</label><label class="expand" for="c-39747669">[1 more]</label></div><br/><div class="children"><div class="content">In theory yes, this could be problematic with alternative Rust implementations, but it works fine and many people are using it in this case. We can be sure LLVM and Cranelift codegen backends will be implementing the same binary ABI, because binary ABI decisions are made in the shared code.</div><br/></div></div><div id="39749026" class="c"><input type="checkbox" id="c-39749026" checked=""/><div class="controls bullet"><span class="by">devit</span><span>|</span><a href="#39745528">root</a><span>|</span><a href="#39747136">parent</a><span>|</span><a href="#39747669">prev</a><span>|</span><a href="#39747691">next</a><span>|</span><label class="collapse" for="c-39749026">[-]</label><label class="expand" for="c-39749026">[1 more]</label></div><br/><div class="children"><div class="content">Presumably the LLVM and Cranelift backends in the same rustc version generate code with the same ABI, and it would be a bug if that wasn&#x27;t the case.</div><br/></div></div><div id="39747691" class="c"><input type="checkbox" id="c-39747691" checked=""/><div class="controls bullet"><span class="by">aw1621107</span><span>|</span><a href="#39745528">root</a><span>|</span><a href="#39747136">parent</a><span>|</span><a href="#39749026">prev</a><span>|</span><a href="#39749166">next</a><span>|</span><label class="collapse" for="c-39747691">[-]</label><label class="expand" for="c-39747691">[5 more]</label></div><br/><div class="children"><div class="content">I suppose that might depend on how the ABI is represented internally. If the ABI is fully described by (one of?) the lowered IR the backends consume (e.g., does MIR fully describe struct layouts&#x2F;etc.) then I wouldn&#x27;t expect there to be any issues outside &quot;regular&quot; bugs since all the relevant information would be contained in the inputs.</div><br/><div id="39747775" class="c"><input type="checkbox" id="c-39747775" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#39745528">root</a><span>|</span><a href="#39747691">parent</a><span>|</span><a href="#39749166">next</a><span>|</span><label class="collapse" for="c-39747775">[-]</label><label class="expand" for="c-39747775">[4 more]</label></div><br/><div class="children"><div class="content">It is done in the shared code, see <a href="https:&#x2F;&#x2F;rustc-dev-guide.rust-lang.org&#x2F;backend&#x2F;backend-agnostic.html" rel="nofollow">https:&#x2F;&#x2F;rustc-dev-guide.rust-lang.org&#x2F;backend&#x2F;backend-agnost...</a> for details.</div><br/><div id="39748781" class="c"><input type="checkbox" id="c-39748781" checked=""/><div class="controls bullet"><span class="by">CodesInChaos</span><span>|</span><a href="#39745528">root</a><span>|</span><a href="#39747775">parent</a><span>|</span><a href="#39750287">next</a><span>|</span><label class="collapse" for="c-39748781">[-]</label><label class="expand" for="c-39748781">[2 more]</label></div><br/><div class="children"><div class="content">Struct layout happening in generic code makes sense (and is actually required since you can reference it in `const`s). It seems unlikely that they made function calling fully backend agnostic, since it&#x27;d require assigning the registers for parameters and result in generic code, and not in the backend.<p>I&#x27;d expect the generic code to lower the function parameters to primitive types (pointers, ints, floats, etc.), but the backend would then distribute those over registers and&#x2F;or stack. Keeping that compatible would still require an (unstable) specification implemented by all compatible backends.<p>Unwinding might be tricky as well.</div><br/><div id="39751672" class="c"><input type="checkbox" id="c-39751672" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#39745528">root</a><span>|</span><a href="#39748781">parent</a><span>|</span><a href="#39750287">next</a><span>|</span><label class="collapse" for="c-39751672">[-]</label><label class="expand" for="c-39751672">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I&#x27;d expect the generic code to lower the function parameters to primitive types (pointers, ints, floats, etc.), but the backend would then distribute those over registers and&#x2F;or stack<p>Not sure about that. Calling conventions are defined by the platform ABI which is what the backend implements, so any conforming backend should still be mutually invokable. That&#x27;s why a Rust program can emit an ABI-stable C API and why you can call GCC built libraries from an LLVM built executable. The lowering of parameters to registers is constrained by this because the intermediate representation understands that what are parameters to functions &amp; then follows the platform ABI to lower it to function calls.</div><br/></div></div></div></div><div id="39750287" class="c"><input type="checkbox" id="c-39750287" checked=""/><div class="controls bullet"><span class="by">aw1621107</span><span>|</span><a href="#39745528">root</a><span>|</span><a href="#39747775">parent</a><span>|</span><a href="#39748781">prev</a><span>|</span><a href="#39749166">next</a><span>|</span><label class="collapse" for="c-39750287">[-]</label><label class="expand" for="c-39750287">[1 more]</label></div><br/><div class="children"><div class="content">Ah, I think that&#x27;s about what I had in mind. Just didn&#x27;t know what to look for. Thanks!</div><br/></div></div></div></div></div></div><div id="39747619" class="c"><input type="checkbox" id="c-39747619" checked=""/><div class="controls bullet"><span class="by">Ar-Curunir</span><span>|</span><a href="#39745528">root</a><span>|</span><a href="#39747136">parent</a><span>|</span><a href="#39749166">prev</a><span>|</span><a href="#39749078">next</a><span>|</span><label class="collapse" for="c-39747619">[-]</label><label class="expand" for="c-39747619">[1 more]</label></div><br/><div class="children"><div class="content">This is for local work, so you use the same compiler for your dependencies and for your own code. Only the codegen backend differs.</div><br/></div></div><div id="39749078" class="c"><input type="checkbox" id="c-39749078" checked=""/><div class="controls bullet"><span class="by">PartiallyTyped</span><span>|</span><a href="#39745528">root</a><span>|</span><a href="#39747136">parent</a><span>|</span><a href="#39747619">prev</a><span>|</span><a href="#39743558">next</a><span>|</span><label class="collapse" for="c-39749078">[-]</label><label class="expand" for="c-39749078">[1 more]</label></div><br/><div class="children"><div class="content">If you do the compilation locally, you know the structure of the ABI for the compiled code. You know that regardless of the backend you are using. At this stage, no functions with generics are compiled, only what is non-generic.<p>What is left is to account for the ABI in any monomorphizations that occur at the boundary, i.e. when your own structures monomorphize generic functions in the dependency.<p>When the compiler creates this monomorphic variant, and lowers down, it can provide the necessary details of the ABI.</div><br/></div></div></div></div></div></div><div id="39743558" class="c"><input type="checkbox" id="c-39743558" checked=""/><div class="controls bullet"><span class="by">chrisaycock</span><span>|</span><a href="#39745528">prev</a><span>|</span><a href="#39745829">next</a><span>|</span><label class="collapse" for="c-39743558">[-]</label><label class="expand" for="c-39743558">[18 more]</label></div><br/><div class="children"><div class="content">This article provides an excellent overview of the latest in <i>speed of optimizer</i> vs <i>quality of optimization</i>.<p>In particular, copy-and-patch compilation is still the fastest approach because it uses pre-compiled code, though leaves little room for optimization.<p>Cranelift uses e-graphs to represent equivalence on the IR. This allows for more optimizations than the copy-and-patch approach.<p>Of course, the most optimized output is going to come from a more traditional compiler toolchain like LLVM or GCC. But for users who want to get &quot;fast enough&quot; output as quickly as possible, newer compiler techniques provide a promising alternative.</div><br/><div id="39743635" class="c"><input type="checkbox" id="c-39743635" checked=""/><div class="controls bullet"><span class="by">weinzierl</span><span>|</span><a href="#39743558">parent</a><span>|</span><a href="#39744770">next</a><span>|</span><label class="collapse" for="c-39743635">[-]</label><label class="expand" for="c-39743635">[4 more]</label></div><br/><div class="children"><div class="content">From what I understand, the big advantage of the e-graphs approach is, that the quality of the output is (within limits) a function the time and memory given.<p>The more memory, the more nodes can be generated in the e-graph and the more time for search, the better the selected node.<p>It might never be as fast as copy-and-patch or as good as LLVM or GCC, but this flexibility is a value in itself.</div><br/><div id="39752112" class="c"><input type="checkbox" id="c-39752112" checked=""/><div class="controls bullet"><span class="by">cfallin</span><span>|</span><a href="#39743558">root</a><span>|</span><a href="#39743635">parent</a><span>|</span><a href="#39749744">next</a><span>|</span><label class="collapse" for="c-39752112">[-]</label><label class="expand" for="c-39752112">[1 more]</label></div><br/><div class="children"><div class="content">We actually take a fairly unconventional approach to e-graphs: we have a few linear passes and we do all rewrites eagerly, so we use them to provide a general framework for the fixpoint problem into which we plug in all our rewrites, but we don&#x27;t have the usual &quot;apply as much CPU time as you want to get better results&quot; property of conventional equality saturation.<p>I gave a talk about this approach, aegraphs (acyclic e-graphs), here: slides (<a href="https:&#x2F;&#x2F;cfallin.org&#x2F;pubs&#x2F;egraphs2023_aegraphs_slides.pdf" rel="nofollow">https:&#x2F;&#x2F;cfallin.org&#x2F;pubs&#x2F;egraphs2023_aegraphs_slides.pdf</a>), video (<a href="https:&#x2F;&#x2F;vimeo.com&#x2F;843540328" rel="nofollow">https:&#x2F;&#x2F;vimeo.com&#x2F;843540328</a>)<p>(disclosure: Cranelift tech lead 2020-2022 and main author of the e-graphs mid-end, as well as regalloc, isel and its custom DSL, and other bits, along with the excellent team)</div><br/></div></div><div id="39749744" class="c"><input type="checkbox" id="c-39749744" checked=""/><div class="controls bullet"><span class="by">thechao</span><span>|</span><a href="#39743558">root</a><span>|</span><a href="#39743635">parent</a><span>|</span><a href="#39752112">prev</a><span>|</span><a href="#39744770">next</a><span>|</span><label class="collapse" for="c-39749744">[-]</label><label class="expand" for="c-39749744">[2 more]</label></div><br/><div class="children"><div class="content">When we first reviewed the equality saturation paper, we thought there was one major pro, and one major con: [pro] phase-ordering invariant; [con] at the time there was no (believable) way to extract the transformed graph in a non-hand-wavy-way.<p>Personally, I think e-graphs should be combined with Massalin superoptimization — they&#x27;re natural &quot;duals&quot; — and just turn the whole exercise into a hill-climbing process. You can tune the total effort by the set of passes used, the amount of time to drive the graph to saturation, and the method (and time) for graph extraction.</div><br/><div id="39751682" class="c"><input type="checkbox" id="c-39751682" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#39743558">root</a><span>|</span><a href="#39749744">parent</a><span>|</span><a href="#39744770">next</a><span>|</span><label class="collapse" for="c-39751682">[-]</label><label class="expand" for="c-39751682">[1 more]</label></div><br/><div class="children"><div class="content">Can you memoize across invocations so that the time spent optimizing is cumulative across all builds?</div><br/></div></div></div></div></div></div><div id="39744770" class="c"><input type="checkbox" id="c-39744770" checked=""/><div class="controls bullet"><span class="by">vsnf</span><span>|</span><a href="#39743558">parent</a><span>|</span><a href="#39743635">prev</a><span>|</span><a href="#39743664">next</a><span>|</span><label class="collapse" for="c-39744770">[-]</label><label class="expand" for="c-39744770">[11 more]</label></div><br/><div class="children"><div class="content">Is there any literature or  guidelines on what to do if I&#x27;m willing to spend effectively unlimited CPU cycles in return for a more optimized final output?</div><br/><div id="39744833" class="c"><input type="checkbox" id="c-39744833" checked=""/><div class="controls bullet"><span class="by">thesz</span><span>|</span><a href="#39743558">root</a><span>|</span><a href="#39744770">parent</a><span>|</span><a href="#39745213">next</a><span>|</span><label class="collapse" for="c-39744833">[-]</label><label class="expand" for="c-39744833">[9 more]</label></div><br/><div class="children"><div class="content">Superoptimizers: <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Superoptimization" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Superoptimization</a><p>Also, program distillation: <a href="https:&#x2F;&#x2F;www.researchgate.net&#x2F;publication&#x2F;220989887_Distillation_Extracting_the_essence_of_programs" rel="nofollow">https:&#x2F;&#x2F;www.researchgate.net&#x2F;publication&#x2F;220989887_Distillat...</a></div><br/><div id="39749219" class="c"><input type="checkbox" id="c-39749219" checked=""/><div class="controls bullet"><span class="by">mort96</span><span>|</span><a href="#39743558">root</a><span>|</span><a href="#39744833">parent</a><span>|</span><a href="#39745213">next</a><span>|</span><label class="collapse" for="c-39749219">[-]</label><label class="expand" for="c-39749219">[8 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t it a bit weird that this isn&#x27;t just the standard? Like imagine if Chrome was optimized with such a superoptimizer; let the optimizer spend a couple hours every month or so when cutting a new release. Surely that has to be worth it?</div><br/><div id="39750060" class="c"><input type="checkbox" id="c-39750060" checked=""/><div class="controls bullet"><span class="by">MaxBarraclough</span><span>|</span><a href="#39743558">root</a><span>|</span><a href="#39749219">parent</a><span>|</span><a href="#39749711">next</a><span>|</span><label class="collapse" for="c-39750060">[-]</label><label class="expand" for="c-39750060">[3 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not a compiler engineer, but I think it&#x27;s a diminishing returns issue. Modern optimising compilers are already impressive, and they get more impressive year on year, but only quite slowly. We don&#x27;t see compilers producing code that runs 30% faster than the code generated last year, for instance.<p>Such improvements can happen with compiler updates, but not when the starting point is a compiler that&#x27;s already of decent quality. I recall some GPU driver updates from ATi (years ago) delivered pretty drastic performance improvements, but I believe that&#x27;s because the original drivers were rather primitive.<p>(Perhaps a drastic improvement to autovectorisation could give a 30% boost, or better, but this would apply only to certain programs.)<p>You could grant a compute budget 100x the typical build set-up, but no one has built a production-ready compiler to take advantage of that, and if they did I suspect the improvements would be unimpressive. They may also run into a &#x27;complexity ceiling&#x27; issue, as the compiler would presumably be even more complex than today&#x27;s ordinary optimising compilers, which are already enormous.<p>As Filligree says, superoptimisers tend to only be practical for very short programs. They can&#x27;t be applied to monstrous codebases like Chromium.</div><br/><div id="39751023" class="c"><input type="checkbox" id="c-39751023" checked=""/><div class="controls bullet"><span class="by">thesz</span><span>|</span><a href="#39743558">root</a><span>|</span><a href="#39750060">parent</a><span>|</span><a href="#39750600">next</a><span>|</span><label class="collapse" for="c-39751023">[-]</label><label class="expand" for="c-39751023">[1 more]</label></div><br/><div class="children"><div class="content">The ability of compilers to make code faster is 12 (twelve) times slower than Moore&#x27;s law: they double the program&#x27;s speed in 18 years [1].<p>[1] <a href="https:&#x2F;&#x2F;proebsting.cs.arizona.edu&#x2F;law.html" rel="nofollow">https:&#x2F;&#x2F;proebsting.cs.arizona.edu&#x2F;law.html</a><p>This might seem discouraging, but it is not - one can still reap the benefits of code optimization twelve times as long after Moore&#x27;s law stops working.</div><br/></div></div><div id="39750600" class="c"><input type="checkbox" id="c-39750600" checked=""/><div class="controls bullet"><span class="by">alserio</span><span>|</span><a href="#39743558">root</a><span>|</span><a href="#39750060">parent</a><span>|</span><a href="#39751023">prev</a><span>|</span><a href="#39749711">next</a><span>|</span><label class="collapse" for="c-39750600">[-]</label><label class="expand" for="c-39750600">[1 more]</label></div><br/><div class="children"><div class="content">But maybe you can superoptimize some hot sections, and encode the superoptimizer findings somewhere.
Then the compiler can validate the optimizations and apply them to the particular piece of code for the rest of the program life, untill the preconditions hold.</div><br/></div></div></div></div><div id="39749711" class="c"><input type="checkbox" id="c-39749711" checked=""/><div class="controls bullet"><span class="by">kibwen</span><span>|</span><a href="#39743558">root</a><span>|</span><a href="#39749219">parent</a><span>|</span><a href="#39750060">prev</a><span>|</span><a href="#39749979">next</a><span>|</span><label class="collapse" for="c-39749711">[-]</label><label class="expand" for="c-39749711">[1 more]</label></div><br/><div class="children"><div class="content">Doing a full-fledged release-this-to-millions-of-users build of Chrome or Firefox already takes on the order of 24 hours (or at least it did when last I checked a few years ago).</div><br/></div></div><div id="39749979" class="c"><input type="checkbox" id="c-39749979" checked=""/><div class="controls bullet"><span class="by">Filligree</span><span>|</span><a href="#39743558">root</a><span>|</span><a href="#39749219">parent</a><span>|</span><a href="#39749711">prev</a><span>|</span><a href="#39752024">next</a><span>|</span><label class="collapse" for="c-39749979">[-]</label><label class="expand" for="c-39749979">[1 more]</label></div><br/><div class="children"><div class="content">Is it just a couple hours? Even just ordering the optimisation passes is already an NP-complete process, so I could easily imagine superoptimisers would take trillions of years for a large program...</div><br/></div></div><div id="39752024" class="c"><input type="checkbox" id="c-39752024" checked=""/><div class="controls bullet"><span class="by">kccqzy</span><span>|</span><a href="#39743558">root</a><span>|</span><a href="#39749219">parent</a><span>|</span><a href="#39749979">prev</a><span>|</span><a href="#39749740">next</a><span>|</span><label class="collapse" for="c-39752024">[-]</label><label class="expand" for="c-39752024">[1 more]</label></div><br/><div class="children"><div class="content">Chrome already uses PGO: <a href="https:&#x2F;&#x2F;blog.chromium.org&#x2F;2020&#x2F;08&#x2F;chrome-just-got-faster-with-profile.html" rel="nofollow">https:&#x2F;&#x2F;blog.chromium.org&#x2F;2020&#x2F;08&#x2F;chrome-just-got-faster-wit...</a></div><br/></div></div><div id="39749740" class="c"><input type="checkbox" id="c-39749740" checked=""/><div class="controls bullet"><span class="by">remorses</span><span>|</span><a href="#39743558">root</a><span>|</span><a href="#39749219">parent</a><span>|</span><a href="#39752024">prev</a><span>|</span><a href="#39745213">next</a><span>|</span><label class="collapse" for="c-39749740">[-]</label><label class="expand" for="c-39749740">[1 more]</label></div><br/><div class="children"><div class="content">I am pretty sure the existing release process already takes much more than 2 hours.</div><br/></div></div></div></div></div></div><div id="39745213" class="c"><input type="checkbox" id="c-39745213" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#39743558">root</a><span>|</span><a href="#39744770">parent</a><span>|</span><a href="#39744833">prev</a><span>|</span><a href="#39743664">next</a><span>|</span><label class="collapse" for="c-39745213">[-]</label><label class="expand" for="c-39745213">[1 more]</label></div><br/><div class="children"><div class="content">Have a look at Unison which uses constraint programming to do optimal code generation.<p><a href="https:&#x2F;&#x2F;unison-code.github.io&#x2F;" rel="nofollow">https:&#x2F;&#x2F;unison-code.github.io&#x2F;</a></div><br/></div></div></div></div><div id="39743664" class="c"><input type="checkbox" id="c-39743664" checked=""/><div class="controls bullet"><span class="by">kapilsinha</span><span>|</span><a href="#39743558">parent</a><span>|</span><a href="#39744770">prev</a><span>|</span><a href="#39745829">next</a><span>|</span><label class="collapse" for="c-39743664">[-]</label><label class="expand" for="c-39743664">[2 more]</label></div><br/><div class="children"><div class="content">Agree, especially when developing, I&#x27;d assume speed of optimizer matters more than quality of optimization. I wonder though if LLVM spent less time on that &quot;phase ordering&quot; problem, what is the tradeoff between these two factors?</div><br/><div id="39746245" class="c"><input type="checkbox" id="c-39746245" checked=""/><div class="controls bullet"><span class="by">boomanaiden154</span><span>|</span><a href="#39743558">root</a><span>|</span><a href="#39743664">parent</a><span>|</span><a href="#39745829">next</a><span>|</span><label class="collapse" for="c-39746245">[-]</label><label class="expand" for="c-39746245">[1 more]</label></div><br/><div class="children"><div class="content">LLVM doesn’t spend really any runtime solving the phase ordering problem since the pass pipelines are static. There have been proposals to dynamically adjust the pipeline based on various factors, but those are a ways out from happening.</div><br/></div></div></div></div></div></div><div id="39745829" class="c"><input type="checkbox" id="c-39745829" checked=""/><div class="controls bullet"><span class="by">ad-ops</span><span>|</span><a href="#39743558">prev</a><span>|</span><a href="#39744396">next</a><span>|</span><label class="collapse" for="c-39745829">[-]</label><label class="expand" for="c-39745829">[4 more]</label></div><br/><div class="children"><div class="content">I see that there are many comments on full debug builds, but for me the most important difference are incremental build times when making minor changes. In my opinion this is what speeds up the development iterations.<p>Here are my build times when making a trivial change to a print-statment in a root function, comparing nightly dev vs adding cranelift + mold for rust-analyzer[0] (347_290 LoC) and gleam[1] (76_335 LoC):<p><pre><code>    $ time cargo build
    Compiling rust-analyzer v0.0.0 (&#x2F;home&#x2F;user&#x2F;repos&#x2F;rust-analyzer&#x2F;crates&#x2F;rust-analyzer)
    # nightly
    Finished `dev` profile [unoptimized] target(s) in 6.60s
    cargo build  4.18s user 2.51s system 100% cpu 6.650 total
    
    # cranelift+mold
    Finished `dev` profile [unoptimized] target(s) in 2.25s
    cargo build  1.77s user 0.36s system 92% cpu 2.305 total

    Compiling gleam v1.0.0 (&#x2F;home&#x2F;user&#x2F;repos&#x2F;gleam&#x2F;compiler-cli)
    # nightly
    Finished `dev` profile [unoptimized + debuginfo] target(s) in 4.69s
    cargo build --bin gleam  3.02s user 1.74s system 100% cpu 4.743 total

    # cranelift+mold
    Finished `dev` profile [unoptimized + debuginfo] target(s) in 0.99s
    cargo build --bin gleam  0.71s user 0.20s system 88% cpu 1.033 total
</code></pre>
For me this is the most important metric and it shows a huge improvement. If I compare it to Go building Terraform[2] (371_594 LoC) it is looking promising. This is a bit unfair since it is the release build for Go and this is really nice in the CI&#x2F;CD. Love Go compilation times and I thought it would be nice to compare with another language to show the huge improvements that Rust has made.<p><pre><code>  $ time go build
  go build  3.62s user 0.76s system 171% cpu 2.545 total
</code></pre>
I was looking forward to parallel front-end[3], but I have not seen any improvement for these small changes.<p>[0] <a href="https:&#x2F;&#x2F;github.com&#x2F;rust-lang&#x2F;rust-analyzer">https:&#x2F;&#x2F;github.com&#x2F;rust-lang&#x2F;rust-analyzer</a><p>[1] <a href="https:&#x2F;&#x2F;github.com&#x2F;gleam-lang&#x2F;gleam">https:&#x2F;&#x2F;github.com&#x2F;gleam-lang&#x2F;gleam</a><p>[2] <a href="https:&#x2F;&#x2F;github.com&#x2F;hashicorp&#x2F;terraform">https:&#x2F;&#x2F;github.com&#x2F;hashicorp&#x2F;terraform</a><p>[3] <a href="https:&#x2F;&#x2F;blog.rust-lang.org&#x2F;2023&#x2F;11&#x2F;09&#x2F;parallel-rustc.html" rel="nofollow">https:&#x2F;&#x2F;blog.rust-lang.org&#x2F;2023&#x2F;11&#x2F;09&#x2F;parallel-rustc.html</a><p>*edit: code-comments &amp; links + making it easier to see the differences</div><br/><div id="39746413" class="c"><input type="checkbox" id="c-39746413" checked=""/><div class="controls bullet"><span class="by">ad-ops</span><span>|</span><a href="#39745829">parent</a><span>|</span><a href="#39748598">next</a><span>|</span><label class="collapse" for="c-39746413">[-]</label><label class="expand" for="c-39746413">[1 more]</label></div><br/><div class="children"><div class="content">For completion here is the compilation time for a small project from the axum&#x2F;examples[0] (125 LoC), also comparing nightly dev vs adding cranelift + mold:<p><pre><code>    $ time cargo build
    Compiling example-todos v0.1.0 (&#x2F;home&#x2F;user&#x2F;ws&#x2F;rust&#x2F;example-todos)
    Finished `dev` profile [unoptimized + debuginfo] target(s) in 1.65s
    cargo build  1.49s user 0.58s system 123% cpu 1.685 total

    Compiling example-todos v0.1.0 (&#x2F;home&#x2F;user&#x2F;ws&#x2F;rust&#x2F;example-todos)
    Finished `dev` profile [unoptimized + debuginfo] target(s) in 0.55s
    cargo build  0.47s user 0.13s system 102% cpu 0.590 total
</code></pre>
[0] <a href="https:&#x2F;&#x2F;github.com&#x2F;tokio-rs&#x2F;axum&#x2F;tree&#x2F;main&#x2F;examples&#x2F;todos">https:&#x2F;&#x2F;github.com&#x2F;tokio-rs&#x2F;axum&#x2F;tree&#x2F;main&#x2F;examples&#x2F;todos</a></div><br/></div></div><div id="39748598" class="c"><input type="checkbox" id="c-39748598" checked=""/><div class="controls bullet"><span class="by">kapilsinha</span><span>|</span><a href="#39745829">parent</a><span>|</span><a href="#39746413">prev</a><span>|</span><a href="#39746334">next</a><span>|</span><label class="collapse" for="c-39748598">[-]</label><label class="expand" for="c-39748598">[1 more]</label></div><br/><div class="children"><div class="content">This is most definitely self-promotion (I am the author), but also extremely relevant if you are looking at incremental build speed-ups: <a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39743431">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39743431</a></div><br/></div></div></div></div><div id="39744396" class="c"><input type="checkbox" id="c-39744396" checked=""/><div class="controls bullet"><span class="by">cube2222</span><span>|</span><a href="#39745829">prev</a><span>|</span><a href="#39743936">next</a><span>|</span><label class="collapse" for="c-39744396">[-]</label><label class="expand" for="c-39744396">[3 more]</label></div><br/><div class="children"><div class="content">Slightly off-topic, but if you fancy writing compilers in your free time, Cranelift has a great Rust library[0] for doing code generation - it’s a pleasure to use!<p>[0]: <a href="https:&#x2F;&#x2F;docs.rs&#x2F;cranelift-frontend&#x2F;0.105.3&#x2F;cranelift_frontend&#x2F;index.html" rel="nofollow">https:&#x2F;&#x2F;docs.rs&#x2F;cranelift-frontend&#x2F;0.105.3&#x2F;cranelift_fronten...</a></div><br/><div id="39746159" class="c"><input type="checkbox" id="c-39746159" checked=""/><div class="controls bullet"><span class="by">loeg</span><span>|</span><a href="#39744396">parent</a><span>|</span><a href="#39743936">next</a><span>|</span><label class="collapse" for="c-39746159">[-]</label><label class="expand" for="c-39746159">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve seen it used for really simple JIT in Advent of Code puzzles.</div><br/><div id="39753575" class="c"><input type="checkbox" id="c-39753575" checked=""/><div class="controls bullet"><span class="by">PartiallyTyped</span><span>|</span><a href="#39744396">root</a><span>|</span><a href="#39746159">parent</a><span>|</span><a href="#39743936">next</a><span>|</span><label class="collapse" for="c-39753575">[-]</label><label class="expand" for="c-39753575">[1 more]</label></div><br/><div class="children"><div class="content">Why am i not surprised and even find this amusing? :D</div><br/></div></div></div></div></div></div><div id="39743936" class="c"><input type="checkbox" id="c-39743936" checked=""/><div class="controls bullet"><span class="by">diggan</span><span>|</span><a href="#39744396">prev</a><span>|</span><a href="#39745810">next</a><span>|</span><label class="collapse" for="c-39743936">[-]</label><label class="expand" for="c-39743936">[10 more]</label></div><br/><div class="children"><div class="content">Tried out the instructions from the article on a tiny Bevy project, and compared it to a &quot;normal&quot; build:<p>&gt; cargo build --release  23.93s user 22.85s system 66% cpu 1:09.88 total<p>&gt; cargo +nightly build -Zcodegen-backend  23.52s user 21.98s system 68% cpu 1:06.86 total<p>Seems just marginally faster than a normal release build. Wonder if there is something particular with Bevy that makes this so? The author of the article mentions 40% difference in build speed, but I&#x27;m not seeing anything near that.<p>Edit: just realized I&#x27;m caching my release builds with sccache and a local NAS, hence the release builds being as fast as Cranelift+debug builds. Trying it again with just debug builds and without any caching:<p>&gt; cargo +nightly build  1997.35s user 200.38s system 1878% cpu 1:57.02 total<p>&gt; cargo +nightly build -Zcodegen-backend  280.96s user 73.06s system 657% cpu 53.850 total<p>Definitely an improvement once I realized what I did wrong, about half the time spent compiling now :) Neat!</div><br/><div id="39744017" class="c"><input type="checkbox" id="c-39744017" checked=""/><div class="controls bullet"><span class="by">naasking</span><span>|</span><a href="#39743936">parent</a><span>|</span><a href="#39749608">next</a><span>|</span><label class="collapse" for="c-39744017">[-]</label><label class="expand" for="c-39744017">[4 more]</label></div><br/><div class="children"><div class="content">Maybe your build is not limited by code generation, which seems like the only thing that changed here. There was a good thread recently about the variation in what slows down compilation:<p><a href="https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39721922">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=39721922</a></div><br/><div id="39744095" class="c"><input type="checkbox" id="c-39744095" checked=""/><div class="controls bullet"><span class="by">diggan</span><span>|</span><a href="#39743936">root</a><span>|</span><a href="#39744017">parent</a><span>|</span><a href="#39749608">next</a><span>|</span><label class="collapse" for="c-39744095">[-]</label><label class="expand" for="c-39744095">[3 more]</label></div><br/><div class="children"><div class="content">Edited my comment now, forgot I was caching the release builds with sccache so wasn&#x27;t actually compiling all the units, but fetching a lot of them instead :&#x2F;</div><br/><div id="39747394" class="c"><input type="checkbox" id="c-39747394" checked=""/><div class="controls bullet"><span class="by">vlovich123</span><span>|</span><a href="#39743936">root</a><span>|</span><a href="#39744095">parent</a><span>|</span><a href="#39749608">next</a><span>|</span><label class="collapse" for="c-39747394">[-]</label><label class="expand" for="c-39747394">[2 more]</label></div><br/><div class="children"><div class="content">Which speaks to maybe that cargo should just ship with sccache turned on by default so that the “normal” experience for developing Rust has this seamless experience. Intelligent caching should always win. I’d like to see rustc get sccache integration that works at the MIR level too so that changing something that doesn’t change the code gen meaningfully still gets a cached response (e.g. changing some comments&#x2F;whitespace, moving functions around, etc).<p>Even cooler would be if LLVM itself could also cache internal expensive parts of compilation and optimization across process instances. That would make a huge impact in cutting down incremental builds.</div><br/><div id="39748631" class="c"><input type="checkbox" id="c-39748631" checked=""/><div class="controls bullet"><span class="by">SkiFire13</span><span>|</span><a href="#39743936">root</a><span>|</span><a href="#39747394">parent</a><span>|</span><a href="#39749608">next</a><span>|</span><label class="collapse" for="c-39748631">[-]</label><label class="expand" for="c-39748631">[1 more]</label></div><br/><div class="children"><div class="content">&gt; I’d like to see rustc get sccache integration that works at the MIR level too so that changing something that doesn’t change the code gen meaningfully still gets a cached response (e.g. changing some comments&#x2F;whitespace, moving functions around, etc).<p>Isn&#x27;t incremental compilation already like that?</div><br/></div></div></div></div></div></div></div></div><div id="39749608" class="c"><input type="checkbox" id="c-39749608" checked=""/><div class="controls bullet"><span class="by">kapilsinha</span><span>|</span><a href="#39743936">parent</a><span>|</span><a href="#39744017">prev</a><span>|</span><a href="#39745681">next</a><span>|</span><label class="collapse" for="c-39749608">[-]</label><label class="expand" for="c-39749608">[4 more]</label></div><br/><div class="children"><div class="content">Is that Bevy project open-source by any chance? I&#x27;d love to it out myself</div><br/><div id="39749679" class="c"><input type="checkbox" id="c-39749679" checked=""/><div class="controls bullet"><span class="by">doctor_phil</span><span>|</span><a href="#39743936">root</a><span>|</span><a href="#39749608">parent</a><span>|</span><a href="#39745681">next</a><span>|</span><label class="collapse" for="c-39749679">[-]</label><label class="expand" for="c-39749679">[3 more]</label></div><br/><div class="children"><div class="content">Bevyengine is open source and very findable by most search engines.<p>Here is a direct link: <a href="https:&#x2F;&#x2F;github.com&#x2F;bevyengine&#x2F;bevy">https:&#x2F;&#x2F;github.com&#x2F;bevyengine&#x2F;bevy</a></div><br/><div id="39749719" class="c"><input type="checkbox" id="c-39749719" checked=""/><div class="controls bullet"><span class="by">kapilsinha</span><span>|</span><a href="#39743936">root</a><span>|</span><a href="#39749679">parent</a><span>|</span><a href="#39749710">next</a><span>|</span><label class="collapse" for="c-39749719">[-]</label><label class="expand" for="c-39749719">[1 more]</label></div><br/><div class="children"><div class="content">Ah yep I&#x27;m aware of that. Bevy is a framework&#x2F;engine that devs built on top of. I was referring to the project that OP has built on top of Bevy</div><br/></div></div></div></div></div></div></div></div><div id="39745810" class="c"><input type="checkbox" id="c-39745810" checked=""/><div class="controls bullet"><span class="by">metadat</span><span>|</span><a href="#39743936">prev</a><span>|</span><a href="#39744711">next</a><span>|</span><label class="collapse" for="c-39745810">[-]</label><label class="expand" for="c-39745810">[1 more]</label></div><br/><div class="children"><div class="content">The Equality Graphs link [0] led me to discover ESC&#x2F;Java [1] [2].  Has anyone actually tried or had any success with ESC&#x2F;Java?  It&#x27;s piqued my curiosity to compare with Spot bugs (formerly known as Findbugs).<p>[0] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;E-graph" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;E-graph</a><p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;ESC&#x2F;Java" rel="nofollow">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;ESC&#x2F;Java</a><p>[2] <a href="https:&#x2F;&#x2F;www.kindsoftware.com&#x2F;products&#x2F;opensource&#x2F;escjava2&#x2F;" rel="nofollow">https:&#x2F;&#x2F;www.kindsoftware.com&#x2F;products&#x2F;opensource&#x2F;escjava2&#x2F;</a></div><br/></div></div><div id="39744711" class="c"><input type="checkbox" id="c-39744711" checked=""/><div class="controls bullet"><span class="by">Tehnix</span><span>|</span><a href="#39745810">prev</a><span>|</span><a href="#39744213">next</a><span>|</span><label class="collapse" for="c-39744711">[-]</label><label class="expand" for="c-39744711">[2 more]</label></div><br/><div class="children"><div class="content">Very excited for Cranelift for debug builds to speed up development iteration - in particular for WASM&#x2F;Frontend Rust where iteration speed is competing with the new era of Rust tooling for JS which lands in the sub 1 second builds sometimes (iteration speed in Frontend is crucial).<p>Sadly, it does not yet support ARM macOS, so us M1-3 users will have to wait a bit :&#x2F;</div><br/><div id="39749624" class="c"><input type="checkbox" id="c-39749624" checked=""/><div class="controls bullet"><span class="by">kapilsinha</span><span>|</span><a href="#39744711">parent</a><span>|</span><a href="#39744213">next</a><span>|</span><label class="collapse" for="c-39749624">[-]</label><label class="expand" for="c-39749624">[1 more]</label></div><br/><div class="children"><div class="content">But usually, at least I don&#x27;t build an executable while iterating. And if I do, I try to set up the feature flags so the build is minimal for the tests I am running</div><br/></div></div></div></div><div id="39744213" class="c"><input type="checkbox" id="c-39744213" checked=""/><div class="controls bullet"><span class="by">Deukhoofd</span><span>|</span><a href="#39744711">prev</a><span>|</span><a href="#39752150">next</a><span>|</span><label class="collapse" for="c-39744213">[-]</label><label class="expand" for="c-39744213">[2 more]</label></div><br/><div class="children"><div class="content">Does anyone by chance have benchmarks of runtime (so not the compile time) when using Cranelift? I&#x27;m seeing a mention of &quot;twice as slow&quot; in the article, but that&#x27;s based on data from 2020. Wondering if it has substantially improved since then.</div><br/><div id="39752073" class="c"><input type="checkbox" id="c-39752073" checked=""/><div class="controls bullet"><span class="by">cfallin</span><span>|</span><a href="#39744213">parent</a><span>|</span><a href="#39752150">next</a><span>|</span><label class="collapse" for="c-39752073">[-]</label><label class="expand" for="c-39752073">[1 more]</label></div><br/><div class="children"><div class="content">There are some benchmarks of Cranelift-based Wasm VMs (Wasmtime) vs. LLVM-based Wasm VMs here: <a href="https:&#x2F;&#x2F;00f.net&#x2F;2023&#x2F;01&#x2F;04&#x2F;webassembly-benchmark-2023&#x2F;" rel="nofollow">https:&#x2F;&#x2F;00f.net&#x2F;2023&#x2F;01&#x2F;04&#x2F;webassembly-benchmark-2023&#x2F;</a><p>The (perhaps slightly exaggerated but encouraging to me at least!) money quote there is:<p>&gt; That’s right. The cranelift code generator has become as fast as LLVM. This is extremely impressive considering the fact that cranelift is a relatively young project, written from scratch by a very small (but obviously very talented) team.<p>In practice anywhere from 10%-30% slower maybe is reasonable to expect. Compiler microbenchmarks are interesting because they&#x27;re very &quot;quantized&quot;: for any particular benchmark, often either you get the right transforms and achieve <i>the</i> correct optimized inner loop, or you don&#x27;t. So the game is about getting more and more cases right and we&#x27;re slowly getting there.<p>(disclosure: I was tech lead of Cranelift in 2020-2022)</div><br/></div></div></div></div><div id="39752150" class="c"><input type="checkbox" id="c-39752150" checked=""/><div class="controls bullet"><span class="by">Someone</span><span>|</span><a href="#39744213">prev</a><span>|</span><a href="#39751096">next</a><span>|</span><label class="collapse" for="c-39752150">[-]</label><label class="expand" for="c-39752150">[1 more]</label></div><br/><div class="children"><div class="content">FTA: <i>“Because optimizations run on an E-graph only add information in the form of new annotations, the order of the optimizations does not change the result. As long as the compiler continues running optimizations until they no longer have any new matches (a process known as equality saturation), the E-graph will contain the representation that would have been produced by the optimal ordering of an equivalent sequence of traditional optimization passes […] In practice, Cranelift sets a limit on how many operations are performed on the graph to prevent it from becoming too large.”</i><p>So, in practice, the order of optimizations <i>can</i> change the result? How easy is it to hit that limit?</div><br/></div></div><div id="39751096" class="c"><input type="checkbox" id="c-39751096" checked=""/><div class="controls bullet"><span class="by">namuol</span><span>|</span><a href="#39752150">prev</a><span>|</span><a href="#39746108">next</a><span>|</span><label class="collapse" for="c-39751096">[-]</label><label class="expand" for="c-39751096">[3 more]</label></div><br/><div class="children"><div class="content">Is there no native support for M1-M3 Macs currently, and no Windows support either?<p>Unclear what the roadmap is there, as this update from the most active contributor is inconclusive:<p>&gt; Windows support has been omitted for now. And for macOS currently on supports x86_64 as Apple invented their own calling convention for arm64 for which variadic functions can’t easily be implemented as hack. If you are using an M1 processor, you could try installing the x86_64 version of rustc and then using Rosetta 2. Rosetta 2 will hurt performance though, so you will need to try if it is faster than the LLVM backend with arm64 rustc.<p>Source is from Oct 2023 so this could easily be outdated, but I found nothing in the original article: <a href="https:&#x2F;&#x2F;bjorn3.github.io&#x2F;2023&#x2F;10&#x2F;31&#x2F;progress-report-oct-2023.html" rel="nofollow">https:&#x2F;&#x2F;bjorn3.github.io&#x2F;2023&#x2F;10&#x2F;31&#x2F;progress-report-oct-2023...</a></div><br/><div id="39751615" class="c"><input type="checkbox" id="c-39751615" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#39751096">parent</a><span>|</span><a href="#39751195">next</a><span>|</span><label class="collapse" for="c-39751615">[-]</label><label class="expand" for="c-39751615">[1 more]</label></div><br/><div class="children"><div class="content">Windows is supported. See <a href="https:&#x2F;&#x2F;github.com&#x2F;rust-lang&#x2F;rustc_codegen_cranelift&#x2F;issues&#x2F;977">https:&#x2F;&#x2F;github.com&#x2F;rust-lang&#x2F;rustc_codegen_cranelift&#x2F;issues&#x2F;...</a>.</div><br/></div></div><div id="39751195" class="c"><input type="checkbox" id="c-39751195" checked=""/><div class="controls bullet"><span class="by">bhaney</span><span>|</span><a href="#39751096">parent</a><span>|</span><a href="#39751615">prev</a><span>|</span><a href="#39746108">next</a><span>|</span><label class="collapse" for="c-39751195">[-]</label><label class="expand" for="c-39751195">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;github.com&#x2F;rust-lang&#x2F;rustc_codegen_cranelift#platform-support">https:&#x2F;&#x2F;github.com&#x2F;rust-lang&#x2F;rustc_codegen_cranelift#platfor...</a></div><br/></div></div></div></div><div id="39746108" class="c"><input type="checkbox" id="c-39746108" checked=""/><div class="controls bullet"><span class="by">posix_monad</span><span>|</span><a href="#39751096">prev</a><span>|</span><a href="#39746955">next</a><span>|</span><label class="collapse" for="c-39746108">[-]</label><label class="expand" for="c-39746108">[12 more]</label></div><br/><div class="children"><div class="content">Can anyone explain why Cranelift is expected to be faster than LLVM? And why those improvements can&#x27;t also be applied to LLVM?</div><br/><div id="39746315" class="c"><input type="checkbox" id="c-39746315" checked=""/><div class="controls bullet"><span class="by">Filligree</span><span>|</span><a href="#39746108">parent</a><span>|</span><a href="#39746539">next</a><span>|</span><label class="collapse" for="c-39746315">[-]</label><label class="expand" for="c-39746315">[3 more]</label></div><br/><div class="children"><div class="content">It uses E-graphs, as explained in the article. That’s a completely different approach to compilation, and to use it in LLVM you’d have to rewrite LLVM.<p>It’s also unlikely that the resulting code will ever be as fast as a traditional compiler’s output. It’s great for development, but I wouldn’t use it in a release build.</div><br/><div id="39748151" class="c"><input type="checkbox" id="c-39748151" checked=""/><div class="controls bullet"><span class="by">Rusky</span><span>|</span><a href="#39746108">root</a><span>|</span><a href="#39746315">parent</a><span>|</span><a href="#39746539">next</a><span>|</span><label class="collapse" for="c-39748151">[-]</label><label class="expand" for="c-39748151">[2 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t believe the performance difference between Cranelift and LLVM really has much to do with E-graphs. Cranelift had this same performance profile (faster than LLVM but generating slower output) before they switched to E-graphs.<p>Rather it&#x27;s about how much effort Cranelift puts toward optimizing its output- it has fewer, less involved passes (regardless of whether those &quot;passes&quot; are expressed as part of the E-graph framework). More subtly, this means it is also written to generate &quot;okay&quot; code without as much reliance on those passes- while LLVM on the other hand generates a lot of naive code at -O0 which contributes to its slower compile times in that mode.</div><br/><div id="39752098" class="c"><input type="checkbox" id="c-39752098" checked=""/><div class="controls bullet"><span class="by">cfallin</span><span>|</span><a href="#39746108">root</a><span>|</span><a href="#39748151">parent</a><span>|</span><a href="#39746539">next</a><span>|</span><label class="collapse" for="c-39752098">[-]</label><label class="expand" for="c-39752098">[1 more]</label></div><br/><div class="children"><div class="content">Right, it&#x27;s about algorithmic tradeoffs throughout. A good example I wrote about is here: <a href="https:&#x2F;&#x2F;cfallin.org&#x2F;blog&#x2F;2021&#x2F;01&#x2F;22&#x2F;cranelift-isel-2&#x2F;" rel="nofollow">https:&#x2F;&#x2F;cfallin.org&#x2F;blog&#x2F;2021&#x2F;01&#x2F;22&#x2F;cranelift-isel-2&#x2F;</a> where we use a single-pass algorithm to solve a problem that LLVM has a multi-part fixpoint loop to solve.<p>Most CPU time during compile is in the register allocator and I took a really careful approach to optimization when I rewrote it a few years ago (more details <a href="https:&#x2F;&#x2F;cfallin.org&#x2F;blog&#x2F;2022&#x2F;06&#x2F;09&#x2F;cranelift-regalloc2&#x2F;" rel="nofollow">https:&#x2F;&#x2F;cfallin.org&#x2F;blog&#x2F;2022&#x2F;06&#x2F;09&#x2F;cranelift-regalloc2&#x2F;</a>). We generally try to pay close attention to algorithmic efficiency and avoid altogether the fixpoint loops, etc that one often finds elsewhere. (RA2 does backtrack and have a worklist loop, though it&#x27;s pretty minimal, and also we&#x27;re planning to add a single-pass mode.)<p>(disclosure: I was Cranelift tech lead in 2020-2022)</div><br/></div></div></div></div></div></div><div id="39746539" class="c"><input type="checkbox" id="c-39746539" checked=""/><div class="controls bullet"><span class="by">norman784</span><span>|</span><a href="#39746108">parent</a><span>|</span><a href="#39746315">prev</a><span>|</span><a href="#39749259">next</a><span>|</span><label class="collapse" for="c-39746539">[-]</label><label class="expand" for="c-39746539">[7 more]</label></div><br/><div class="children"><div class="content">&gt; why those improvements can&#x27;t also be applied to LLVM?<p>LLVM is a huge and bloated ecosystem (it has tons of tools and millions of LoC), also the code base itself is pretty old or rather the project has his age, so there&#x27;s a lot of legacy code, other aspect is that is hard to try new&#x2F;radical things because how big the project itself is.</div><br/><div id="39747450" class="c"><input type="checkbox" id="c-39747450" checked=""/><div class="controls bullet"><span class="by">posix_monad</span><span>|</span><a href="#39746108">root</a><span>|</span><a href="#39746539">parent</a><span>|</span><a href="#39746842">next</a><span>|</span><label class="collapse" for="c-39747450">[-]</label><label class="expand" for="c-39747450">[5 more]</label></div><br/><div class="children"><div class="content">X is too bloated! We need Y, which is leaner and more tailored to our use-case.<p><i>(years pass...)</i><p>Y is too bloated! We need Z...</div><br/><div id="39751710" class="c"><input type="checkbox" id="c-39751710" checked=""/><div class="controls bullet"><span class="by">sanxiyn</span><span>|</span><a href="#39746108">root</a><span>|</span><a href="#39747450">parent</a><span>|</span><a href="#39749009">next</a><span>|</span><label class="collapse" for="c-39751710">[-]</label><label class="expand" for="c-39751710">[2 more]</label></div><br/><div class="children"><div class="content">After years of trying (IIRC there was funded effort from Intel), LLVM still can&#x27;t parallelize code generation at function level. Cranelift had function level code generation parallelization from day 1.</div><br/><div id="39752791" class="c"><input type="checkbox" id="c-39752791" checked=""/><div class="controls bullet"><span class="by">azakai</span><span>|</span><a href="#39746108">root</a><span>|</span><a href="#39751710">parent</a><span>|</span><a href="#39749009">next</a><span>|</span><label class="collapse" for="c-39752791">[-]</label><label class="expand" for="c-39752791">[1 more]</label></div><br/><div class="children"><div class="content">I have always been told that LLVM doesn&#x27;t parallelize codegen because the right place for that is in the build system: just do make -j N (or use ninja, which parallelizes automatically). And when you&#x27;re building lots of source files in a normal project that&#x27;s definitely true.<p>It&#x27;s different when you&#x27;re a wasm VM that receives big chunk of wasm that contains many source files, and you get it all at once. And for that reason pretty much every wasm compiler parallelizes codegen: Cranelift as you said, and also V8, SpiderMonkey, JSC, and not just VMs but also optimizers like Binaryen. It&#x27;s a crucial part of their design.<p>For LLVM, the right design may be what it has today: single-core codegen. (LTO is the main issue there, but that&#x27;s what thin LTO is for.)</div><br/></div></div></div></div><div id="39749009" class="c"><input type="checkbox" id="c-39749009" checked=""/><div class="controls bullet"><span class="by">dymk</span><span>|</span><a href="#39746108">root</a><span>|</span><a href="#39747450">parent</a><span>|</span><a href="#39751710">prev</a><span>|</span><a href="#39750412">next</a><span>|</span><label class="collapse" for="c-39749009">[-]</label><label class="expand" for="c-39749009">[1 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t get it. Is your point that new software shouldn&#x27;t be written to fulfill new usecases? We should always bolt new patterns and architecture onto existing codebases?</div><br/></div></div><div id="39750412" class="c"><input type="checkbox" id="c-39750412" checked=""/><div class="controls bullet"><span class="by">alserio</span><span>|</span><a href="#39746108">root</a><span>|</span><a href="#39747450">parent</a><span>|</span><a href="#39749009">prev</a><span>|</span><a href="#39746842">next</a><span>|</span><label class="collapse" for="c-39750412">[-]</label><label class="expand" for="c-39750412">[1 more]</label></div><br/><div class="children"><div class="content">Yes. But in the meantime one can experiment with new things, void some assumptions.
And I don&#x27;t know about the LLVM project, but its quite more difficult to try different ideas in a big and old organization and codebase than it is in a greenfield project.
Maybe they won&#x27;t be successful, maybe they will move the whole field ahead.</div><br/></div></div></div></div><div id="39746842" class="c"><input type="checkbox" id="c-39746842" checked=""/><div class="controls bullet"><span class="by">papruapap</span><span>|</span><a href="#39746108">root</a><span>|</span><a href="#39746539">parent</a><span>|</span><a href="#39747450">prev</a><span>|</span><a href="#39749259">next</a><span>|</span><label class="collapse" for="c-39746842">[-]</label><label class="expand" for="c-39746842">[1 more]</label></div><br/><div class="children"><div class="content">iirc Zig want to replace LLVM for similar reasons, also hard to debug.</div><br/></div></div></div></div><div id="39749259" class="c"><input type="checkbox" id="c-39749259" checked=""/><div class="controls bullet"><span class="by">mort96</span><span>|</span><a href="#39746108">parent</a><span>|</span><a href="#39746539">prev</a><span>|</span><a href="#39746955">next</a><span>|</span><label class="collapse" for="c-39749259">[-]</label><label class="expand" for="c-39749259">[1 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve heard compiler writers complain that LLVM&#x27;s API is inherently slow. Something about using runtime polymorphism everywhere. I don&#x27;t know how much there is to this though, I haven&#x27;t investigated it myself.</div><br/></div></div></div></div><div id="39746955" class="c"><input type="checkbox" id="c-39746955" checked=""/><div class="controls bullet"><span class="by">mmoskal</span><span>|</span><a href="#39746108">prev</a><span>|</span><a href="#39746756">next</a><span>|</span><label class="collapse" for="c-39746955">[-]</label><label class="expand" for="c-39746955">[1 more]</label></div><br/><div class="children"><div class="content">&gt; JIT compilers often use techniques, such as speculative optimizations, that make it difficult to reuse the compiler outside its original context, since they encode so many assumptions about the specific language for which they were designed.<p>&gt; The developers of Cranelift chose to use a more generic architecture, which means that Cranelift is usable outside of the confines of WebAssembly.<p>One would think this has more to do with Wasm being the source language, as it&#x27;s fairly generic (compared to JS or Python), so there are no specific assumptions to encode.<p>Great article though. It&#x27;s quite interesting to see E-matching used in compilers, took me down a memory lane (and found myself cited on Wikipedia page for e-graphs).</div><br/></div></div><div id="39746756" class="c"><input type="checkbox" id="c-39746756" checked=""/><div class="controls bullet"><span class="by">rayiner</span><span>|</span><a href="#39746955">prev</a><span>|</span><a href="#39743447">next</a><span>|</span><label class="collapse" for="c-39746756">[-]</label><label class="expand" for="c-39746756">[1 more]</label></div><br/><div class="children"><div class="content">Very interesting article. I had not heard of equality graphs before. Here&#x27;s some pretty good background reading on the subject:  <a href="https:&#x2F;&#x2F;inst.eecs.berkeley.edu&#x2F;~cs294-260&#x2F;sp24&#x2F;2024-03-04-eqsat-paper" rel="nofollow">https:&#x2F;&#x2F;inst.eecs.berkeley.edu&#x2F;~cs294-260&#x2F;sp24&#x2F;2024-03-04-eq...</a></div><br/></div></div><div id="39743447" class="c"><input type="checkbox" id="c-39743447" checked=""/><div class="controls bullet"><span class="by">k_bx</span><span>|</span><a href="#39746756">prev</a><span>|</span><a href="#39744150">next</a><span>|</span><label class="collapse" for="c-39743447">[-]</label><label class="expand" for="c-39743447">[12 more]</label></div><br/><div class="children"><div class="content">Any fresh compilation time benchmarks and comparisons to LLVM?</div><br/><div id="39744001" class="c"><input type="checkbox" id="c-39744001" checked=""/><div class="controls bullet"><span class="by">nindalf</span><span>|</span><a href="#39743447">parent</a><span>|</span><a href="#39743555">next</a><span>|</span><label class="collapse" for="c-39744001">[-]</label><label class="expand" for="c-39744001">[2 more]</label></div><br/><div class="children"><div class="content">Few reports comparing Cranelift to LLVM from this day old reddit thread [1]<p>- 29.52s -&gt; 24.47s (17.1%)<p>- 27s -&gt; 19s (29.6%)<p>- 11.5s -&gt; 8.4s (26.9%)<p>- 37.5s -&gt; 29.6s (28.7%) - this measurement from TFA.<p>To put these numbers in context, all the perf improvements over the last 4 years have helped the compiler become faster on a variety of workloads by 7%, 17%, 13% and 15%, for an overall speed gain of 37% over 4 years. [2] So one large change providing a 20-30% improvement is very impressive.<p>When you add that to the parallel frontend [3] and support for linking with LLD [4], Rust compilation could be substantially faster by this time next year.<p>[1] - <a href="https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;rust&#x2F;comments&#x2F;1bgyo8a&#x2F;try_cranelift_codegen_backend_for_faster_compile&#x2F;" rel="nofollow">https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;rust&#x2F;comments&#x2F;1bgyo8a&#x2F;try_cranelift...</a><p>[2] - <a href="https:&#x2F;&#x2F;nnethercote.github.io&#x2F;2024&#x2F;03&#x2F;06&#x2F;how-to-speed-up-the-rust-compiler-in-march-2024.html" rel="nofollow">https:&#x2F;&#x2F;nnethercote.github.io&#x2F;2024&#x2F;03&#x2F;06&#x2F;how-to-speed-up-the...</a><p>[3] - <a href="https:&#x2F;&#x2F;blog.rust-lang.org&#x2F;2023&#x2F;11&#x2F;09&#x2F;parallel-rustc.html" rel="nofollow">https:&#x2F;&#x2F;blog.rust-lang.org&#x2F;2023&#x2F;11&#x2F;09&#x2F;parallel-rustc.html</a><p>[4] - <a href="https:&#x2F;&#x2F;github.com&#x2F;rust-lang&#x2F;rust&#x2F;issues&#x2F;39915">https:&#x2F;&#x2F;github.com&#x2F;rust-lang&#x2F;rust&#x2F;issues&#x2F;39915</a></div><br/><div id="39746654" class="c"><input type="checkbox" id="c-39746654" checked=""/><div class="controls bullet"><span class="by">k_bx</span><span>|</span><a href="#39743447">root</a><span>|</span><a href="#39744001">parent</a><span>|</span><a href="#39743555">next</a><span>|</span><label class="collapse" for="c-39746654">[-]</label><label class="expand" for="c-39746654">[1 more]</label></div><br/><div class="children"><div class="content">Thanks, that&#x27;s really great news. The only thing I wish for now is aarch64 support on Apple Silicon</div><br/></div></div></div></div><div id="39743555" class="c"><input type="checkbox" id="c-39743555" checked=""/><div class="controls bullet"><span class="by">boxed</span><span>|</span><a href="#39743447">parent</a><span>|</span><a href="#39744001">prev</a><span>|</span><a href="#39743589">next</a><span>|</span><label class="collapse" for="c-39743555">[-]</label><label class="expand" for="c-39743555">[1 more]</label></div><br/><div class="children"><div class="content">The very last paragraph:<p>&gt; A full debug build of Cranelift itself using the Cranelift backend took 29.6 seconds on my computer, compared to 37.5 with LLVM (a reduction in wall-clock time of 20%). Those wall-clock times don&#x27;t tell the full story, however, because of parallelism in the build system. Compiling with Cranelift took 125 CPU-seconds, whereas LLVM took 211 CPU-seconds, a difference of 40%. Incremental builds — rebuilding only Cranelift itself, and none of its dependencies — were faster with both backends. 66ms of CPU time compared to 90ms.</div><br/></div></div><div id="39743589" class="c"><input type="checkbox" id="c-39743589" checked=""/><div class="controls bullet"><span class="by">AlexErrant</span><span>|</span><a href="#39743447">parent</a><span>|</span><a href="#39743555">prev</a><span>|</span><a href="#39743516">next</a><span>|</span><label class="collapse" for="c-39743589">[-]</label><label class="expand" for="c-39743589">[2 more]</label></div><br/><div class="children"><div class="content">Not quite fresh, but<p>&gt; A paper from 2020 [0] showed that Cranelift was an order of magnitude faster than LLVM, while producing code that was approximately twice as slow on some benchmarks.<p>[0] <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2011.13127.pdf" rel="nofollow">https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2011.13127.pdf</a></div><br/><div id="39744601" class="c"><input type="checkbox" id="c-39744601" checked=""/><div class="controls bullet"><span class="by">Findecanor</span><span>|</span><a href="#39743447">root</a><span>|</span><a href="#39743589">parent</a><span>|</span><a href="#39743516">next</a><span>|</span><label class="collapse" for="c-39744601">[-]</label><label class="expand" for="c-39744601">[1 more]</label></div><br/><div class="children"><div class="content">The changes in Cranelift since 2020 have been quite significant so I would not put any trust in those benchmarks.</div><br/></div></div></div></div><div id="39743516" class="c"><input type="checkbox" id="c-39743516" checked=""/><div class="controls bullet"><span class="by">zokier</span><span>|</span><a href="#39743447">parent</a><span>|</span><a href="#39743589">prev</a><span>|</span><a href="#39744150">next</a><span>|</span><label class="collapse" for="c-39743516">[-]</label><label class="expand" for="c-39743516">[6 more]</label></div><br/><div class="children"><div class="content">From the article:<p>&gt; A full debug build of Cranelift itself using the Cranelift backend took 29.6 seconds on my computer, compared to 37.5 with LLVM (a reduction in wall-clock time of 20%)<p>That seems much smaller difference than what I would have expected</div><br/><div id="39743719" class="c"><input type="checkbox" id="c-39743719" checked=""/><div class="controls bullet"><span class="by">__s</span><span>|</span><a href="#39743447">root</a><span>|</span><a href="#39743516">parent</a><span>|</span><a href="#39744150">next</a><span>|</span><label class="collapse" for="c-39743719">[-]</label><label class="expand" for="c-39743719">[5 more]</label></div><br/><div class="children"><div class="content">Those numbers are the entire compile time, parsing&#x2F;typecheck&#x2F;MIR+MIR optimization&#x2F;linking<p>Don&#x27;t have numbers handy, so hard to say how much faster Cranelift is making the codegen portion, but gets into Amdahl&#x27;s Law</div><br/><div id="39746332" class="c"><input type="checkbox" id="c-39746332" checked=""/><div class="controls bullet"><span class="by">Filligree</span><span>|</span><a href="#39743447">root</a><span>|</span><a href="#39743719">parent</a><span>|</span><a href="#39744150">next</a><span>|</span><label class="collapse" for="c-39746332">[-]</label><label class="expand" for="c-39746332">[4 more]</label></div><br/><div class="children"><div class="content">And there are faster linkers than the default.<p>…why is it still the default?</div><br/><div id="39749781" class="c"><input type="checkbox" id="c-39749781" checked=""/><div class="controls bullet"><span class="by">kibwen</span><span>|</span><a href="#39743447">root</a><span>|</span><a href="#39746332">parent</a><span>|</span><a href="#39747732">next</a><span>|</span><label class="collapse" for="c-39749781">[-]</label><label class="expand" for="c-39749781">[1 more]</label></div><br/><div class="children"><div class="content">Because the linker provided by the system itself is the one that&#x27;s guaranteed to be the most broadly compatible.  For a toolchain to begin distributing an alternative linker is a lot of work on every supported platform (future versions of Rust may sidestep this issue by making LLD the default on certain platforms, which they get &quot;for free&quot; by dint of shipping alongside LLVM). But also, because there are about ten people on planet Earth qualified to write a production-grade linker, which is why LLD still isn&#x27;t broadly recommended yet.</div><br/></div></div><div id="39747732" class="c"><input type="checkbox" id="c-39747732" checked=""/><div class="controls bullet"><span class="by">3836293648</span><span>|</span><a href="#39743447">root</a><span>|</span><a href="#39746332">parent</a><span>|</span><a href="#39749781">prev</a><span>|</span><a href="#39744150">next</a><span>|</span><label class="collapse" for="c-39747732">[-]</label><label class="expand" for="c-39747732">[2 more]</label></div><br/><div class="children"><div class="content">Compatibility with niche usecases</div><br/><div id="39749452" class="c"><input type="checkbox" id="c-39749452" checked=""/><div class="controls bullet"><span class="by">Filligree</span><span>|</span><a href="#39743447">root</a><span>|</span><a href="#39747732">parent</a><span>|</span><a href="#39744150">next</a><span>|</span><label class="collapse" for="c-39749452">[-]</label><label class="expand" for="c-39749452">[1 more]</label></div><br/><div class="children"><div class="content">That sounds like a good reason to keep the existing one as an opt-in possibility.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="39744150" class="c"><input type="checkbox" id="c-39744150" checked=""/><div class="controls bullet"><span class="by">Dowwie</span><span>|</span><a href="#39743447">prev</a><span>|</span><a href="#39745339">next</a><span>|</span><label class="collapse" for="c-39744150">[-]</label><label class="expand" for="c-39744150">[2 more]</label></div><br/><div class="children"><div class="content">Would it be naive to assume a general compile-time reduction of 20% for all Rust projects by swapping llvm with cranelift?</div><br/><div id="39744670" class="c"><input type="checkbox" id="c-39744670" checked=""/><div class="controls bullet"><span class="by">mrklol</span><span>|</span><a href="#39744150">parent</a><span>|</span><a href="#39745339">next</a><span>|</span><label class="collapse" for="c-39744670">[-]</label><label class="expand" for="c-39744670">[1 more]</label></div><br/><div class="children"><div class="content">There are still projects out there which won’t hit 20% or even some %, if the bottleneck isn’t code gen (for example). So the part with&quot;all&quot; can be wrong, but beside that 20% is a good number.</div><br/></div></div></div></div><div id="39745339" class="c"><input type="checkbox" id="c-39745339" checked=""/><div class="controls bullet"><span class="by">Ericson2314</span><span>|</span><a href="#39744150">prev</a><span>|</span><a href="#39746072">next</a><span>|</span><label class="collapse" for="c-39745339">[-]</label><label class="expand" for="c-39745339">[5 more]</label></div><br/><div class="children"><div class="content">Really looking forward to the death of non-e-graph-based compilation :)</div><br/><div id="39749455" class="c"><input type="checkbox" id="c-39749455" checked=""/><div class="controls bullet"><span class="by">convolvatron</span><span>|</span><a href="#39745339">parent</a><span>|</span><a href="#39746072">next</a><span>|</span><label class="collapse" for="c-39749455">[-]</label><label class="expand" for="c-39749455">[4 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve tried to look into this a couple times, including today. To me this looks alot like unification? but I don&#x27;t really understand how operationally one gets from equivalence classes to instructions. is there an e-graphs for dummies writeup?</div><br/><div id="39752535" class="c"><input type="checkbox" id="c-39752535" checked=""/><div class="controls bullet"><span class="by">cfallin</span><span>|</span><a href="#39745339">root</a><span>|</span><a href="#39749455">parent</a><span>|</span><a href="#39749816">next</a><span>|</span><label class="collapse" for="c-39752535">[-]</label><label class="expand" for="c-39752535">[1 more]</label></div><br/><div class="children"><div class="content">The approach that Cranelift uses is what we call the &quot;aegraph&quot; (talk I gave about it: slides <a href="https:&#x2F;&#x2F;cfallin.org&#x2F;pubs&#x2F;egraphs2023_aegraphs_slides.pdf" rel="nofollow">https:&#x2F;&#x2F;cfallin.org&#x2F;pubs&#x2F;egraphs2023_aegraphs_slides.pdf</a>, video <a href="https:&#x2F;&#x2F;vimeo.com&#x2F;843540328" rel="nofollow">https:&#x2F;&#x2F;vimeo.com&#x2F;843540328</a>). The basic idea is that eclasses hold sets of operator expressions (think sea-of-node IR), and we keep that alongside the CFG with a &quot;skeleton&quot; of side-effecting ops. We build that, do rewrites, then extract out of it back to a conventional CFG-of-basic-blocks for lowering. The &quot;equivalence&quot; part comes in when doing rewrites: the main difference between an egraph and a conventional sea-of-nodes IR with rewrite rules is that one keeps all representations in the equivalence class around, then chooses the best one later.<p>We had to solve a few novel problems in working out how to handle control flow, and we&#x27;re still polishing off some rough edges (search recent issues in the repo for egraphs); but we&#x27;re mostly happy how it turned out!<p>(Disclosure: tech lead of CL for a while; the e-graphs optimizer is &quot;my fault&quot;)</div><br/></div></div><div id="39749816" class="c"><input type="checkbox" id="c-39749816" checked=""/><div class="controls bullet"><span class="by">tekknolagi</span><span>|</span><a href="#39745339">root</a><span>|</span><a href="#39749455">parent</a><span>|</span><a href="#39752535">prev</a><span>|</span><a href="#39746072">next</a><span>|</span><label class="collapse" for="c-39749816">[-]</label><label class="expand" for="c-39749816">[2 more]</label></div><br/><div class="children"><div class="content">If by instructions you mean machine instructions, you don&#x27;t; it&#x27;s used for internal optimization passes only.</div><br/><div id="39750109" class="c"><input type="checkbox" id="c-39750109" checked=""/><div class="controls bullet"><span class="by">convolvatron</span><span>|</span><a href="#39745339">root</a><span>|</span><a href="#39749816">parent</a><span>|</span><a href="#39746072">next</a><span>|</span><label class="collapse" for="c-39750109">[-]</label><label class="expand" for="c-39750109">[1 more]</label></div><br/><div class="children"><div class="content">that helps, so this is really an alternative to pushing analysis attributes aronud a dataflow graph</div><br/></div></div></div></div></div></div></div></div><div id="39746072" class="c"><input type="checkbox" id="c-39746072" checked=""/><div class="controls bullet"><span class="by">rishav_sharan</span><span>|</span><a href="#39745339">prev</a><span>|</span><a href="#39743915">next</a><span>|</span><label class="collapse" for="c-39746072">[-]</label><label class="expand" for="c-39746072">[3 more]</label></div><br/><div class="children"><div class="content">It sucks that there is no way to use cranelift from outside of rust to create your own toy language. I would have loved to use cranelift in a toy compiler, but I am not ready to pay the Rust price of complexity.</div><br/><div id="39746495" class="c"><input type="checkbox" id="c-39746495" checked=""/><div class="controls bullet"><span class="by">kibwen</span><span>|</span><a href="#39746072">parent</a><span>|</span><a href="#39746444">next</a><span>|</span><label class="collapse" for="c-39746495">[-]</label><label class="expand" for="c-39746495">[1 more]</label></div><br/><div class="children"><div class="content">As far as I know, you should be able to generate textual CLIF files and feed them to Cranelift: <a href="https:&#x2F;&#x2F;github.com&#x2F;bytecodealliance&#x2F;wasmtime&#x2F;blob&#x2F;main&#x2F;cranelift&#x2F;docs&#x2F;ir.md">https:&#x2F;&#x2F;github.com&#x2F;bytecodealliance&#x2F;wasmtime&#x2F;blob&#x2F;main&#x2F;crane...</a></div><br/></div></div><div id="39746444" class="c"><input type="checkbox" id="c-39746444" checked=""/><div class="controls bullet"><span class="by">sigsev_251</span><span>|</span><a href="#39746072">parent</a><span>|</span><a href="#39746495">prev</a><span>|</span><a href="#39743915">next</a><span>|</span><label class="collapse" for="c-39746444">[-]</label><label class="expand" for="c-39746444">[1 more]</label></div><br/><div class="children"><div class="content">There is always qbe if you need something simpler</div><br/></div></div></div></div><div id="39743915" class="c"><input type="checkbox" id="c-39743915" checked=""/><div class="controls bullet"><span class="by">pjmlp</span><span>|</span><a href="#39746072">prev</a><span>|</span><a href="#39746300">next</a><span>|</span><label class="collapse" for="c-39743915">[-]</label><label class="expand" for="c-39743915">[1 more]</label></div><br/><div class="children"><div class="content">Finally, looking forward for wider adoption.</div><br/></div></div><div id="39746125" class="c"><input type="checkbox" id="c-39746125" checked=""/><div class="controls bullet"><span class="by">tsegratis</span><span>|</span><a href="#39746300">prev</a><span>|</span><label class="collapse" for="c-39746125">[-]</label><label class="expand" for="c-39746125">[1 more]</label></div><br/><div class="children"><div class="content">I feel like im reading advertising blurb reading that article<p>I wish them every success, but i hope for a more balanced overview of pros and cons rather than gushing praise at every step...</div><br/></div></div></div></div></div></div></div></body></html>