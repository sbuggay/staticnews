<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1694595675911" as="style"/><link rel="stylesheet" href="styles.css?v=1694595675911"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://github.com/turboderp/exllamav2">70B Llama 2 at 35tokens/second on 4090</a> <span class="domain">(<a href="https://github.com">github.com</a>)</span></div><div class="subtext"><span>Palmik</span> | <span>21 comments</span></div><br/><div><div id="37493047" class="c"><input type="checkbox" id="c-37493047" checked=""/><div class="controls bullet"><span class="by">Palmik</span><span>|</span><a href="#37493030">next</a><span>|</span><label class="collapse" for="c-37493047">[-]</label><label class="expand" for="c-37493047">[2 more]</label></div><br/><div class="children"><div class="content">Why this is interesting: To my knowledge, this is the first time you are able to run the largest Llama, at competitive speed, on a consumer GPU (or something like A40).<p>On many tasks, fine-tuned Llama can outperform GPT-3.5-turbo or even GPT-4, but with a naive approach to serving (like HuggingFace + FastAPI), you will have hard time beating the cost of GPT-3.5-turbo even with the smaller Llama models, especially if your utilization is low.</div><br/><div id="37493883" class="c"><input type="checkbox" id="c-37493883" checked=""/><div class="controls bullet"><span class="by">politelemon</span><span>|</span><a href="#37493047">parent</a><span>|</span><a href="#37493030">next</a><span>|</span><label class="collapse" for="c-37493883">[-]</label><label class="expand" for="c-37493883">[1 more]</label></div><br/><div class="children"><div class="content">Could you explain what you mean by naive approach to serving?</div><br/></div></div></div></div><div id="37493030" class="c"><input type="checkbox" id="c-37493030" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37493047">prev</a><span>|</span><a href="#37493351">next</a><span>|</span><label class="collapse" for="c-37493030">[-]</label><label class="expand" for="c-37493030">[3 more]</label></div><br/><div class="children"><div class="content">Amazing.<p>But at what cost? Have there been any perplexity benchmarks for ELX2?<p>Its annoying that Facebook made llama v2 70B instead of 65B, even with the memory saving changes... 5B less parameters, and the squeeze would be far easier.</div><br/><div id="37493071" class="c"><input type="checkbox" id="c-37493071" checked=""/><div class="controls bullet"><span class="by">Palmik</span><span>|</span><a href="#37493030">parent</a><span>|</span><a href="#37493351">next</a><span>|</span><label class="collapse" for="c-37493071">[-]</label><label class="expand" for="c-37493071">[2 more]</label></div><br/><div class="children"><div class="content">Good point -- hopefully the quality impact is still worth it, remains to be seen. Agree on the size -- hopefully something they will keep in mind for future models.</div><br/><div id="37493837" class="c"><input type="checkbox" id="c-37493837" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37493030">root</a><span>|</span><a href="#37493071">parent</a><span>|</span><a href="#37493351">next</a><span>|</span><label class="collapse" for="c-37493837">[-]</label><label class="expand" for="c-37493837">[1 more]</label></div><br/><div class="children"><div class="content">If its better than the equivalent 30B model, that&#x27;s still a huge achievement.<p>Llama.cpp&#x27;s Q2_K quant is 2.5625 bpw with perplexity just barely better than the next step down: <a href="https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;1684">https:&#x2F;&#x2F;github.com&#x2F;ggerganov&#x2F;llama.cpp&#x2F;pull&#x2F;1684</a><p>But subjectively, the Q2 quant &quot;feels&quot; worse than its high wikitext perplexity would suggest.<p>That&#x27;s apples to oranges, as this quantization is different than Q2_K, but I just hope the quality hit in practice isn&#x27;t so bad.</div><br/></div></div></div></div></div></div><div id="37493351" class="c"><input type="checkbox" id="c-37493351" checked=""/><div class="controls bullet"><span class="by">pulse7</span><span>|</span><a href="#37493030">prev</a><span>|</span><a href="#37493484">next</a><span>|</span><label class="collapse" for="c-37493351">[-]</label><label class="expand" for="c-37493351">[4 more]</label></div><br/><div class="children"><div class="content">How does &quot;70B Llama 2&quot; compare with ChatGPT 3.5&#x2F;4.0 in reality? Are they on par? In all areas?</div><br/><div id="37494076" class="c"><input type="checkbox" id="c-37494076" checked=""/><div class="controls bullet"><span class="by">Palmik</span><span>|</span><a href="#37493351">parent</a><span>|</span><a href="#37493512">next</a><span>|</span><label class="collapse" for="c-37494076">[-]</label><label class="expand" for="c-37494076">[1 more]</label></div><br/><div class="children"><div class="content">Where fine tuning shines imo:<p>Things that require consistency: e.g. you want the chat &#x2F; output to have certain &quot;personality&quot;, consistent level of conciseness or formatting.<p>Things where examples are hard to fit into prompt: e.g. summarization, or other longer form tasks.<p>High volume, simpler tasks: Various data extraction tasks.<p>Two of my side projects (links in bio) use AI for summarization, and indeed consistency is a big issue there.</div><br/></div></div><div id="37493512" class="c"><input type="checkbox" id="c-37493512" checked=""/><div class="controls bullet"><span class="by">andai</span><span>|</span><a href="#37493351">parent</a><span>|</span><a href="#37494076">prev</a><span>|</span><a href="#37493736">next</a><span>|</span><label class="collapse" for="c-37493512">[-]</label><label class="expand" for="c-37493512">[1 more]</label></div><br/><div class="children"><div class="content">It seems to depend on the task. I&#x27;d say it beats GPT-3 in quality most of the time (but not in speed and cost!) and GPT-4 approximately never. It&#x27;s perfect if what you need is &quot;less good than GPT-4 but 50x cheaper.&quot;</div><br/></div></div><div id="37493736" class="c"><input type="checkbox" id="c-37493736" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37493351">parent</a><span>|</span><a href="#37493512">prev</a><span>|</span><a href="#37493484">next</a><span>|</span><label class="collapse" for="c-37493736">[-]</label><label class="expand" for="c-37493736">[1 more]</label></div><br/><div class="children"><div class="content">Some 70B finetunes excel at certain niches, like specific non-English languages, roleplay, fictional writing, or topics GPT4 would refuse to discuss. Some of these are hard to evaluate, but try out (for instance) MythoMax for fiction writing, Airoboros for &quot;uncensored&quot; general use, and Samantha for therapist style chat.<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;models?sort=modified&amp;search=70b" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;models?sort=modified&amp;search=70b</a></div><br/></div></div></div></div><div id="37493484" class="c"><input type="checkbox" id="c-37493484" checked=""/><div class="controls bullet"><span class="by">CodeCompost</span><span>|</span><a href="#37493351">prev</a><span>|</span><a href="#37493717">next</a><span>|</span><label class="collapse" for="c-37493484">[-]</label><label class="expand" for="c-37493484">[3 more]</label></div><br/><div class="children"><div class="content">What about the 4070 which &#x27;only&#x27; has 12GB of VRAM?</div><br/><div id="37493667" class="c"><input type="checkbox" id="c-37493667" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37493484">parent</a><span>|</span><a href="#37493717">next</a><span>|</span><label class="collapse" for="c-37493667">[-]</label><label class="expand" for="c-37493667">[2 more]</label></div><br/><div class="children"><div class="content">Thats a good fit for 13B at the moment. Or older 30B models with CPU offloading.<p>I&#x27;m not certain 30B models will fit completely on 12GB, even with this quantization.</div><br/><div id="37493947" class="c"><input type="checkbox" id="c-37493947" checked=""/><div class="controls bullet"><span class="by">CodeCompost</span><span>|</span><a href="#37493484">root</a><span>|</span><a href="#37493667">parent</a><span>|</span><a href="#37493717">next</a><span>|</span><label class="collapse" for="c-37493947">[-]</label><label class="expand" for="c-37493947">[1 more]</label></div><br/><div class="children"><div class="content">Maybe a stupid question, I&#x27;m not familiar with how video ports work, but does the concept of memory swapping exist for video cards? Or is that physically impossible?<p>Obviously that would be useless for games, but for LLMs it may be an option?</div><br/></div></div></div></div></div></div><div id="37493717" class="c"><input type="checkbox" id="c-37493717" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#37493484">prev</a><span>|</span><a href="#37493757">next</a><span>|</span><label class="collapse" for="c-37493717">[-]</label><label class="expand" for="c-37493717">[4 more]</label></div><br/><div class="children"><div class="content">This is quantized down to 2.5 bits per weight, whereas single precision accuracy is 32 bits.</div><br/><div id="37493868" class="c"><input type="checkbox" id="c-37493868" checked=""/><div class="controls bullet"><span class="by">tutfbhuf</span><span>|</span><a href="#37493717">parent</a><span>|</span><a href="#37493749">next</a><span>|</span><label class="collapse" for="c-37493868">[-]</label><label class="expand" for="c-37493868">[2 more]</label></div><br/><div class="children"><div class="content">How is 2.5 bits possible? Is it an average between 2 and 3 over all weights?</div><br/><div id="37494022" class="c"><input type="checkbox" id="c-37494022" checked=""/><div class="controls bullet"><span class="by">ftxbro</span><span>|</span><a href="#37493717">root</a><span>|</span><a href="#37493868">parent</a><span>|</span><a href="#37493749">next</a><span>|</span><label class="collapse" for="c-37494022">[-]</label><label class="expand" for="c-37494022">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Is it an average between 2 and 3 over all weights?<p>Yes I think it&#x27;s an average where different quantization levels are used for different layers or weights. Here are more details about the quantization scheme: <a href="https:&#x2F;&#x2F;github.com&#x2F;turboderp&#x2F;exllamav2#exl2-quantization">https:&#x2F;&#x2F;github.com&#x2F;turboderp&#x2F;exllamav2#exl2-quantization</a></div><br/></div></div></div></div><div id="37493749" class="c"><input type="checkbox" id="c-37493749" checked=""/><div class="controls bullet"><span class="by">3abiton</span><span>|</span><a href="#37493717">parent</a><span>|</span><a href="#37493868">prev</a><span>|</span><a href="#37493757">next</a><span>|</span><label class="collapse" for="c-37493749">[-]</label><label class="expand" for="c-37493749">[1 more]</label></div><br/><div class="children"><div class="content">Any benchmarks on performance?</div><br/></div></div></div></div><div id="37493757" class="c"><input type="checkbox" id="c-37493757" checked=""/><div class="controls bullet"><span class="by">yieldcrv</span><span>|</span><a href="#37493717">prev</a><span>|</span><label class="collapse" for="c-37493757">[-]</label><label class="expand" for="c-37493757">[4 more]</label></div><br/><div class="children"><div class="content">What’s a useful amount of tokens&#x2F;second?<p>What’s a speed like ChatGPT 4?<p>I’m trying to understand if there are agreed upon metrics such as “frames per second” in other fields or niches</div><br/><div id="37493922" class="c"><input type="checkbox" id="c-37493922" checked=""/><div class="controls bullet"><span class="by">sigmoid10</span><span>|</span><a href="#37493757">parent</a><span>|</span><a href="#37493892">next</a><span>|</span><label class="collapse" for="c-37493922">[-]</label><label class="expand" for="c-37493922">[1 more]</label></div><br/><div class="children"><div class="content">&gt;What’s a useful amount of tokens&#x2F;second?<p>Depends entirely on your application. If you want a real time chatbot, anything more than 10 tokens&#x2F;s is probably generating text faster than the user can read, so it&#x27;s fine. Do you want code suggestions or completions? Probabaly also fine, but a bit slow. Do you want to create summaries for 1000 documents? That&#x27;s gonna be really slow. But at this frontier of the field, tokens&#x2F;s is not really the issue. Performance vs. quality is. If you lose a whole lot of accuracy by quantizing floats down to single digit precisions but in turn are able to run 70B parameter models, you often still get better results than less thoroughly quantized 7B parameter models. It&#x27;s all about getting big models to run on memory limited GPUs.</div><br/></div></div><div id="37493892" class="c"><input type="checkbox" id="c-37493892" checked=""/><div class="controls bullet"><span class="by">brucethemoose2</span><span>|</span><a href="#37493757">parent</a><span>|</span><a href="#37493922">prev</a><span>|</span><label class="collapse" for="c-37493892">[-]</label><label class="expand" for="c-37493892">[2 more]</label></div><br/><div class="children"><div class="content">More than ~10 tokens&#x2F;s streamed feels OK to read. I can kinda live with 4-5 tokens&#x2F;s (which is what I get on llama 70B on my 3090 with llama.cpp CPU offload).<p>What feels &quot;good&quot; depends on the person and the type of content, but 35 tokens&#x2F;s should feel very fast.</div><br/><div id="37494048" class="c"><input type="checkbox" id="c-37494048" checked=""/><div class="controls bullet"><span class="by">enjeyw</span><span>|</span><a href="#37493757">root</a><span>|</span><a href="#37493892">parent</a><span>|</span><label class="collapse" for="c-37494048">[-]</label><label class="expand" for="c-37494048">[1 more]</label></div><br/><div class="children"><div class="content">I wonder if anyone will end up employing the &quot;human&quot; solution of padding sentences out with filler words like &#x27;Ummmm&#x27; or &#x27;So like&#x27; while the generator is determining the actual next token...</div><br/></div></div></div></div></div></div></div></div></div></div></div></body></html>