<!DOCTYPE html><html lang="en"><head><title>Static News</title><meta charSet="utf-8"/><meta name="description" content="Static delayed Hacker News."/><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1d1f21"/><meta name="viewport" content="width=device-width,initial-scale=1.0"/><meta name="application-name" content="Static News"/><meta name="apple-mobile-web-app-title" content="Static News"/><meta name="apple-mobile-web-app-capable" content="yes"/><meta name="mobile-web-app-capable" content="yes"/><meta name="apple-mobile-web-app-status-bar-style" content="#1d1f21"/><link rel="preload" href="styles.css?v=1700557264901" as="style"/><link rel="stylesheet" href="styles.css?v=1700557264901"/></head><body><div id="container"><div id="inner"><header><a href="/">Static News</a><a href="/about">about</a></header><div id="content"><div><div id="title"><a href="https://arxiv.org/abs/2311.08516">LLMs cannot find reasoning errors, but can correct them</a> <span class="domain">(<a href="https://arxiv.org">arxiv.org</a>)</span></div><div class="subtext"><span>koie</span> | <span>102 comments</span></div><br/><div><div id="38355821" class="c"><input type="checkbox" id="c-38355821" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#38354692">next</a><span>|</span><label class="collapse" for="c-38355821">[-]</label><label class="expand" for="c-38355821">[37 more]</label></div><br/><div class="children"><div class="content">Stop doing self-correction within the context of the model&#x27;s own generation.<p>The previous paper on self correction told the model &quot;you previously said X - are there errors with this?&quot;<p>This one has the mistakes statically added to the prompt in a task prompt and response without additional context immediately before asking if it has any errors.<p>Think about the training data.<p>How often does the training data of most of the Internet reflect users identifying issues with their own output?<p>How often does the training data reflect users identifying issues with someone else&#x27;s output?<p>Try doing self-correction by setting up the context of &quot;this was someone else&#x27;s answer&quot;. It is still technically self-correction if a model is reviewing its own output in that context - it just isn&#x27;t set up as &quot;correct your own answer.&quot;<p>This may even be part of why the classifier did a better job at identifying issues - less the fine tuning and more the context (unfortunately I don&#x27;t see the training&#x2F;prompts for the classifier in their GitHub repo).<p>It really seems like the aversion to anthropomorphizing LLMs is leading people to ignore or overlook relevant patterns in the highly anthropomorphic training data fed into them. We might not want to entertain that a LLM has a concept of self vs other or a bias between critiques based on such a differentiation, and yet the training data almost certainly reflects such a concept and bias.<p>I&#x27;d strongly encourage future work on self-correction to explicitly define the thing being evaluated as the work of another. (Or ideally even compare self-correction rates between critiques in the context of their own output vs another&#x27;s output.)</div><br/><div id="38357413" class="c"><input type="checkbox" id="c-38357413" checked=""/><div class="controls bullet"><span class="by">bongodongobob</span><span>|</span><a href="#38355821">parent</a><span>|</span><a href="#38356566">next</a><span>|</span><label class="collapse" for="c-38357413">[-]</label><label class="expand" for="c-38357413">[12 more]</label></div><br/><div class="children"><div class="content">I see lots of people trying to prompt with incomplete sentences, not capitalizing, using slang, bad grammar, imprecise terminology etc. And it still works. <i>However</i>, I find that you get a noticable a
quality boost if you use proper English and treat it more like a human.<p>&quot;i want a python app that calculates a roadtrip for me&quot;<p>vs<p>&quot;Please write me a Python program using a map API that measures the distance between two locations as a car would drive. Think carefully about the program architecture and be sure to use a human readable Pythonic style. Please show me the complete program in it&#x27;s entirety.&quot;<p>The former game me a high level overview with a ton of explanation and didn&#x27;t write any code. You can try to walk it through the process of all the steps it needs, but it will write &quot;confused&quot;, albeit working, code after a few prompts. The latter just wrote working code on the first response. Moving forward, the context is just so more concise and correct that everything after will be of much higher quality.<p>I rarely go past 5-10 responses due to what I&#x27;d call &quot;context poisoning&quot;. If it makes a simple syntax error or something small, I&#x27;ll shoot it the error and let it correct itself. But as soon as it invents a function or otherwise hallucinates, it gets copy pasted into a new prompt saying &quot;here&#x27;s some bad code, fix this&quot; and it is far more likely to come up with an elegant solution rather that rewriting everything or making huge changes to solve a one off error or something it&#x27;s previous context was preventing it from grasping.<p>What you&#x27;re saying is almost the meta of using good grammer and context, and I completely agree.</div><br/><div id="38360889" class="c"><input type="checkbox" id="c-38360889" checked=""/><div class="controls bullet"><span class="by">avereveard</span><span>|</span><a href="#38355821">root</a><span>|</span><a href="#38357413">parent</a><span>|</span><a href="#38359908">next</a><span>|</span><label class="collapse" for="c-38360889">[-]</label><label class="expand" for="c-38360889">[1 more]</label></div><br/><div class="children"><div class="content">Smallish model (7b) require a somewhat simplified grammar tho. Especially with longer complex instruction I found more luck by joining all the conditions with ands and to have everything that&#x27;s a follow up and need to happen in order joined by then, instead of having more natural sentences.<p>So instead of &quot;write a short story of a person that&#x27;s satisfied at work&quot; something along the line of &quot;write a short story and the protagonist must be a person and the protagonist must be happy at work&quot; boost comprension especially as the condition list becomes longer.</div><br/></div></div><div id="38359908" class="c"><input type="checkbox" id="c-38359908" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#38355821">root</a><span>|</span><a href="#38357413">parent</a><span>|</span><a href="#38360889">prev</a><span>|</span><a href="#38358245">next</a><span>|</span><label class="collapse" for="c-38359908">[-]</label><label class="expand" for="c-38359908">[1 more]</label></div><br/><div class="children"><div class="content">A recent paper along these lines you might be interested in was <i>Large Language Models Understand and Can be Enhanced by Emotional Stimuli</i>: <a href="https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.11760" rel="nofollow noreferrer">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2307.11760</a><p>It makes complete sense and has been a part of my own usage for well over a year now, but it&#x27;s been cool seeing it demonstrated in research across multiple models.</div><br/></div></div><div id="38358245" class="c"><input type="checkbox" id="c-38358245" checked=""/><div class="controls bullet"><span class="by">galaxyLogic</span><span>|</span><a href="#38355821">root</a><span>|</span><a href="#38357413">parent</a><span>|</span><a href="#38359908">prev</a><span>|</span><a href="#38359464">next</a><span>|</span><label class="collapse" for="c-38358245">[-]</label><label class="expand" for="c-38358245">[2 more]</label></div><br/><div class="children"><div class="content">If your prompt-input is high quality it is more likely to match high-quality training inputs</div><br/><div id="38360470" class="c"><input type="checkbox" id="c-38360470" checked=""/><div class="controls bullet"><span class="by">euroderf</span><span>|</span><a href="#38355821">root</a><span>|</span><a href="#38358245">parent</a><span>|</span><a href="#38359464">next</a><span>|</span><label class="collapse" for="c-38360470">[-]</label><label class="expand" for="c-38360470">[1 more]</label></div><br/><div class="children"><div class="content">This seems intuitively true but has it been established ?</div><br/></div></div></div></div><div id="38359464" class="c"><input type="checkbox" id="c-38359464" checked=""/><div class="controls bullet"><span class="by">CtrlAltmanDel</span><span>|</span><a href="#38355821">root</a><span>|</span><a href="#38357413">parent</a><span>|</span><a href="#38358245">prev</a><span>|</span><a href="#38358595">next</a><span>|</span><label class="collapse" for="c-38359464">[-]</label><label class="expand" for="c-38359464">[5 more]</label></div><br/><div class="children"><div class="content">Using a common search engine for &quot;python app calculate roadtrip&quot;<p>is way faster, free, doesn&#x27;t require a phone number or login, and gives much better results.</div><br/><div id="38359737" class="c"><input type="checkbox" id="c-38359737" checked=""/><div class="controls bullet"><span class="by">cosmojg</span><span>|</span><a href="#38355821">root</a><span>|</span><a href="#38359464">parent</a><span>|</span><a href="#38360447">next</a><span>|</span><label class="collapse" for="c-38359737">[-]</label><label class="expand" for="c-38359737">[3 more]</label></div><br/><div class="children"><div class="content">Not nearly as quickly or directly, though. LLMs augmented by search engines (or vice versa) seem to be an obvious and permanent innovation, especially for the general public who are notoriously awful at personally generating optimal keywords for a desired search query.</div><br/><div id="38360291" class="c"><input type="checkbox" id="c-38360291" checked=""/><div class="controls bullet"><span class="by">Roark66</span><span>|</span><a href="#38355821">root</a><span>|</span><a href="#38359737">parent</a><span>|</span><a href="#38360447">next</a><span>|</span><label class="collapse" for="c-38360291">[-]</label><label class="expand" for="c-38360291">[2 more]</label></div><br/><div class="children"><div class="content">I&#x27;m not convinced. On these few occasions where an AI chat bot went out, did a Google search and responded with results the quality of that answer was always much worse than if it just replied from it&#x27;s training data. This of course excludes things that happened after training data ends.<p>For example, ask chatgpt about writing a python script that does anything with AWS inspector 2. It will do very badly, it will hallucinate, etc. Even with Internet access. Ask about doing the same with some other API that was well represented in the training set and it&#x27;s great.<p>This is why I think predicting death for sites like stackoverflow is very premature. What happens 10 years down the line once everything chatgpt knows is old tech? It can&#x27;t be simply trained with more recrnt data, because unless stackoverflow regains it&#x27;s popularity there will be very little training data. Of course various data generation techniques will be invented and tried, but no one will match the gold standard of human generated data.<p>Unfortunately I have to predict inevitable enshittification of general purpose chat bots.</div><br/><div id="38361129" class="c"><input type="checkbox" id="c-38361129" checked=""/><div class="controls bullet"><span class="by">dwattttt</span><span>|</span><a href="#38355821">root</a><span>|</span><a href="#38360291">parent</a><span>|</span><a href="#38360447">next</a><span>|</span><label class="collapse" for="c-38361129">[-]</label><label class="expand" for="c-38361129">[1 more]</label></div><br/><div class="children"><div class="content"><a href="https:&#x2F;&#x2F;www.inf.ufpr.br&#x2F;renato&#x2F;profession.html" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.inf.ufpr.br&#x2F;renato&#x2F;profession.html</a></div><br/></div></div></div></div></div></div><div id="38360447" class="c"><input type="checkbox" id="c-38360447" checked=""/><div class="controls bullet"><span class="by">jibal</span><span>|</span><a href="#38355821">root</a><span>|</span><a href="#38359464">parent</a><span>|</span><a href="#38359737">prev</a><span>|</span><a href="#38358595">next</a><span>|</span><label class="collapse" for="c-38360447">[-]</label><label class="expand" for="c-38360447">[1 more]</label></div><br/><div class="children"><div class="content">Utterly false. A google search for that phrase yields &quot;It looks like there aren&#x27;t many great matches for your search&quot;. And no search engine will yield the <i>code</i> for such an app unless the engine is LLM-based.</div><br/></div></div></div></div><div id="38358595" class="c"><input type="checkbox" id="c-38358595" checked=""/><div class="controls bullet"><span class="by">lupire</span><span>|</span><a href="#38355821">root</a><span>|</span><a href="#38357413">parent</a><span>|</span><a href="#38359464">prev</a><span>|</span><a href="#38356566">next</a><span>|</span><label class="collapse" for="c-38358595">[-]</label><label class="expand" for="c-38358595">[2 more]</label></div><br/><div class="children"><div class="content">Your example confounds many variables.</div><br/><div id="38360342" class="c"><input type="checkbox" id="c-38360342" checked=""/><div class="controls bullet"><span class="by">bongodongobob</span><span>|</span><a href="#38355821">root</a><span>|</span><a href="#38358595">parent</a><span>|</span><a href="#38356566">next</a><span>|</span><label class="collapse" for="c-38360342">[-]</label><label class="expand" for="c-38360342">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re definitely right. I&#x27;m painting with very broad strokes to make a point of what I&#x27;ve been seeing.</div><br/></div></div></div></div></div></div><div id="38356566" class="c"><input type="checkbox" id="c-38356566" checked=""/><div class="controls bullet"><span class="by">PoignardAzur</span><span>|</span><a href="#38355821">parent</a><span>|</span><a href="#38357413">prev</a><span>|</span><a href="#38355930">next</a><span>|</span><label class="collapse" for="c-38356566">[-]</label><label class="expand" for="c-38356566">[10 more]</label></div><br/><div class="children"><div class="content">&gt; <i>Think about the training data.</i><p>&gt; <i>How often does the training data of most of the Internet reflect users identifying issues with their own output?</i><p>&gt; <i>How often does the training data reflect users identifying issues with someone else&#x27;s output?</i><p>I wouldn&#x27;t put too much weight into just-so theories like this.<p>We still don&#x27;t understand too much about how LLMs process information internally; it could be that their understanding of the concept of &quot;correcting a previous mistake&quot; is good enough that they can access it without prompt engineering to mimic the way it happens in training data. Or maybe not (after all, there&#x27;s an entire management concept called &quot;pre-mortems&quot; which is basically doing what you suggest, as a human).</div><br/><div id="38356744" class="c"><input type="checkbox" id="c-38356744" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#38355821">root</a><span>|</span><a href="#38356566">parent</a><span>|</span><a href="#38358820">next</a><span>|</span><label class="collapse" for="c-38356744">[-]</label><label class="expand" for="c-38356744">[1 more]</label></div><br/><div class="children"><div class="content">This depends less on the internals vs the patterns in the training data.<p>Even if the model has the capacity to abstract beyond the patterns, the patterns are still very likely to have influence on its ability to do so.<p>For example, early after GPT-4 was released it was being claimed it couldn&#x27;t solve variations on the goat, wolf, and cabbage problem.<p>I found that it <i>could</i> solve these variations fine 100% of the time, you just needed to explicitly prompt for it to repeat adjectives with nouns and change the nouns to emojis. The repeating worked similar to CoT by biasing the generation towards the variation and away from the original form, and the emojis in place of the nouns further broke the token associations which was leading it to fail by extending the original solution.<p>So while it&#x27;s possible that with enough finessing you could get a model to perform self-critique as well as its critique of others, if the training data has a clear pattern of bias between those two, why actively ignore it?<p>It&#x27;s a bit like sanding against the grain vs with it. You <i>can</i> sand against the grain of the training data and with enough effort potentially get the result you want with sophisticated enough models. But maybe your life will be a lot easier if you identify the grain in the data first and sand along with it instead?</div><br/></div></div><div id="38358820" class="c"><input type="checkbox" id="c-38358820" checked=""/><div class="controls bullet"><span class="by">galaxyLogic</span><span>|</span><a href="#38355821">root</a><span>|</span><a href="#38356566">parent</a><span>|</span><a href="#38356744">prev</a><span>|</span><a href="#38360541">next</a><span>|</span><label class="collapse" for="c-38358820">[-]</label><label class="expand" for="c-38358820">[7 more]</label></div><br/><div class="children"><div class="content">&gt; We still don&#x27;t understand too much about how LLMs process information internally<p>I admit I personally don&#x27;t know too much about how &quot;LLMs process information internally&quot;. But, I would find it curious if programmers who created the system wouldn&#x27;t understand what it is doing. Is there any evidence that the LLM programmers don&#x27;t understand how the program they created works?</div><br/><div id="38359787" class="c"><input type="checkbox" id="c-38359787" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#38355821">root</a><span>|</span><a href="#38358820">parent</a><span>|</span><a href="#38360088">next</a><span>|</span><label class="collapse" for="c-38359787">[-]</label><label class="expand" for="c-38359787">[3 more]</label></div><br/><div class="children"><div class="content">LLMs aren&#x27;t programmed and it&#x27;s why the neural network working as it does is black box to everyone, developers included.<p>Imagine a billion black boxes with hamsters put in them. You put in a bag of equally mixed Skittles in one end of each box and then rate each box based on how well it does to get rid of the yellow and green Skittles but push out the others. The ones that do the best at this you mate the hamsters and go again, over and over. Eventually you should have hamsters in boxes that almost always get rid of yellow and green Skittles and output the rest.<p>But is it because you bred in a preference to eat those color Skittles? An aversion to the other colors? Are they using those colors for nesting? Do they find the red and blue and orange ones too stimulating so they push those out but leave the others alone?<p>There could be a myriad of reasons why your training was successful, and without the ability to introspect the result you just won&#x27;t know what&#x27;s correct.<p>This is a <i>huge</i> simplification by way of loose analogy for what&#x27;s going on with training a transformer based LLM, but no one is sitting there &#x27;programming&#x27; it. They are just setting up the conditions for it to self-optimize around the training goals given the data, and the &#x27;programming&#x27; just has to do with improving the efficiency of the training process. Analyzing the final network itself is like trying to understand what each variable in a billion variable math equation is doing to the result.</div><br/><div id="38360118" class="c"><input type="checkbox" id="c-38360118" checked=""/><div class="controls bullet"><span class="by">galaxyLogic</span><span>|</span><a href="#38355821">root</a><span>|</span><a href="#38359787">parent</a><span>|</span><a href="#38360088">next</a><span>|</span><label class="collapse" for="c-38360118">[-]</label><label class="expand" for="c-38360118">[2 more]</label></div><br/><div class="children"><div class="content">When you train an LLM you do that by executing some computer code with some inputs. The programmers who wrote the code you execute know exactly what it does. Just like Google knows exactly how its search-algorithm works. An LLM   uses statistics and Markov-chains and what have you to generate the output for a given input.<p>It&#x27;s like with any optimization algorithm. You cannot predict what exactly will be the result of a given optimization-run. But you know how the optimization algorithm works. The (more or less) optimal solution you get back might surprise you, might be counter-intuitive. But programmers who wrote the code that did the optimization, and have the source-code, know exactly how it works.<p>When you get a result from LLM you don&#x27;t say &quot;I can&#x27;t possibly understand why it came up with this result?&quot;. You can understand that, it&#x27;s just following the rules it was programmed to follow. You might not know those rules, you might not understand them, but programmers who wrote them do.</div><br/><div id="38360613" class="c"><input type="checkbox" id="c-38360613" checked=""/><div class="controls bullet"><span class="by">IanCal</span><span>|</span><a href="#38355821">root</a><span>|</span><a href="#38360118">parent</a><span>|</span><a href="#38360088">next</a><span>|</span><label class="collapse" for="c-38360613">[-]</label><label class="expand" for="c-38360613">[1 more]</label></div><br/><div class="children"><div class="content">You&#x27;re mixing up what we mean by what rules it&#x27;s following or how it&#x27;s working.<p>If I ask how it&#x27;s able to write a poem given a request and you tell me you know - it multiplies and adds this set of 1.8 trillion numbers together X times with this set of accumulators, I would argue you don&#x27;t understand how it works enough to make any useful predictions.<p>Kind of like how you understand what insane spaghetti code is doing - it&#x27;s running this code - but can have absolutely no idea what business logic it encodes.</div><br/></div></div></div></div></div></div><div id="38360088" class="c"><input type="checkbox" id="c-38360088" checked=""/><div class="controls bullet"><span class="by">seanhunter</span><span>|</span><a href="#38355821">root</a><span>|</span><a href="#38358820">parent</a><span>|</span><a href="#38359787">prev</a><span>|</span><a href="#38360541">next</a><span>|</span><label class="collapse" for="c-38360088">[-]</label><label class="expand" for="c-38360088">[3 more]</label></div><br/><div class="children"><div class="content">People understand how the program works but not how the network produces the outputs it does from the inputs and training it receives. The mechanics of how these models work at a very high level are:<p>1. Tokenize some input so you have some big vectors<p>2. &lt;bunch of linear algebra involving these vectors and some sets of matrices of weights&gt;<p>3. Take the output vector and turn it back into tokens<p>Each of these steps are well understood in and of themselves. So maybe the magic is in the way the matrices of weights are generated and trained? Well we know they typically start as random matrices, and can explain how as the network is trained these weights are tweaked in various ways.<p>All of that is known.  What’s unclear is specifically how the weights in the matrices correspond to our understanding of the concepts in the input and output and how it all seems to add up to a system that works as well as it does. I think that’s what they meant by not understanding how they process information internally.</div><br/><div id="38360209" class="c"><input type="checkbox" id="c-38360209" checked=""/><div class="controls bullet"><span class="by">galaxyLogic</span><span>|</span><a href="#38355821">root</a><span>|</span><a href="#38360088">parent</a><span>|</span><a href="#38360541">next</a><span>|</span><label class="collapse" for="c-38360209">[-]</label><label class="expand" for="c-38360209">[2 more]</label></div><br/><div class="children"><div class="content">&gt; that’s what they meant by not understanding how they process information internally.<p>There is no other &quot;internal information processing&quot; happening in an LLM than the process it was programmed to execute. Is there?<p>The code an LLM executes is not too complicated for humans to understand. After all it was written by humans. The outputs may be surprising but so it is with lottery. Why did I win the jackpot this week, when I didn&#x27;t win anything in the last 10 years? Very counter-intuitive. I can&#x27;t possibly understand that? Yes I can, it is just statistics and probability.</div><br/><div id="38360427" class="c"><input type="checkbox" id="c-38360427" checked=""/><div class="controls bullet"><span class="by">seanhunter</span><span>|</span><a href="#38355821">root</a><span>|</span><a href="#38360209">parent</a><span>|</span><a href="#38360541">next</a><span>|</span><label class="collapse" for="c-38360427">[-]</label><label class="expand" for="c-38360427">[1 more]</label></div><br/><div class="children"><div class="content">As I tried to explain, it&#x27;s not the code that people don&#x27;t understand. People understand the code they wrote.<p>It&#x27;s <i>why</i> the bunch of linear algebra on the weights works to do this particular task, and how it will respond to any particular task that is a bit mysterious.<p>Like imagine someone gave you the Taylor series expansion of the inverse Kepler equation[1]. So you just have a bunch of crazy fractions of powers of x that you add up. And then they say ok now the this function will very accurately explain the orbit of the planets.<p>You&#x27;d be able to do the steps - you&#x27;re just adding up fractions. You&#x27;d be able to verify the answer you got corresponded to the orbit of a given celestial body.<p>But if you didn&#x27;t have all the pieces in the middle (calculus mainly) there&#x27;s no way you&#x27;d be able to explain why this particular set of fractions corresponds to the movement of the planets and some other set doesn&#x27;t.<p>[1] <a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Kepler%27s_equation" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Kepler%27s_equation</a> scroll down a bit</div><br/></div></div></div></div></div></div></div></div><div id="38360541" class="c"><input type="checkbox" id="c-38360541" checked=""/><div class="controls bullet"><span class="by">jibal</span><span>|</span><a href="#38355821">root</a><span>|</span><a href="#38356566">parent</a><span>|</span><a href="#38358820">prev</a><span>|</span><a href="#38355930">next</a><span>|</span><label class="collapse" for="c-38360541">[-]</label><label class="expand" for="c-38360541">[1 more]</label></div><br/><div class="children"><div class="content">They aren&#x27;t just-so theories ... this is how LLMs work. We actually understand exactly how they process information internally, but since their very nature is to extract statistical patterns from the training data and that training data is massive, we can&#x27;t anticipate what patterns have been extracted. We just know that, whatever patterns are there to be abstracted--e.g., users tending to identify issues with someone else&#x27;s output rather than their own--those patterns will be reflected in the output.</div><br/></div></div></div></div><div id="38355930" class="c"><input type="checkbox" id="c-38355930" checked=""/><div class="controls bullet"><span class="by">andai</span><span>|</span><a href="#38355821">parent</a><span>|</span><a href="#38356566">prev</a><span>|</span><a href="#38360010">next</a><span>|</span><label class="collapse" for="c-38355930">[-]</label><label class="expand" for="c-38355930">[5 more]</label></div><br/><div class="children"><div class="content">That&#x27;s hilarious. Does this imply LLMs inherited the human tendency to get attached to a perspective despite evidence to the contrary? I&#x27;ll often try to coax the right answer out of GPT-3 when I know it&#x27;s wrong, and it&#x27;ll often insist that it&#x27;s right several times in a row.</div><br/><div id="38355963" class="c"><input type="checkbox" id="c-38355963" checked=""/><div class="controls bullet"><span class="by">OmarShehata</span><span>|</span><a href="#38355821">root</a><span>|</span><a href="#38355930">parent</a><span>|</span><a href="#38360394">next</a><span>|</span><label class="collapse" for="c-38355963">[-]</label><label class="expand" for="c-38355963">[2 more]</label></div><br/><div class="children"><div class="content">I think it does indeed suggest this, but I think this may be good news.<p>Part of what makes humans able to make progress in difficult, vague, and uncertain fields is a willingness to hold onto a point of view in the face of criticism to try &amp; fix itl. This is, as a matter of fact, how science progresses, depending on if you ask scientists or historians of science. See Thomas Kuhn&#x27;s Structure of Scientific Revolutions for more on this.</div><br/><div id="38360496" class="c"><input type="checkbox" id="c-38360496" checked=""/><div class="controls bullet"><span class="by">jibal</span><span>|</span><a href="#38355821">root</a><span>|</span><a href="#38355963">parent</a><span>|</span><a href="#38360394">next</a><span>|</span><label class="collapse" for="c-38360496">[-]</label><label class="expand" for="c-38360496">[1 more]</label></div><br/><div class="children"><div class="content">But LLMs don&#x27;t do these things ... they just produce text that statistically matches patterns in the training data. Since the humans who authored the training data have personality patterns, the <i>outputs</i> of LLMs show these personality patterns. But LLMs do not internalize such patterns--they have no cognitive functions of their own.</div><br/></div></div></div></div><div id="38360394" class="c"><input type="checkbox" id="c-38360394" checked=""/><div class="controls bullet"><span class="by">mkl</span><span>|</span><a href="#38355821">root</a><span>|</span><a href="#38355930">parent</a><span>|</span><a href="#38355963">prev</a><span>|</span><a href="#38360471">next</a><span>|</span><label class="collapse" for="c-38360394">[-]</label><label class="expand" for="c-38360394">[1 more]</label></div><br/><div class="children"><div class="content">Getting attached to a perspective despite evidence to the contrary would require perspective and distinguishing fact from fiction, but just copying humans protesting that they&#x27;re right (regardless of context) seems plausible, as there&#x27;s a lot of that to learn from.</div><br/></div></div><div id="38360471" class="c"><input type="checkbox" id="c-38360471" checked=""/><div class="controls bullet"><span class="by">jibal</span><span>|</span><a href="#38355821">root</a><span>|</span><a href="#38355930">parent</a><span>|</span><a href="#38360394">prev</a><span>|</span><a href="#38360010">next</a><span>|</span><label class="collapse" for="c-38360471">[-]</label><label class="expand" for="c-38360471">[1 more]</label></div><br/><div class="children"><div class="content">Everything in the <i>output</i> of LLMs is inherited from human tendencies ... that&#x27;s the very essence of how they work. But LLMs themselves don&#x27;t have any of these tendencies ... they are just statistical engines that extract patterns from the training data.</div><br/></div></div></div></div><div id="38360010" class="c"><input type="checkbox" id="c-38360010" checked=""/><div class="controls bullet"><span class="by">seanhunter</span><span>|</span><a href="#38355821">parent</a><span>|</span><a href="#38355930">prev</a><span>|</span><a href="#38359748">next</a><span>|</span><label class="collapse" for="c-38360010">[-]</label><label class="expand" for="c-38360010">[1 more]</label></div><br/><div class="children"><div class="content">The obvious way to do this would be as adversarial networks like in GANs for image generation. Have the existing LLM as the generator trained exactly as at present but with an additional penalty for being found to have committed an error and have another network trained at the same time as a validator where its fitness function is finding errors in the output of the generator.<p>People must be doing this, probably just takes a while for the research to bear fruit.<p>Some of these errors are so obvious I can’t imagine this would be too hard.  For an example, try asking an LLM “generate me a system of two equations in two unknowns. Both the coefficients and the solutions must be integers between -10 and 10”. In my experience it will generate a valid system. Some of the time the coefficients will be in the range specified. Probably about a third to a half the time the solution it gives will be wrong and when you ask for an explanation of the solution it will make some basic arithmetic error (eg flipping a sign etc). Then when you point out the error it will correct.</div><br/></div></div><div id="38359748" class="c"><input type="checkbox" id="c-38359748" checked=""/><div class="controls bullet"><span class="by">NoToP</span><span>|</span><a href="#38355821">parent</a><span>|</span><a href="#38360010">prev</a><span>|</span><a href="#38356225">next</a><span>|</span><label class="collapse" for="c-38359748">[-]</label><label class="expand" for="c-38359748">[2 more]</label></div><br/><div class="children"><div class="content">With due respect (and I actually mean due respect), this embodies exactly what is wrong with the modern approach to AI. Who cares that there&#x27;s no examples in the training set. True AI should be able of taking a few steps out of book without getting flummoxed. When you learn your first language, teacher does not stand before the class and provide examples of ungrammatical statements, yet you figure out the rules of grammar just fine.<p>There is something fundamentally flawed in the approach not in the data.</div><br/><div id="38360149" class="c"><input type="checkbox" id="c-38360149" checked=""/><div class="controls bullet"><span class="by">seanhunter</span><span>|</span><a href="#38355821">root</a><span>|</span><a href="#38359748">parent</a><span>|</span><a href="#38356225">next</a><span>|</span><label class="collapse" for="c-38360149">[-]</label><label class="expand" for="c-38360149">[1 more]</label></div><br/><div class="children"><div class="content">There are training methodologies that do this but they don’t necessarily work in this case (or noone has got them to work that well yet).<p>For example reinforcement learning, like when AlphaZero famously learned by playing itself at chess and go and became much stronger than the purpose-built “alphago” first version.<p>Or another example generative adversarial networks where you have a generator network generating images and a validator network trying to spot fake images.<p>In both these examples it’s easy to see how you build the loss functions for the training because they are quite constrained. For a domain like a game you penalize versions of the model that lose games and reward those that win. For GANs the initial insight was huge but having had that it’s easy to see how you move forward - you reward the generator for slipping fake images past the validator and you reward the validator for finding fakes in a stream of images that includes some real images and some generated images.<p>For an open-ended general model like an LLM it’s not so easy to see how you do this in the general case. GPT models are actually pretty good at “zero shot” learning (without examples) and “transfer” learning (where lessons from a domain are applied to an associated domain).<p>Your example of a language is interesting, because you don’t learn your first language from any sort of teacher - you learn it from your parents and others talking around you and to you.  So you have lots of examples to draw on.  You then try out various sounds and words and everyone looks confused but becomes more excited as you get closer to saying something that is a real word eventually you hit on the magic recipe and say the word “DUCK!” (Or whatever) and everyone loses their minds.  So you have lots of positive reinforcement that you’re on the right track and you have a huge number of examples. You’re not just fed the hackernews comment section, some papers on quantum mechanics and all the english literature that has fallen out of copyright and left to get on with it.</div><br/></div></div></div></div><div id="38356225" class="c"><input type="checkbox" id="c-38356225" checked=""/><div class="controls bullet"><span class="by">og_kalu</span><span>|</span><a href="#38355821">parent</a><span>|</span><a href="#38359748">prev</a><span>|</span><a href="#38356497">next</a><span>|</span><label class="collapse" for="c-38356225">[-]</label><label class="expand" for="c-38356225">[1 more]</label></div><br/><div class="children"><div class="content">&gt;It really seems like the aversion to anthropomorphizing LLMs is leading people to ignore or overlook relevant patterns in the highly anthropomorphic training data fed into them.<p>This exactly. Not anthropomizing when anthropomization is producing better predictive models of what to expect in output is not smart, it&#x27;s just silly.</div><br/></div></div><div id="38356497" class="c"><input type="checkbox" id="c-38356497" checked=""/><div class="controls bullet"><span class="by">4death4</span><span>|</span><a href="#38355821">parent</a><span>|</span><a href="#38356225">prev</a><span>|</span><a href="#38354692">next</a><span>|</span><label class="collapse" for="c-38356497">[-]</label><label class="expand" for="c-38356497">[5 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t agree about your point regarding training data. The internet is infamous for pedants who will correct even the smallest factual or logical errors. Take this comment for instance...  It seems like the training set would be filled with proposition X, followed by a corrective assertion Y.</div><br/><div id="38356581" class="c"><input type="checkbox" id="c-38356581" checked=""/><div class="controls bullet"><span class="by">xp84</span><span>|</span><a href="#38355821">root</a><span>|</span><a href="#38356497">parent</a><span>|</span><a href="#38354692">next</a><span>|</span><label class="collapse" for="c-38356581">[-]</label><label class="expand" for="c-38356581">[4 more]</label></div><br/><div class="children"><div class="content">I think you&#x27;re agreeing with GP.<p>That&#x27;s the point: The internet IS full of pedants correcting others&#x27; statements. (Hopefully those pedants are right enough of the time for this to be helpful training data, heh.)<p>I think GP (kromem) was pointing out that those corrections are more likely to be phrased as &quot;You&#x27;re wrong, here&#x27;s why...&quot; than as &quot;I&#x27;m sorry, I was mistaken&quot; because humans are full of sass for other humans and not as full of first-person admitted errors.</div><br/><div id="38358867" class="c"><input type="checkbox" id="c-38358867" checked=""/><div class="controls bullet"><span class="by">galaxyLogic</span><span>|</span><a href="#38355821">root</a><span>|</span><a href="#38356581">parent</a><span>|</span><a href="#38356627">next</a><span>|</span><label class="collapse" for="c-38358867">[-]</label><label class="expand" for="c-38358867">[1 more]</label></div><br/><div class="children"><div class="content">Good point. That is why LLMs are incapable of humility. And that may be their downfall.</div><br/></div></div><div id="38356627" class="c"><input type="checkbox" id="c-38356627" checked=""/><div class="controls bullet"><span class="by">kromem</span><span>|</span><a href="#38355821">root</a><span>|</span><a href="#38356581">parent</a><span>|</span><a href="#38358867">prev</a><span>|</span><a href="#38354692">next</a><span>|</span><label class="collapse" for="c-38356627">[-]</label><label class="expand" for="c-38356627">[2 more]</label></div><br/><div class="children"><div class="content">Exactly.</div><br/><div id="38359722" class="c"><input type="checkbox" id="c-38359722" checked=""/><div class="controls bullet"><span class="by">4death4</span><span>|</span><a href="#38355821">root</a><span>|</span><a href="#38356627">parent</a><span>|</span><a href="#38354692">next</a><span>|</span><label class="collapse" for="c-38359722">[-]</label><label class="expand" for="c-38359722">[1 more]</label></div><br/><div class="children"><div class="content">Isn’t the entire promise of LLMs that they’re supposed to generalize, though?</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38354692" class="c"><input type="checkbox" id="c-38354692" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#38355821">prev</a><span>|</span><a href="#38355614">next</a><span>|</span><label class="collapse" for="c-38354692">[-]</label><label class="expand" for="c-38354692">[14 more]</label></div><br/><div class="children"><div class="content">I was just testing Bard with some very simple coding exercises and it did well.<p>I noticed that they automatically create at least three other draft responses.<p>I assume that this is a technique that allows them to try multiple times and then select the best one.<p>Just mentioning it because it seems like another example of not strictly &quot;zero-shot&quot;ing a response. Which seems important for getting good results with these models.<p>I&#x27;m guessing they use batching for this. I wonder if it might become more common to run multiple inference subtasks for the same main task inside of a batch, for purposes of self-correcting agent swarms or something. The outputs from step one are reviewed by the group in step 2, then they try again in step 3.<p>I guess that only applies for a small department where there is frequently just one person using it at a time.</div><br/><div id="38355129" class="c"><input type="checkbox" id="c-38355129" checked=""/><div class="controls bullet"><span class="by">MillionOClock</span><span>|</span><a href="#38354692">parent</a><span>|</span><a href="#38355574">next</a><span>|</span><label class="collapse" for="c-38355129">[-]</label><label class="expand" for="c-38355129">[6 more]</label></div><br/><div class="children"><div class="content">IIRC there were some OpenAI docs that recommended doing exactly this, make n generations and use a smaller fine tuned model to select the best one</div><br/><div id="38355526" class="c"><input type="checkbox" id="c-38355526" checked=""/><div class="controls bullet"><span class="by">DaiPlusPlus</span><span>|</span><a href="#38354692">root</a><span>|</span><a href="#38355129">parent</a><span>|</span><a href="#38355411">next</a><span>|</span><label class="collapse" for="c-38355526">[-]</label><label class="expand" for="c-38355526">[4 more]</label></div><br/><div class="children"><div class="content">...does this directly relate to the high operating costs of LLMs-as-a-service, if for every request they have to run n-many redundant LLM requests? So if they could improve things so that a single prompt&#x2F;request+response has a higher chance of being high-quality they wouldn&#x27;t need to run alternatives?</div><br/><div id="38355588" class="c"><input type="checkbox" id="c-38355588" checked=""/><div class="controls bullet"><span class="by">ilaksh</span><span>|</span><a href="#38354692">root</a><span>|</span><a href="#38355526">parent</a><span>|</span><a href="#38359310">next</a><span>|</span><label class="collapse" for="c-38355588">[-]</label><label class="expand" for="c-38355588">[1 more]</label></div><br/><div class="children"><div class="content">A lot of people don&#x27;t run multiple at a time.<p>It can make it more expensive if that option becomes popular.<p>But I think in most cases batching is actually the biggest _improvement_ in terms of cost effectiveness for operators, since it enables them to use the parallel throughout of the graphics device more fully by handling multiple inference requests (often from different customers) at once. (Unless they work like Bard by default).</div><br/></div></div><div id="38359310" class="c"><input type="checkbox" id="c-38359310" checked=""/><div class="controls bullet"><span class="by">MillionOClock</span><span>|</span><a href="#38354692">root</a><span>|</span><a href="#38355526">parent</a><span>|</span><a href="#38355588">prev</a><span>|</span><a href="#38355411">next</a><span>|</span><label class="collapse" for="c-38359310">[-]</label><label class="expand" for="c-38359310">[2 more]</label></div><br/><div class="children"><div class="content">Another point: Now that I think about it, I doubt this is compatible with streaming the output to the user, which might be an issue in some cases.</div><br/><div id="38359494" class="c"><input type="checkbox" id="c-38359494" checked=""/><div class="controls bullet"><span class="by">DaiPlusPlus</span><span>|</span><a href="#38354692">root</a><span>|</span><a href="#38359310">parent</a><span>|</span><a href="#38355411">next</a><span>|</span><label class="collapse" for="c-38359494">[-]</label><label class="expand" for="c-38359494">[1 more]</label></div><br/><div class="children"><div class="content">I thought the char-by-char teleprinter thing was just an effect (y’know, for user-engagement and to make the interaction feel more genuine) - and that these systems just return output in buffered blocks&#x2F;pages or whatever-it-is that they wired-up their network to do.<p>It’s not like DALL-E outputs pixels in scanout order - or in brushstroke order (…er… or does it?)</div><br/></div></div></div></div></div></div><div id="38355411" class="c"><input type="checkbox" id="c-38355411" checked=""/><div class="controls bullet"><span class="by">Tostino</span><span>|</span><a href="#38354692">root</a><span>|</span><a href="#38355129">parent</a><span>|</span><a href="#38355526">prev</a><span>|</span><a href="#38355574">next</a><span>|</span><label class="collapse" for="c-38355411">[-]</label><label class="expand" for="c-38355411">[1 more]</label></div><br/><div class="children"><div class="content">Right, most inference servers support this already.</div><br/></div></div></div></div><div id="38355574" class="c"><input type="checkbox" id="c-38355574" checked=""/><div class="controls bullet"><span class="by">erhaetherth</span><span>|</span><a href="#38354692">parent</a><span>|</span><a href="#38355129">prev</a><span>|</span><a href="#38355130">next</a><span>|</span><label class="collapse" for="c-38355574">[-]</label><label class="expand" for="c-38355574">[3 more]</label></div><br/><div class="children"><div class="content">I don&#x27;t like this. It forces me to read 2 prompts instead of 1 so that I can help train their LLM. ChatGPT and Bard already have regenerate buttons if I don&#x27;t like their response, it doesn&#x27;t need to be that in my face.</div><br/><div id="38355716" class="c"><input type="checkbox" id="c-38355716" checked=""/><div class="controls bullet"><span class="by">moritzwarhier</span><span>|</span><a href="#38354692">root</a><span>|</span><a href="#38355574">parent</a><span>|</span><a href="#38357709">next</a><span>|</span><label class="collapse" for="c-38355716">[-]</label><label class="expand" for="c-38355716">[1 more]</label></div><br/><div class="children"><div class="content">I think there is an argument that it would be beneficial for this to be common, despite the cognitive burden.<p>It forces you to remind yourself of the stochastic nature of the model and RILHF, maybe the data even helps to improve the latter.<p>I liked this trait of Bard from the start and hope they keep it.<p>It provides a sense of agency and reminds to not anthropomorphize the transformer chatbot too much.</div><br/></div></div><div id="38357709" class="c"><input type="checkbox" id="c-38357709" checked=""/><div class="controls bullet"><span class="by">bongodongobob</span><span>|</span><a href="#38354692">root</a><span>|</span><a href="#38355574">parent</a><span>|</span><a href="#38355716">prev</a><span>|</span><a href="#38355130">next</a><span>|</span><label class="collapse" for="c-38357709">[-]</label><label class="expand" for="c-38357709">[1 more]</label></div><br/><div class="children"><div class="content">How else do you expect the LLM you use to become better? I&#x27;m more than happy to provide feedback. Unless you want it to only scrape data, I can&#x27;t imagine why you&#x27;d be opposed to improving a product you use especially when that&#x27;s really the only way to do it. If you don&#x27;t care, just pick one and don&#x27;t think about it, they&#x27;re usually extremely similar anyway. I&#x27;m not sure I&#x27;ve come across an option where one was acceptable and one wasn&#x27;t. They are literally giving you options that that don&#x27;t need to give and you&#x27;re complaining.</div><br/></div></div></div></div><div id="38355130" class="c"><input type="checkbox" id="c-38355130" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#38354692">parent</a><span>|</span><a href="#38355574">prev</a><span>|</span><a href="#38355614">next</a><span>|</span><label class="collapse" for="c-38355130">[-]</label><label class="expand" for="c-38355130">[4 more]</label></div><br/><div class="children"><div class="content">Isn&#x27;t that textbook MoE?</div><br/><div id="38355482" class="c"><input type="checkbox" id="c-38355482" checked=""/><div class="controls bullet"><span class="by">Tostino</span><span>|</span><a href="#38354692">root</a><span>|</span><a href="#38355130">parent</a><span>|</span><a href="#38355614">next</a><span>|</span><label class="collapse" for="c-38355482">[-]</label><label class="expand" for="c-38355482">[3 more]</label></div><br/><div class="children"><div class="content">No, like the other comment said, it&#x27;s just using the `n` parameter in an OpenAI style API. For example, vLLM and llamacpp have support for it.</div><br/><div id="38355519" class="c"><input type="checkbox" id="c-38355519" checked=""/><div class="controls bullet"><span class="by">stavros</span><span>|</span><a href="#38354692">root</a><span>|</span><a href="#38355482">parent</a><span>|</span><a href="#38355614">next</a><span>|</span><label class="collapse" for="c-38355519">[-]</label><label class="expand" for="c-38355519">[2 more]</label></div><br/><div class="children"><div class="content">Ah, it&#x27;s the same model, multiple runs, then? Not actually N different models?</div><br/><div id="38355632" class="c"><input type="checkbox" id="c-38355632" checked=""/><div class="controls bullet"><span class="by">Tostino</span><span>|</span><a href="#38354692">root</a><span>|</span><a href="#38355519">parent</a><span>|</span><a href="#38355614">next</a><span>|</span><label class="collapse" for="c-38355632">[-]</label><label class="expand" for="c-38355632">[1 more]</label></div><br/><div class="children"><div class="content">Correct.</div><br/></div></div></div></div></div></div></div></div></div></div><div id="38355614" class="c"><input type="checkbox" id="c-38355614" checked=""/><div class="controls bullet"><span class="by">pton_xd</span><span>|</span><a href="#38354692">prev</a><span>|</span><a href="#38354173">next</a><span>|</span><label class="collapse" for="c-38355614">[-]</label><label class="expand" for="c-38355614">[5 more]</label></div><br/><div class="children"><div class="content">I&#x27;ve also noticed LLMs seem to lack conviction on the correctness of their answers. As the paper notes, you can easily convince the transformer that a correct answer is wrong, and needs adjustment. Ultimately they&#x27;re just trying to please you. For example with ChatGPT 3.5 (abbreviated):<p>me: what is sin -pi&#x2F;2<p>gpt: -1<p>me: that&#x27;s not right<p>gpt: I apologize, let me clarify, the answer is 1</div><br/><div id="38355660" class="c"><input type="checkbox" id="c-38355660" checked=""/><div class="controls bullet"><span class="by">hellcow</span><span>|</span><a href="#38355614">parent</a><span>|</span><a href="#38355674">next</a><span>|</span><label class="collapse" for="c-38355660">[-]</label><label class="expand" for="c-38355660">[1 more]</label></div><br/><div class="children"><div class="content">I just re-ran this on GPT-4 and it apologized, told me I was right, and then said again that the answer was -1. So while it lacked conviction it at least kept the correct answer.</div><br/></div></div><div id="38355674" class="c"><input type="checkbox" id="c-38355674" checked=""/><div class="controls bullet"><span class="by">muzani</span><span>|</span><a href="#38355614">parent</a><span>|</span><a href="#38355660">prev</a><span>|</span><a href="#38355720">next</a><span>|</span><label class="collapse" for="c-38355674">[-]</label><label class="expand" for="c-38355674">[2 more]</label></div><br/><div class="children"><div class="content">gpt-4: Actually, the value of \(\sin(-\pi&#x2F;2)\) is indeed \(-1\). The sine function represents the y-coordinate of a point on the unit circle corresponding to a given angle. At \(-\pi&#x2F;2\) radians, which is equivalent to 270 degrees or a quarter circle in the negative direction, the point on the unit circle is at the bottom with coordinates (0, -1). Therefore, the sine of \(-\pi&#x2F;2\) is \(-1\).<p>=====<p>The smarter it is, the more conviction it has. GPT-3.5 has a lot of impostor syndrome and it&#x27;s probably deserved lol. But GPT-4 starts to stutter when you give it enough math questions, which aren&#x27;t its forte.</div><br/><div id="38355738" class="c"><input type="checkbox" id="c-38355738" checked=""/><div class="controls bullet"><span class="by">muzani</span><span>|</span><a href="#38355614">root</a><span>|</span><a href="#38355674">parent</a><span>|</span><a href="#38355720">next</a><span>|</span><label class="collapse" for="c-38355738">[-]</label><label class="expand" for="c-38355738">[1 more]</label></div><br/><div class="children"><div class="content">If anything, GPT-4 has the opposite problem. Ask it to check your homework and it&#x27;ll go &quot;20&#x2F;5 is not 4. The correct answer is 4&quot;</div><br/></div></div></div></div><div id="38355720" class="c"><input type="checkbox" id="c-38355720" checked=""/><div class="controls bullet"><span class="by">kaiokendev</span><span>|</span><a href="#38355614">parent</a><span>|</span><a href="#38355674">prev</a><span>|</span><a href="#38354173">next</a><span>|</span><label class="collapse" for="c-38355720">[-]</label><label class="expand" for="c-38355720">[1 more]</label></div><br/><div class="children"><div class="content">This is due to the RLHF alignment, only product-focused. It would be very annoying for users to fight back and forth with the LLM on the correctness of the answer, especially when it is so prone to hallucination.</div><br/></div></div></div></div><div id="38354173" class="c"><input type="checkbox" id="c-38354173" checked=""/><div class="controls bullet"><span class="by">valine</span><span>|</span><a href="#38355614">prev</a><span>|</span><a href="#38353799">next</a><span>|</span><label class="collapse" for="c-38354173">[-]</label><label class="expand" for="c-38354173">[9 more]</label></div><br/><div class="children"><div class="content">I wonder if separate LLMs can find each other’s logical mistakes. If I ask llama to find the logical mistake in Yi output, would that work better than llama finding a mistake in llama output?<p>A logical mistake might imply a blind spot inherent to the model, a blind spot that might not be present in all models.</div><br/><div id="38354722" class="c"><input type="checkbox" id="c-38354722" checked=""/><div class="controls bullet"><span class="by">sevagh</span><span>|</span><a href="#38354173">parent</a><span>|</span><a href="#38354541">next</a><span>|</span><label class="collapse" for="c-38354722">[-]</label><label class="expand" for="c-38354722">[1 more]</label></div><br/><div class="children"><div class="content">I frequently share responses between ChatGPT (paid version with GPT4) and Copilot-X to break an impasse when trying to generate or fix a tricky piece of code.</div><br/></div></div><div id="38354541" class="c"><input type="checkbox" id="c-38354541" checked=""/><div class="controls bullet"><span class="by">EricMausler</span><span>|</span><a href="#38354173">parent</a><span>|</span><a href="#38354722">prev</a><span>|</span><a href="#38353799">next</a><span>|</span><label class="collapse" for="c-38354541">[-]</label><label class="expand" for="c-38354541">[7 more]</label></div><br/><div class="children"><div class="content">wouldn&#x27;t this effectively be using a &quot;model&quot; twice the size?<p>Would it be better to just double the size of one of the models rather than house both?<p>Genuine question</div><br/><div id="38355216" class="c"><input type="checkbox" id="c-38355216" checked=""/><div class="controls bullet"><span class="by">avereveard</span><span>|</span><a href="#38354173">root</a><span>|</span><a href="#38354541">parent</a><span>|</span><a href="#38355431">next</a><span>|</span><label class="collapse" for="c-38355216">[-]</label><label class="expand" for="c-38355216">[1 more]</label></div><br/><div class="children"><div class="content">Parsing is faster than generating, so having a small model produce a whole output and then have Goliath only produce &quot;good&#x2F;bad&quot; single token response evaluation would be faster than having Goliath produce everything. This would be the extreme, adhoc and iterative version of speculative decoding, which is already a thing and would probably give the best compromise</div><br/></div></div><div id="38355431" class="c"><input type="checkbox" id="c-38355431" checked=""/><div class="controls bullet"><span class="by">raincole</span><span>|</span><a href="#38354173">root</a><span>|</span><a href="#38354541">parent</a><span>|</span><a href="#38355216">prev</a><span>|</span><a href="#38354649">next</a><span>|</span><label class="collapse" for="c-38355431">[-]</label><label class="expand" for="c-38355431">[1 more]</label></div><br/><div class="children"><div class="content">I think the relationship between model size and training time isn&#x27;t linear. So if you want a twice bigger model it&#x27;ll take more resources to train it than two original models.</div><br/></div></div><div id="38354649" class="c"><input type="checkbox" id="c-38354649" checked=""/><div class="controls bullet"><span class="by">valine</span><span>|</span><a href="#38354173">root</a><span>|</span><a href="#38354541">parent</a><span>|</span><a href="#38355431">prev</a><span>|</span><a href="#38353799">next</a><span>|</span><label class="collapse" for="c-38354649">[-]</label><label class="expand" for="c-38354649">[4 more]</label></div><br/><div class="children"><div class="content">Maybe. Goliath 120B took two different llama variants and interwove the layers. Surprisingly Goliath 120B quantized to 2bit is outperforming llama 70B 4bit in many benchmarks.<p><a href="https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;17vcr9d&#x2F;llm_comparisontest_2x_34b_yi_dolphin_nous&#x2F;" rel="nofollow noreferrer">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;17vcr9d&#x2F;llm_com...</a></div><br/><div id="38356843" class="c"><input type="checkbox" id="c-38356843" checked=""/><div class="controls bullet"><span class="by">ghotli</span><span>|</span><a href="#38354173">root</a><span>|</span><a href="#38354649">parent</a><span>|</span><a href="#38353799">next</a><span>|</span><label class="collapse" for="c-38356843">[-]</label><label class="expand" for="c-38356843">[3 more]</label></div><br/><div class="children"><div class="content">Do you happen to have a link to where that interwoven layers bit is described? As far as I can tell it&#x27;s not clear on the model cards.</div><br/><div id="38357858" class="c"><input type="checkbox" id="c-38357858" checked=""/><div class="controls bullet"><span class="by">valine</span><span>|</span><a href="#38354173">root</a><span>|</span><a href="#38356843">parent</a><span>|</span><a href="#38353799">next</a><span>|</span><label class="collapse" for="c-38357858">[-]</label><label class="expand" for="c-38357858">[2 more]</label></div><br/><div class="children"><div class="content">The model page is the only info I’ve found on it. As far as I can tell there’s no paper published on the technique.<p>In the “Merge Process” section they at least give the layer ranges.<p><a href="https:&#x2F;&#x2F;huggingface.co&#x2F;alpindale&#x2F;goliath-120b" rel="nofollow noreferrer">https:&#x2F;&#x2F;huggingface.co&#x2F;alpindale&#x2F;goliath-120b</a></div><br/><div id="38358360" class="c"><input type="checkbox" id="c-38358360" checked=""/><div class="controls bullet"><span class="by">ghotli</span><span>|</span><a href="#38354173">root</a><span>|</span><a href="#38357858">parent</a><span>|</span><a href="#38353799">next</a><span>|</span><label class="collapse" for="c-38358360">[-]</label><label class="expand" for="c-38358360">[1 more]</label></div><br/><div class="children"><div class="content">Ah, actually reviewing that more closely I found a link to it in the acknowledgements.<p><a href="https:&#x2F;&#x2F;github.com&#x2F;cg123&#x2F;mergekit">https:&#x2F;&#x2F;github.com&#x2F;cg123&#x2F;mergekit</a></div><br/></div></div></div></div></div></div></div></div></div></div></div></div><div id="38353799" class="c"><input type="checkbox" id="c-38353799" checked=""/><div class="controls bullet"><span class="by">seeknotfind</span><span>|</span><a href="#38354173">prev</a><span>|</span><a href="#38359148">next</a><span>|</span><label class="collapse" for="c-38353799">[-]</label><label class="expand" for="c-38353799">[5 more]</label></div><br/><div class="children"><div class="content">It can also &quot;correct&quot; proper reasoning. :)<p>~&quot;When told where it&#x27;s wrong, LLM can correct itself to improve accuracy.&quot;<p>Similar to cheating in chess- a master only needs to be told the value of a few positions to have an advantage.</div><br/><div id="38354067" class="c"><input type="checkbox" id="c-38354067" checked=""/><div class="controls bullet"><span class="by">tines</span><span>|</span><a href="#38353799">parent</a><span>|</span><a href="#38355386">next</a><span>|</span><label class="collapse" for="c-38354067">[-]</label><label class="expand" for="c-38354067">[2 more]</label></div><br/><div class="children"><div class="content">This is said in the abstract as well:<p>&gt; recent attempts to self-correct logical or reasoning errors often cause correct answers to become incorrect, resulting in worse performances overall (Huang et al., 2023)</div><br/><div id="38358403" class="c"><input type="checkbox" id="c-38358403" checked=""/><div class="controls bullet"><span class="by">seeknotfind</span><span>|</span><a href="#38353799">root</a><span>|</span><a href="#38354067">parent</a><span>|</span><a href="#38355386">next</a><span>|</span><label class="collapse" for="c-38358403">[-]</label><label class="expand" for="c-38358403">[1 more]</label></div><br/><div class="children"><div class="content">Yeah but in this complex way that really glosses over what&#x27;s going on here.<p>Plus, sometimes the corrections aren&#x27;t accurate. So of course if you tell it where it&#x27;s wrong, and it gets a second chance, the error rate will be less...</div><br/></div></div></div></div><div id="38355386" class="c"><input type="checkbox" id="c-38355386" checked=""/><div class="controls bullet"><span class="by">mark_l_watson</span><span>|</span><a href="#38353799">parent</a><span>|</span><a href="#38354067">prev</a><span>|</span><a href="#38359148">next</a><span>|</span><label class="collapse" for="c-38355386">[-]</label><label class="expand" for="c-38355386">[2 more]</label></div><br/><div class="children"><div class="content">I have noticed this several times. When I give feedback that a mistake was made (with no details on what the mistake is), often smaller and medium size LLMs then give a correct response.</div><br/><div id="38355607" class="c"><input type="checkbox" id="c-38355607" checked=""/><div class="controls bullet"><span class="by">erhaetherth</span><span>|</span><a href="#38353799">root</a><span>|</span><a href="#38355386">parent</a><span>|</span><a href="#38359148">next</a><span>|</span><label class="collapse" for="c-38355607">[-]</label><label class="expand" for="c-38355607">[1 more]</label></div><br/><div class="children"><div class="content">Which I take full advantage of when the output is like 90% correct but the &quot;fix&quot; requires a bit of refactoring, I just tell it what I want and presto. Faster than doing it by hand.</div><br/></div></div></div></div></div></div><div id="38359148" class="c"><input type="checkbox" id="c-38359148" checked=""/><div class="controls bullet"><span class="by">eschaton</span><span>|</span><a href="#38353799">prev</a><span>|</span><a href="#38356386">next</a><span>|</span><label class="collapse" for="c-38359148">[-]</label><label class="expand" for="c-38359148">[4 more]</label></div><br/><div class="children"><div class="content">LLMs can’t find reasoning errors because *LLMs don’t reason*.<p>It’s incredible how uninformed the average Hackernews is about artificial intelligence. But the average Hackernews never met a hype train they wouldn’t try to jump on.</div><br/><div id="38361040" class="c"><input type="checkbox" id="c-38361040" checked=""/><div class="controls bullet"><span class="by">dmichulke</span><span>|</span><a href="#38359148">parent</a><span>|</span><a href="#38359425">next</a><span>|</span><label class="collapse" for="c-38361040">[-]</label><label class="expand" for="c-38361040">[2 more]</label></div><br/><div class="children"><div class="content">&gt; LLMs can’t find reasoning errors because <i>LLMs don’t reason</i>.<p>I have several experiences where people belittle me when I say the same thing. To the extent I rarely say it anymore. For everybody else AGI is around the corner and it&#x27;s gonna dominate the world.<p>&gt; never met a hype train they wouldn’t try to jump on<p>Crypto-currencies</div><br/><div id="38361063" class="c"><input type="checkbox" id="c-38361063" checked=""/><div class="controls bullet"><span class="by">rsynnott</span><span>|</span><a href="#38359148">root</a><span>|</span><a href="#38361040">parent</a><span>|</span><a href="#38359425">next</a><span>|</span><label class="collapse" for="c-38361063">[-]</label><label class="expand" for="c-38361063">[1 more]</label></div><br/><div class="children"><div class="content">&gt; Crypto-currencies<p>HN _eventually_ largely gave up on these, but it was basically a True Believer space from 2011 to the early days of NFTs; it was more credulous than just about any other community which had known about cryptocurrencies since the early days.</div><br/></div></div></div></div><div id="38359425" class="c"><input type="checkbox" id="c-38359425" checked=""/><div class="controls bullet"><span class="by">cmrdporcupine</span><span>|</span><a href="#38359148">parent</a><span>|</span><a href="#38361040">prev</a><span>|</span><a href="#38356386">next</a><span>|</span><label class="collapse" for="c-38359425">[-]</label><label class="expand" for="c-38359425">[1 more]</label></div><br/><div class="children"><div class="content">I agree they can&#x27;t reason, but you shouldn&#x27;t be so quick to be dismissive, you need to give your definition of reasoning and you should be able to back it up with papers. Because part of the reason <i>some</i> commenters on HN reflect what you&#x27;re smearing the whole community with is that... they don&#x27;t actually have a definition of what reasoning is, or have a different one.<p>There have been some good ones on this topic that have come over HN, and I do think they show that LLMs don&#x27;t reason -- but they certainly give the <i>appearance</i> of doing so with the right prompts. But the good papers are combined with a formal definition of what &quot;reasoning&quot; is.<p>The typical counter argument is usually that &quot;how do we know the human brain isn&#x27;t like this, too&quot;, or &quot;there&#x27;s lots of humans who also don&#x27;t reason&quot; etc. Which I think is a bad faith argument.</div><br/></div></div></div></div><div id="38356386" class="c"><input type="checkbox" id="c-38356386" checked=""/><div class="controls bullet"><span class="by">RagnarD</span><span>|</span><a href="#38359148">prev</a><span>|</span><a href="#38358160">next</a><span>|</span><label class="collapse" for="c-38356386">[-]</label><label class="expand" for="c-38356386">[7 more]</label></div><br/><div class="children"><div class="content">Hmm. I quickly made up an example of a logical fallacy and asked GPT-4 about it. Seemed to do a pretty good job, to me.<p>&gt; All apples are red. All apples are fruit. My car is red, therefore my car is a fruit. Find the logical error<p>-----<p>The logical error in the statement &quot;All apples are red. All apples are fruit. My car is red, therefore my car is a fruit.&quot; is a classic example of the fallacy of the undistributed middle. This fallacy occurs when a syllogism (a form of reasoning in which a conclusion is drawn from two given or assumed propositions) is improperly formed because the middle term is not distributed at least once in the premises.<p>Here&#x27;s a breakdown of the fallacy in your statement:<p>Major Premise: All apples are fruit.
Minor Premise: All apples are red.
Conclusion: My car is red, therefore my car is a fruit.
In this syllogism, &quot;apples&quot; is the middle term. However, it is not distributed correctly. The major premise talks about all apples being fruits, and the minor premise talks about all apples being red. However, there&#x27;s no premise that states all things that are red are apples. Without this, you cannot logically conclude that other red things, like a car, are fruits.<p>In simpler terms, just because all apples are red and fruits, it doesn&#x27;t mean that all red things are fruits. The color of the apples (red) is not an exclusive property that defines the category of fruits. Your car shares the property of being red with apples, but it doesn&#x27;t share the essential property of being a fruit.</div><br/><div id="38356694" class="c"><input type="checkbox" id="c-38356694" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#38356386">parent</a><span>|</span><a href="#38356810">next</a><span>|</span><label class="collapse" for="c-38356694">[-]</label><label class="expand" for="c-38356694">[4 more]</label></div><br/><div class="children"><div class="content">You said &quot;Find the logical error&quot;, meaning you told it the logical error is there. That doesn&#x27;t show the model can find errors when it isn&#x27;t told that the error is there.<p>Your post shows how the model can correct a reasoning error. That is different from finding an error when it isn&#x27;t pointed out, that is why he title to this post is &quot;LLMs cannot find reasoning errors, but can correct them&quot;. You using the phrasing &quot;find the logical error&quot; doesn&#x27;t contradict the title.</div><br/><div id="38356759" class="c"><input type="checkbox" id="c-38356759" checked=""/><div class="controls bullet"><span class="by">kaoD</span><span>|</span><a href="#38356386">root</a><span>|</span><a href="#38356694">parent</a><span>|</span><a href="#38356810">next</a><span>|</span><label class="collapse" for="c-38356759">[-]</label><label class="expand" for="c-38356759">[3 more]</label></div><br/><div class="children"><div class="content">&gt; All apples are red. All apples are fruit. My car is red, therefore my car is a fruit.<p>&gt; The conclusion &quot;My car is a fruit&quot; is not logically valid. This is an example of the fallacy of the undistributed middle. The logic goes as follows:<p>1. All apples are red. (Premise)<p>2. All apples are fruit. (Premise)<p>3. My car is red. (Premise)<p>4. Therefore, my car is a fruit. (Conclusion)<p>The fallacy arises because the premises do not establish a shared property between &quot;red things&quot; and &quot;fruit&quot; in a way that would include the car. Just because both apples and the car share the property of being red, it does not mean they share all properties of apples, such as being a fruit.</div><br/><div id="38357230" class="c"><input type="checkbox" id="c-38357230" checked=""/><div class="controls bullet"><span class="by">Jensson</span><span>|</span><a href="#38356386">root</a><span>|</span><a href="#38356759">parent</a><span>|</span><a href="#38356810">next</a><span>|</span><label class="collapse" for="c-38357230">[-]</label><label class="expand" for="c-38357230">[2 more]</label></div><br/><div class="children"><div class="content">&gt; All apples are red. All apples are fruit. My car is red, therefore my car is a fruit.<p>I Googled that exact phrase and got solutions. A logical problem that can be solved by a search engine isn&#x27;t a valid example, the LLM knows that it is a logical puzzle just by how you phrased it just like Google knows that it is a logical puzzle.<p>And no, doing tiny alterations to that until you no longer get any Google hits isn&#x27;t a proof ChatGPT can do logic, it is proof that ChatGPT can parse general structure and find patterns better than a search engine can. You need to do logical problems that can&#x27;t easily be translated to standard problems that there are tons of examples of in the wild.</div><br/><div id="38359476" class="c"><input type="checkbox" id="c-38359476" checked=""/><div class="controls bullet"><span class="by">cmrdporcupine</span><span>|</span><a href="#38356386">root</a><span>|</span><a href="#38357230">parent</a><span>|</span><a href="#38356810">next</a><span>|</span><label class="collapse" for="c-38359476">[-]</label><label class="expand" for="c-38359476">[1 more]</label></div><br/><div class="children"><div class="content">Exactly. And when you realize how weak GPT is on this is by giving it complicated type system programming problems and watch it fall over and get stuck in circular, illogical patterns, and then get even crazier as you try to correct it.<p>It can&#x27;t &quot;reason things through&quot;, it just builds logic-like patterns based on the distillation of the work of other minds which <i>did</i> reason -- which works about 80% of the time, but when it fails it can&#x27;t retrace its steps.<p>Even a really &quot;stupid&quot; human (c&#x27;est moi) can be made to work through and find their errors when given guidance by a patient teacher. In my experience, dialectical guidance actually makes ChatGPT <i>worse</i>.</div><br/></div></div></div></div></div></div></div></div><div id="38356810" class="c"><input type="checkbox" id="c-38356810" checked=""/><div class="controls bullet"><span class="by">realharo</span><span>|</span><a href="#38356386">parent</a><span>|</span><a href="#38356694">prev</a><span>|</span><a href="#38357249">next</a><span>|</span><label class="collapse" for="c-38356810">[-]</label><label class="expand" for="c-38356810">[1 more]</label></div><br/><div class="children"><div class="content"><i>&gt;All apples are red. All apples are fruit. My car is red, therefore my car is a fruit.</i><p>This is an extremely common example of an error. I wish people would put more effort into coming up with examples that aren&#x27;t so common all over the internet.<p>The output even calls it <i>&quot;a classic example&quot;</i>.</div><br/></div></div><div id="38357249" class="c"><input type="checkbox" id="c-38357249" checked=""/><div class="controls bullet"><span class="by">couchand</span><span>|</span><a href="#38356386">parent</a><span>|</span><a href="#38356810">prev</a><span>|</span><a href="#38358160">next</a><span>|</span><label class="collapse" for="c-38357249">[-]</label><label class="expand" for="c-38357249">[1 more]</label></div><br/><div class="children"><div class="content">It did a terrible job: it got the answer close but confidently wrong.<p>The middle term in the fallacy of the excluded middle here is &quot;red&quot;, not &quot;apple&quot;.</div><br/></div></div></div></div><div id="38358160" class="c"><input type="checkbox" id="c-38358160" checked=""/><div class="controls bullet"><span class="by">ggm</span><span>|</span><a href="#38356386">prev</a><span>|</span><a href="#38355822">next</a><span>|</span><label class="collapse" for="c-38358160">[-]</label><label class="expand" for="c-38358160">[1 more]</label></div><br/><div class="children"><div class="content">How does an LLM question it&#x27;s priors and axioms?<p>People have to do this all the time. Bringing skepticism to that table &quot;excel made for you&quot; is a vital part of heading off bad reasoning. For an LLM its a given.</div><br/></div></div><div id="38355822" class="c"><input type="checkbox" id="c-38355822" checked=""/><div class="controls bullet"><span class="by">sumthingsumthng</span><span>|</span><a href="#38358160">prev</a><span>|</span><a href="#38357027">next</a><span>|</span><label class="collapse" for="c-38355822">[-]</label><label class="expand" for="c-38355822">[1 more]</label></div><br/><div class="children"><div class="content">I have not read the essay, yet but when &#x27;we&#x27; talk about 
&gt; reasoning errors, 
we do not mean reason in some natural, universal, scientific kind of sense, right?<p>Given that the training data can only contain human reasoning and computational logic, reason in the sense of LLM&#x27;s can only be interpreted as &quot;rational facts AND nonsense humans made up to create systems that would support consumerism-driven sanity&quot;, correct?????<p>Please understand, I&#x27;m not mocking, I&#x27;m genuinely interested 
in the ways human reasoning radiates into the code LLM&#x27;s learn while they realize (the computational equivalent of a new-born&#x27;s eyes opening) their cognitive (&amp;) sensory 
(that which triggers&#x2F;causes&#x2F;elicits&#x2F;prompts&#x2F;influences) their origins (every whatever-second&#x2F;moment of their existence).</div><br/></div></div><div id="38357027" class="c"><input type="checkbox" id="c-38357027" checked=""/><div class="controls bullet"><span class="by">itissid</span><span>|</span><a href="#38355822">prev</a><span>|</span><a href="#38357345">next</a><span>|</span><label class="collapse" for="c-38357027">[-]</label><label class="expand" for="c-38357027">[1 more]</label></div><br/><div class="children"><div class="content">&gt; We fine-tuned a PaLM 2-XS-Otter model based on our available data for 20k steps and choose the checkpoint with the best validation results.<p>What is the &quot;available data for 20k steps&quot;?</div><br/></div></div><div id="38357345" class="c"><input type="checkbox" id="c-38357345" checked=""/><div class="controls bullet"><span class="by">hiddencost</span><span>|</span><a href="#38357027">prev</a><span>|</span><a href="#38356707">next</a><span>|</span><label class="collapse" for="c-38357345">[-]</label><label class="expand" for="c-38357345">[1 more]</label></div><br/><div class="children"><div class="content">What a stupid title.<p>&quot;The LLMs we tested couldn&#x27;t find reasoning errors but can correct them&quot; is accurate. Trying small language golf experiments on existing models just tells you about their training data.<p>It&#x27;s quite likely that a transformer model could successfully be trained for this task.<p>Also, many of these models get new capabilities each release.</div><br/></div></div><div id="38356707" class="c"><input type="checkbox" id="c-38356707" checked=""/><div class="controls bullet"><span class="by">Davidzheng</span><span>|</span><a href="#38357345">prev</a><span>|</span><a href="#38354919">next</a><span>|</span><label class="collapse" for="c-38356707">[-]</label><label class="expand" for="c-38356707">[1 more]</label></div><br/><div class="children"><div class="content">By the way the next training run of openai will for the first time have a huge amount of data which the model may recognize as being generated by &quot;itself&quot; and therefore will be forced to model itself. Could have implications wrt logic among other things</div><br/></div></div><div id="38354919" class="c"><input type="checkbox" id="c-38354919" checked=""/><div class="controls bullet"><span class="by">bandrami</span><span>|</span><a href="#38356707">prev</a><span>|</span><a href="#38353832">next</a><span>|</span><label class="collapse" for="c-38354919">[-]</label><label class="expand" for="c-38354919">[1 more]</label></div><br/><div class="children"><div class="content">I noticed early on that GPT35 can successfully create a false sentence but has a <i>whole lot</i> of trouble creating an invalid syllogism, and tends to end up making false but valid ones. Not sure if that&#x27;s changed but it&#x27;s interesting what that might say about its training.</div><br/></div></div><div id="38353832" class="c"><input type="checkbox" id="c-38353832" checked=""/><div class="controls bullet"><span class="by">agentultra</span><span>|</span><a href="#38354919">prev</a><span>|</span><a href="#38358745">next</a><span>|</span><label class="collapse" for="c-38353832">[-]</label><label class="expand" for="c-38353832">[2 more]</label></div><br/><div class="children"><div class="content">This might deserve some context here from experts. Wouldn’t solving mistake finding, in the general case, be the same as solving SAT (NP-Hard)?<p>From the abstract it sounds to me like they’re talking about heuristics for particular problems. Is that accurate?</div><br/><div id="38354549" class="c"><input type="checkbox" id="c-38354549" checked=""/><div class="controls bullet"><span class="by">helen___keller</span><span>|</span><a href="#38353832">parent</a><span>|</span><a href="#38358745">next</a><span>|</span><label class="collapse" for="c-38354549">[-]</label><label class="expand" for="c-38354549">[1 more]</label></div><br/><div class="children"><div class="content">Computational complexity isn’t really related here. Complexity has to do with formal languages and asymptotics, this is about natural language and fixed size data sets.</div><br/></div></div></div></div><div id="38358745" class="c"><input type="checkbox" id="c-38358745" checked=""/><div class="controls bullet"><span class="by">quantum_state</span><span>|</span><a href="#38353832">prev</a><span>|</span><a href="#38354816">next</a><span>|</span><label class="collapse" for="c-38358745">[-]</label><label class="expand" for="c-38358745">[1 more]</label></div><br/><div class="children"><div class="content">It’s fitting to call it Naive Intelligent…</div><br/></div></div><div id="38354816" class="c"><input type="checkbox" id="c-38354816" checked=""/><div class="controls bullet"><span class="by">nextworddev</span><span>|</span><a href="#38358745">prev</a><span>|</span><a href="#38359535">next</a><span>|</span><label class="collapse" for="c-38354816">[-]</label><label class="expand" for="c-38354816">[5 more]</label></div><br/><div class="children"><div class="content">If this is the case, then just run it X times till error rate drops near 0. AGI solved.</div><br/><div id="38354851" class="c"><input type="checkbox" id="c-38354851" checked=""/><div class="controls bullet"><span class="by">westurner</span><span>|</span><a href="#38354816">parent</a><span>|</span><a href="#38355424">next</a><span>|</span><label class="collapse" for="c-38354851">[-]</label><label class="expand" for="c-38354851">[1 more]</label></div><br/><div class="children"><div class="content">This is called (Algorithmic) <i>Convergence</i>; does the model stably converge upon one answer which it believes is most correct? After how much resources and time?<p>Convergence (evolutionary computing) 
<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Convergence_(evolutionary_computing)" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Convergence_(evolutionary_comp...</a><p>Convergence (disambiguation) &gt; Science, technology, and mathematics 
<a href="https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Convergence#Science,_technology,_and_mathematics" rel="nofollow noreferrer">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Convergence#Science,_technolog...</a></div><br/></div></div><div id="38355424" class="c"><input type="checkbox" id="c-38355424" checked=""/><div class="controls bullet"><span class="by">bee_rider</span><span>|</span><a href="#38354816">parent</a><span>|</span><a href="#38354851">prev</a><span>|</span><a href="#38355612">next</a><span>|</span><label class="collapse" for="c-38355424">[-]</label><label class="expand" for="c-38355424">[1 more]</label></div><br/><div class="children"><div class="content">I don’t think it would solve AGI, but having multiple models arguing with each other seems sort of similar to how we work things out when we’re thinking hard, right? Consider a hypothesis, argue for or against it in your head.</div><br/></div></div><div id="38355612" class="c"><input type="checkbox" id="c-38355612" checked=""/><div class="controls bullet"><span class="by">ming0308</span><span>|</span><a href="#38354816">parent</a><span>|</span><a href="#38355424">prev</a><span>|</span><a href="#38359535">next</a><span>|</span><label class="collapse" for="c-38355612">[-]</label><label class="expand" for="c-38355612">[2 more]</label></div><br/><div class="children"><div class="content">As the paper suggested, the LLM cannot identify their own mistakes yet though.
And they can only fix their mistakes if the mistake location is given.</div><br/><div id="38359358" class="c"><input type="checkbox" id="c-38359358" checked=""/><div class="controls bullet"><span class="by">Hitton</span><span>|</span><a href="#38354816">root</a><span>|</span><a href="#38355612">parent</a><span>|</span><a href="#38359535">next</a><span>|</span><label class="collapse" for="c-38359358">[-]</label><label class="expand" for="c-38359358">[1 more]</label></div><br/><div class="children"><div class="content">They would fix a &quot;mistake&quot; even if they were given location where there is none.</div><br/></div></div></div></div></div></div><div id="38359535" class="c"><input type="checkbox" id="c-38359535" checked=""/><div class="controls bullet"><span class="by">cmrdporcupine</span><span>|</span><a href="#38354816">prev</a><span>|</span><a href="#38354496">next</a><span>|</span><label class="collapse" for="c-38359535">[-]</label><label class="expand" for="c-38359535">[1 more]</label></div><br/><div class="children"><div class="content">It can &quot;correct&quot; because it just goes out and finds and produces a pattern template that matches the problem better&#x2F;different (often just different, and it fails in new ways, in my experience). It never used reasoning to find the answer in the first place, and doesn&#x27;t use reason to find the corrected answer.<p>The papers referenced here get into this: <a href="https:&#x2F;&#x2F;cacm.acm.org&#x2F;blogs&#x2F;blog-cacm&#x2F;276268-can-llms-really-reason-and-plan&#x2F;fulltext" rel="nofollow noreferrer">https:&#x2F;&#x2F;cacm.acm.org&#x2F;blogs&#x2F;blog-cacm&#x2F;276268-can-llms-really-...</a></div><br/></div></div><div id="38354496" class="c"><input type="checkbox" id="c-38354496" checked=""/><div class="controls bullet"><span class="by">einpoklum</span><span>|</span><a href="#38359535">prev</a><span>|</span><label class="collapse" for="c-38354496">[-]</label><label class="expand" for="c-38354496">[5 more]</label></div><br/><div class="children"><div class="content">No, they can&#x27;t &quot;correct reasoning errors&quot;, and that&#x27;s a clickbait title.</div><br/><div id="38354761" class="c"><input type="checkbox" id="c-38354761" checked=""/><div class="controls bullet"><span class="by">swatcoder</span><span>|</span><a href="#38354496">parent</a><span>|</span><a href="#38355517">next</a><span>|</span><label class="collapse" for="c-38354761">[-]</label><label class="expand" for="c-38354761">[1 more]</label></div><br/><div class="children"><div class="content">They can produce text that is more sound than prior text that appeared earlier in the same input, when interim text indicates that something in the earlier block was unsound. (Sometimes)<p>It&#x27;s the same pattern you&#x27;d see in a pedagological article about correcting reasoning errors, except that it&#x27;s able to generate some share of the article content on its own.<p>With more layers of post-processing behind a curtain, you might be able to build an assembly over this behavior that looked convincingly like it was correcting reasoning errors on its own.<p>So... yes and no.</div><br/></div></div><div id="38355517" class="c"><input type="checkbox" id="c-38355517" checked=""/><div class="controls bullet"><span class="by">ming0308</span><span>|</span><a href="#38354496">parent</a><span>|</span><a href="#38354761">prev</a><span>|</span><label class="collapse" for="c-38355517">[-]</label><label class="expand" for="c-38355517">[3 more]</label></div><br/><div class="children"><div class="content">If you look at the paper, they only claim LLM can correct the errors if the mistake location is given. And the mistake finding part is not yet solved.</div><br/><div id="38356646" class="c"><input type="checkbox" id="c-38356646" checked=""/><div class="controls bullet"><span class="by">einpoklum</span><span>|</span><a href="#38354496">root</a><span>|</span><a href="#38355517">parent</a><span>|</span><label class="collapse" for="c-38356646">[-]</label><label class="expand" for="c-38356646">[2 more]</label></div><br/><div class="children"><div class="content">They don&#x27;t correct errors even then. They just generate something which sounds like what one might say in a conversation when constrained not to express the error. If there&#x27;s essentially just one option for that, its the correct one - but then it&#x27;s like telling someone that the answer to a yes&#x2F;no question is not the one they generated. Not much &quot;error correction&quot; to do then.</div><br/><div id="38359581" class="c"><input type="checkbox" id="c-38359581" checked=""/><div class="controls bullet"><span class="by">cmrdporcupine</span><span>|</span><a href="#38354496">root</a><span>|</span><a href="#38356646">parent</a><span>|</span><label class="collapse" for="c-38359581">[-]</label><label class="expand" for="c-38359581">[1 more]</label></div><br/><div class="children"><div class="content">Yep. Where you can see them really get tripped up is if there&#x27;s multiple &quot;levers&quot; or points for potential contradiction. Lots of dependent clauses, chains of predicates that all have to line up for something to make sense. When they get one item wrong, they don&#x27;t &quot;see&quot; the consequences for the others. And if you get them to correct one, they&#x27;ll then often turn around and mess up the others.<p>Because at no point is the &quot;mind&quot; involved doing a step by step reduction of the problem. It doesn&#x27;t do formal reasoning.<p>Humans usually don&#x27;t either, <i>but</i> they can almost all do a form of it when required to. Either under the assistance of a teacher, or in extremis when they need to. We&#x27;ve all had the experience of being flustered, taking a deep breath, and then &quot;working through&quot; something. After spending time with GPT, etc it becomes clear they&#x27;re not doing that.<p>It&#x27;s not that reasoning comes intrinsic to all human thoughts -- we&#x27;re far lazier than that -- but when we need to, we can usually do it.</div><br/></div></div></div></div></div></div></div></div></div></div></div></div></div></body></html>